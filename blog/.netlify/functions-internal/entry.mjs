import * as adapter from '@astrojs/netlify/netlify-functions.js';
import { h as h$1, Component } from 'preact';
import render from 'preact-render-to-string';
import { defineComponent, h as h$2, createSSRApp } from 'vue';
import { renderToString as renderToString$1 } from 'vue/server-renderer';
import { escape } from 'html-escaper';
import { parse } from 'node-html-parser';
import 'axios';
/* empty css                                  *//* empty css                                  *//* empty css                                  */import { storyblokInit, apiPlugin } from '@storyblok/js';
/* empty css                                 *//* empty css                                     */import cloneDeep from 'clone-deep';
import slugify from 'slugify';
import { BUNDLED_LANGUAGES, getHighlighter as getHighlighter$1 } from 'shiki';
/* empty css                                  */import camelcase from 'camelcase';
/* empty css                                     */import rss from '@astrojs/rss';
/* empty css                                  */import 'mime';
import 'cookie';
import 'kleur/colors';
import 'string-width';
import 'path-browserify';
import { compile } from 'path-to-regexp';

const contexts = /* @__PURE__ */ new WeakMap();
function getContext(result) {
  if (contexts.has(result)) {
    return contexts.get(result);
  }
  let ctx = {
    c: 0,
    get id() {
      return "p" + this.c.toString();
    },
    signals: /* @__PURE__ */ new Map(),
    propsToSignals: /* @__PURE__ */ new Map()
  };
  contexts.set(result, ctx);
  return ctx;
}
function incrementId(ctx) {
  let id = ctx.id;
  ctx.c++;
  return id;
}

function isSignal(x) {
  return x != null && typeof x === "object" && typeof x.peek === "function" && "value" in x;
}
function restoreSignalsOnProps(ctx, props) {
  let propMap;
  if (ctx.propsToSignals.has(props)) {
    propMap = ctx.propsToSignals.get(props);
  } else {
    propMap = /* @__PURE__ */ new Map();
    ctx.propsToSignals.set(props, propMap);
  }
  for (const [key, signal] of propMap) {
    props[key] = signal;
  }
  return propMap;
}
function serializeSignals(ctx, props, attrs, map) {
  const signals = {};
  for (const [key, value] of Object.entries(props)) {
    if (isSignal(value)) {
      props[key] = value.peek();
      map.set(key, value);
      let id;
      if (ctx.signals.has(value)) {
        id = ctx.signals.get(value);
      } else {
        id = incrementId(ctx);
        ctx.signals.set(value, id);
      }
      signals[key] = id;
    }
  }
  if (Object.keys(signals).length) {
    attrs["data-preact-signals"] = JSON.stringify(signals);
  }
}

const StaticHtml$1 = ({ value, name }) => {
  if (!value)
    return null;
  return h$1("astro-slot", { name, dangerouslySetInnerHTML: { __html: value } });
};
StaticHtml$1.shouldComponentUpdate = () => false;
var static_html_default = StaticHtml$1;

const slotName$1 = (str) => str.trim().replace(/[-_]([a-z])/g, (_, w) => w.toUpperCase());
let originalConsoleError$1;
let consoleFilterRefs$1 = 0;
function check$2(Component$1, props, children) {
  if (typeof Component$1 !== "function")
    return false;
  if (Component$1.prototype != null && typeof Component$1.prototype.render === "function") {
    return Component.isPrototypeOf(Component$1);
  }
  useConsoleFilter$1();
  try {
    try {
      const { html } = renderToStaticMarkup$2.call(this, Component$1, props, children);
      if (typeof html !== "string") {
        return false;
      }
      return !/\<undefined\>/.test(html);
    } catch (err) {
      return false;
    }
  } finally {
    finishUsingConsoleFilter$1();
  }
}
function renderToStaticMarkup$2(Component, props, { default: children, ...slotted }) {
  const ctx = getContext(this.result);
  const slots = {};
  for (const [key, value] of Object.entries(slotted)) {
    const name = slotName$1(key);
    slots[name] = h$1(static_html_default, { value, name });
  }
  let propsMap = restoreSignalsOnProps(ctx, props);
  const newProps = { ...props, ...slots };
  const attrs = {};
  serializeSignals(ctx, props, attrs, propsMap);
  const html = render(
    h$1(Component, newProps, children != null ? h$1(static_html_default, { value: children }) : children)
  );
  return {
    attrs,
    html
  };
}
function useConsoleFilter$1() {
  consoleFilterRefs$1++;
  if (!originalConsoleError$1) {
    originalConsoleError$1 = console.error;
    try {
      console.error = filteredConsoleError$1;
    } catch (error) {
    }
  }
}
function finishUsingConsoleFilter$1() {
  consoleFilterRefs$1--;
}
function filteredConsoleError$1(msg, ...rest) {
  if (consoleFilterRefs$1 > 0 && typeof msg === "string") {
    const isKnownReactHookError = msg.includes("Warning: Invalid hook call.") && msg.includes("https://reactjs.org/link/invalid-hook-call");
    if (isKnownReactHookError)
      return;
  }
  originalConsoleError$1(msg, ...rest);
}
var server_default$1 = {
  check: check$2,
  renderToStaticMarkup: renderToStaticMarkup$2
};

/**
 * Astro passes `children` as a string of HTML, so we need
 * a wrapper `div` to render that content as VNodes.
 *
 * This is the Vue + JSX equivalent of using `<div v-html="value" />`
 */
const StaticHtml = defineComponent({
	props: {
		value: String,
	},
	setup({ value }) {
		if (!value) return () => null;
		return () => h$2('astro-fragment', { innerHTML: value });
	},
});

function check$1(Component) {
	return !!Component['ssrRender'];
}

async function renderToStaticMarkup$1(Component, props, children) {
	const slots = {};
	if (children != null) {
		slots.default = () => h$2(StaticHtml, { value: children });
	}
	const app = createSSRApp({ render: () => h$2(Component, props, slots) });
	const html = await renderToString$1(app);
	return { html };
}

const _renderer1 = {
	check: check$1,
	renderToStaticMarkup: renderToStaticMarkup$1,
};

function baseCreateComponent(cb, moduleId) {
  cb.isAstroComponentFactory = true;
  cb.moduleId = moduleId;
  return cb;
}
function createComponentWithOptions(opts) {
  const cb = baseCreateComponent(opts.factory, opts.moduleId);
  cb.propagation = opts.propagation;
  return cb;
}
function createComponent(arg1, moduleId) {
  if (typeof arg1 === "function") {
    return baseCreateComponent(arg1, moduleId);
  } else {
    return createComponentWithOptions(arg1);
  }
}

const ASTRO_VERSION = "1.9.2";

function createDeprecatedFetchContentFn() {
  return () => {
    throw new Error("Deprecated: Astro.fetchContent() has been replaced with Astro.glob().");
  };
}
function createAstroGlobFn() {
  const globHandler = (importMetaGlobResult, globValue) => {
    let allEntries = [...Object.values(importMetaGlobResult)];
    if (allEntries.length === 0) {
      throw new Error(`Astro.glob(${JSON.stringify(globValue())}) - no matches found.`);
    }
    return Promise.all(allEntries.map((fn) => fn()));
  };
  return globHandler;
}
function createAstro(filePathname, _site, projectRootStr) {
  const site = _site ? new URL(_site) : void 0;
  const referenceURL = new URL(filePathname, `http://localhost`);
  const projectRoot = new URL(projectRootStr);
  return {
    site,
    generator: `Astro v${ASTRO_VERSION}`,
    fetchContent: createDeprecatedFetchContentFn(),
    glob: createAstroGlobFn(),
    resolve(...segments) {
      let resolved = segments.reduce((u, segment) => new URL(segment, u), referenceURL).pathname;
      if (resolved.startsWith(projectRoot.pathname)) {
        resolved = "/" + resolved.slice(projectRoot.pathname.length);
      }
      return resolved;
    }
  };
}

const escapeHTML = escape;
class HTMLBytes extends Uint8Array {
}
Object.defineProperty(HTMLBytes.prototype, Symbol.toStringTag, {
  get() {
    return "HTMLBytes";
  }
});
class HTMLString extends String {
  get [Symbol.toStringTag]() {
    return "HTMLString";
  }
}
const markHTMLString = (value) => {
  if (value instanceof HTMLString) {
    return value;
  }
  if (typeof value === "string") {
    return new HTMLString(value);
  }
  return value;
};
function isHTMLString(value) {
  return Object.prototype.toString.call(value) === "[object HTMLString]";
}
function markHTMLBytes(bytes) {
  return new HTMLBytes(bytes);
}
async function* unescapeChunksAsync(iterable) {
  for await (const chunk of iterable) {
    yield unescapeHTML(chunk);
  }
}
function* unescapeChunks(iterable) {
  for (const chunk of iterable) {
    yield unescapeHTML(chunk);
  }
}
function unescapeHTML(str) {
  if (!!str && typeof str === "object") {
    if (str instanceof Uint8Array) {
      return markHTMLBytes(str);
    } else if (str instanceof Response && str.body) {
      const body = str.body;
      return unescapeChunksAsync(body);
    } else if (typeof str.then === "function") {
      return Promise.resolve(str).then((value) => {
        return unescapeHTML(value);
      });
    } else if (Symbol.iterator in str) {
      return unescapeChunks(str);
    } else if (Symbol.asyncIterator in str) {
      return unescapeChunksAsync(str);
    }
  }
  return markHTMLString(str);
}

var idle_prebuilt_default = `(self.Astro=self.Astro||{}).idle=t=>{const e=async()=>{await(await t())()};"requestIdleCallback"in window?window.requestIdleCallback(e):setTimeout(e,200)},window.dispatchEvent(new Event("astro:idle"));`;

var load_prebuilt_default = `(self.Astro=self.Astro||{}).load=a=>{(async()=>await(await a())())()},window.dispatchEvent(new Event("astro:load"));`;

var media_prebuilt_default = `(self.Astro=self.Astro||{}).media=(s,a)=>{const t=async()=>{await(await s())()};if(a.value){const e=matchMedia(a.value);e.matches?t():e.addEventListener("change",t,{once:!0})}},window.dispatchEvent(new Event("astro:media"));`;

var only_prebuilt_default = `(self.Astro=self.Astro||{}).only=t=>{(async()=>await(await t())())()},window.dispatchEvent(new Event("astro:only"));`;

var visible_prebuilt_default = `(self.Astro=self.Astro||{}).visible=(s,c,n)=>{const r=async()=>{await(await s())()};let i=new IntersectionObserver(e=>{for(const t of e)if(!!t.isIntersecting){i.disconnect(),r();break}});for(let e=0;e<n.children.length;e++){const t=n.children[e];i.observe(t)}},window.dispatchEvent(new Event("astro:visible"));`;

var astro_island_prebuilt_default = `var l;{const c={0:t=>t,1:t=>JSON.parse(t,o),2:t=>new RegExp(t),3:t=>new Date(t),4:t=>new Map(JSON.parse(t,o)),5:t=>new Set(JSON.parse(t,o)),6:t=>BigInt(t),7:t=>new URL(t),8:t=>new Uint8Array(JSON.parse(t)),9:t=>new Uint16Array(JSON.parse(t)),10:t=>new Uint32Array(JSON.parse(t))},o=(t,s)=>{if(t===""||!Array.isArray(s))return s;const[e,n]=s;return e in c?c[e](n):void 0};customElements.get("astro-island")||customElements.define("astro-island",(l=class extends HTMLElement{constructor(){super(...arguments);this.hydrate=()=>{if(!this.hydrator||this.parentElement&&this.parentElement.closest("astro-island[ssr]"))return;const s=this.querySelectorAll("astro-slot"),e={},n=this.querySelectorAll("template[data-astro-template]");for(const r of n){const i=r.closest(this.tagName);!i||!i.isSameNode(this)||(e[r.getAttribute("data-astro-template")||"default"]=r.innerHTML,r.remove())}for(const r of s){const i=r.closest(this.tagName);!i||!i.isSameNode(this)||(e[r.getAttribute("name")||"default"]=r.innerHTML)}const a=this.hasAttribute("props")?JSON.parse(this.getAttribute("props"),o):{};this.hydrator(this)(this.Component,a,e,{client:this.getAttribute("client")}),this.removeAttribute("ssr"),window.removeEventListener("astro:hydrate",this.hydrate),window.dispatchEvent(new CustomEvent("astro:hydrate"))}}connectedCallback(){!this.hasAttribute("await-children")||this.firstChild?this.childrenConnectedCallback():new MutationObserver((s,e)=>{e.disconnect(),this.childrenConnectedCallback()}).observe(this,{childList:!0})}async childrenConnectedCallback(){window.addEventListener("astro:hydrate",this.hydrate);let s=this.getAttribute("before-hydration-url");s&&await import(s),this.start()}start(){const s=JSON.parse(this.getAttribute("opts")),e=this.getAttribute("client");if(Astro[e]===void 0){window.addEventListener(\`astro:\${e}\`,()=>this.start(),{once:!0});return}Astro[e](async()=>{const n=this.getAttribute("renderer-url"),[a,{default:r}]=await Promise.all([import(this.getAttribute("component-url")),n?import(n):()=>()=>{}]),i=this.getAttribute("component-export")||"default";if(!i.includes("."))this.Component=a[i];else{this.Component=a;for(const d of i.split("."))this.Component=this.Component[d]}return this.hydrator=r,this.hydrate},s,this)}attributeChangedCallback(){this.hydrator&&this.hydrate()}},l.observedAttributes=["props"],l))}`;

function determineIfNeedsHydrationScript(result) {
  if (result._metadata.hasHydrationScript) {
    return false;
  }
  return result._metadata.hasHydrationScript = true;
}
const hydrationScripts = {
  idle: idle_prebuilt_default,
  load: load_prebuilt_default,
  only: only_prebuilt_default,
  media: media_prebuilt_default,
  visible: visible_prebuilt_default
};
function determinesIfNeedsDirectiveScript(result, directive) {
  if (result._metadata.hasDirectives.has(directive)) {
    return false;
  }
  result._metadata.hasDirectives.add(directive);
  return true;
}
function getDirectiveScriptText(directive) {
  if (!(directive in hydrationScripts)) {
    throw new Error(`Unknown directive: ${directive}`);
  }
  const directiveScriptText = hydrationScripts[directive];
  return directiveScriptText;
}
function getPrescripts(type, directive) {
  switch (type) {
    case "both":
      return `<style>astro-island,astro-slot{display:contents}</style><script>${getDirectiveScriptText(directive) + astro_island_prebuilt_default}<\/script>`;
    case "directive":
      return `<script>${getDirectiveScriptText(directive)}<\/script>`;
  }
  return "";
}

const headAndContentSym = Symbol.for("astro.headAndContent");
function isHeadAndContent(obj) {
  return typeof obj === "object" && !!obj[headAndContentSym];
}

function serializeListValue(value) {
  const hash = {};
  push(value);
  return Object.keys(hash).join(" ");
  function push(item) {
    if (item && typeof item.forEach === "function")
      item.forEach(push);
    else if (item === Object(item))
      Object.keys(item).forEach((name) => {
        if (item[name])
          push(name);
      });
    else {
      item = item === false || item == null ? "" : String(item).trim();
      if (item) {
        item.split(/\s+/).forEach((name) => {
          hash[name] = true;
        });
      }
    }
  }
}
function isPromise(value) {
  return !!value && typeof value === "object" && typeof value.then === "function";
}

var _a$6;
const renderTemplateResultSym = Symbol.for("astro.renderTemplateResult");
class RenderTemplateResult {
  constructor(htmlParts, expressions) {
    this[_a$6] = true;
    this.htmlParts = htmlParts;
    this.error = void 0;
    this.expressions = expressions.map((expression) => {
      if (isPromise(expression)) {
        return Promise.resolve(expression).catch((err) => {
          if (!this.error) {
            this.error = err;
            throw err;
          }
        });
      }
      return expression;
    });
  }
  get [(_a$6 = renderTemplateResultSym, Symbol.toStringTag)]() {
    return "AstroComponent";
  }
  async *[Symbol.asyncIterator]() {
    const { htmlParts, expressions } = this;
    for (let i = 0; i < htmlParts.length; i++) {
      const html = htmlParts[i];
      const expression = expressions[i];
      yield markHTMLString(html);
      yield* renderChild(expression);
    }
  }
}
function isRenderTemplateResult(obj) {
  return typeof obj === "object" && !!obj[renderTemplateResultSym];
}
async function* renderAstroTemplateResult(component) {
  for await (const value of component) {
    if (value || value === 0) {
      for await (const chunk of renderChild(value)) {
        switch (chunk.type) {
          case "directive": {
            yield chunk;
            break;
          }
          default: {
            yield markHTMLString(chunk);
            break;
          }
        }
      }
    }
  }
}
function renderTemplate(htmlParts, ...expressions) {
  return new RenderTemplateResult(htmlParts, expressions);
}

function isAstroComponentFactory(obj) {
  return obj == null ? false : obj.isAstroComponentFactory === true;
}
async function renderToString(result, componentFactory, props, children) {
  const factoryResult = await componentFactory(result, props, children);
  if (factoryResult instanceof Response) {
    const response = factoryResult;
    throw response;
  }
  let parts = new HTMLParts();
  const templateResult = isHeadAndContent(factoryResult) ? factoryResult.content : factoryResult;
  for await (const chunk of renderAstroTemplateResult(templateResult)) {
    parts.append(chunk, result);
  }
  return parts.toString();
}
function isAPropagatingComponent(result, factory) {
  let hint = factory.propagation || "none";
  if (factory.moduleId && result.propagation.has(factory.moduleId) && hint === "none") {
    hint = result.propagation.get(factory.moduleId);
  }
  return hint === "in-tree" || hint === "self";
}

const defineErrors = (errs) => errs;
const AstroErrorData = defineErrors({
  UnknownCompilerError: {
    title: "Unknown compiler error.",
    code: 1e3
  },
  StaticRedirectNotAvailable: {
    title: "`Astro.redirect` is not available in static mode.",
    code: 3001,
    message: "Redirects are only available when using `output: 'server'`. Update your Astro config if you need SSR features.",
    hint: "See https://docs.astro.build/en/guides/server-side-rendering/#enabling-ssr-in-your-project for more information on how to enable SSR."
  },
  ClientAddressNotAvailable: {
    title: "`Astro.clientAddress` is not available in current adapter.",
    code: 3002,
    message: (adapterName) => `\`Astro.clientAddress\` is not available in the \`${adapterName}\` adapter. File an issue with the adapter to add support.`
  },
  StaticClientAddressNotAvailable: {
    title: "`Astro.clientAddress` is not available in static mode.",
    code: 3003,
    message: "`Astro.clientAddress` is only available when using `output: 'server'`. Update your Astro config if you need SSR features.",
    hint: "See https://docs.astro.build/en/guides/server-side-rendering/#enabling-ssr-in-your-project for more information on how to enable SSR."
  },
  NoMatchingStaticPathFound: {
    title: "No static path found for requested path.",
    code: 3004,
    message: (pathName) => `A \`getStaticPaths()\` route pattern was matched, but no matching static path was found for requested path \`${pathName}\`.`,
    hint: (possibleRoutes) => `Possible dynamic routes being matched: ${possibleRoutes.join(", ")}.`
  },
  OnlyResponseCanBeReturned: {
    title: "Invalid type returned by Astro page.",
    code: 3005,
    message: (route, returnedValue) => `Route \`${route ? route : ""}\` returned a \`${returnedValue}\`. Only a [Response](https://developer.mozilla.org/en-US/docs/Web/API/Response) can be returned from Astro files.`,
    hint: "See https://docs.astro.build/en/guides/server-side-rendering/#response for more information."
  },
  MissingMediaQueryDirective: {
    title: "Missing value for `client:media` directive.",
    code: 3006,
    message: 'Media query not provided for `client:media` directive. A media query similar to `client:media="(max-width: 600px)"` must be provided'
  },
  NoMatchingRenderer: {
    title: "No matching renderer found.",
    code: 3007,
    message: (componentName, componentExtension, plural, validRenderersCount) => `Unable to render \`${componentName}\`.

${validRenderersCount > 0 ? `There ${plural ? "are." : "is."} ${validRenderersCount} renderer${plural ? "s." : ""} configured in your \`astro.config.mjs\` file,
but ${plural ? "none were." : "it was not."} able to server-side render \`${componentName}\`.` : `No valid renderer was found ${componentExtension ? `for the \`.${componentExtension}\` file extension.` : `for this file extension.`}`}`,
    hint: (probableRenderers) => `Did you mean to enable the ${probableRenderers} integration?

See https://docs.astro.build/en/core-concepts/framework-components/ for more information on how to install and configure integrations.`
  },
  NoClientEntrypoint: {
    title: "No client entrypoint specified in renderer.",
    code: 3008,
    message: (componentName, clientDirective, rendererName) => `\`${componentName}\` component has a \`client:${clientDirective}\` directive, but no client entrypoint was provided by \`${rendererName}\`.`,
    hint: "See https://docs.astro.build/en/reference/integrations-reference/#addrenderer-option for more information on how to configure your renderer."
  },
  NoClientOnlyHint: {
    title: "Missing hint on client:only directive.",
    code: 3009,
    message: (componentName) => `Unable to render \`${componentName}\`. When using the \`client:only\` hydration strategy, Astro needs a hint to use the correct renderer.`,
    hint: (probableRenderers) => `Did you mean to pass \`client:only="${probableRenderers}"\`? See https://docs.astro.build/en/reference/directives-reference/#clientonly for more information on client:only`
  },
  InvalidGetStaticPathParam: {
    title: "Invalid value returned by a `getStaticPaths` path.",
    code: 3010,
    message: (paramType) => `Invalid params given to \`getStaticPaths\` path. Expected an \`object\`, got \`${paramType}\``,
    hint: "See https://docs.astro.build/en/reference/api-reference/#getstaticpaths for more information on getStaticPaths."
  },
  InvalidGetStaticPathsReturn: {
    title: "Invalid value returned by getStaticPaths.",
    code: 3011,
    message: (returnType) => `Invalid type returned by \`getStaticPaths\`. Expected an \`array\`, got \`${returnType}\``,
    hint: "See https://docs.astro.build/en/reference/api-reference/#getstaticpaths for more information on getStaticPaths."
  },
  GetStaticPathsRemovedRSSHelper: {
    title: "getStaticPaths RSS helper is not available anymore.",
    code: 3012,
    message: "The RSS helper has been removed from `getStaticPaths`. Try the new @astrojs/rss package instead.",
    hint: "See https://docs.astro.build/en/guides/rss/ for more information."
  },
  GetStaticPathsExpectedParams: {
    title: "Missing params property on `getStaticPaths` route.",
    code: 3013,
    message: "Missing or empty required `params` property on `getStaticPaths` route.",
    hint: "See https://docs.astro.build/en/reference/api-reference/#getstaticpaths for more information on getStaticPaths."
  },
  GetStaticPathsInvalidRouteParam: {
    title: "Invalid value for `getStaticPaths` route parameter.",
    code: 3014,
    message: (key, value, valueType) => `Invalid getStaticPaths route parameter for \`${key}\`. Expected undefined, a string or a number, received \`${valueType}\` (\`${value}\`)`,
    hint: "See https://docs.astro.build/en/reference/api-reference/#getstaticpaths for more information on getStaticPaths."
  },
  GetStaticPathsRequired: {
    title: "`getStaticPaths()` function required for dynamic routes.",
    code: 3015,
    message: "`getStaticPaths()` function is required for dynamic routes. Make sure that you `export` a `getStaticPaths` function from your dynamic route.",
    hint: `See https://docs.astro.build/en/core-concepts/routing/#dynamic-routes for more information on dynamic routes.

Alternatively, set \`output: "server"\` in your Astro config file to switch to a non-static server build.
See https://docs.astro.build/en/guides/server-side-rendering/ for more information on non-static rendering.`
  },
  ReservedSlotName: {
    title: "Invalid slot name.",
    code: 3016,
    message: (slotName) => `Unable to create a slot named \`${slotName}\`. \`${slotName}\` is a reserved slot name. Please update the name of this slot.`
  },
  NoAdapterInstalled: {
    title: "Cannot use Server-side Rendering without an adapter.",
    code: 3017,
    message: `Cannot use \`output: 'server'\` without an adapter. Please install and configure the appropriate server adapter for your final deployment.`,
    hint: "See https://docs.astro.build/en/guides/server-side-rendering/ for more information."
  },
  NoMatchingImport: {
    title: "No import found for component.",
    code: 3018,
    message: (componentName) => `Could not render \`${componentName}\`. No matching import has been found for \`${componentName}\`.`,
    hint: "Please make sure the component is properly imported."
  },
  InvalidPrerenderExport: {
    title: "Invalid prerender export.",
    code: 3019,
    message: (prefix, suffix) => {
      let msg = `A \`prerender\` export has been detected, but its value cannot be statically analyzed.`;
      if (prefix !== "const")
        msg += `
Expected \`const\` declaration but got \`${prefix}\`.`;
      if (suffix !== "true")
        msg += `
Expected \`true\` value but got \`${suffix}\`.`;
      return msg;
    },
    hint: "Mutable values declared at runtime are not supported. Please make sure to use exactly `export const prerender = true`."
  },
  UnknownViteError: {
    title: "Unknown Vite Error.",
    code: 4e3
  },
  FailedToLoadModuleSSR: {
    title: "Could not import file.",
    code: 4001,
    message: (importName) => `Could not import \`${importName}\`.`,
    hint: "This is often caused by a typo in the import path. Please make sure the file exists."
  },
  InvalidGlob: {
    title: "Invalid glob pattern.",
    code: 4002,
    message: (globPattern) => `Invalid glob pattern: \`${globPattern}\`. Glob patterns must start with './', '../' or '/'.`,
    hint: "See https://docs.astro.build/en/guides/imports/#glob-patterns for more information on supported glob patterns."
  },
  UnknownCSSError: {
    title: "Unknown CSS Error.",
    code: 5e3
  },
  CSSSyntaxError: {
    title: "CSS Syntax Error.",
    code: 5001
  },
  UnknownMarkdownError: {
    title: "Unknown Markdown Error.",
    code: 6e3
  },
  MarkdownFrontmatterParseError: {
    title: "Failed to parse Markdown frontmatter.",
    code: 6001
  },
  MarkdownContentSchemaValidationError: {
    title: "Content collection frontmatter invalid.",
    code: 6002,
    message: (collection, entryId, error) => {
      return [
        `${String(collection)} \u2192 ${String(entryId)} frontmatter does not match collection schema.`,
        ...error.errors.map((zodError) => zodError.message)
      ].join("\n");
    },
    hint: "See https://docs.astro.build/en/guides/content-collections/ for more information on content schemas."
  },
  UnknownConfigError: {
    title: "Unknown configuration error.",
    code: 7e3
  },
  ConfigNotFound: {
    title: "Specified configuration file not found.",
    code: 7001,
    message: (configFile) => `Unable to resolve \`--config "${configFile}"\`. Does the file exist?`
  },
  ConfigLegacyKey: {
    title: "Legacy configuration detected.",
    code: 7002,
    message: (legacyConfigKey) => `Legacy configuration detected: \`${legacyConfigKey}\`.`,
    hint: "Please update your configuration to the new format.\nSee https://astro.build/config for more information."
  },
  UnknownCLIError: {
    title: "Unknown CLI Error.",
    code: 8e3
  },
  GenerateContentTypesError: {
    title: "Failed to generate content types.",
    code: 8001,
    message: "`astro sync` command failed to generate content collection types.",
    hint: "Check your `src/content/config.*` file for typos."
  },
  UnknownError: {
    title: "Unknown Error.",
    code: 99999
  }
});

function normalizeLF(code) {
  return code.replace(/\r\n|\r(?!\n)|\n/g, "\n");
}
function getErrorDataByCode(code) {
  const entry = Object.entries(AstroErrorData).find((data) => data[1].code === code);
  if (entry) {
    return {
      name: entry[0],
      data: entry[1]
    };
  }
}

function codeFrame(src, loc) {
  if (!loc || loc.line === void 0 || loc.column === void 0) {
    return "";
  }
  const lines = normalizeLF(src).split("\n").map((ln) => ln.replace(/\t/g, "  "));
  const visibleLines = [];
  for (let n = -2; n <= 2; n++) {
    if (lines[loc.line + n])
      visibleLines.push(loc.line + n);
  }
  let gutterWidth = 0;
  for (const lineNo of visibleLines) {
    let w = `> ${lineNo}`;
    if (w.length > gutterWidth)
      gutterWidth = w.length;
  }
  let output = "";
  for (const lineNo of visibleLines) {
    const isFocusedLine = lineNo === loc.line - 1;
    output += isFocusedLine ? "> " : "  ";
    output += `${lineNo + 1} | ${lines[lineNo]}
`;
    if (isFocusedLine)
      output += `${Array.from({ length: gutterWidth }).join(" ")}  | ${Array.from({
        length: loc.column
      }).join(" ")}^
`;
  }
  return output;
}

class AstroError extends Error {
  constructor(props, ...params) {
    var _a;
    super(...params);
    this.type = "AstroError";
    const { code, name, title, message, stack, location, hint, frame } = props;
    this.errorCode = code;
    if (name && name !== "Error") {
      this.name = name;
    } else {
      this.name = ((_a = getErrorDataByCode(this.errorCode)) == null ? void 0 : _a.name) ?? "UnknownError";
    }
    this.title = title;
    if (message)
      this.message = message;
    this.stack = stack ? stack : this.stack;
    this.loc = location;
    this.hint = hint;
    this.frame = frame;
  }
  setErrorCode(errorCode) {
    this.errorCode = errorCode;
  }
  setLocation(location) {
    this.loc = location;
  }
  setName(name) {
    this.name = name;
  }
  setMessage(message) {
    this.message = message;
  }
  setHint(hint) {
    this.hint = hint;
  }
  setFrame(source, location) {
    this.frame = codeFrame(source, location);
  }
  static is(err) {
    return err.type === "AstroError";
  }
}

const PROP_TYPE = {
  Value: 0,
  JSON: 1,
  RegExp: 2,
  Date: 3,
  Map: 4,
  Set: 5,
  BigInt: 6,
  URL: 7,
  Uint8Array: 8,
  Uint16Array: 9,
  Uint32Array: 10
};
function serializeArray(value, metadata = {}, parents = /* @__PURE__ */ new WeakSet()) {
  if (parents.has(value)) {
    throw new Error(`Cyclic reference detected while serializing props for <${metadata.displayName} client:${metadata.hydrate}>!

Cyclic references cannot be safely serialized for client-side usage. Please remove the cyclic reference.`);
  }
  parents.add(value);
  const serialized = value.map((v) => {
    return convertToSerializedForm(v, metadata, parents);
  });
  parents.delete(value);
  return serialized;
}
function serializeObject(value, metadata = {}, parents = /* @__PURE__ */ new WeakSet()) {
  if (parents.has(value)) {
    throw new Error(`Cyclic reference detected while serializing props for <${metadata.displayName} client:${metadata.hydrate}>!

Cyclic references cannot be safely serialized for client-side usage. Please remove the cyclic reference.`);
  }
  parents.add(value);
  const serialized = Object.fromEntries(
    Object.entries(value).map(([k, v]) => {
      return [k, convertToSerializedForm(v, metadata, parents)];
    })
  );
  parents.delete(value);
  return serialized;
}
function convertToSerializedForm(value, metadata = {}, parents = /* @__PURE__ */ new WeakSet()) {
  const tag = Object.prototype.toString.call(value);
  switch (tag) {
    case "[object Date]": {
      return [PROP_TYPE.Date, value.toISOString()];
    }
    case "[object RegExp]": {
      return [PROP_TYPE.RegExp, value.source];
    }
    case "[object Map]": {
      return [
        PROP_TYPE.Map,
        JSON.stringify(serializeArray(Array.from(value), metadata, parents))
      ];
    }
    case "[object Set]": {
      return [
        PROP_TYPE.Set,
        JSON.stringify(serializeArray(Array.from(value), metadata, parents))
      ];
    }
    case "[object BigInt]": {
      return [PROP_TYPE.BigInt, value.toString()];
    }
    case "[object URL]": {
      return [PROP_TYPE.URL, value.toString()];
    }
    case "[object Array]": {
      return [PROP_TYPE.JSON, JSON.stringify(serializeArray(value, metadata, parents))];
    }
    case "[object Uint8Array]": {
      return [PROP_TYPE.Uint8Array, JSON.stringify(Array.from(value))];
    }
    case "[object Uint16Array]": {
      return [PROP_TYPE.Uint16Array, JSON.stringify(Array.from(value))];
    }
    case "[object Uint32Array]": {
      return [PROP_TYPE.Uint32Array, JSON.stringify(Array.from(value))];
    }
    default: {
      if (value !== null && typeof value === "object") {
        return [PROP_TYPE.Value, serializeObject(value, metadata, parents)];
      } else {
        return [PROP_TYPE.Value, value];
      }
    }
  }
}
function serializeProps(props, metadata) {
  const serialized = JSON.stringify(serializeObject(props, metadata));
  return serialized;
}

const HydrationDirectivesRaw = ["load", "idle", "media", "visible", "only"];
const HydrationDirectives = new Set(HydrationDirectivesRaw);
const HydrationDirectiveProps = new Set(HydrationDirectivesRaw.map((n) => `client:${n}`));
function extractDirectives(displayName, inputProps) {
  let extracted = {
    isPage: false,
    hydration: null,
    props: {}
  };
  for (const [key, value] of Object.entries(inputProps)) {
    if (key.startsWith("server:")) {
      if (key === "server:root") {
        extracted.isPage = true;
      }
    }
    if (key.startsWith("client:")) {
      if (!extracted.hydration) {
        extracted.hydration = {
          directive: "",
          value: "",
          componentUrl: "",
          componentExport: { value: "" }
        };
      }
      switch (key) {
        case "client:component-path": {
          extracted.hydration.componentUrl = value;
          break;
        }
        case "client:component-export": {
          extracted.hydration.componentExport.value = value;
          break;
        }
        case "client:component-hydration": {
          break;
        }
        case "client:display-name": {
          break;
        }
        default: {
          extracted.hydration.directive = key.split(":")[1];
          extracted.hydration.value = value;
          if (!HydrationDirectives.has(extracted.hydration.directive)) {
            throw new Error(
              `Error: invalid hydration directive "${key}". Supported hydration methods: ${Array.from(
                HydrationDirectiveProps
              ).join(", ")}`
            );
          }
          if (extracted.hydration.directive === "media" && typeof extracted.hydration.value !== "string") {
            throw new AstroError(AstroErrorData.MissingMediaQueryDirective);
          }
          break;
        }
      }
    } else if (key === "class:list") {
      if (value) {
        extracted.props[key.slice(0, -5)] = serializeListValue(value);
      }
    } else {
      extracted.props[key] = value;
    }
  }
  for (const sym of Object.getOwnPropertySymbols(inputProps)) {
    extracted.props[sym] = inputProps[sym];
  }
  return extracted;
}
async function generateHydrateScript(scriptOptions, metadata) {
  const { renderer, result, astroId, props, attrs } = scriptOptions;
  const { hydrate, componentUrl, componentExport } = metadata;
  if (!componentExport.value) {
    throw new Error(
      `Unable to resolve a valid export for "${metadata.displayName}"! Please open an issue at https://astro.build/issues!`
    );
  }
  const island = {
    children: "",
    props: {
      uid: astroId
    }
  };
  if (attrs) {
    for (const [key, value] of Object.entries(attrs)) {
      island.props[key] = escapeHTML(value);
    }
  }
  island.props["component-url"] = await result.resolve(decodeURI(componentUrl));
  if (renderer.clientEntrypoint) {
    island.props["component-export"] = componentExport.value;
    island.props["renderer-url"] = await result.resolve(decodeURI(renderer.clientEntrypoint));
    island.props["props"] = escapeHTML(serializeProps(props, metadata));
  }
  island.props["ssr"] = "";
  island.props["client"] = hydrate;
  let beforeHydrationUrl = await result.resolve("astro:scripts/before-hydration.js");
  if (beforeHydrationUrl.length) {
    island.props["before-hydration-url"] = beforeHydrationUrl;
  }
  island.props["opts"] = escapeHTML(
    JSON.stringify({
      name: metadata.displayName,
      value: metadata.hydrateArgs || ""
    })
  );
  return island;
}

var _a$5;
const astroComponentInstanceSym = Symbol.for("astro.componentInstance");
class AstroComponentInstance {
  constructor(result, props, slots, factory) {
    this[_a$5] = true;
    this.result = result;
    this.props = props;
    this.factory = factory;
    this.slotValues = {};
    for (const name in slots) {
      this.slotValues[name] = slots[name]();
    }
  }
  async init() {
    this.returnValue = this.factory(this.result, this.props, this.slotValues);
    return this.returnValue;
  }
  async *render() {
    if (this.returnValue === void 0) {
      await this.init();
    }
    let value = this.returnValue;
    if (isPromise(value)) {
      value = await value;
    }
    if (isHeadAndContent(value)) {
      yield* value.content;
    } else {
      yield* renderChild(value);
    }
  }
}
_a$5 = astroComponentInstanceSym;
function validateComponentProps(props, displayName) {
  if (props != null) {
    for (const prop of Object.keys(props)) {
      if (HydrationDirectiveProps.has(prop)) {
        console.warn(
          `You are attempting to render <${displayName} ${prop} />, but ${displayName} is an Astro component. Astro components do not render in the client and should not have a hydration directive. Please use a framework component for client rendering.`
        );
      }
    }
  }
}
function createAstroComponentInstance(result, displayName, factory, props, slots = {}) {
  validateComponentProps(props, displayName);
  const instance = new AstroComponentInstance(result, props, slots, factory);
  if (isAPropagatingComponent(result, factory) && !result.propagators.has(factory)) {
    result.propagators.set(factory, instance);
  }
  return instance;
}
function isAstroComponentInstance(obj) {
  return typeof obj === "object" && !!obj[astroComponentInstanceSym];
}

async function* renderChild(child) {
  child = await child;
  if (child instanceof SlotString) {
    if (child.instructions) {
      yield* child.instructions;
    }
    yield child;
  } else if (isHTMLString(child)) {
    yield child;
  } else if (Array.isArray(child)) {
    for (const value of child) {
      yield markHTMLString(await renderChild(value));
    }
  } else if (typeof child === "function") {
    yield* renderChild(child());
  } else if (typeof child === "string") {
    yield markHTMLString(escapeHTML(child));
  } else if (!child && child !== 0) ; else if (isRenderTemplateResult(child)) {
    yield* renderAstroTemplateResult(child);
  } else if (isAstroComponentInstance(child)) {
    yield* child.render();
  } else if (ArrayBuffer.isView(child)) {
    yield child;
  } else if (typeof child === "object" && (Symbol.asyncIterator in child || Symbol.iterator in child)) {
    yield* child;
  } else {
    yield child;
  }
}

const slotString = Symbol.for("astro:slot-string");
class SlotString extends HTMLString {
  constructor(content, instructions) {
    super(content);
    this.instructions = instructions;
    this[slotString] = true;
  }
}
function isSlotString(str) {
  return !!str[slotString];
}
async function renderSlot(_result, slotted, fallback) {
  if (slotted) {
    let iterator = renderChild(slotted);
    let content = "";
    let instructions = null;
    for await (const chunk of iterator) {
      if (chunk.type === "directive") {
        if (instructions === null) {
          instructions = [];
        }
        instructions.push(chunk);
      } else {
        content += chunk;
      }
    }
    return markHTMLString(new SlotString(content, instructions));
  }
  return fallback;
}
async function renderSlots(result, slots = {}) {
  let slotInstructions = null;
  let children = {};
  if (slots) {
    await Promise.all(
      Object.entries(slots).map(
        ([key, value]) => renderSlot(result, value).then((output) => {
          if (output.instructions) {
            if (slotInstructions === null) {
              slotInstructions = [];
            }
            slotInstructions.push(...output.instructions);
          }
          children[key] = output;
        })
      )
    );
  }
  return { slotInstructions, children };
}

const Fragment = Symbol.for("astro:fragment");
const Renderer = Symbol.for("astro:renderer");
const encoder = new TextEncoder();
const decoder = new TextDecoder();
function stringifyChunk(result, chunk) {
  switch (chunk.type) {
    case "directive": {
      const { hydration } = chunk;
      let needsHydrationScript = hydration && determineIfNeedsHydrationScript(result);
      let needsDirectiveScript = hydration && determinesIfNeedsDirectiveScript(result, hydration.directive);
      let prescriptType = needsHydrationScript ? "both" : needsDirectiveScript ? "directive" : null;
      if (prescriptType) {
        let prescripts = getPrescripts(prescriptType, hydration.directive);
        return markHTMLString(prescripts);
      } else {
        return "";
      }
    }
    default: {
      if (isSlotString(chunk)) {
        let out = "";
        const c = chunk;
        if (c.instructions) {
          for (const instr of c.instructions) {
            out += stringifyChunk(result, instr);
          }
        }
        out += chunk.toString();
        return out;
      }
      return chunk.toString();
    }
  }
}
class HTMLParts {
  constructor() {
    this.parts = "";
  }
  append(part, result) {
    if (ArrayBuffer.isView(part)) {
      this.parts += decoder.decode(part);
    } else {
      this.parts += stringifyChunk(result, part);
    }
  }
  toString() {
    return this.parts;
  }
  toArrayBuffer() {
    return encoder.encode(this.parts);
  }
}

const ClientOnlyPlaceholder = "astro-client-only";
class Skip {
  constructor(vnode) {
    this.vnode = vnode;
    this.count = 0;
  }
  increment() {
    this.count++;
  }
  haveNoTried() {
    return this.count === 0;
  }
  isCompleted() {
    return this.count > 2;
  }
}
Skip.symbol = Symbol("astro:jsx:skip");
let originalConsoleError;
let consoleFilterRefs = 0;
async function renderJSX(result, vnode) {
  switch (true) {
    case vnode instanceof HTMLString:
      if (vnode.toString().trim() === "") {
        return "";
      }
      return vnode;
    case typeof vnode === "string":
      return markHTMLString(escapeHTML(vnode));
    case typeof vnode === "function":
      return vnode;
    case (!vnode && vnode !== 0):
      return "";
    case Array.isArray(vnode):
      return markHTMLString(
        (await Promise.all(vnode.map((v) => renderJSX(result, v)))).join("")
      );
  }
  let skip;
  if (vnode.props) {
    if (vnode.props[Skip.symbol]) {
      skip = vnode.props[Skip.symbol];
    } else {
      skip = new Skip(vnode);
    }
  } else {
    skip = new Skip(vnode);
  }
  return renderJSXVNode(result, vnode, skip);
}
async function renderJSXVNode(result, vnode, skip) {
  if (isVNode(vnode)) {
    switch (true) {
      case !vnode.type: {
        throw new Error(`Unable to render ${result._metadata.pathname} because it contains an undefined Component!
Did you forget to import the component or is it possible there is a typo?`);
      }
      case vnode.type === Symbol.for("astro:fragment"):
        return renderJSX(result, vnode.props.children);
      case vnode.type.isAstroComponentFactory: {
        let props = {};
        let slots = {};
        for (const [key, value] of Object.entries(vnode.props ?? {})) {
          if (key === "children" || value && typeof value === "object" && value["$$slot"]) {
            slots[key === "children" ? "default" : key] = () => renderJSX(result, value);
          } else {
            props[key] = value;
          }
        }
        return markHTMLString(await renderToString(result, vnode.type, props, slots));
      }
      case (!vnode.type && vnode.type !== 0):
        return "";
      case (typeof vnode.type === "string" && vnode.type !== ClientOnlyPlaceholder):
        return markHTMLString(await renderElement$1(result, vnode.type, vnode.props ?? {}));
    }
    if (vnode.type) {
      let extractSlots2 = function(child) {
        if (Array.isArray(child)) {
          return child.map((c) => extractSlots2(c));
        }
        if (!isVNode(child)) {
          _slots.default.push(child);
          return;
        }
        if ("slot" in child.props) {
          _slots[child.props.slot] = [..._slots[child.props.slot] ?? [], child];
          delete child.props.slot;
          return;
        }
        _slots.default.push(child);
      };
      if (typeof vnode.type === "function" && vnode.type["astro:renderer"]) {
        skip.increment();
      }
      if (typeof vnode.type === "function" && vnode.props["server:root"]) {
        const output2 = await vnode.type(vnode.props ?? {});
        return await renderJSX(result, output2);
      }
      if (typeof vnode.type === "function") {
        if (skip.haveNoTried() || skip.isCompleted()) {
          useConsoleFilter();
          try {
            const output2 = await vnode.type(vnode.props ?? {});
            let renderResult;
            if (output2 && output2[AstroJSX]) {
              renderResult = await renderJSXVNode(result, output2, skip);
              return renderResult;
            } else if (!output2) {
              renderResult = await renderJSXVNode(result, output2, skip);
              return renderResult;
            }
          } catch (e) {
            if (skip.isCompleted()) {
              throw e;
            }
            skip.increment();
          } finally {
            finishUsingConsoleFilter();
          }
        } else {
          skip.increment();
        }
      }
      const { children = null, ...props } = vnode.props ?? {};
      const _slots = {
        default: []
      };
      extractSlots2(children);
      for (const [key, value] of Object.entries(props)) {
        if (value["$$slot"]) {
          _slots[key] = value;
          delete props[key];
        }
      }
      const slotPromises = [];
      const slots = {};
      for (const [key, value] of Object.entries(_slots)) {
        slotPromises.push(
          renderJSX(result, value).then((output2) => {
            if (output2.toString().trim().length === 0)
              return;
            slots[key] = () => output2;
          })
        );
      }
      await Promise.all(slotPromises);
      props[Skip.symbol] = skip;
      let output;
      if (vnode.type === ClientOnlyPlaceholder && vnode.props["client:only"]) {
        output = await renderComponentToIterable(
          result,
          vnode.props["client:display-name"] ?? "",
          null,
          props,
          slots
        );
      } else {
        output = await renderComponentToIterable(
          result,
          typeof vnode.type === "function" ? vnode.type.name : vnode.type,
          vnode.type,
          props,
          slots
        );
      }
      if (typeof output !== "string" && Symbol.asyncIterator in output) {
        let parts = new HTMLParts();
        for await (const chunk of output) {
          parts.append(chunk, result);
        }
        return markHTMLString(parts.toString());
      } else {
        return markHTMLString(output);
      }
    }
  }
  return markHTMLString(`${vnode}`);
}
async function renderElement$1(result, tag, { children, ...props }) {
  return markHTMLString(
    `<${tag}${spreadAttributes(props)}${markHTMLString(
      (children == null || children == "") && voidElementNames.test(tag) ? `/>` : `>${children == null ? "" : await renderJSX(result, children)}</${tag}>`
    )}`
  );
}
function useConsoleFilter() {
  consoleFilterRefs++;
  if (!originalConsoleError) {
    originalConsoleError = console.error;
    try {
      console.error = filteredConsoleError;
    } catch (error) {
    }
  }
}
function finishUsingConsoleFilter() {
  consoleFilterRefs--;
}
function filteredConsoleError(msg, ...rest) {
  if (consoleFilterRefs > 0 && typeof msg === "string") {
    const isKnownReactHookError = msg.includes("Warning: Invalid hook call.") && msg.includes("https://reactjs.org/link/invalid-hook-call");
    if (isKnownReactHookError)
      return;
  }
  originalConsoleError(msg, ...rest);
}

/**
 * shortdash - https://github.com/bibig/node-shorthash
 *
 * @license
 *
 * (The MIT License)
 *
 * Copyright (c) 2013 Bibig <bibig@me.com>
 *
 * Permission is hereby granted, free of charge, to any person
 * obtaining a copy of this software and associated documentation
 * files (the "Software"), to deal in the Software without
 * restriction, including without limitation the rights to use,
 * copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the
 * Software is furnished to do so, subject to the following
 * conditions:
 *
 * The above copyright notice and this permission notice shall be
 * included in all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
 * OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
 * HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
 * WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
 * OTHER DEALINGS IN THE SOFTWARE.
 */
const dictionary = "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXY";
const binary = dictionary.length;
function bitwise(str) {
  let hash = 0;
  if (str.length === 0)
    return hash;
  for (let i = 0; i < str.length; i++) {
    const ch = str.charCodeAt(i);
    hash = (hash << 5) - hash + ch;
    hash = hash & hash;
  }
  return hash;
}
function shorthash(text) {
  let num;
  let result = "";
  let integer = bitwise(text);
  const sign = integer < 0 ? "Z" : "";
  integer = Math.abs(integer);
  while (integer >= binary) {
    num = integer % binary;
    integer = Math.floor(integer / binary);
    result = dictionary[num] + result;
  }
  if (integer > 0) {
    result = dictionary[integer] + result;
  }
  return sign + result;
}

const voidElementNames = /^(area|base|br|col|command|embed|hr|img|input|keygen|link|meta|param|source|track|wbr)$/i;
const htmlBooleanAttributes = /^(allowfullscreen|async|autofocus|autoplay|controls|default|defer|disabled|disablepictureinpicture|disableremoteplayback|formnovalidate|hidden|loop|nomodule|novalidate|open|playsinline|readonly|required|reversed|scoped|seamless|itemscope)$/i;
const htmlEnumAttributes = /^(contenteditable|draggable|spellcheck|value)$/i;
const svgEnumAttributes = /^(autoReverse|externalResourcesRequired|focusable|preserveAlpha)$/i;
const STATIC_DIRECTIVES = /* @__PURE__ */ new Set(["set:html", "set:text"]);
const toIdent = (k) => k.trim().replace(/(?:(?!^)\b\w|\s+|[^\w]+)/g, (match, index) => {
  if (/[^\w]|\s/.test(match))
    return "";
  return index === 0 ? match : match.toUpperCase();
});
const toAttributeString = (value, shouldEscape = true) => shouldEscape ? String(value).replace(/&/g, "&#38;").replace(/"/g, "&#34;") : value;
const kebab = (k) => k.toLowerCase() === k ? k : k.replace(/[A-Z]/g, (match) => `-${match.toLowerCase()}`);
const toStyleString = (obj) => Object.entries(obj).map(([k, v]) => `${kebab(k)}:${v}`).join(";");
function defineScriptVars(vars) {
  let output = "";
  for (const [key, value] of Object.entries(vars)) {
    output += `const ${toIdent(key)} = ${JSON.stringify(value)};
`;
  }
  return markHTMLString(output);
}
function formatList(values) {
  if (values.length === 1) {
    return values[0];
  }
  return `${values.slice(0, -1).join(", ")} or ${values[values.length - 1]}`;
}
function addAttribute(value, key, shouldEscape = true) {
  if (value == null) {
    return "";
  }
  if (value === false) {
    if (htmlEnumAttributes.test(key) || svgEnumAttributes.test(key)) {
      return markHTMLString(` ${key}="false"`);
    }
    return "";
  }
  if (STATIC_DIRECTIVES.has(key)) {
    console.warn(`[astro] The "${key}" directive cannot be applied dynamically at runtime. It will not be rendered as an attribute.

Make sure to use the static attribute syntax (\`${key}={value}\`) instead of the dynamic spread syntax (\`{...{ "${key}": value }}\`).`);
    return "";
  }
  if (key === "class:list") {
    const listValue = toAttributeString(serializeListValue(value), shouldEscape);
    if (listValue === "") {
      return "";
    }
    return markHTMLString(` ${key.slice(0, -5)}="${listValue}"`);
  }
  if (key === "style" && !(value instanceof HTMLString) && typeof value === "object") {
    return markHTMLString(` ${key}="${toAttributeString(toStyleString(value), shouldEscape)}"`);
  }
  if (key === "className") {
    return markHTMLString(` class="${toAttributeString(value, shouldEscape)}"`);
  }
  if (value === true && (key.startsWith("data-") || htmlBooleanAttributes.test(key))) {
    return markHTMLString(` ${key}`);
  } else {
    return markHTMLString(` ${key}="${toAttributeString(value, shouldEscape)}"`);
  }
}
function internalSpreadAttributes(values, shouldEscape = true) {
  let output = "";
  for (const [key, value] of Object.entries(values)) {
    output += addAttribute(value, key, shouldEscape);
  }
  return markHTMLString(output);
}
function renderElement(name, { props: _props, children = "" }, shouldEscape = true) {
  const { lang: _, "data-astro-id": astroId, "define:vars": defineVars, ...props } = _props;
  if (defineVars) {
    if (name === "style") {
      delete props["is:global"];
      delete props["is:scoped"];
    }
    if (name === "script") {
      delete props.hoist;
      children = defineScriptVars(defineVars) + "\n" + children;
    }
  }
  if ((children == null || children == "") && voidElementNames.test(name)) {
    return `<${name}${internalSpreadAttributes(props, shouldEscape)} />`;
  }
  return `<${name}${internalSpreadAttributes(props, shouldEscape)}>${children}</${name}>`;
}

function componentIsHTMLElement(Component) {
  return typeof HTMLElement !== "undefined" && HTMLElement.isPrototypeOf(Component);
}
async function renderHTMLElement(result, constructor, props, slots) {
  const name = getHTMLElementName(constructor);
  let attrHTML = "";
  for (const attr in props) {
    attrHTML += ` ${attr}="${toAttributeString(await props[attr])}"`;
  }
  return markHTMLString(
    `<${name}${attrHTML}>${await renderSlot(result, slots == null ? void 0 : slots.default)}</${name}>`
  );
}
function getHTMLElementName(constructor) {
  const definedName = customElements.getName(constructor);
  if (definedName)
    return definedName;
  const assignedName = constructor.name.replace(/^HTML|Element$/g, "").replace(/[A-Z]/g, "-$&").toLowerCase().replace(/^-/, "html-");
  return assignedName;
}

const rendererAliases = /* @__PURE__ */ new Map([["solid", "solid-js"]]);
function guessRenderers(componentUrl) {
  const extname = componentUrl == null ? void 0 : componentUrl.split(".").pop();
  switch (extname) {
    case "svelte":
      return ["@astrojs/svelte"];
    case "vue":
      return ["@astrojs/vue"];
    case "jsx":
    case "tsx":
      return ["@astrojs/react", "@astrojs/preact", "@astrojs/solid", "@astrojs/vue (jsx)"];
    default:
      return [
        "@astrojs/react",
        "@astrojs/preact",
        "@astrojs/solid",
        "@astrojs/vue",
        "@astrojs/svelte"
      ];
  }
}
function isFragmentComponent(Component) {
  return Component === Fragment;
}
function isHTMLComponent(Component) {
  return Component && typeof Component === "object" && Component["astro:html"];
}
async function renderFrameworkComponent(result, displayName, Component, _props, slots = {}) {
  var _a, _b;
  if (!Component && !_props["client:only"]) {
    throw new Error(
      `Unable to render ${displayName} because it is ${Component}!
Did you forget to import the component or is it possible there is a typo?`
    );
  }
  const { renderers } = result._metadata;
  const metadata = { displayName };
  const { hydration, isPage, props } = extractDirectives(displayName, _props);
  let html = "";
  let attrs = void 0;
  if (hydration) {
    metadata.hydrate = hydration.directive;
    metadata.hydrateArgs = hydration.value;
    metadata.componentExport = hydration.componentExport;
    metadata.componentUrl = hydration.componentUrl;
  }
  const probableRendererNames = guessRenderers(metadata.componentUrl);
  const validRenderers = renderers.filter((r) => r.name !== "astro:jsx");
  const { children, slotInstructions } = await renderSlots(result, slots);
  let renderer;
  if (metadata.hydrate !== "only") {
    let isTagged = false;
    try {
      isTagged = Component && Component[Renderer];
    } catch {
    }
    if (isTagged) {
      const rendererName = Component[Renderer];
      renderer = renderers.find(({ name }) => name === rendererName);
    }
    if (!renderer) {
      let error;
      for (const r of renderers) {
        try {
          if (await r.ssr.check.call({ result }, Component, props, children)) {
            renderer = r;
            break;
          }
        } catch (e) {
          error ?? (error = e);
        }
      }
      if (!renderer && error) {
        throw error;
      }
    }
    if (!renderer && typeof HTMLElement === "function" && componentIsHTMLElement(Component)) {
      const output = renderHTMLElement(result, Component, _props, slots);
      return output;
    }
  } else {
    if (metadata.hydrateArgs) {
      const passedName = metadata.hydrateArgs;
      const rendererName = rendererAliases.has(passedName) ? rendererAliases.get(passedName) : passedName;
      renderer = renderers.find(
        ({ name }) => name === `@astrojs/${rendererName}` || name === rendererName
      );
    }
    if (!renderer && validRenderers.length === 1) {
      renderer = validRenderers[0];
    }
    if (!renderer) {
      const extname = (_a = metadata.componentUrl) == null ? void 0 : _a.split(".").pop();
      renderer = renderers.filter(
        ({ name }) => name === `@astrojs/${extname}` || name === extname
      )[0];
    }
  }
  if (!renderer) {
    if (metadata.hydrate === "only") {
      throw new AstroError({
        ...AstroErrorData.NoClientOnlyHint,
        message: AstroErrorData.NoClientOnlyHint.message(metadata.displayName),
        hint: AstroErrorData.NoClientOnlyHint.hint(
          probableRendererNames.map((r) => r.replace("@astrojs/", "")).join("|")
        )
      });
    } else if (typeof Component !== "string") {
      const matchingRenderers = validRenderers.filter(
        (r) => probableRendererNames.includes(r.name)
      );
      const plural = validRenderers.length > 1;
      if (matchingRenderers.length === 0) {
        throw new AstroError({
          ...AstroErrorData.NoMatchingRenderer,
          message: AstroErrorData.NoMatchingRenderer.message(
            metadata.displayName,
            (_b = metadata == null ? void 0 : metadata.componentUrl) == null ? void 0 : _b.split(".").pop(),
            plural,
            validRenderers.length
          ),
          hint: AstroErrorData.NoMatchingRenderer.hint(
            formatList(probableRendererNames.map((r) => "`" + r + "`"))
          )
        });
      } else if (matchingRenderers.length === 1) {
        renderer = matchingRenderers[0];
        ({ html, attrs } = await renderer.ssr.renderToStaticMarkup.call(
          { result },
          Component,
          props,
          children,
          metadata
        ));
      } else {
        throw new Error(`Unable to render ${metadata.displayName}!

This component likely uses ${formatList(probableRendererNames)},
but Astro encountered an error during server-side rendering.

Please ensure that ${metadata.displayName}:
1. Does not unconditionally access browser-specific globals like \`window\` or \`document\`.
   If this is unavoidable, use the \`client:only\` hydration directive.
2. Does not conditionally return \`null\` or \`undefined\` when rendered on the server.

If you're still stuck, please open an issue on GitHub or join us at https://astro.build/chat.`);
      }
    }
  } else {
    if (metadata.hydrate === "only") {
      html = await renderSlot(result, slots == null ? void 0 : slots.fallback);
    } else {
      ({ html, attrs } = await renderer.ssr.renderToStaticMarkup.call(
        { result },
        Component,
        props,
        children,
        metadata
      ));
    }
  }
  if (renderer && !renderer.clientEntrypoint && renderer.name !== "@astrojs/lit" && metadata.hydrate) {
    throw new AstroError({
      ...AstroErrorData.NoClientEntrypoint,
      message: AstroErrorData.NoClientEntrypoint.message(
        displayName,
        metadata.hydrate,
        renderer.name
      )
    });
  }
  if (!html && typeof Component === "string") {
    const Tag = sanitizeElementName(Component);
    const childSlots = Object.values(children).join("");
    const iterable = renderAstroTemplateResult(
      await renderTemplate`<${Tag}${internalSpreadAttributes(props)}${markHTMLString(
        childSlots === "" && voidElementNames.test(Tag) ? `/>` : `>${childSlots}</${Tag}>`
      )}`
    );
    html = "";
    for await (const chunk of iterable) {
      html += chunk;
    }
  }
  if (!hydration) {
    return async function* () {
      if (slotInstructions) {
        yield* slotInstructions;
      }
      if (isPage || (renderer == null ? void 0 : renderer.name) === "astro:jsx") {
        yield html;
      } else {
        yield markHTMLString(html.replace(/\<\/?astro-slot\>/g, ""));
      }
    }();
  }
  const astroId = shorthash(
    `<!--${metadata.componentExport.value}:${metadata.componentUrl}-->
${html}
${serializeProps(
      props,
      metadata
    )}`
  );
  const island = await generateHydrateScript(
    { renderer, result, astroId, props, attrs },
    metadata
  );
  let unrenderedSlots = [];
  if (html) {
    if (Object.keys(children).length > 0) {
      for (const key of Object.keys(children)) {
        if (!html.includes(key === "default" ? `<astro-slot>` : `<astro-slot name="${key}">`)) {
          unrenderedSlots.push(key);
        }
      }
    }
  } else {
    unrenderedSlots = Object.keys(children);
  }
  const template = unrenderedSlots.length > 0 ? unrenderedSlots.map(
    (key) => `<template data-astro-template${key !== "default" ? `="${key}"` : ""}>${children[key]}</template>`
  ).join("") : "";
  island.children = `${html ?? ""}${template}`;
  if (island.children) {
    island.props["await-children"] = "";
  }
  async function* renderAll() {
    if (slotInstructions) {
      yield* slotInstructions;
    }
    yield { type: "directive", hydration, result };
    yield markHTMLString(renderElement("astro-island", island, false));
  }
  return renderAll();
}
function sanitizeElementName(tag) {
  const unsafe = /[&<>'"\s]+/g;
  if (!unsafe.test(tag))
    return tag;
  return tag.trim().split(unsafe)[0].trim();
}
async function renderFragmentComponent(result, slots = {}) {
  const children = await renderSlot(result, slots == null ? void 0 : slots.default);
  if (children == null) {
    return children;
  }
  return markHTMLString(children);
}
async function renderHTMLComponent(result, Component, _props, slots = {}) {
  const { slotInstructions, children } = await renderSlots(result, slots);
  const html = Component.render({ slots: children });
  const hydrationHtml = slotInstructions ? slotInstructions.map((instr) => stringifyChunk(result, instr)).join("") : "";
  return markHTMLString(hydrationHtml + html);
}
function renderComponent(result, displayName, Component, props, slots = {}) {
  if (isPromise(Component)) {
    return Promise.resolve(Component).then((Unwrapped) => {
      return renderComponent(result, displayName, Unwrapped, props, slots);
    });
  }
  if (isFragmentComponent(Component)) {
    return renderFragmentComponent(result, slots);
  }
  if (isHTMLComponent(Component)) {
    return renderHTMLComponent(result, Component, props, slots);
  }
  if (isAstroComponentFactory(Component)) {
    return createAstroComponentInstance(result, displayName, Component, props, slots);
  }
  return renderFrameworkComponent(result, displayName, Component, props, slots);
}
function renderComponentToIterable(result, displayName, Component, props, slots = {}) {
  const renderResult = renderComponent(result, displayName, Component, props, slots);
  if (isAstroComponentInstance(renderResult)) {
    return renderResult.render();
  }
  return renderResult;
}

const uniqueElements = (item, index, all) => {
  const props = JSON.stringify(item.props);
  const children = item.children;
  return index === all.findIndex((i) => JSON.stringify(i.props) === props && i.children == children);
};
async function* renderExtraHead(result, base) {
  yield base;
  for (const part of result.extraHead) {
    yield* renderChild(part);
  }
}
function renderAllHeadContent(result) {
  const styles = Array.from(result.styles).filter(uniqueElements).map((style) => renderElement("style", style));
  result.styles.clear();
  const scripts = Array.from(result.scripts).filter(uniqueElements).map((script, i) => {
    return renderElement("script", script, false);
  });
  const links = Array.from(result.links).filter(uniqueElements).map((link) => renderElement("link", link, false));
  const baseHeadContent = markHTMLString(links.join("\n") + styles.join("\n") + scripts.join("\n"));
  if (result.extraHead.length > 0) {
    return renderExtraHead(result, baseHeadContent);
  } else {
    return baseHeadContent;
  }
}
function createRenderHead(result) {
  result._metadata.hasRenderedHead = true;
  return renderAllHeadContent.bind(null, result);
}
const renderHead = createRenderHead;
async function* maybeRenderHead(result) {
  if (result._metadata.hasRenderedHead) {
    return;
  }
  yield createRenderHead(result)();
}

typeof process === "object" && Object.prototype.toString.call(process) === "[object process]";

function spreadAttributes(values, _name, { class: scopedClassName } = {}) {
  let output = "";
  if (scopedClassName) {
    if (typeof values.class !== "undefined") {
      values.class += ` ${scopedClassName}`;
    } else if (typeof values["class:list"] !== "undefined") {
      values["class:list"] = [values["class:list"], scopedClassName];
    } else {
      values.class = scopedClassName;
    }
  }
  for (const [key, value] of Object.entries(values)) {
    output += addAttribute(value, key, true);
  }
  return markHTMLString(output);
}

const AstroJSX = "astro:jsx";
const Empty = Symbol("empty");
const toSlotName = (slotAttr) => slotAttr;
function isVNode(vnode) {
  return vnode && typeof vnode === "object" && vnode[AstroJSX];
}
function transformSlots(vnode) {
  if (typeof vnode.type === "string")
    return vnode;
  const slots = {};
  if (isVNode(vnode.props.children)) {
    const child = vnode.props.children;
    if (!isVNode(child))
      return;
    if (!("slot" in child.props))
      return;
    const name = toSlotName(child.props.slot);
    slots[name] = [child];
    slots[name]["$$slot"] = true;
    delete child.props.slot;
    delete vnode.props.children;
  }
  if (Array.isArray(vnode.props.children)) {
    vnode.props.children = vnode.props.children.map((child) => {
      if (!isVNode(child))
        return child;
      if (!("slot" in child.props))
        return child;
      const name = toSlotName(child.props.slot);
      if (Array.isArray(slots[name])) {
        slots[name].push(child);
      } else {
        slots[name] = [child];
        slots[name]["$$slot"] = true;
      }
      delete child.props.slot;
      return Empty;
    }).filter((v) => v !== Empty);
  }
  Object.assign(vnode.props, slots);
}
function markRawChildren(child) {
  if (typeof child === "string")
    return markHTMLString(child);
  if (Array.isArray(child))
    return child.map((c) => markRawChildren(c));
  return child;
}
function transformSetDirectives(vnode) {
  if (!("set:html" in vnode.props || "set:text" in vnode.props))
    return;
  if ("set:html" in vnode.props) {
    const children = markRawChildren(vnode.props["set:html"]);
    delete vnode.props["set:html"];
    Object.assign(vnode.props, { children });
    return;
  }
  if ("set:text" in vnode.props) {
    const children = vnode.props["set:text"];
    delete vnode.props["set:text"];
    Object.assign(vnode.props, { children });
    return;
  }
}
function createVNode(type, props) {
  const vnode = {
    [Renderer]: "astro:jsx",
    [AstroJSX]: true,
    type,
    props: props ?? {}
  };
  transformSetDirectives(vnode);
  transformSlots(vnode);
  return vnode;
}

const slotName = (str) => str.trim().replace(/[-_]([a-z])/g, (_, w) => w.toUpperCase());
async function check(Component, props, { default: children = null, ...slotted } = {}) {
  if (typeof Component !== "function")
    return false;
  const slots = {};
  for (const [key, value] of Object.entries(slotted)) {
    const name = slotName(key);
    slots[name] = value;
  }
  try {
    const result = await Component({ ...props, ...slots, children });
    return result[AstroJSX];
  } catch (e) {
  }
  return false;
}
async function renderToStaticMarkup(Component, props = {}, { default: children = null, ...slotted } = {}) {
  const slots = {};
  for (const [key, value] of Object.entries(slotted)) {
    const name = slotName(key);
    slots[name] = value;
  }
  const { result } = this;
  const html = await renderJSX(result, createVNode(Component, { ...props, ...slots, children }));
  return { html };
}
var server_default = {
  check,
  renderToStaticMarkup
};

const __vite_glob_0_0$2 = "<svg width=\"72\" height=\"96\" viewBox=\"0 0 72 96\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path d=\"M35.826 22.5192C35.826 15.0572 30.1011 9 22.9714 9C15.8417 9 10.1168 15.0572 10.1168 22.5192C10.1168 22.9271 10.1168 23.35 10.1923 23.8485C5.2378 26.4164 1.824 31.7184 1.824 37.8511C1.824 41.9144 3.31942 45.5699 5.72117 48.3795C2.98711 50.7812 1.25 54.3461 1.25 58.3339C1.25 64.3005 5.07164 69.2852 10.283 70.7655C10.2074 71.5057 10.1168 72.261 10.1168 73.0011C10.1168 80.4631 15.8417 86.5203 22.9714 86.5203C30.1011 86.5203 35.826 80.4631 35.826 73.0011V22.5192Z\" stroke=\"#AAB8CD\" stroke-width=\"2\"/>\r\n<path d=\"M35.6606 58.9229H29.7695\" stroke=\"#AAB8CD\" stroke-width=\"2\"/>\r\n<path d=\"M25.7961 62.5634C27.8051 62.5634 29.4516 60.932 29.4516 58.9079C29.4516 56.8838 27.8202 55.2524 25.7961 55.2524C23.772 55.2524 22.1406 56.8838 22.1406 58.9079C22.1406 60.932 23.772 62.5634 25.7961 62.5634Z\" stroke=\"#AAB8CD\" stroke-width=\"2\"/>\r\n<path d=\"M5.48047 48.395H11.3715\" stroke=\"#AAB8CD\" stroke-width=\"2\"/>\r\n<path d=\"M15.343 52.0351C17.352 52.0351 18.9985 50.4037 18.9985 48.3796C18.9985 46.3555 17.3671 44.7241 15.343 44.7241C13.3189 44.7241 11.6875 46.3555 11.6875 48.3796C11.6875 50.4037 13.3189 52.0351 15.343 52.0351V52.0351Z\" stroke=\"#AAB8CD\" stroke-width=\"2\"/>\r\n<path d=\"M35.6593 22.5195H27.291\" stroke=\"#AAB8CD\" stroke-width=\"2\"/>\r\n<path d=\"M23.3039 26.1596C25.3129 26.1596 26.9594 24.5282 26.9594 22.5041C26.9594 20.48 25.328 18.8486 23.3039 18.8486C21.2798 18.8486 19.6484 20.48 19.6484 22.5041C19.6484 24.5282 21.2798 26.1596 23.3039 26.1596Z\" stroke=\"#AAB8CD\" stroke-width=\"2\"/>\r\n<path d=\"M10.207 23.8486C10.207 23.8486 10.207 33.2139 20.4937 35.2984\" stroke=\"#AAB8CD\" stroke-width=\"2\"/>\r\n<path d=\"M10.2832 70.7806C10.2832 70.7806 18.3343 72.7745 23.8024 67.8804\" stroke=\"#AAB8CD\" stroke-width=\"2\"/>\r\n<path d=\"M35.8262 73.107C35.8262 80.569 41.5511 86.6263 48.6808 86.6263C55.8105 86.6263 61.5354 80.569 61.5354 73.107C61.5354 72.6992 61.5354 72.2762 61.4599 71.7778C66.4295 69.2099 69.8282 63.8928 69.8282 57.7601C69.8282 53.6967 68.3328 50.0413 65.931 47.2317C68.6651 44.8299 70.4022 41.2651 70.4022 37.2773C70.4022 31.3107 66.5805 26.326 61.3692 24.8456C61.4447 24.1055 61.5354 23.3502 61.5354 22.6101C61.5354 15.148 55.8105 9.09082 48.6808 9.09082C41.5511 9.09082 35.8262 15.148 35.8262 22.6101\" stroke=\"#AAB8CD\" stroke-width=\"2\"/>\r\n<path d=\"M35.9922 36.7031H41.8832\" stroke=\"#AAB8CD\" stroke-width=\"2\"/>\r\n<path d=\"M45.9465 40.3437C47.9555 40.3437 49.602 38.7123 49.602 36.6882C49.602 34.6641 47.9706 33.0327 45.9465 33.0327C43.9224 33.0327 42.291 34.6641 42.291 36.6882C42.291 38.7123 43.9224 40.3437 45.9465 40.3437V40.3437Z\" stroke=\"#AAB8CD\" stroke-width=\"2\"/>\r\n<path d=\"M66.1872 47.2314H60.3867\" stroke=\"#AAB8CD\" stroke-width=\"2\"/>\r\n<path d=\"M56.3098 50.872C58.3188 50.872 59.9653 49.2406 59.9653 47.2165C59.9653 45.1924 58.3339 43.561 56.3098 43.561C54.2857 43.561 52.6543 45.1924 52.6543 47.2165C52.6543 49.2406 54.2857 50.872 56.3098 50.872Z\" stroke=\"#AAB8CD\" stroke-width=\"2\"/>\r\n<path d=\"M35.9922 73.1069H44.3605\" stroke=\"#AAB8CD\" stroke-width=\"2\"/>\r\n<path d=\"M48.4387 76.747C50.4477 76.747 52.0942 75.1156 52.0942 73.0915C52.0942 71.0674 50.4628 69.436 48.4387 69.436C46.4146 69.436 44.7832 71.0674 44.7832 73.0915C44.7832 75.1156 46.4146 76.747 48.4387 76.747V76.747Z\" stroke=\"#AAB8CD\" stroke-width=\"2\"/>\r\n<path d=\"M61.5348 71.7774C61.5348 71.7774 61.5347 62.4122 51.248 60.3276\" stroke=\"#AAB8CD\" stroke-width=\"2\"/>\r\n<path d=\"M61.3688 24.8458C61.3688 24.8458 53.3177 22.8519 47.8496 27.746\" stroke=\"#AAB8CD\" stroke-width=\"2\"/>\r\n</svg>\r\n";

const __vite_glob_0_1$2 = "<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 384 512\"><!--! Font Awesome Pro 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license (Commercial License) Copyright 2022 Fonticons, Inc. --><path d=\"M192 384c-8.188 0-16.38-3.125-22.62-9.375l-160-160c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L192 306.8l137.4-137.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-160 160C208.4 380.9 200.2 384 192 384z\"/></svg>";

const __vite_glob_0_2$2 = "<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 384 512\">\r\n  <path\r\n    d=\"M374.6 310.6l-160 160C208.4 476.9 200.2 480 192 480s-16.38-3.125-22.62-9.375l-160-160c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 370.8V64c0-17.69 14.33-31.1 31.1-31.1S224 46.31 224 64v306.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0S387.1 298.1 374.6 310.6z\" />\r\n</svg>";

const __vite_glob_0_3$2 = "<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 448 512\">\r\n  <path\r\n    d=\"M447.1 256C447.1 273.7 433.7 288 416 288H109.3l105.4 105.4c12.5 12.5 12.5 32.75 0 45.25C208.4 444.9 200.2 448 192 448s-16.38-3.125-22.62-9.375l-160-160c-12.5-12.5-12.5-32.75 0-45.25l160-160c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25L109.3 224H416C433.7 224 447.1 238.3 447.1 256z\" />\r\n</svg>";

const __vite_glob_0_4$2 = "<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 448 512\">\r\n  <path\r\n    d=\"M438.6 278.6l-160 160C272.4 444.9 264.2 448 256 448s-16.38-3.125-22.62-9.375c-12.5-12.5-12.5-32.75 0-45.25L338.8 288H32C14.33 288 .0016 273.7 .0016 256S14.33 224 32 224h306.8l-105.4-105.4c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0l160 160C451.1 245.9 451.1 266.1 438.6 278.6z\" />\r\n</svg>";

const __vite_glob_0_5$2 = "<svg xmlns=\"http://www.w3.org/2000/svg\" fill=\"none\" viewBox=\"0 0 24 24\" stroke-width=\"1.5\" stroke=\"currentColor\">\n  <path stroke-linecap=\"round\" stroke-linejoin=\"round\" d=\"M13.5 6H5.25A2.25 2.25 0 003 8.25v10.5A2.25 2.25 0 005.25 21h10.5A2.25 2.25 0 0018 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25\" />\n</svg>\n\n";

const __vite_glob_0_6$2 = "<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 384 512\">\r\n  <path\r\n    d=\"M374.6 246.6C368.4 252.9 360.2 256 352 256s-16.38-3.125-22.62-9.375L224 141.3V448c0 17.69-14.33 31.1-31.1 31.1S160 465.7 160 448V141.3L54.63 246.6c-12.5 12.5-32.75 12.5-45.25 0s-12.5-32.75 0-45.25l160-160c12.5-12.5 32.75-12.5 45.25 0l160 160C387.1 213.9 387.1 234.1 374.6 246.6z\" />\r\n</svg>";

const __vite_glob_0_7$2 = "\r\n<svg width=\"30\" height=\"40\" viewBox=\"0 0 30 40\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path d=\"M29 10.5846V39H1V1H19.6299\" stroke=\"#D7DFEB\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M28.999 10.5846L19.6289 1V10.5846H28.999Z\" stroke=\"#D7DFEB\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M9.5807 19.2292C10.2972 20.1313 10.7382 21.3153 10.7382 22.612C10.7382 24.0779 10.187 25.3746 9.25 26.3331\" stroke=\"#D7DFEB\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M12.9199 15.9929C14.2006 17.8218 14.9934 20.0571 14.9934 22.5633C14.9934 25.4082 14.0176 27.9145 12.3711 29.8111\" stroke=\"#D7DFEB\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M16.2508 13.3093C18.3453 15.79 19.613 19.0601 19.613 22.612C19.613 26.5022 18.1248 29.9978 15.6445 32.5912\" stroke=\"#D7DFEB\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n</svg>";

const __vite_glob_0_8$2 = "\n<svg width=\"22\" height=\"25\" viewBox=\"0 0 22 25\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M7.85714 0.125H14.1429V24.125H7.85714V0.125ZM0 10.4107H6.28571V24.125H0V10.4107ZM22 3.55357V24.125H15.7143V3.55357H22Z\" fill=\"#D7DFEB\"/>\n</svg>\n";

const __vite_glob_0_9$2 = "<svg width=\"134\" height=\"96\" viewBox=\"0 0 134 96\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path d=\"M16.2195 77.7701H12.9609V28.0771H16.2195\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M11.9617 73.6972H8.70312V31.3359H11.9617\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M7.70191 70.4387H4.44336V35.4092H7.70191\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M3.44392 66.365H1V39.4819H3.44392\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M61.0297 38.4916V82.047H17.5371V23.8003H46.4751\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M61.0292 38.4916L46.4746 23.8003V38.4916H61.0292Z\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M31.4336 52.5894C32.5466 53.9721 33.2315 55.7869 33.2315 57.7745C33.2315 60.0214 32.3754 62.0091 30.9199 63.4782\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M36.6221 47.6284C38.6113 50.4317 39.8427 53.858 39.8427 57.6996C39.8427 62.0603 38.3271 65.9019 35.7695 68.809\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M41.7933 43.5151C45.0467 47.3176 47.0159 52.3299 47.0159 57.7743C47.0159 63.7373 44.7042 69.0953 40.8516 73.0706\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M132.672 38.4916V82.047H89.1797V23.8003H118.118\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M132.672 38.4916L118.117 23.8003V38.4916H132.672Z\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<line x1=\"96.6973\" y1=\"44.1846\" x2=\"124.839\" y2=\"44.1846\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-linecap=\"round\"/>\r\n<line x1=\"96.6973\" y1=\"49.0723\" x2=\"124.839\" y2=\"49.0723\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-linecap=\"round\"/>\r\n<line x1=\"96.6973\" y1=\"53.9604\" x2=\"124.839\" y2=\"53.9604\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-linecap=\"round\"/>\r\n<line x1=\"96.6973\" y1=\"58.8481\" x2=\"124.839\" y2=\"58.8481\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-linecap=\"round\"/>\r\n<line x1=\"96.6973\" y1=\"63.7358\" x2=\"124.839\" y2=\"63.7358\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-linecap=\"round\"/>\r\n<line x1=\"96.6973\" y1=\"68.6235\" x2=\"124.839\" y2=\"68.6235\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-linecap=\"round\"/>\r\n<line x1=\"96.6973\" y1=\"73.5117\" x2=\"124.839\" y2=\"73.5117\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-linecap=\"round\"/>\r\n<path d=\"M69.1159 38.4916L82.7161 20.7936H76.8172L79.4389 13H70.7544L66.4941 25.827H73.0485L69.1159 38.4916Z\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M76.4874 50.8871L81.2151 55.6147L76.4874 60.3424\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-linecap=\"round\"/>\r\n<path d=\"M80.8323 55.6147H69.7031\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-linecap=\"round\"/>\r\n</svg>\r\n";

const __vite_glob_0_10$2 = "<svg width=\"20\" height=\"24\" viewBox=\"0 0 20 24\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path d=\"M20 15.75V2.25C20 1.03125 19.0179 0 17.8571 0H4.28571C1.875 0 0 2.01562 0 4.5V19.5C0 22.0312 1.875 24 4.28571 24H18.5714C19.3304 24 20 23.3438 20 22.5469C20 22.2107 19.8672 21.8944 19.6633 21.6371C19.4607 21.3813 19.2857 21.0869 19.2857 20.7606V17.8543C19.2857 17.6156 19.3759 17.3881 19.5133 17.1929C19.7995 16.7861 20 16.3132 20 15.75ZM6.38393 6H14.9554C15.3571 6 15.7143 6.375 15.7143 6.75C15.7143 7.17188 15.3571 7.5 15 7.5H6.38393C6.02679 7.5 5.71429 7.17188 5.71429 6.75C5.71429 6.375 6.02679 6 6.38393 6ZM6.38393 9H14.9554C15.3571 9 15.7143 9.375 15.7143 9.75C15.7143 10.1719 15.3571 10.5 15 10.5H6.38393C6.02679 10.5 5.71429 10.1719 5.71429 9.75C5.71429 9.375 6.02679 9 6.38393 9ZM17.1429 20C17.1429 20.5523 16.6951 21 16.1429 21H4.28571C3.48214 21 2.85714 20.3438 2.85714 19.5C2.85714 18.7031 3.48214 18 4.28571 18H16.1429C16.6951 18 17.1429 18.4477 17.1429 19V20Z\" fill=\"#D7DFEB\"/>\r\n</svg>\r\n";

const __vite_glob_0_11$2 = "<svg width=\"24\" height=\"16\" viewBox=\"0 0 24 16\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path d=\"M8.56576 6.5975L12.0161 1.11718L2.60257 0.00678773C2.34004 -0.0290313 2.11502 0.0784259 2.0025 0.329159L0.0522817 3.9827C-0.0977355 4.34089 0.089786 4.7349 0.427325 4.80654L7.81567 6.84823C8.1157 6.91987 8.41574 6.81241 8.56576 6.5975ZM23.9425 3.9827L21.9923 0.29334C21.8798 0.0784259 21.6548 -0.0290313 21.3922 0.00678773L12.0161 1.11718L15.429 6.5975C15.5791 6.81241 15.8791 6.91987 16.1416 6.84823L23.5675 4.80654C23.9425 4.69909 24.0925 4.30508 23.9425 3.9827ZM15.9541 8.06608C15.3165 8.06608 14.7165 7.74371 14.4164 7.24224L12.0161 3.4096L9.57837 7.20642C9.27834 7.74371 8.67827 8.06608 8.0407 8.06608C7.85317 8.06608 7.70316 8.03026 7.55314 7.99444L2.41505 6.5975V12.9017C2.41505 13.4389 2.75259 13.8688 3.31515 14.012L11.4161 15.9463C11.7911 16.0179 12.2037 16.0179 12.5787 15.9463L20.6796 14.012C21.2047 13.9046 21.6172 13.4389 21.6172 12.9375V6.5975L16.4792 7.99444C16.2916 8.03026 16.1416 8.06608 15.9541 8.06608Z\" fill=\"#D7DFEB\"/>\r\n</svg>\r\n";

const __vite_glob_0_12$2 = "<svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M6.75 17.25C6.75 17.6719 7.07813 18 7.5 18C7.875 18 8.25 17.6719 8.25 17.25C8.25 16.875 7.875 16.5 7.5 16.5C7.07813 16.5 6.75 16.875 6.75 17.25ZM17.25 6.75C17.25 6.375 16.875 6 16.5 6C16.0781 6 15.75 6.375 15.75 6.75C15.75 7.17188 16.0781 7.5 16.5 7.5C16.875 7.5 17.25 7.17188 17.25 6.75ZM8.625 0C7.26563 0 6.1875 1.03125 6.04688 2.34375C4.73438 2.67188 3.75 3.84375 3.75 5.25C3.75 5.53125 3.75 5.8125 3.84375 6.09375C2.48438 6.375 1.5 7.54688 1.5 9C1.5 9.70313 1.73438 10.3594 2.10938 10.875C0.890625 11.4844 0 12.75 0 14.25C0 15.8438 0.984375 17.2031 2.4375 17.7656C2.29688 18.0469 2.25 18.375 2.25 18.75C2.25 20.3906 3.60938 21.75 5.25 21.75C5.48438 21.75 5.76563 21.7031 6.04688 21.6563C6.14063 22.9688 7.26563 24 8.625 24C10.0781 24 11.25 22.8281 11.25 21.375V12.75H9C8.57813 12.75 8.25 13.125 8.25 13.5V15.5625C8.90625 15.8438 9.375 16.5 9.375 17.25C9.375 18.3281 8.53125 19.125 7.5 19.125C6.42188 19.125 5.625 18.3281 5.625 17.25C5.625 16.5 6.04688 15.8438 6.75 15.5625V13.5C6.75 12.2813 7.73438 11.25 9 11.25H11.25V7.5H9.1875C8.90625 8.20313 8.25 8.625 7.5 8.625C6.42188 8.625 5.625 7.82813 5.625 6.75C5.625 5.71875 6.42188 4.875 7.5 4.875C8.25 4.875 8.90625 5.34375 9.1875 6H11.25V2.625C11.25 1.17188 10.0781 0 8.625 0ZM6.75 6.75C6.75 7.17188 7.07813 7.5 7.5 7.5C7.875 7.5 8.25 7.17188 8.25 6.75C8.25 6.375 7.875 6 7.5 6C7.07813 6 6.75 6.375 6.75 6.75ZM24 14.25C24 12.75 23.1094 11.4844 21.8438 10.875C22.2656 10.3594 22.5 9.70313 22.5 9C22.5 7.54688 21.4688 6.375 20.1094 6.09375C20.2031 5.8125 20.2031 5.53125 20.2031 5.25C20.2031 3.84375 19.2188 2.67188 17.9063 2.34375C17.8125 1.03125 16.7344 0 15.375 0C13.9219 0 12.75 1.17188 12.75 2.625V11.25H15C15.375 11.25 15.75 10.9219 15.75 10.5V8.48438C15.0469 8.20313 14.625 7.54688 14.625 6.75C14.625 5.71875 15.4219 4.875 16.5 4.875C17.5313 4.875 18.375 5.71875 18.375 6.75C18.375 7.54688 17.9063 8.20313 17.25 8.48438V10.5C17.25 11.7656 16.2188 12.75 15 12.75H12.75V16.5H14.7656C15.0469 15.8438 15.7031 15.375 16.5 15.375C17.5313 15.375 18.375 16.2188 18.375 17.25C18.375 18.3281 17.5313 19.125 16.5 19.125C15.7031 19.125 15.0469 18.7031 14.7656 18H12.75V21.375C12.75 22.8281 13.9219 24 15.375 24C16.7344 24 17.8594 22.9688 17.9531 21.6563C18.2344 21.7031 18.4219 21.75 18.75 21.75C20.3906 21.75 21.75 20.3906 21.75 18.75C21.75 18.375 21.6563 18.0469 21.5625 17.7656C22.9219 17.2031 24 15.8438 24 14.25ZM17.25 17.25C17.25 16.875 16.875 16.5 16.5 16.5C16.0781 16.5 15.75 16.875 15.75 17.25C15.75 17.6719 16.0781 18 16.5 18C16.875 18 17.25 17.6719 17.25 17.25Z\" fill=\"#D7DFEB\"/>\n</svg>";

const __vite_glob_0_13$2 = "<svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M12 8.25V8.29688L13.9688 6.32813L13.7813 5.53125C13.2188 5.39063 12.6094 5.25 12 5.25C8.25 5.25 5.25 8.29688 5.25 12C5.25 15.75 8.25 18.75 12 18.75C15.7031 18.75 18.75 15.75 18.75 12C18.75 11.3906 18.6094 10.7813 18.4688 10.2188L17.6719 10.0313L15.7031 12H15.75C15.75 14.1094 14.0625 15.75 12 15.75C9.89063 15.75 8.25 14.1094 8.25 12C8.25 9.9375 9.89063 8.25 12 8.25ZM23.3906 8.29688L21.7969 9.84375C21.5625 10.125 21.1875 10.3125 20.8125 10.4531C20.9063 10.9688 21 11.4844 21 12C21 16.9688 16.9688 21 12 21C6.98438 21 3 17.0156 3 12C3 7.03125 6.98438 3 12 3C12.5156 3 13.0313 3.09375 13.5469 3.1875C13.6875 2.8125 13.875 2.48438 14.1563 2.20313L15.7031 0.609375C14.5313 0.234375 13.2656 0 12 0C5.34375 0 0 5.39063 0 12C0 18.6563 5.34375 24 12 24C18.6094 24 24 18.6563 24 12C24 10.7344 23.7656 9.46875 23.3906 8.29688ZM15 3.9375L15.5625 6.84375L11.5781 10.8281C11.1094 11.2969 11.1094 12 11.5781 12.4219C11.7656 12.6563 12.0469 12.75 12.375 12.75C12.6563 12.75 12.9375 12.6563 13.125 12.4219L17.1563 8.4375L20.0625 9C20.1094 9 20.1563 9.04688 20.25 9.04688C20.4375 9.04688 20.625 8.95313 20.7656 8.8125L23.7656 5.8125C23.9531 5.625 24.0469 5.34375 23.9531 5.0625C23.8594 4.78125 23.625 4.59375 23.3906 4.54688L20.25 3.79688L19.4531 0.609375C19.4063 0.375 19.2188 0.140625 18.9375 0.046875C18.6563 -0.046875 18.375 0.046875 18.1875 0.234375L15.1875 3.23438C15 3.42188 14.9531 3.65625 15 3.9375Z\" fill=\"#D7DFEB\"/>\n</svg>\n";

const __vite_glob_0_14$2 = "<svg width=\"24\" height=\"22\" viewBox=\"0 0 24 22\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M23.4491 3.42082C24.0587 2.905 24.1994 1.96715 23.6836 1.31065C23.1678 0.701043 22.183 0.560365 21.5734 1.07619L14.9615 6.37507L9.89711 2.57675C9.3344 2.15472 8.58411 2.15472 8.06829 2.57675L0.565452 8.57903C-0.0910466 9.09485 -0.184832 10.0796 0.330988 10.6892C0.846808 11.3457 1.78466 11.4395 2.44116 10.9237L9.00615 5.67168L14.0706 9.46999C14.6333 9.89202 15.3836 9.89202 15.9463 9.4231L23.4491 3.42082ZM7.50558 11.2519V20.2553C7.50558 21.0994 8.16208 21.7559 9.00615 21.7559C9.80332 21.7559 10.5067 21.0994 10.5067 20.2553V11.2519C10.5067 10.4547 9.80332 9.75135 9.00615 9.75135C8.16208 9.75135 7.50558 10.4547 7.50558 11.2519ZM1.50331 15.7536V20.2553C1.50331 21.0994 2.15981 21.7559 3.00388 21.7559C3.80105 21.7559 4.50444 21.0994 4.50444 20.2553V15.7536C4.50444 14.9564 3.80105 14.2531 3.00388 14.2531C2.15981 14.2531 1.50331 14.9564 1.50331 15.7536ZM15.0084 12.7525C14.1643 12.7525 13.5079 13.4559 13.5079 14.2531V20.2553C13.5079 21.0994 14.1643 21.7559 15.0084 21.7559C15.8056 21.7559 16.509 21.0994 16.509 20.2553V14.2531C16.509 13.4559 15.8056 12.7525 15.0084 12.7525ZM19.5101 11.2519V20.2553C19.5101 21.0994 20.1666 21.7559 21.0107 21.7559C21.8079 21.7559 22.5113 21.0994 22.5113 20.2553V11.2519C22.5113 10.4547 21.8079 9.75135 21.0107 9.75135C20.1666 9.75135 19.5101 10.4547 19.5101 11.2519Z\" fill=\"#D7DFEB\"/>\n</svg>";

const __vite_glob_0_15$1 = "<svg width=\"84\" height=\"96\" viewBox=\"0 0 84 96\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<mask id=\"path-1-inside-1_698_14930\" fill=\"white\">\r\n<path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M70.1127 16L83.2676 29.1549L45.9581 66.4643L46.042 66.5482L32.8871 79.7031L32.8033 79.6192L32.7195 79.703L19.5646 66.5481L19.6484 66.4643L0 46.8159L13.1549 33.6611L32.8033 53.3095L70.1127 16Z\"/>\r\n</mask>\r\n<path d=\"M83.2676 29.1549L84.6818 30.5691L86.096 29.1549L84.6818 27.7406L83.2676 29.1549ZM70.1127 16L71.5269 14.5858L70.1127 13.1716L68.6985 14.5858L70.1127 16ZM45.9581 66.4643L44.5439 65.0501L43.1297 66.4643L44.5439 67.8785L45.9581 66.4643ZM46.042 66.5482L47.4562 67.9624L48.8704 66.5482L47.4562 65.134L46.042 66.5482ZM32.8871 79.7031L31.473 81.1173L32.8872 82.5315L34.3014 81.1173L32.8871 79.7031ZM32.8033 79.6192L34.2174 78.2049L32.8032 76.7908L31.389 78.205L32.8033 79.6192ZM32.7195 79.703L31.3052 81.1172L32.7195 82.5314L34.1337 81.1172L32.7195 79.703ZM19.5646 66.5481L18.1504 65.1339L16.7361 66.5481L18.1504 67.9623L19.5646 66.5481ZM19.6484 66.4643L21.0626 67.8786L22.4768 66.4643L21.0626 65.0501L19.6484 66.4643ZM0 46.8159L-1.41421 45.4017L-2.82843 46.8159L-1.41421 48.2301L0 46.8159ZM13.1549 33.6611L14.5691 32.2469L13.1549 30.8326L11.7406 32.2469L13.1549 33.6611ZM32.8033 53.3095L31.389 54.7237L32.8033 56.1379L34.2175 54.7237L32.8033 53.3095ZM84.6818 27.7406L71.5269 14.5858L68.6985 17.4142L81.8534 30.5691L84.6818 27.7406ZM47.3723 67.8785L84.6818 30.5691L81.8534 27.7406L44.5439 65.0501L47.3723 67.8785ZM44.5439 67.8785L44.6278 67.9624L47.4562 65.134L47.3723 65.0501L44.5439 67.8785ZM44.6278 65.134L31.4729 78.2889L34.3014 81.1173L47.4562 67.9624L44.6278 65.134ZM34.3013 78.2888L34.2174 78.2049L31.3891 81.0334L31.473 81.1173L34.3013 78.2888ZM34.1337 81.1172L34.2175 81.0334L31.389 78.205L31.3052 78.2888L34.1337 81.1172ZM18.1504 67.9623L31.3052 81.1172L34.1337 78.2888L20.9788 65.1339L18.1504 67.9623ZM18.2342 65.0501L18.1504 65.1339L20.9788 67.9624L21.0626 67.8786L18.2342 65.0501ZM21.0626 65.0501L1.41421 45.4017L-1.41421 48.2301L18.2342 67.8785L21.0626 65.0501ZM1.41421 48.2301L14.5691 35.0753L11.7406 32.2469L-1.41421 45.4017L1.41421 48.2301ZM11.7406 35.0753L31.389 54.7237L34.2175 51.8952L14.5691 32.2469L11.7406 35.0753ZM68.6985 14.5858L31.389 51.8952L34.2175 54.7237L71.5269 17.4142L68.6985 14.5858Z\" fill=\"#AAB8CD\" mask=\"url(#path-1-inside-1_698_14930)\"/>\r\n</svg>\r\n";

const __vite_glob_0_16$1 = "<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\r\n  <path stroke-linecap=\"round\" stroke-linejoin=\"round\" d=\"M15 19l-7-7 7-7\" />\r\n</svg>";

const __vite_glob_0_17$1 = "<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\r\n  <path stroke-linecap=\"round\" stroke-linejoin=\"round\" d=\"M9 5l7 7-7 7\" />\r\n</svg>";

const __vite_glob_0_18$1 = "<svg width=\"182\" height=\"96\" viewBox=\"0 0 182 96\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path d=\"M84.4023 47.6689C83.8501 47.6689 83.4023 48.1167 83.4023 48.6689C83.4023 49.2212 83.8501 49.6689 84.4023 49.6689V47.6689ZM98.8534 49.3761C99.2439 48.9855 99.2439 48.3524 98.8534 47.9618L92.4894 41.5979C92.0989 41.2074 91.4657 41.2074 91.0752 41.5979C90.6847 41.9884 90.6847 42.6216 91.0752 43.0121L96.7321 48.6689L91.0752 54.3258C90.6847 54.7163 90.6847 55.3495 91.0752 55.74C91.4657 56.1305 92.0989 56.1305 92.4894 55.74L98.8534 49.3761ZM84.4023 49.6689H98.1463V47.6689H84.4023V49.6689Z\" fill=\"#AAB8CD\"/>\r\n<g opacity=\"0.7\">\r\n<path d=\"M35.248 18.5809V17.5955V14.5234\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M42.0176 24.9568V22.0586\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M48.7852 37.187V35.6799V34.5786\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M55.4629 31.7964V28.2026\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M62.2305 37.3609V36.0857V34.9844\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M68.998 35.3325V33.5356\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M75.7656 40.839V39.5059\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M1.5 34.9268V33.1299\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M8.17773 37.3609V36.0277V34.9844\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M14.9453 30.7535V30.9274V26.9858\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M21.7129 36.6655V35.2164V34.0571\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M28.4805 25.3044V22.4062\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n</g>\r\n<g opacity=\"0.5\">\r\n<path d=\"M35.248 72.8755V73.8029V76.875\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M42.0176 67.1948V70.093\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M48.7852 77.6865V79.1356V80.2949\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M55.4629 81.4541V84.9899\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M62.2305 75.3101V76.5853V77.6866\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M68.998 66.6733V68.4702\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M75.7656 68.644V69.9772\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M1.5 66.8472V68.5861\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M8.17773 77.5127V78.7879V79.8892\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M14.9453 84.0626V83.8887V87.8882\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M21.7129 78.1504V79.5995V80.7587\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M28.4805 66.9053V69.8034\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n</g>\r\n<path d=\"M35.248 61.0785V52.1521V23.75\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M42.0176 56.0936V29.3145\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M48.7852 65.7154V52.1519V41.6025\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M55.4629 69.1352V36.2119\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M62.2305 64.2086V52.1522V42.1245\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M68.998 56.0935V39.6318\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M75.7656 52.1523V45.3706\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M1.5 56.0935V39.6318\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M8.17773 64.2086V52.1522V42.1245\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M14.9453 71.3958V73.1347V36.2119\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M21.7129 65.7154V52.1519V41.6025\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M28.4805 56.0936V29.3145\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<g opacity=\"0.5\">\r\n<path d=\"M35.25 11.7786V10.7933V7.72119\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M42.0176 18.1545V15.2563\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M48.7852 30.3847V28.8777V27.7764\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M55.4629 24.9941V21.4004\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M62.2305 30.5591V29.2839V28.1826\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M68.998 28.5303V26.7334\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M75.7656 34.0368V32.7036\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M1.5 28.1245V26.3276\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M8.17773 30.5591V29.226V28.1826\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M14.9453 23.9512V24.1251V20.1836\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M21.7129 29.8632V28.4142V27.2549\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M28.4805 18.5022V15.604\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n</g>\r\n<g opacity=\"0.7\">\r\n<path d=\"M35.248 64.9385V65.8659V68.938\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M42.0176 59.2578V62.156\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M48.7852 69.7495V71.1986V72.3579\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M55.4629 73.5171V77.0529\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M62.2305 67.373V68.6482V69.7496\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M68.998 58.7363V60.5332\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M75.7656 60.707V62.0402\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M1.5 58.9102V60.6491\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M8.17773 69.5757V70.8509V71.9522\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M14.9453 76.1256V75.9517V79.9511\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M21.7129 70.2129V71.662V72.8212\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M28.4805 58.9683V61.8664\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n</g>\r\n<path d=\"M139.967 61.0785V52.1521V23.75\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M146.734 56.0936V29.3145\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M153.504 65.7154V52.1519V41.6025\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M160.182 69.1352V36.2119\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M166.949 64.2086V52.1522V42.1245\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M173.717 56.0935V39.6318\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M180.484 52.1523V45.3706\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M106.219 56.0935V39.6318\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M112.896 64.2086V52.1522V42.1245\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M119.664 71.3958V73.1347V36.2119\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M126.432 65.7154V52.1519V41.6025\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M133.199 56.0936V29.3145\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n</svg>\r\n";

const __vite_glob_0_19$1 = "<svg width=\"166\" height=\"96\" viewBox=\"0 0 166 96\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path d=\"M68.3225 85.3888L86.01 58.0527L103.697 85.3888L68.3225 85.3888Z\" stroke=\"#AAB8CD\" stroke-width=\"2\"/>\r\n<path d=\"M6.33984 56.2119H164.137\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-linecap=\"round\"/>\r\n<path d=\"M59.6523 35.4042C57.7034 37.3531 57.7102 40.5477 59.7022 42.5398C61.6943 44.5318 64.889 44.5386 66.8379 42.5897C68.7868 40.6408 68.78 37.4461 66.7879 35.4541C64.7959 33.4621 61.6012 33.4553 59.6523 35.4042Z\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\"/>\r\n<path d=\"M55.9388 31.6894C51.962 35.6663 51.988 42.1682 56.0319 46.2121C60.0757 50.2559 66.5777 50.282 70.5546 46.3051C74.5314 42.3283 74.5053 35.8263 70.4615 31.7825C66.4177 27.7386 59.9157 27.7125 55.9388 31.6894Z\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\"/>\r\n<path d=\"M51.8101 27.5609C45.58 33.7911 45.6275 43.9679 51.9511 50.2915C58.2747 56.6151 68.4515 56.6626 74.6816 50.4325C80.9118 44.2023 80.8643 34.0255 74.5406 27.7019C68.217 21.3783 58.0403 21.3308 51.8101 27.5609Z\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\"/>\r\n<path d=\"M49.2147 25.0339L49.203 21.3187L47.0685 19.1843L47.1141 22.9333\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\"/>\r\n<path d=\"M63.3613 39.1802L49.1747 24.9935L45.4784 25.0006L43.3654 22.8877L47.0953 22.9142\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\"/>\r\n<path d=\"M138.84 41.9148C138.84 44.0176 140.819 45.8201 143.222 45.8201C145.624 45.8201 147.603 44.1678 147.603 41.9148C147.603 39.8119 146.048 38.0094 143.222 38.0094C140.395 38.0094 138.84 36.3572 138.84 34.1041C138.84 32.0012 140.819 30.1987 143.222 30.1987C145.624 30.1987 147.603 31.851 147.603 34.1041\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M143.221 30.0486V27.4951\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M143.221 48.6741V46.1206\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M143.221 54.9826C152.589 54.9826 160.183 47.3834 160.183 38.0093C160.183 28.6353 152.589 21.0361 143.221 21.0361C133.854 21.0361 126.26 28.6353 126.26 38.0093C126.26 47.3834 133.854 54.9826 143.221 54.9826Z\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M40.9851 39.3342C40.9851 48.0248 33.945 55.0688 25.2621 55.0688C16.5792 55.0688 9.53906 48.0248 9.53906 39.3342C9.53906 30.6436 16.5792 23.5996 25.2621 23.5996C33.945 23.5996 40.9851 30.6436 40.9851 39.3342Z\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M25.2402 22.5004L25.2402 17.2778\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M25.459 26.4175L25.459 39.4738L30.6779 34.6865\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M20.459 16.8979H30.1175\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M18.7727 36.0757H4.3125\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M15.9569 41.1567H1\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M18.7727 46.2383H4.3125\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n</svg>\r\n";

const __vite_glob_0_20$1 = "<svg width=\"24\" height=\"17\" viewBox=\"0 0 24 17\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path d=\"M4.8 3.42857C4.8 2.87629 5.24772 2.42857 5.8 2.42857H18.2C18.7523 2.42857 19.2 2.87629 19.2 3.42857V11.1429C19.2 11.6951 19.6477 12.1429 20.2 12.1429H20.6C21.1523 12.1429 21.6 11.6951 21.6 11.1429V1.82143C21.6 0.834821 20.775 0 19.8 0H4.2C3.1875 0 2.4 0.834821 2.4 1.82143V11.1429C2.4 11.6951 2.84772 12.1429 3.4 12.1429H3.8C4.35229 12.1429 4.8 11.6951 4.8 11.1429V3.42857ZM23.4 13.3571H0.6C0.2625 13.3571 0 13.6607 0 13.9643V14.5714C0 15.9375 1.05 17 2.4 17H21.6C22.9125 17 24 15.9375 24 14.5714V13.9643C24 13.6607 23.7 13.3571 23.4 13.3571ZM13.6875 9.67634C13.8375 9.82813 14.025 9.86607 14.25 9.86607C14.4375 9.86607 14.625 9.82813 14.775 9.67634L16.575 7.85491C16.8375 7.55134 16.8375 7.05804 16.575 6.75446L14.775 4.93304C14.475 4.66741 13.9875 4.66741 13.6875 4.93304C13.425 5.23661 13.425 5.72991 13.6875 6.03348L14.2468 6.58284C14.6417 6.97067 14.6465 7.60551 14.2574 7.9992L13.6875 8.57589C13.425 8.87946 13.425 9.37277 13.6875 9.67634ZM10.275 4.93304C9.975 4.66741 9.4875 4.66741 9.1875 4.93304L7.3875 6.75446C7.125 7.05804 7.125 7.55134 7.3875 7.85491L9.1875 9.67634C9.3375 9.82813 9.525 9.86607 9.75 9.86607C9.9375 9.86607 10.125 9.82813 10.275 9.67634C10.5375 9.37277 10.5375 8.87946 10.275 8.57589L9.70509 7.99919C9.31603 7.60551 9.32077 6.97067 9.71566 6.58284L10.275 6.03348C10.5375 5.72991 10.5375 5.23661 10.275 4.93304Z\" fill=\"#D7DFEB\"/>\r\n</svg>\r\n";

const __vite_glob_0_21$1 = "<svg width=\"24\" height=\"20\" viewBox=\"0 0 24 20\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path d=\"M15.6 1.25C15.6 0.859375 15.3 0 14.4 0C13.875 0 13.3875 0.390625 13.2375 0.9375L8.4375 18.4375C8.4 18.5547 8.3625 18.6719 8.3625 18.75C8.3625 19.1797 8.7 20 9.6 20C10.0875 20 10.575 19.6484 10.725 19.1016L15.525 1.60156C15.5625 1.48437 15.6 1.36719 15.6 1.25ZM6.6 5.625C6.6 4.88281 6.0375 4.375 5.4 4.375C5.0625 4.375 4.7625 4.49219 4.5375 4.72656L0.3375 9.10156C0.1125 9.375 0 9.6875 0 10C0 10.2734 0.1125 10.6641 0.3375 10.8984L4.5375 15.2734C4.7625 15.5078 5.0625 15.625 5.4 15.625C6.0375 15.625 6.6 15.0781 6.6 14.375C6.6 14.0234 6.45 13.7109 6.225 13.4766L2.8875 10L6.225 6.48437C6.45 6.28906 6.6 5.97656 6.6 5.625ZM24 10C24 9.64844 23.85 9.33594 23.625 9.10156L19.425 4.72656C19.2 4.53125 18.9 4.375 18.6 4.375C17.925 4.375 17.4 4.88281 17.4 5.625C17.4 5.9375 17.5125 6.25 17.7375 6.48437L21.075 10L17.7375 13.4766C17.5125 13.75 17.4 14.0625 17.4 14.375C17.4 15.0781 17.925 15.625 18.6 15.625C18.9 15.625 19.2 15.4687 19.425 15.2344L23.625 10.8594C23.85 10.6641 24 10.3516 24 10Z\" fill=\"#D7DFEB\"/>\r\n</svg>\r\n";

const __vite_glob_0_22$1 = "<svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path d=\"M21.0087 2.4155C22.9178 4.02886 23.8952 6.38055 23.9957 9.36118C24.0056 9.72215 23.9967 10.1475 23.9865 10.6358C23.9778 11.0476 23.9683 11.5041 23.9683 12.0046V14.6115C23.7308 20.8705 20.1684 24 13.281 24H3.23322V18.8591H13.1349C13.7082 18.9069 14.2852 18.8404 14.8325 18.6636C15.3798 18.4868 15.8864 18.2031 16.3228 17.8291C17.0989 16.9134 17.4944 15.736 17.4281 14.5385C17.5011 13.8731 17.5011 13.0254 17.5011 11.9681C17.5011 10.9107 17.4737 10.0722 17.4281 9.43411C17.4549 8.84174 17.3638 8.24994 17.16 7.69294C16.9562 7.13595 16.6437 6.62482 16.2406 6.18914C15.3264 5.42617 14.1496 5.05 12.9613 5.1409H6.47628V12.0049H0V0.000386006L13.1349 0C16.5238 0 19.0997 0.802127 21.0087 2.4155Z\" fill=\"#D7DFEB\"/>\r\n</svg>\r\n";

const __vite_glob_0_23$1 = "<svg role=\"img\" viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\"><title>Dribbble</title><path d=\"M12 24C5.385 24 0 18.615 0 12S5.385 0 12 0s12 5.385 12 12-5.385 12-12 12zm10.12-10.358c-.35-.11-3.17-.953-6.384-.438 1.34 3.684 1.887 6.684 1.992 7.308 2.3-1.555 3.936-4.02 4.395-6.87zm-6.115 7.808c-.153-.9-.75-4.032-2.19-7.77l-.066.02c-5.79 2.015-7.86 6.025-8.04 6.4 1.73 1.358 3.92 2.166 6.29 2.166 1.42 0 2.77-.29 4-.814zm-11.62-2.58c.232-.4 3.045-5.055 8.332-6.765.135-.045.27-.084.405-.12-.26-.585-.54-1.167-.832-1.74C7.17 11.775 2.206 11.71 1.756 11.7l-.004.312c0 2.633.998 5.037 2.634 6.855zm-2.42-8.955c.46.008 4.683.026 9.477-1.248-1.698-3.018-3.53-5.558-3.8-5.928-2.868 1.35-5.01 3.99-5.676 7.17zM9.6 2.052c.282.38 2.145 2.914 3.822 6 3.645-1.365 5.19-3.44 5.373-3.702-1.81-1.61-4.19-2.586-6.795-2.586-.825 0-1.63.1-2.4.285zm10.335 3.483c-.218.29-1.935 2.493-5.724 4.04.24.49.47.985.68 1.486.08.18.15.36.22.53 3.41-.43 6.8.26 7.14.33-.02-2.42-.88-4.64-2.31-6.38z\"/></svg>";

const __vite_glob_0_24$1 = "<svg width=\"101\" height=\"96\" viewBox=\"0 0 101 96\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path d=\"M50.1387 49.8038C71.5668 49.8038 88.9376 41.4846 88.9376 31.2222C88.9376 20.9599 71.5668 12.6406 50.1387 12.6406C28.7107 12.6406 11.3398 20.9599 11.3398 31.2222C11.3398 41.4846 28.7107 49.8038 50.1387 49.8038Z\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\"/>\r\n<path d=\"M11.3418 31.2227V54.8502C11.3418 65.1122 28.7115 73.4318 50.1407 73.4318C71.5698 73.4318 88.9396 65.1122 88.9396 54.8502V31.2227\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\"/>\r\n<path d=\"M88.8345 44.1914C95.3468 48.1823 99.2286 53.2031 99.2286 58.6593C99.2286 71.6452 77.2501 82.1695 50.1411 82.1695C23.0321 82.1695 1.05078 71.6452 1.05078 58.6593C1.05078 53.2617 4.84937 48.2856 11.2368 44.317\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\"/>\r\n<path d=\"M32.8569 34.4797C31.6721 35.2026 30.2431 35.5654 28.5811 35.5654C27.4101 35.5654 26.4251 35.4007 25.6288 35.0686C24.8352 34.7393 24.2081 34.2844 23.7447 33.7011C23.2869 33.1234 22.9955 32.4396 22.8762 31.6526C22.7569 30.8767 22.7763 30.0395 22.929 29.1464C23.1176 28.2645 23.445 27.4691 23.914 26.7546C24.3774 26.0485 24.9434 25.4429 25.6121 24.935C26.2753 24.4298 27.0161 24.0419 27.8347 23.7656C28.6504 23.4893 29.5106 23.3525 30.4207 23.3525C31.6055 23.3525 32.56 23.5479 33.287 23.9414C34.0195 24.3377 34.5855 24.8457 34.9879 25.4652C35.393 26.0932 35.6455 26.7853 35.7454 27.5416C35.8453 28.3091 35.8369 29.0487 35.7204 29.7576H25.2375C25.1182 30.2767 25.096 30.7707 25.1681 31.2423C25.2403 31.7196 25.4262 32.141 25.7259 32.5122C26.0255 32.8861 26.4445 33.1848 26.98 33.408C27.5156 33.6341 28.1759 33.7457 28.9584 33.7457C29.9629 33.7457 30.8175 33.5448 31.5139 33.1429C32.2076 32.7438 32.7098 32.1438 33.0178 31.3456H35.3208C34.8491 32.7131 34.0278 33.7625 32.8569 34.4769V34.4797ZM33.3147 26.8188C33.1649 26.4281 32.9374 26.0932 32.6349 25.8169C32.3325 25.5406 31.9634 25.3201 31.5278 25.1582C31.0922 24.9964 30.5955 24.9154 30.0406 24.9154C29.4856 24.9154 28.9362 24.9964 28.4479 25.1582C27.9567 25.3201 27.5183 25.5434 27.1354 25.828C26.7497 26.1155 26.4223 26.4476 26.1559 26.8328C25.8868 27.2179 25.6926 27.6309 25.5705 28.0747H33.509C33.5312 27.6337 33.4673 27.2151 33.3175 26.8216L33.3147 26.8188Z\" fill=\"#AAB8CD\"/>\r\n<path d=\"M50.5197 35.2409C50.1063 35.4558 49.5292 35.5647 48.7939 35.5647C48.1723 35.5647 47.6784 35.4112 47.3177 35.107C46.9598 34.8028 46.7878 34.3088 46.8072 33.6278C46.1357 34.3088 45.356 34.8028 44.4681 35.107C43.5774 35.4112 42.6229 35.5647 41.6018 35.5647C40.9414 35.5647 40.3199 35.4977 39.7344 35.3665C39.1517 35.2354 38.6578 35.0288 38.2499 34.7497C37.8448 34.4707 37.5368 34.1106 37.3287 33.6641C37.1206 33.2231 37.0485 32.6929 37.1123 32.0761C37.1817 31.3868 37.3731 30.8286 37.6895 30.3988C38.003 29.9718 38.3915 29.6257 38.8576 29.3606C39.321 29.0982 39.8399 28.8973 40.417 28.7633C40.9914 28.6294 41.5768 28.5177 42.1706 28.4284C42.8033 28.3252 43.3998 28.247 43.9659 28.194C44.5291 28.1438 45.0286 28.0684 45.4587 27.9735C45.8887 27.8786 46.2328 27.7391 46.4853 27.5549C46.7378 27.3707 46.871 27.1056 46.8793 26.7567C46.8904 26.352 46.8072 26.0311 46.6296 25.7855C46.452 25.5427 46.2245 25.3585 45.9415 25.2329C45.6584 25.1045 45.3393 25.0208 44.9842 24.9789C44.629 24.9371 44.2794 24.9147 43.9298 24.9147C42.9919 24.9147 42.2011 25.0599 41.5546 25.3501C40.9053 25.6404 40.5197 26.1957 40.4031 27.019H38.1501C38.2527 26.3241 38.483 25.7436 38.841 25.2775C39.1989 24.8115 39.6456 24.4375 40.1895 24.1528C40.7305 23.8682 41.3354 23.6644 42.0041 23.5416C42.6728 23.4188 43.3859 23.3574 44.1407 23.3574C44.74 23.3574 45.3366 23.3909 45.9248 23.4607C46.5158 23.5305 47.0486 23.67 47.5258 23.8849C48.0031 24.0998 48.3888 24.4012 48.6773 24.7919C48.9687 25.1854 49.1102 25.7018 49.1046 26.3437L49.0491 32.3468C49.0436 32.8213 49.0741 33.1729 49.1379 33.3962C49.2018 33.6222 49.4182 33.7339 49.79 33.7339C49.9953 33.7339 50.2367 33.7004 50.517 33.6362V35.2493L50.5197 35.2409ZM46.7961 28.9866C46.5047 29.1652 46.1218 29.2964 45.6501 29.3801C45.1784 29.4638 44.6845 29.5308 44.1684 29.5838C43.6523 29.6369 43.1307 29.6983 42.6007 29.7653C42.0735 29.8322 41.5963 29.9439 41.1689 30.0946C40.7416 30.2453 40.3837 30.463 40.1007 30.7476C39.8149 31.0323 39.6512 31.423 39.6096 31.9198C39.5818 32.2491 39.6373 32.5282 39.776 32.7571C39.9148 32.9859 40.1035 33.1729 40.3449 33.318C40.5835 33.4632 40.8693 33.5664 41.1995 33.6306C41.5297 33.6948 41.8821 33.7255 42.2539 33.7255C43.0363 33.7255 43.7106 33.6334 44.2766 33.4492C44.8427 33.265 45.3033 33.0362 45.6668 32.7571C46.0275 32.4808 46.2939 32.1821 46.4687 31.8612C46.6435 31.543 46.7323 31.2444 46.7406 30.9653L46.7933 28.9866H46.7961Z\" fill=\"#AAB8CD\"/>\r\n<path d=\"M54.5002 32.4896C54.7332 32.7994 55.0329 33.0449 55.3992 33.2291C55.7682 33.4133 56.1816 33.5473 56.6422 33.6254C57.1029 33.7064 57.5746 33.7455 58.0574 33.7455C58.4292 33.7455 58.8176 33.7203 59.2255 33.6729C59.6306 33.6254 60.0024 33.5361 60.3354 33.4078C60.6684 33.2794 60.9375 33.0924 61.1401 32.844C61.3398 32.5984 61.4259 32.2858 61.3926 31.9063C61.3482 31.39 61.0873 31.002 60.6101 30.7397C60.1356 30.4773 59.5529 30.268 58.862 30.1062C58.1739 29.9471 57.4275 29.802 56.6284 29.6736C55.8293 29.5452 55.0884 29.3638 54.4058 29.1294C53.726 28.8977 53.1572 28.5712 52.7021 28.1553C52.2499 27.7423 52.0196 27.1702 52.0112 26.4473C52.0029 25.8864 52.15 25.4119 52.4469 25.0156C52.7438 24.6221 53.1267 24.3039 53.5956 24.0611C54.0618 23.8183 54.5917 23.6397 55.18 23.5225C55.7682 23.4053 56.3537 23.3467 56.9364 23.3467C57.6911 23.3467 58.3903 23.3969 59.0313 23.5002C59.675 23.6034 60.2522 23.7821 60.7655 24.0388C61.2788 24.2956 61.7061 24.6444 62.0418 25.0882C62.3804 25.5347 62.6107 26.0929 62.73 26.7711H60.4797C60.4159 26.4111 60.2771 26.1124 60.0662 25.8752C59.8554 25.6408 59.5973 25.451 59.2893 25.3087C58.9841 25.1663 58.6484 25.0659 58.2877 25.0017C57.9269 24.9375 57.5662 24.9068 57.2 24.9068C56.8698 24.9068 56.5368 24.9291 56.1983 24.971C55.8626 25.0128 55.5546 25.0882 55.2826 25.1942C55.0079 25.3003 54.786 25.4426 54.6195 25.6184C54.4502 25.7971 54.3698 26.0287 54.3809 26.3162C54.3919 26.6343 54.5362 26.8995 54.8193 27.1088C55.1023 27.3209 55.4574 27.4967 55.8903 27.6362C56.3232 27.7758 56.8087 27.893 57.3498 27.9907C57.8909 28.0884 58.4319 28.186 58.973 28.2893C59.5529 28.3926 60.119 28.5209 60.6767 28.6689C61.2344 28.8196 61.7366 29.0177 62.1834 29.2661C62.6301 29.5145 63.0047 29.8299 63.3043 30.2122C63.6068 30.5946 63.7871 31.0746 63.8482 31.6495C63.9259 32.3891 63.8093 33.0087 63.4958 33.5054C63.1823 34.005 62.7439 34.4125 62.1806 34.7223C61.6173 35.0348 60.9653 35.2525 60.2327 35.3753C59.4974 35.5009 58.7594 35.5623 58.0241 35.5623C57.2138 35.5623 56.4341 35.487 55.6933 35.339C54.9524 35.1911 54.2976 34.9483 53.726 34.6106C53.1572 34.2757 52.6994 33.8376 52.3553 33.2989C52.014 32.7631 51.8226 32.1184 51.7754 31.3676H54.1062C54.1367 31.8058 54.2671 32.177 54.4974 32.4868L54.5002 32.4896Z\" fill=\"#AAB8CD\"/>\r\n<path d=\"M72.899 37.022C72.7187 37.6695 72.5244 38.2193 72.3191 38.6686C72.111 39.1207 71.8585 39.4919 71.5644 39.785C71.2675 40.0752 70.9179 40.2901 70.5183 40.4241C70.116 40.558 69.6415 40.625 69.0921 40.625C68.798 40.625 68.5011 40.6083 68.2042 40.572C67.9073 40.5357 67.6132 40.4743 67.3302 40.385L67.0805 38.3337C67.3024 38.4202 67.5577 38.4928 67.8463 38.5542C68.1321 38.6156 68.3735 38.6463 68.5677 38.6463C69.0699 38.6463 69.475 38.5375 69.7803 38.3226C70.0855 38.1077 70.2963 37.8007 70.4101 37.4099L71.076 35.2219L63.9922 23.6035H66.4672L71.9001 33.0087H71.9556L74.1532 23.6035H76.4756L72.9018 37.0276L72.899 37.022Z\" fill=\"#AAB8CD\"/>\r\n</svg>\r\n";

const __vite_glob_0_25$1 = "<svg role=\"img\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 512 512\">\r\n  <path\r\n    d=\"M464 64C490.5 64 512 85.49 512 112C512 127.1 504.9 141.3 492.8 150.4L275.2 313.6C263.8 322.1 248.2 322.1 236.8 313.6L19.2 150.4C7.113 141.3 0 127.1 0 112C0 85.49 21.49 64 48 64H464zM217.6 339.2C240.4 356.3 271.6 356.3 294.4 339.2L512 176V384C512 419.3 483.3 448 448 448H64C28.65 448 0 419.3 0 384V176L217.6 339.2z\" />\r\n</svg>";

const __vite_glob_0_26$1 = "<svg role=\"img\" viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\"><title>Facebook</title><path d=\"M24 12.073c0-6.627-5.373-12-12-12s-12 5.373-12 12c0 5.99 4.388 10.954 10.125 11.854v-8.385H7.078v-3.47h3.047V9.43c0-3.007 1.792-4.669 4.533-4.669 1.312 0 2.686.235 2.686.235v2.953H15.83c-1.491 0-1.956.925-1.956 1.874v2.25h3.328l-.532 3.47h-2.796v8.385C19.612 23.027 24 18.062 24 12.073z\"/></svg>";

const __vite_glob_0_27$1 = "<svg width=\"94\" height=\"96\" viewBox=\"0 0 94 96\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path d=\"M92.9073 54.3809C92.9073 73.7581 77.199 89.4664 57.8218 89.4664C38.4446 89.4664 22.7363 73.7581 22.7363 54.3809C22.7363 35.0037 38.4446 19.2954 57.8218 19.2954C77.199 19.2954 92.9073 35.0037 92.9073 54.3809Z\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M57.7773 18.0809L57.7773 6.81934\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M58.2461 26.5273L58.2461 54.6813L69.5077 44.3582\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M47.4609 6H68.3024\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M39.3491 47.3545H8.14648\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M33.2745 58.311H1\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path d=\"M39.3491 69.2676H8.14648\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-miterlimit=\"10\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n</svg>\r\n";

const __vite_glob_0_28$1 = "<svg width=\"18\" height=\"24\" viewBox=\"0 0 18 24\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path d=\"M13.7071 1.70711C13.0771 1.07714 12 1.52331 12 2.41421V5C12 5.55228 12.4477 6 13 6H15.5858C16.4767 6 16.9229 4.92286 16.2929 4.29289L13.7071 1.70711ZM10.5 6V1C10.5 0.447715 10.0523 0 9.5 0H2.25C0.984375 0 0 1.03125 0 2.25V21.75C0 23.0156 0.984375 24 2.25 24H15.75C16.9688 24 18 23.0156 18 21.75V8.5C18 7.94771 17.5523 7.5 17 7.5H12C11.1562 7.5 10.5 6.84375 10.5 6ZM12.75 19.5H5.25C4.82812 19.5 4.5 19.1719 4.5 18.75C4.5 18.375 4.82812 18 5.25 18H12.75C13.125 18 13.5 18.375 13.5 18.75C13.5 19.1719 13.125 19.5 12.75 19.5ZM12.75 16.5H5.25C4.82812 16.5 4.5 16.1719 4.5 15.75C4.5 15.375 4.82812 15 5.25 15H12.75C13.125 15 13.5 15.375 13.5 15.75C13.5 16.1719 13.125 16.5 12.75 16.5ZM13.5 12.75C13.5 13.1719 13.125 13.5 12.75 13.5H5.25C4.82812 13.5 4.5 13.1719 4.5 12.75C4.5 12.375 4.82812 12 5.25 12H12.75C13.125 12 13.5 12.375 13.5 12.75Z\" fill=\"#D7DFEB\"/>\r\n</svg>\r\n";

const __vite_glob_0_29$1 = "<svg role=\"img\" viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\"><title>GitHub</title><path d=\"M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12\"/></svg>";

const __vite_glob_0_30$1 = "<svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path d=\"M8.55 0.55C5.2 1.55 2.45 4.05 1.05 7.2H6.05C6.5 4.45 7.4 2.1 8.55 0.55ZM7.25 12C7.25 13.15 7.3 14.2 7.4 15.2H16.65C16.75 14.2 16.85 13.15 16.85 12C16.85 10.9 16.75 9.85 16.65 8.8H7.4C7.3 9.85 7.25 10.9 7.25 12ZM16.4 7.2C15.6 2.8 13.75 0 12.05 0C10.3 0 8.45 2.8 7.65 7.2H16.4ZM23 7.2C21.6 4.05 18.85 1.55 15.5 0.55C16.65 2.1 17.55 4.45 18 7.2H23ZM15.5 23.5C18.85 22.5 21.6 20 23 16.8H18C17.55 19.6 16.65 21.95 15.5 23.5ZM23.6 8.8H18.25C18.35 9.85 18.4 10.9 18.4 12C18.4 13.15 18.35 14.2 18.25 15.2H23.6C23.85 14.2 24 13.15 24 12C24 10.9 23.85 9.85 23.6 8.8ZM7.65 16.8C8.45 21.25 10.3 24 12 24C13.75 24 15.6 21.25 16.4 16.8H7.65ZM5.65 12C5.65 10.9 5.7 9.85 5.8 8.8H0.45C0.2 9.85 0 10.9 0 12C0 13.15 0.2 14.2 0.45 15.2H5.8C5.7 14.2 5.65 13.15 5.65 12ZM1.05 16.8C2.45 20 5.15 22.5 8.55 23.5C7.4 21.95 6.5 19.6 6.05 16.8H1.05Z\" fill=\"#D7DFEB\"/>\r\n</svg>\r\n";

const __vite_glob_0_31$1 = "<svg width=\"24\" height=\"17\" viewBox=\"0 0 24 17\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path d=\"M23.3625 3.97711L12.75 0.142379C12.225 -0.0474595 11.6625 -0.0474595 11.1375 0.142379L0.6 3.97711C0.225 4.12898 0 4.47069 0 4.85036C0 5.26801 0.225 5.60972 0.6 5.76159L1.57579 6.10737C2.20937 6.33189 2.43681 7.10793 2.17989 7.72907C2.12776 7.85512 2.08075 7.98364 2.03931 8.11464C1.94255 8.4205 1.71826 8.67157 1.50524 8.91143C1.31696 9.12344 1.2 9.40203 1.2 9.71022C1.2 9.88784 1.23283 10.0572 1.29465 10.2104C1.42386 10.5307 1.57952 10.8667 1.52525 11.2078L0.7125 16.3166C0.6375 16.6963 0.9375 17 1.275 17H3.4875C3.825 17 4.125 16.6963 4.05 16.3166L3.23725 11.2078C3.18298 10.8667 3.33651 10.5329 3.48369 10.2205C3.55713 10.0646 3.6 9.89176 3.6 9.71022C3.6 9.5683 3.56856 9.43169 3.51548 9.30633C3.36024 8.93969 3.15669 8.54219 3.30883 8.17426C3.40053 7.9525 3.51242 7.74158 3.64126 7.54315C3.86172 7.20358 4.29831 7.10449 4.67895 7.24242L11.175 9.59631C11.7 9.78615 12.2625 9.78615 12.7875 9.59631L23.3625 5.76159C23.7375 5.60972 24 5.26801 24 4.85036C24 4.47069 23.7375 4.12898 23.3625 3.97711ZM13.1625 10.7353C12.7875 10.8872 12.375 10.9252 12 10.9252C11.5875 10.9252 11.175 10.8872 10.8 10.7353L6.53571 9.19762C5.92188 8.97627 5.26393 9.39258 5.20113 10.0421L4.8 14.1904C4.8 15.7471 7.9875 17 12 17C15.9375 17 19.2 15.7471 19.2 14.1904L18.7692 10.0322C18.7022 9.38596 18.0465 8.97416 17.4353 9.19455L13.1625 10.7353Z\" fill=\"#D7DFEB\"/>\r\n</svg>\r\n";

const __vite_glob_0_32$1 = "<svg width=\"20\" height=\"15\" viewBox=\"0 0 20 15\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path d=\"M7.08398 13.7239C7.43555 14.0754 8.0332 14.0754 8.38477 13.7239L18.7207 3.38794C19.0723 3.03638 19.0723 2.43872 18.7207 2.08716L17.4551 0.821533C17.1035 0.469971 16.541 0.469971 16.1895 0.821533L7.75195 9.25903L3.7793 5.32153C3.42773 4.96997 2.86523 4.96997 2.51367 5.32153L1.24805 6.58716C0.896484 6.93872 0.896484 7.53638 1.24805 7.88794L7.08398 13.7239Z\" fill=\"#38EDAC\"/>\r\n</svg>\r\n";

const __vite_glob_0_33$1 = "<svg width=\"22\" height=\"24\" viewBox=\"0 0 22 24\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path d=\"M21.9017 11.8164C20.9981 9.79687 19.7933 5.32812 18.7606 3.82422C17.0824 1.41797 14.3286 0 11.4457 0H10.9723V4.64062C12.6074 5.41406 13.7262 7.04687 13.7262 8.9375C13.7262 11.6016 11.5317 13.75 8.90696 13.75C6.28221 13.75 4.08774 11.6016 4.08774 8.9375C4.08774 7.04687 5.20649 5.41406 6.84158 4.64062V0.128906C2.96899 0.816406 0 4.16797 0 8.25C0 10.6133 0.989663 12.8477 2.75384 14.3945V24H13.7692V19.25H16.48C18.0291 19.25 19.2339 18.0469 19.2339 16.543V13.75H20.5678C21.0411 13.75 21.4714 13.5352 21.6865 13.1484C22.0308 12.7617 22.0738 12.2461 21.9017 11.8164ZM5.50769 8.9375C5.50769 10.8711 7.0137 12.375 8.90696 12.375C10.7572 12.375 12.3062 10.8711 12.3062 8.9375C12.3062 7.04687 10.8433 5.5 8.94999 5.5C7.0137 5.5 5.50769 7.04687 5.50769 8.9375ZM8.90696 7.5625C9.63845 7.5625 10.2839 8.20703 10.2839 8.9375C10.2839 9.71094 9.63845 10.3125 8.90696 10.3125C8.13245 10.3125 7.53004 9.71094 7.53004 8.9375C7.53004 8.20703 8.17547 7.5625 8.90696 7.5625Z\" fill=\"#D7DFEB\"/>\r\n</svg>\r\n";

const __vite_glob_0_34$1 = "<svg width=\"24\" height=\"20\" viewBox=\"0 0 24 20\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path d=\"M24 9.35564V10.6079C24 10.9601 23.7 11.2341 23.4 11.2341H19.3422C18.9574 11.2341 18.6067 11.4549 18.4404 11.802L16.65 15.5388C16.425 16.0084 15.975 16.2823 15.45 16.204C14.9625 16.1649 14.55 15.8127 14.4375 15.304L13.2056 10.6591C12.9346 9.63767 11.4707 9.67996 11.2592 10.7154L9.5625 19.0217C9.45 19.5695 8.9625 20 8.4375 20H8.4C7.8375 20 7.3875 19.6478 7.2375 19.0999L5.26532 11.9675C5.14548 11.5342 4.75113 11.2341 4.30149 11.2341H0.6C0.2625 11.2341 0 10.9601 0 10.6471V9.35564C0 9.04257 0.2625 8.76864 0.6 8.76864H6C6.525 8.76864 6.975 9.15997 7.125 9.70784V9.70784C7.40023 10.6983 8.8209 10.6491 9.02705 9.64207L10.8 0.981036C10.9125 0.394031 11.4 0.00269451 11.9625 0.00269451C12.4875 -0.0364392 13.0125 0.354898 13.125 0.902769L15.3246 9.3091C15.524 10.0711 16.5616 10.1782 16.9125 9.47304V9.47304C17.1 9.00344 17.5125 8.7295 18 8.7295H23.4C23.7 8.7295 24 9.04257 24 9.35564Z\" fill=\"#D7DFEB\"/>\r\n</svg>\r\n";

const __vite_glob_0_35$1 = "<svg xmlns=\"http://www.w3.org/2000/svg\">\r\n<path d=\"M12 2C6.48 2 2 6.48 2 12C2 17.52 6.48 22 12 22C17.52 22 22 17.52 22 12C22 6.48 17.52 2 12 2ZM13 17H11V11H13V17ZM13 9H11V7H13V9Z\"/>\r\n</svg>";

const __vite_glob_0_36$1 = "<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M0 0h24v24H0z\" fill=\"none\"/><path d=\"M11.99 2C6.47 2 2 6.48 2 12s4.47 10 9.99 10C17.52 22 22 17.52 22 12S17.52 2 11.99 2zm6.93 6h-2.95c-.32-1.25-.78-2.45-1.38-3.56 1.84.63 3.37 1.91 4.33 3.56zM12 4.04c.83 1.2 1.48 2.53 1.91 3.96h-3.82c.43-1.43 1.08-2.76 1.91-3.96zM4.26 14C4.1 13.36 4 12.69 4 12s.1-1.36.26-2h3.38c-.08.66-.14 1.32-.14 2 0 .68.06 1.34.14 2H4.26zm.82 2h2.95c.32 1.25.78 2.45 1.38 3.56-1.84-.63-3.37-1.9-4.33-3.56zm2.95-8H5.08c.96-1.66 2.49-2.93 4.33-3.56C8.81 5.55 8.35 6.75 8.03 8zM12 19.96c-.83-1.2-1.48-2.53-1.91-3.96h3.82c-.43 1.43-1.08 2.76-1.91 3.96zM14.34 14H9.66c-.09-.66-.16-1.32-.16-2 0-.68.07-1.35.16-2h4.68c.09.65.16 1.32.16 2 0 .68-.07 1.34-.16 2zm.25 5.56c.6-1.11 1.06-2.31 1.38-3.56h2.95c-.96 1.65-2.49 2.93-4.33 3.56zM16.36 14c.08-.66.14-1.32.14-2 0-.68-.06-1.34-.14-2h3.38c.16.64.26 1.31.26 2s-.1 1.36-.26 2h-3.38z\"/></svg>";

const __vite_glob_0_37$1 = "<svg role=\"img\" viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\"><title>LinkedIn</title><path d=\"M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z\"/></svg>";

const __vite_glob_0_38$1 = "<svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path d=\"M2.7768 15.9642C5.88264 19.0794 10.7048 19.5233 14.2998 17.2959L20.9836 23.9999L24 20.9744L17.3133 14.2676C17.5957 13.8073 17.8347 13.327 18.0303 12.8328C17.9095 12.5983 17.7931 12.3253 17.6787 12.0161C17.5309 11.6169 17.402 11.2611 17.2869 10.9431C16.9929 10.1314 16.7884 9.56685 16.588 9.15936C16.5483 9.07859 16.5131 9.01362 16.4825 8.96159C16.4722 8.98248 16.4614 9.00518 16.4501 9.02981C16.2704 9.42095 16.1208 9.97637 15.9439 10.6529L15.9261 10.721C15.7677 11.3277 15.5821 12.0381 15.3302 12.5979C15.1983 12.891 15.0168 13.2175 14.7538 13.4818C14.4731 13.764 14.0706 14 13.5568 14C12.8626 14 12.4063 13.5471 12.155 13.2022C11.8921 12.8416 11.6911 12.3849 11.528 11.9339C11.2243 11.0943 10.9634 10.013 10.7158 8.98675L10.6551 8.73516C10.3788 7.59343 10.1143 6.54245 9.80814 5.77812C9.77787 5.70256 9.75025 5.62267 9.72379 5.54612C9.65092 5.33534 9.5868 5.14988 9.50023 5.14988C9.41367 5.14988 9.34955 5.33533 9.27668 5.54611C9.25022 5.62266 9.2226 5.70256 9.19233 5.77812C8.88614 6.54245 8.62164 7.59343 8.3454 8.73516L8.28465 8.98672C8.03704 10.013 7.77613 11.0943 7.47249 11.9339C7.30937 12.3849 7.10833 12.8416 6.84547 13.2022C6.59413 13.5471 6.13789 14 5.44371 14C4.92979 14 4.52723 13.7639 4.24656 13.4815C3.9837 13.2171 3.80231 12.8905 3.67058 12.5974C3.41931 12.0383 3.23427 11.3287 3.07621 10.7226L3.05794 10.6526C2.88134 9.97611 2.73184 9.42093 2.55222 9.0301C2.54057 9.00475 2.52944 8.98146 2.51884 8.96007C2.48807 9.01236 2.45255 9.0778 2.41245 9.15936C2.21207 9.56685 2.00763 10.1313 1.7136 10.9431C1.59845 11.261 1.46957 11.6169 1.32181 12.0161C1.18579 12.3836 1.04693 12.7 0.901039 12.9616C0.870203 13.0168 0.837542 13.0724 0.802836 13.1276C1.26301 14.1597 1.92099 15.1245 2.7768 15.9642Z\" fill=\"#D7DFEB\"/>\r\n<path d=\"M0.00989956 9.77451C-0.107232 7.23162 0.815069 4.64948 2.7768 2.72462C6.39872 -0.908206 12.3548 -0.908206 15.9767 2.72462C17.6939 4.44699 18.5969 6.69561 18.6858 8.97148C18.5817 8.71061 18.4816 8.47787 18.3828 8.27685C18.208 7.92139 17.9876 7.55479 17.6733 7.27567C17.3221 6.96378 16.8981 6.8 16.423 6.8C15.3792 6.8 14.8623 7.69475 14.6325 8.19519C14.3693 8.76842 14.179 9.49658 14.0179 10.1129L14.009 10.1471C13.8509 10.7518 13.7164 11.2597 13.5636 11.6421C13.5149 11.5329 13.4631 11.404 13.4087 11.2536C13.1447 10.5234 12.909 9.54783 12.651 8.48018L12.599 8.26484C12.3308 7.15657 12.0345 5.95755 11.6647 5.03438C11.4808 4.57534 11.2514 4.11227 10.95 3.74858C10.647 3.38304 10.1672 3 9.50023 3C8.83322 3 8.35347 3.38304 8.0505 3.74858C7.74906 4.11227 7.51965 4.57534 7.33576 5.03438C6.96594 5.95755 6.66963 7.15657 6.40149 8.26484L6.34944 8.48018C6.09151 9.54783 5.85581 10.5234 5.59172 11.2536C5.53748 11.4036 5.48578 11.5322 5.43724 11.6413C5.28484 11.2591 5.15082 10.7516 4.99309 10.1474L4.98462 10.115C4.82365 9.49821 4.63332 8.76898 4.36948 8.1949C4.13885 7.69309 3.6213 6.8 2.57746 6.8C2.1024 6.8 1.67839 6.96378 1.32719 7.27567C1.01289 7.55479 0.792464 7.92139 0.617682 8.27685C0.426373 8.66591 0.230294 9.17382 0.00989956 9.77451Z\" fill=\"#D7DFEB\"/>\r\n</svg>\r\n";

const __vite_glob_0_39$1 = "<svg width=\"17\" height=\"24\" viewBox=\"0 0 17 24\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M8.5 0C5.94034 0 3.86364 2.01562 3.86364 4.5V12C3.86364 14.4844 5.94034 16.5 8.5 16.5C11.0597 16.5 13.1364 14.4844 13.1364 12V4.5C13.1364 2.01562 11.0597 0 8.5 0ZM2.31818 10.125C2.31818 9.51562 1.78693 9 1.15909 9C0.482955 9 0 9.51562 0 10.125V12C0 16.2188 3.1875 19.6406 7.34091 20.2031V21.75H5.02273C4.34659 21.75 3.86364 22.2656 3.86364 22.875C3.86364 23.5312 4.34659 24 5.02273 24H8.5H11.9773C12.6051 24 13.1364 23.5312 13.1364 22.875C13.1364 22.2656 12.6051 21.75 11.9773 21.75H9.65909V20.2031C13.7642 19.6406 17 16.2188 17 12V10.125C17 9.51562 16.4687 9 15.8409 9C15.1648 9 14.6818 9.51562 14.6818 10.125V12C14.6818 15.3281 11.8807 18 8.5 18C5.07102 18 2.31818 15.3281 2.31818 12V10.125Z\" fill=\"#D7DFEB\"/>\n</svg>\n";

const __vite_glob_0_40$1 = "<svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M9.27344 2.96326C9.27344 3.40017 9.36916 3.81498 9.54102 4.18837L4.9742 6.4811C4.48231 5.77637 3.6594 5.31445 2.72726 5.31445C1.22104 5.31445 0 6.52054 0 8.00833C0 9.49612 1.22104 10.7022 2.72726 10.7022C3.08284 10.7022 3.42254 10.635 3.73402 10.5127L4.92392 12.0488L3.73341 13.5857C3.42209 13.4636 3.08261 13.3965 2.72726 13.3965C1.22104 13.3965 0 14.6026 0 16.0904C0 17.5781 1.22104 18.7842 2.72726 18.7842C3.65973 18.7842 4.48291 18.322 4.97473 17.6168L9.50594 19.8917C9.35618 20.2442 9.27344 20.6313 9.27344 21.0375C9.27344 22.674 10.6166 24.0007 12.2734 24.0007C13.9303 24.0007 15.2734 22.674 15.2734 21.0375C15.2734 20.3323 15.0241 19.6847 14.6077 19.176L19.2843 14.4803C19.7705 14.8157 20.362 15.0125 21 15.0125C22.6569 15.0125 24 13.6858 24 12.0492C24 10.4126 22.6569 9.08594 21 9.08594C20.3622 9.08594 19.7708 9.28256 19.2847 9.61778L14.5636 4.87741C15.0064 4.36105 15.2734 3.69296 15.2734 2.96326C15.2734 1.3267 13.9303 0 12.2734 0C10.6166 0 9.27344 1.3267 9.27344 2.96326ZM13.7918 5.51952C13.3464 5.77819 12.8274 5.92653 12.2734 5.92653C11.8609 5.92653 11.4679 5.8443 11.1102 5.69554L7.89169 9.85052L9.64473 10.6201C10.1555 9.70549 11.1412 9.08594 12.2734 9.08594C13.7576 9.08594 14.9901 10.1505 15.2308 11.5488H18.0426C18.121 11.0937 18.3044 10.6739 18.5674 10.3147L13.7918 5.51952ZM10.26 5.16009L6.94776 9.43613L5.35376 8.73635C5.41941 8.50478 5.45452 8.26061 5.45452 8.00833C5.45452 7.7972 5.42993 7.59174 5.38342 7.3946L10.1136 5.01986C10.1608 5.06821 10.2096 5.11498 10.26 5.16009ZM10.0514 19.0466C10.1041 18.9892 10.159 18.9339 10.2161 18.8808L6.94889 14.663L5.35399 15.3632C5.41949 15.5945 5.45452 15.8384 5.45452 16.0904C5.45452 16.3012 5.43001 16.5063 5.38364 16.7032L10.0514 19.0466ZM7.89282 14.2486L11.0538 18.3293C11.4264 18.1654 11.8392 18.0742 12.2734 18.0742C12.8527 18.0742 13.3937 18.2364 13.8523 18.5173L18.5671 13.7833C18.3041 13.424 18.1208 13.0041 18.0425 12.5488H15.2309C14.9905 13.9475 13.7579 15.0125 12.2734 15.0125C11.1416 15.0125 10.1561 14.3933 9.64527 13.4793L7.89282 14.2486ZM9.31321 12.5329L7.26152 13.4336L6.18884 12.0488L7.26039 10.6655L9.31302 11.5666C9.28698 11.7237 9.27344 11.8849 9.27344 12.0492C9.27344 12.2139 9.28704 12.3755 9.31321 12.5329ZM6.31759 13.848L4.90483 14.4682C4.80882 14.3427 4.70193 14.2257 4.58555 14.1186L5.55638 12.8653L6.31759 13.848ZM6.31647 10.2511L4.9043 9.63116C4.80855 9.75629 4.702 9.8729 4.58602 9.97965L5.55638 11.2323L6.31647 10.2511Z\" fill=\"#D7DFEB\"/>\r\n</svg>\r\n";

const __vite_glob_0_41$1 = "<svg width=\"24\" height=\"22\" viewBox=\"0 0 24 22\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path d=\"M22.5 0H6C5.15625 0 4.5 0.736607 4.5 1.57143V18.0714C4.5 18.5134 4.125 18.8571 3.75 18.8571C3.32812 18.8571 3 18.5134 3 18.0714V3.14286H1.5C0.65625 3.14286 0 3.87946 0 4.71429V18.8571C0 20.625 1.3125 22 3 22H21C22.6406 22 24 20.625 24 18.8571V1.57143C24 0.736607 23.2969 0 22.5 0ZM12.75 18.8571H8.25C7.82812 18.8571 7.5 18.5134 7.5 18.0714C7.5 17.6786 7.82812 17.2857 8.25 17.2857H12.75C13.125 17.2857 13.5 17.6786 13.5 18.0714C13.5 18.5134 13.125 18.8571 12.75 18.8571ZM12.75 14.1429H8.25C7.82812 14.1429 7.5 13.7991 7.5 13.3571C7.5 12.9643 7.82812 12.5714 8.25 12.5714H12.75C13.125 12.5714 13.5 12.9643 13.5 13.3571C13.5 13.7991 13.125 14.1429 12.75 14.1429ZM20.25 18.8571H15.75C15.3281 18.8571 15 18.5134 15 18.0714C15 17.6786 15.3281 17.2857 15.75 17.2857H20.25C20.625 17.2857 21 17.6786 21 18.0714C21 18.5134 20.625 18.8571 20.25 18.8571ZM20.25 14.1429H15.75C15.3281 14.1429 15 13.7991 15 13.3571C15 12.9643 15.3281 12.5714 15.75 12.5714H20.25C20.625 12.5714 21 12.9643 21 13.3571C21 13.7991 20.625 14.1429 20.25 14.1429ZM21 8.64286C21 9.08482 20.625 9.42857 20.25 9.42857H8.25C7.82812 9.42857 7.5 9.08482 7.5 8.64286V3.92857C7.5 3.53571 7.82812 3.14286 8.25 3.14286H20.25C20.625 3.14286 21 3.53571 21 3.92857V8.64286Z\" fill=\"#D7DFEB\"/>\r\n</svg>\r\n";

const __vite_glob_0_42$1 = "<svg width=\"24\" height=\"22\" viewBox=\"0 0 24 22\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path d=\"M23.3333 19.2461H21.3333V4.87406C21.3333 3.71225 20.4167 2.72256 19.3333 2.72256L14.6667 2.76559V5.47648H18.6667V22H23.3333C23.6667 22 24 21.6988 24 21.3546V20.0206C24 19.5903 23.6667 19.2461 23.3333 19.2461ZM11.6667 0.0546998L3.66667 2.16317C3.04167 2.33529 2.66667 2.89468 2.66667 3.54013V19.2461H0.666667C0.291667 19.2461 0 19.5903 0 19.9346V21.2685C0 21.6988 0.291667 22 0.666667 22H13.3333V1.43166C13.3333 0.485 12.5 -0.20348 11.6667 0.0546998ZM9.66667 12.3613C9.08333 12.3613 8.66667 11.7589 8.66667 11.0274C8.66667 10.2958 9.08333 9.69342 9.66667 9.69342C10.2083 9.69342 10.6667 10.2528 10.6667 10.9843C10.6667 11.7589 10.2083 12.3613 9.66667 12.3613Z\" fill=\"#D7DFEB\"/>\r\n</svg>\r\n";

const __vite_glob_0_43$1 = "<svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path d=\"M12 0C5.34375 0 0 5.39062 0 12C0 18.6562 5.34375 24 12 24C18.6094 24 24 18.6562 24 12C24 5.39062 18.6094 0 12 0ZM12 18.75C11.1562 18.75 10.5 18.0938 10.5 17.25C10.5 16.4062 11.1094 15.75 12 15.75C12.7969 15.75 13.5 16.4062 13.5 17.25C13.5 18.0938 12.7969 18.75 12 18.75ZM15.2344 12.0938L13.125 13.4062V13.5C13.125 14.1094 12.6094 14.625 12 14.625C11.3906 14.625 10.875 14.1094 10.875 13.5V12.75C10.875 12.375 11.0625 12 11.4375 11.7656L14.1094 10.1719C14.4375 9.98438 14.625 9.65625 14.625 9.28125C14.625 8.71875 14.1094 8.25 13.5469 8.25H11.1562C10.5469 8.25 10.125 8.71875 10.125 9.28125C10.125 9.89062 9.60938 10.4062 9 10.4062C8.39062 10.4062 7.875 9.89062 7.875 9.28125C7.875 7.45312 9.32812 6 11.1094 6H13.5C15.4219 6 16.875 7.45312 16.875 9.28125C16.875 10.4062 16.2656 11.4844 15.2344 12.0938Z\" fill=\"#D7DFEB\"/>\r\n</svg>\r\n";

const __vite_glob_0_44$1 = "<svg width=\"24\" height=\"20\" viewBox=\"0 0 24 20\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M0 10C0 9.6875 0.1125 9.375 0.3375 9.14062C0.5625 8.90625 0.8625 8.75 1.2 8.75H2.4V16.25H1.2C0.8625 16.25 0.5625 16.1328 0.3375 15.8984C0.1125 15.6641 0 15.3516 0 15V10ZM13.2 3.75H17.4C19.05 3.75 20.4 5.15625 20.4 6.875V17.5C20.4 18.9062 19.3125 20 18 20H6C4.65 20 3.6 18.9062 3.6 17.5V6.875C3.6 5.15625 4.9125 3.75 6.6 3.75H10.8V1.25C10.8 0.585938 11.325 0 12 0C12.6375 0 13.2 0.585938 13.2 1.25V3.75ZM6.9 10C6.9 10.8984 7.5375 11.5625 8.4 11.5625C9.225 11.5625 9.9 10.8984 9.9 10C9.9 9.14062 9.225 8.4375 8.4 8.4375C7.5375 8.4375 6.9 9.14062 6.9 10ZM14.1 10C14.1 10.8984 14.7375 11.5625 15.6 11.5625C16.425 11.5625 17.1 10.8984 17.1 10C17.1 9.14062 16.425 8.4375 15.6 8.4375C14.7375 8.4375 14.1 9.14062 14.1 10ZM22.8 8.75C23.1 8.75 23.4 8.90625 23.625 9.14062C23.85 9.375 24 9.6875 24 10V15C24 15.3516 23.85 15.6641 23.625 15.8984C23.4 16.1328 23.1 16.25 22.8 16.25H21.6V8.75H22.8ZM8.64453 14.3523C9.10605 14.8166 10.0857 15.3589 11.2725 15.5278C12.4838 15.7002 13.9397 15.4877 15.3115 14.3902L14.6868 13.6094C13.5586 14.5119 12.3895 14.6767 11.4134 14.5378C10.4127 14.3954 9.64229 13.9376 9.35381 13.6473L8.64453 14.3523Z\" fill=\"#D7DFEB\"/>\r\n</svg>\r\n";

const __vite_glob_0_45$1 = "<svg width=\"44\" height=\"96\" viewBox=\"0 0 44 96\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path d=\"M12.9925 64.7492C9.91633 66.8768 8.2835 70.1428 8.60423 73.4901L8.63339 73.7883C8.88123 76.2547 10.1642 78.5585 12.2635 80.2796L12.9487 80.8488L12.7738 79.6698C12.5843 78.3824 13.5465 77.1762 15.0189 76.8374C16.6372 76.4715 18.3138 77.2847 18.7511 78.6534C19.101 79.724 18.8095 80.8624 17.9639 81.7026C16.5497 83.1255 16.0978 85.1719 16.8559 86.9878C18.0368 89.766 21.3899 90.7417 23.9412 92.0969L23.6351 91.1889C23.1394 89.7389 22.7312 88.0855 23.2414 86.5813C23.7663 85.0092 24.3203 84.0606 25.4137 82.8003C26.9591 80.9979 27.5714 78.938 28.6065 76.9052C29.8457 77.4337 32.2658 80.5507 32.5865 81.7026L32.8052 81.418C34.1465 79.6427 35.1378 77.6912 35.721 75.6449C36.2167 73.8967 36.1729 72.0672 35.5898 70.3461C34.8171 68.0558 33.1114 66.0501 30.7933 64.7085\" stroke=\"#AAB8CD\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\r\n<path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M2 25.8099C2 14.9191 10.8611 6.0918 21.8129 6.0918C32.7646 6.0918 41.6258 14.9191 41.6258 25.8099C41.6258 32.0861 38.6735 37.6865 34.0847 41.2947L34.059 41.315L34.0346 41.3369C32.9414 42.3189 32.0618 43.489 31.4251 44.7759H12.2114C11.5752 43.4928 10.6971 42.3249 9.6072 41.3391L9.32156 41.0808H9.27303C4.82634 37.4683 2 31.9651 2 25.8099ZM12.9798 46.7759C13.2597 47.7743 13.4052 48.8153 13.4052 49.8736V50.2173H23.5549V52.2173H13.4052V55.6455H23.5549V57.6455H13.4052V59.1041C13.4052 60.1896 14.2924 61.0868 15.4017 61.0868H28.2376C29.3393 61.0868 30.234 60.1972 30.234 59.1041V57.6455H26.4414V55.6455H30.234V52.2173H26.4414V50.2173H30.234V49.8736C30.234 48.8167 30.3793 47.7752 30.6586 46.7759H12.9798ZM21.8129 4.0918C9.76518 4.0918 0 13.806 0 25.8099C0 32.7265 3.24201 38.897 8.30589 42.8677L8.36846 42.9168C10.3094 44.7237 11.4052 47.2346 11.4052 49.8736V59.1041C11.4052 61.2969 13.1906 63.0868 15.4017 63.0868H15.5709C15.7107 64.5733 16.3688 65.9042 17.3762 66.907L17.3812 66.912C18.515 68.0247 20.0716 68.73 21.8069 68.73C25.0641 68.73 27.747 66.2582 28.0549 63.0868H28.2376C30.4293 63.0868 32.234 61.3162 32.234 59.1041V49.8736C32.234 47.1969 33.3611 44.6425 35.3476 42.8459C40.3808 38.8755 43.6258 32.7156 43.6258 25.8099C43.6258 13.806 33.8606 4.0918 21.8129 4.0918ZM26.0407 63.0868H17.5839C17.7135 64.0193 18.1438 64.8481 18.7846 65.4871C19.5672 66.2539 20.6281 66.73 21.8069 66.73C23.9653 66.73 25.7434 65.1443 26.0407 63.0868Z\" fill=\"#AAB8CD\"/>\r\n</svg>\r\n";

const __vite_glob_0_46$1 = "<svg width=\"24\" height=\"22\" viewBox=\"0 0 24 22\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path d=\"M11.9711 1.83343C16.3572 1.83343 20.0246 4.97163 20.8717 9.17194C20.9309 9.46551 20.6996 9.73087 20.4001 9.73088C20.0097 9.7309 19.7739 10.1629 19.9852 10.4913L20.9068 11.9235C21.3002 12.535 22.1941 12.5354 22.5881 11.9243L23.5489 10.4345C23.7447 10.1308 23.5267 9.73072 23.1653 9.73074C22.9334 9.73075 22.7393 9.55624 22.7045 9.32697C21.8959 4.00077 17.3664 -4.86786e-07 11.9711 0C7.79193 3.77056e-07 4.11857 2.43509 2.33008 6.01315C2.10517 6.4631 2.27628 7.01601 2.71225 7.24812C3.14822 7.48024 3.68397 7.30365 3.90887 6.8537C5.39759 3.87537 8.46763 1.83343 11.9711 1.83343Z\" fill=\"#D7DFEB\"/>\r\n<path d=\"M3.13355 12.8281C3.07434 12.5345 3.30565 12.2691 3.60513 12.2691C3.99559 12.2691 4.23133 11.8371 4.02005 11.5087L3.0985 10.0765C2.70503 9.46502 1.81121 9.46458 1.41714 10.0757L0.456399 11.5655C0.260539 11.8692 0.478595 12.2693 0.839993 12.2693C1.0719 12.2692 1.26598 12.4438 1.30079 12.673C2.10941 17.9992 6.63883 22 12.0342 22C16.2133 22 19.8867 19.5649 21.6752 15.9869C21.9001 15.5369 21.729 14.984 21.293 14.7519C20.857 14.5198 20.3213 14.6964 20.0964 15.1463C18.6077 18.1246 15.5376 20.1666 12.0342 20.1666C7.64808 20.1666 3.98066 17.0284 3.13355 12.8281Z\" fill=\"#D7DFEB\"/>\r\n<path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M18.5251 12.7766C18.6639 12.8626 18.7472 13.0345 18.7472 13.1777C18.7472 13.6934 17.5259 16.0138 16.9707 16.0424C16.9336 16.0424 16.8965 16.0367 16.8618 16.0254C16.7775 15.9977 16.7099 15.9336 16.6336 15.8883L16.0617 15.5492C15.6926 15.3304 15.2298 15.3792 14.8737 15.6185C14.723 15.7198 14.5681 15.8135 14.4088 15.8991C14.0417 16.0964 13.7786 16.4617 13.7786 16.8785V17.7326C13.7786 17.9618 13.612 18.1337 13.39 18.191C12.9181 18.3056 12.4184 18.3342 11.9743 18.3342C11.5024 18.3342 11.0306 18.2769 10.5587 18.191C10.3644 18.1337 10.2256 17.9618 10.2256 17.7326V16.8892C10.2256 16.4672 9.95573 16.0989 9.58206 15.9028C9.41673 15.816 9.25731 15.7208 9.10316 15.6179C8.74629 15.3796 8.28383 15.3304 7.91472 15.5492L7.34283 15.8883C7.26645 15.9336 7.19893 15.9977 7.11457 16.0254C7.07991 16.0367 7.0428 16.0424 7.00569 16.0424C6.39502 16.0424 5.2292 13.6074 5.2292 13.1777C5.2292 13.0345 5.31247 12.8626 5.45126 12.7766L6.15397 12.3755C6.51601 12.1688 6.70411 11.7593 6.68178 11.343C6.67566 11.2289 6.6726 11.1147 6.6726 11.0005C6.6726 10.8937 6.67528 10.7896 6.68065 10.6865C6.70232 10.2702 6.51601 9.86092 6.15397 9.65423L5.45126 9.25303C5.31247 9.16709 5.2292 8.99521 5.2292 8.82333C5.2292 8.33632 6.45054 5.98724 7.00569 5.98724C7.0428 5.98724 7.07991 5.99293 7.11457 6.00431C7.19893 6.03202 7.26645 6.09613 7.34284 6.14142L7.89359 6.46795C8.27287 6.69281 8.74884 6.63271 9.11696 6.38999C9.26603 6.2917 9.42003 6.20252 9.57952 6.12129C9.95558 5.92977 10.2256 5.56252 10.2256 5.1405V4.29705C10.2256 4.06788 10.3644 3.89599 10.5587 3.8387C11.0306 3.72411 11.5024 3.66682 12.0021 3.66682C12.474 3.66682 12.9458 3.72411 13.4177 3.8387C13.612 3.86735 13.7786 4.06788 13.7786 4.29705V5.1512C13.7786 5.56795 14.0419 5.93221 14.4114 6.12494C14.5649 6.20501 14.7144 6.29278 14.8599 6.38934C15.2273 6.63318 15.7035 6.69281 16.0828 6.46795L16.6336 6.14142C16.7099 6.09613 16.7775 6.03202 16.8618 6.00431C16.8965 5.99293 16.9336 5.98724 16.9707 5.98724C17.5814 5.98724 18.7472 8.42226 18.7472 8.82333C18.7472 8.99521 18.6639 9.16709 18.5251 9.25303L17.8224 9.65423C17.4604 9.86092 17.2778 10.2691 17.3158 10.6842C17.3257 10.7921 17.3316 10.8978 17.3316 11.0005C17.3316 11.1155 17.3253 11.2304 17.315 11.3454C17.2777 11.7606 17.4604 12.1688 17.8224 12.3755L18.5251 12.7766ZM9.78146 11.0005C9.78146 12.2896 10.753 13.2923 12.0021 13.2923C13.2234 13.2923 14.2227 12.2896 14.2227 11.0005C14.2227 9.74004 13.2234 8.70874 12.0021 8.70874C10.753 8.70874 9.78146 9.74004 9.78146 11.0005Z\" fill=\"#D7DFEB\"/>\r\n</svg>\r\n";

const __vite_glob_0_47$1 = "<svg width=\"24\" height=\"20\" viewBox=\"0 0 24 20\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M9.6151 12.6196C9.6151 12.0187 9.65266 12.3192 6.38508 5.82156C6.08461 5.1455 5.44612 4.80748 4.80762 4.80748C4.16913 4.80748 3.49308 5.1455 3.19261 5.82156C-0.0749718 12.3567 0.000145136 12.0563 0.000145136 12.6196C0.000145136 14.3098 2.14098 15.6243 4.77007 15.6243C7.3616 15.6243 9.6151 14.3098 9.6151 12.6196ZM4.77007 6.61028L7.51183 12.0187H2.06586L4.77007 6.61028ZM19.493 17.4271H12.9202V5.85912C13.9343 5.55865 14.723 4.6948 14.9484 3.60561H19.5305C20.0188 3.60561 20.3944 3.23002 20.3944 2.74177C20.3944 2.21595 20.0188 1.8028 19.493 1.8028H14.723C14.2723 0.751169 13.2207 0 12.0188 0C10.7794 0 9.72778 0.751169 9.23952 1.8028H4.50716C3.98134 1.8028 3.60575 2.21595 3.60575 2.70421C3.60575 3.15491 3.98134 3.60561 4.50716 3.60561H9.05173C9.27708 4.6948 10.0658 5.55865 11.0799 5.85912V17.4271H4.50716C3.98134 17.4271 3.60575 17.8403 3.60575 18.3285C3.60575 18.7792 3.98134 19.2299 4.50716 19.2299H19.493C19.9812 19.2299 20.3568 18.8543 20.3568 18.3661C20.3944 17.8403 20.0188 17.4271 19.493 17.4271ZM12.0188 4.20654C11.3428 4.20654 10.817 3.68073 10.817 3.00467C10.817 2.36618 11.3803 1.8028 12.0188 1.8028C12.6198 1.8028 13.1832 2.36618 13.1832 3.00467C13.1832 3.68073 12.6573 4.20654 12.0188 4.20654ZM24 12.6196C24 12.0187 24.0375 12.3192 20.77 5.82156C20.507 5.1455 19.831 4.80748 19.1925 4.80748C18.554 4.80748 17.878 5.1455 17.5775 5.82156C14.3475 12.3567 14.4226 12.0563 14.4226 12.6196C14.4226 14.3098 16.5634 15.6243 19.2301 15.6243C21.8592 15.6243 24 14.3098 24 12.6196ZM16.4883 12.0187C16.4883 12.0187 18.4789 8.0375 19.1925 6.61028L21.8967 12.0187H16.4883Z\" fill=\"#D7DFEB\"/>\n</svg>";

const __vite_glob_0_48$1 = "<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 512 512\"><!--! Font Awesome Pro 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license (Commercial License) Copyright 2022 Fonticons, Inc. --><path d=\"M504.1 471l-134-134C399.1 301.5 415.1 256.8 415.1 208c0-114.9-93.13-208-208-208S-.0002 93.13-.0002 208S93.12 416 207.1 416c48.79 0 93.55-16.91 129-45.04l134 134C475.7 509.7 481.9 512 488 512s12.28-2.344 16.97-7.031C514.3 495.6 514.3 480.4 504.1 471zM48 208c0-88.22 71.78-160 160-160s160 71.78 160 160s-71.78 160-160 160S48 296.2 48 208z\"/></svg>";

const __vite_glob_0_49$1 = "<svg width=\"20\" height=\"18\" viewBox=\"0 0 20 18\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n<path d=\"M18.75 10.2857H1.25C0.546875 10.2857 0 10.8884 0 11.5714V16.7143C0 17.4375 0.546875 18 1.25 18H18.75C19.4141 18 20 17.4375 20 16.7143V11.5714C20 10.8884 19.4141 10.2857 18.75 10.2857ZM13.75 15.1071C13.2031 15.1071 12.8125 14.7054 12.8125 14.1429C12.8125 13.6205 13.2031 13.1786 13.75 13.1786C14.2578 13.1786 14.6875 13.6205 14.6875 14.1429C14.6875 14.7054 14.2578 15.1071 13.75 15.1071ZM16.25 15.1071C15.7031 15.1071 15.3125 14.7054 15.3125 14.1429C15.3125 13.6205 15.7031 13.1786 16.25 13.1786C16.7578 13.1786 17.1875 13.6205 17.1875 14.1429C17.1875 14.7054 16.7578 15.1071 16.25 15.1071ZM18.75 0H1.25C0.546875 0 0 0.602679 0 1.28571V6.42857C0 7.15179 0.546875 7.71429 1.25 7.71429H18.75C19.4141 7.71429 20 7.15179 20 6.42857V1.28571C20 0.602679 19.4141 0 18.75 0ZM13.75 4.82143C13.2031 4.82143 12.8125 4.41964 12.8125 3.85714C12.8125 3.33482 13.2031 2.89286 13.75 2.89286C14.2578 2.89286 14.6875 3.33482 14.6875 3.85714C14.6875 4.41964 14.2578 4.82143 13.75 4.82143ZM16.25 4.82143C15.7031 4.82143 15.3125 4.41964 15.3125 3.85714C15.3125 3.33482 15.7031 2.89286 16.25 2.89286C16.7578 2.89286 17.1875 3.33482 17.1875 3.85714C17.1875 4.41964 16.7578 4.82143 16.25 4.82143Z\" fill=\"#D7DFEB\"/>\n</svg>\n";

const __vite_glob_0_50$1 = "<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 448 512\">\r\n  <!--! Font Awesome Pro 6.1.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license (Commercial License) Copyright 2022 Fonticons, Inc. -->\r\n  <path\r\n    d=\"M285.4 197.1L191.3 244.1C191.8 248 191.1 251.1 191.1 256C191.1 260 191.8 263.1 191.3 267.9L285.4 314.9C302.6 298.2 326.1 288 352 288C405 288 448 330.1 448 384C448 437 405 480 352 480C298.1 480 256 437 256 384C256 379.1 256.2 376 256.7 372.1L162.6 325.1C145.4 341.8 121.9 352 96 352C42.98 352 0 309 0 256C0 202.1 42.98 160 96 160C121.9 160 145.4 170.2 162.6 186.9L256.7 139.9C256.2 135.1 256 132 256 128C256 74.98 298.1 32 352 32C405 32 448 74.98 448 128C448 181 405 224 352 224C326.1 224 302.6 213.8 285.4 197.1L285.4 197.1z\" />\r\n</svg>";

const __vite_glob_0_51$1 = "<svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path d=\"M15.1875 4.875L18 6L19.125 8.8125C19.2188 8.95312 19.3594 9 19.5 9C19.6406 9 19.7344 8.95312 19.8281 8.8125L21 6L23.7656 4.875C23.9062 4.78125 24 4.64062 24 4.5C24 4.35938 23.9062 4.26562 23.7656 4.17188L21 3L19.8281 0.234375C19.7344 0.09375 19.6406 0 19.5 0C19.3594 0 19.2188 0.09375 19.125 0.234375L18 3L15.1875 4.17188C15.0469 4.26562 15 4.35938 15 4.5C15 4.64062 15.0469 4.78125 15.1875 4.875ZM23.7656 19.1719L21 18L19.8281 15.2344C19.7344 15.0938 19.6406 15 19.5 15C19.3594 15 19.2188 15.0938 19.125 15.2344L18 18L15.1875 19.1719C15.0469 19.2656 15 19.3594 15 19.5C15 19.6406 15.0469 19.7812 15.1875 19.875L18 21L19.125 23.8125C19.2188 23.9531 19.3594 24 19.5 24C19.6406 24 19.7344 23.9531 19.8281 23.8125L21 21L23.7656 19.875C23.9062 19.7812 24 19.6406 24 19.5C24 19.3594 23.9062 19.2656 23.7656 19.1719ZM18 12C18 11.7188 17.8125 11.4375 17.5781 11.3438L12.2812 8.67188L9.65625 3.42188C9.375 2.90625 8.57812 2.90625 8.29688 3.42188L5.67188 8.67188L0.375 11.3438C0.140625 11.4375 0 11.7188 0 12C0 12.2812 0.140625 12.5625 0.375 12.6562L5.67188 15.3281L8.29688 20.625C8.4375 20.8594 8.67188 21 9 21C9.23438 21 9.51562 20.8594 9.65625 20.625L12.2812 15.3281L17.5781 12.6562C17.8125 12.5625 18 12.2812 18 12Z\" fill=\"#D7DFEB\"/>\r\n</svg>\r\n";

const __vite_glob_0_52$1 = "<svg width=\"24\" height=\"23\" viewBox=\"0 0 24 23\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path d=\"M12.7838 0.572631L15.2472 8.16281H23.1748C23.981 8.16281 24.2946 9.19579 23.6675 9.64491L17.2179 14.3158L19.6813 21.9509C19.95 22.6695 19.0543 23.2982 18.4272 22.8491L12.0224 18.1333L5.57278 22.8042C4.94574 23.2533 4.04996 22.6695 4.31869 21.906L6.78208 14.3158L0.332473 9.64491C-0.294573 9.19579 0.01895 8.16281 0.825152 8.16281H8.7528L11.2162 0.572631C11.4849 -0.190877 12.5151 -0.190877 12.7838 0.572631Z\" fill=\"#D7DFEB\"/>\r\n</svg>\r\n";

const __vite_glob_0_53$1 = "<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 448 512\">\n  <!--! Font Awesome Pro 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license (Commercial License) Copyright 2022 Fonticons, Inc. -->\n  <path\n    d=\"M0 80V229.5c0 17 6.7 33.3 18.7 45.3l176 176c25 25 65.5 25 90.5 0L418.7 317.3c25-25 25-65.5 0-90.5l-176-176c-12-12-28.3-18.7-45.3-18.7H48C21.5 32 0 53.5 0 80zm112 96c-17.7 0-32-14.3-32-32s14.3-32 32-32s32 14.3 32 32s-14.3 32-32 32z\" />\n</svg>";

const __vite_glob_0_54$1 = "<svg role=\"img\" viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\"><title>Twitch</title><path d=\"M11.571 4.714h1.715v5.143H11.57zm4.715 0H18v5.143h-1.714zM6 0L1.714 4.286v15.428h5.143V24l4.286-4.286h3.428L22.286 12V0zm14.571 11.143l-3.428 3.428h-3.429l-3 3v-3H6.857V1.714h13.714Z\"/></svg>";

const __vite_glob_0_55$1 = "<svg role=\"img\" viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\"><title>Twitter</title><path d=\"M23.953 4.57a10 10 0 01-2.825.775 4.958 4.958 0 002.163-2.723c-.951.555-2.005.959-3.127 1.184a4.92 4.92 0 00-8.384 4.482C7.69 8.095 4.067 6.13 1.64 3.162a4.822 4.822 0 00-.666 2.475c0 1.71.87 3.213 2.188 4.096a4.904 4.904 0 01-2.228-.616v.06a4.923 4.923 0 003.946 4.827 4.996 4.996 0 01-2.212.085 4.936 4.936 0 004.604 3.417 9.867 9.867 0 01-6.102 2.105c-.39 0-.779-.023-1.17-.067a13.995 13.995 0 007.557 2.209c9.053 0 13.998-7.496 13.998-13.985 0-.21 0-.42-.015-.63A9.935 9.935 0 0024 4.59z\"/></svg>";

const __vite_glob_0_56$1 = "<svg viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path d=\"M1 21H23L12 2L1 21ZM13 18H11V16H13V18ZM13 14H11V10H13V14Z\" />\r\n</svg>";

const __vite_glob_0_57$1 = "<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 448 512\"><!--! Font Awesome Pro 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license (Commercial License) Copyright 2022 Fonticons, Inc. --><path d=\"M393.4 41.37C405.9 28.88 426.1 28.88 438.6 41.37C451.1 53.87 451.1 74.13 438.6 86.63L269.3 255.1L438.6 425.4C451.1 437.9 451.1 458.1 438.6 470.6C426.1 483.1 405.9 483.1 393.4 470.6L223.1 301.3L54.63 470.6C42.13 483.1 21.87 483.1 9.372 470.6C-3.124 458.1-3.124 437.9 9.372 425.4L178.7 255.1L9.372 86.63C-3.124 74.13-3.124 53.87 9.372 41.37C21.87 28.88 42.13 28.88 54.63 41.37L223.1 210.7L393.4 41.37z\"/></svg>";

const __vite_glob_0_58$1 = "<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 320 512\"><!--! Font Awesome Pro 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license (Commercial License) Copyright 2022 Fonticons, Inc. --><path d=\"M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z\"/></svg>";

const __vite_glob_0_59$1 = "<svg role=\"img\" viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\"><title>YouTube</title><path d=\"M23.498 6.186a3.016 3.016 0 0 0-2.122-2.136C19.505 3.545 12 3.545 12 3.545s-7.505 0-9.377.505A3.017 3.017 0 0 0 .502 6.186C0 8.07 0 12 0 12s0 3.93.502 5.814a3.016 3.016 0 0 0 2.122 2.136c1.871.505 9.376.505 9.376.505s7.505 0 9.377-.505a3.015 3.015 0 0 0 2.122-2.136C24 15.93 24 12 24 12s0-3.93-.502-5.814zM9.545 15.568V8.432L15.818 12l-6.273 3.568z\"/></svg>";

const $$Astro$1p = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/general/Icon.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Icon = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$1p, $$props, $$slots);
  Astro2.self = $$Icon;
  function getSVG(name) {
    const filepath = `/src/shared/assets/icons/${name}.svg`;
    const files = /* #__PURE__ */ Object.assign({"/src/shared/assets/icons/ai-brain.svg": __vite_glob_0_0$2,"/src/shared/assets/icons/angle-down.svg": __vite_glob_0_1$2,"/src/shared/assets/icons/arrow-down.svg": __vite_glob_0_2$2,"/src/shared/assets/icons/arrow-left.svg": __vite_glob_0_3$2,"/src/shared/assets/icons/arrow-right.svg": __vite_glob_0_4$2,"/src/shared/assets/icons/arrow-top-right-on-square.svg": __vite_glob_0_5$2,"/src/shared/assets/icons/arrow-up.svg": __vite_glob_0_6$2,"/src/shared/assets/icons/audiofile.svg": __vite_glob_0_7$2,"/src/shared/assets/icons/bar-chart.svg": __vite_glob_0_8$2,"/src/shared/assets/icons/batch-transcription-bolt.svg": __vite_glob_0_9$2,"/src/shared/assets/icons/book.svg": __vite_glob_0_10$2,"/src/shared/assets/icons/box.svg": __vite_glob_0_11$2,"/src/shared/assets/icons/brain-circuit.svg": __vite_glob_0_12$2,"/src/shared/assets/icons/bullseye.svg": __vite_glob_0_13$2,"/src/shared/assets/icons/chart.svg": __vite_glob_0_14$2,"/src/shared/assets/icons/check-mark-outline.svg": __vite_glob_0_15$1,"/src/shared/assets/icons/chevron-left.svg": __vite_glob_0_16$1,"/src/shared/assets/icons/chevron-right.svg": __vite_glob_0_17$1,"/src/shared/assets/icons/cleaned-audio.svg": __vite_glob_0_18$1,"/src/shared/assets/icons/clock-target-cost-scale.svg": __vite_glob_0_19$1,"/src/shared/assets/icons/code-laptop.svg": __vite_glob_0_20$1,"/src/shared/assets/icons/code.svg": __vite_glob_0_21$1,"/src/shared/assets/icons/deepgram-d.svg": __vite_glob_0_22$1,"/src/shared/assets/icons/dribbble.svg": __vite_glob_0_23$1,"/src/shared/assets/icons/easy-button.svg": __vite_glob_0_24$1,"/src/shared/assets/icons/email.svg": __vite_glob_0_25$1,"/src/shared/assets/icons/facebook.svg": __vite_glob_0_26$1,"/src/shared/assets/icons/fast-timer.svg": __vite_glob_0_27$1,"/src/shared/assets/icons/file.svg": __vite_glob_0_28$1,"/src/shared/assets/icons/github.svg": __vite_glob_0_29$1,"/src/shared/assets/icons/globe.svg": __vite_glob_0_30$1,"/src/shared/assets/icons/grad.svg": __vite_glob_0_31$1,"/src/shared/assets/icons/greencheck.svg": __vite_glob_0_32$1,"/src/shared/assets/icons/head-headphones.svg": __vite_glob_0_33$1,"/src/shared/assets/icons/heartbeat.svg": __vite_glob_0_34$1,"/src/shared/assets/icons/info.svg": __vite_glob_0_35$1,"/src/shared/assets/icons/language.svg": __vite_glob_0_36$1,"/src/shared/assets/icons/linkedin.svg": __vite_glob_0_37$1,"/src/shared/assets/icons/mag-glass-soundwave.svg": __vite_glob_0_38$1,"/src/shared/assets/icons/microphone.svg": __vite_glob_0_39$1,"/src/shared/assets/icons/model.svg": __vite_glob_0_40$1,"/src/shared/assets/icons/newspaper.svg": __vite_glob_0_41$1,"/src/shared/assets/icons/open-door.svg": __vite_glob_0_42$1,"/src/shared/assets/icons/question.svg": __vite_glob_0_43$1,"/src/shared/assets/icons/robot-head.svg": __vite_glob_0_44$1,"/src/shared/assets/icons/rocket-bulb.svg": __vite_glob_0_45$1,"/src/shared/assets/icons/rotating-gear.svg": __vite_glob_0_46$1,"/src/shared/assets/icons/scale.svg": __vite_glob_0_47$1,"/src/shared/assets/icons/search.svg": __vite_glob_0_48$1,"/src/shared/assets/icons/server-icon.svg": __vite_glob_0_49$1,"/src/shared/assets/icons/share.svg": __vite_glob_0_50$1,"/src/shared/assets/icons/sparkles.svg": __vite_glob_0_51$1,"/src/shared/assets/icons/star.svg": __vite_glob_0_52$1,"/src/shared/assets/icons/tag.svg": __vite_glob_0_53$1,"/src/shared/assets/icons/twitch.svg": __vite_glob_0_54$1,"/src/shared/assets/icons/twitter.svg": __vite_glob_0_55$1,"/src/shared/assets/icons/warning.svg": __vite_glob_0_56$1,"/src/shared/assets/icons/xmark-large.svg": __vite_glob_0_57$1,"/src/shared/assets/icons/xmark.svg": __vite_glob_0_58$1,"/src/shared/assets/icons/youtube.svg": __vite_glob_0_59$1

});
    if (!(filepath in files)) {
      throw new Error(`${filepath} not found`);
    }
    const root = parse(files[filepath]);
    const svg = root.querySelector("svg");
    const { attributes: attributes2, innerHTML: innerHTML2 } = svg;
    return {
      attributes: attributes2,
      innerHTML: innerHTML2
    };
  }
  const { icon, ...attributes } = Astro2.props;
  const { attributes: baseAttributes, innerHTML } = getSVG(icon);
  const svgAttributes = { ...baseAttributes, ...attributes };
  return renderTemplate`${maybeRenderHead($$result)}<svg${spreadAttributes(svgAttributes)}>${unescapeHTML(innerHTML)}</svg>`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/general/Icon.astro");

var S = Object.defineProperty, h = Object.getOwnPropertySymbols, R = Object.prototype.hasOwnProperty, _ = Object.prototype.propertyIsEnumerable, p = (r, e, t) => e in r ? S(r, e, { enumerable: !0, configurable: !0, writable: !0, value: t }) : r[e] = t, C = (r, e) => {
  for (var t in e || (e = {}))
    R.call(e, t) && p(r, t, e[t]);
  if (h)
    for (var t of h(e))
      _.call(e, t) && p(r, t, e[t]);
  return r;
};
const $ = function(r, e) {
  if (!r)
    return null;
  let t = {};
  for (let o in r) {
    let n = r[o];
    e.indexOf(o) > -1 && n !== null && (t[o] = n);
  }
  return t;
}, O = (r) => r === "email";
var N = {
  nodes: {
    horizontal_rule() {
      return {
        singleTag: "hr"
      };
    },
    blockquote() {
      return {
        tag: "blockquote"
      };
    },
    bullet_list() {
      return {
        tag: "ul"
      };
    },
    code_block(r) {
      return {
        tag: [
          "pre",
          {
            tag: "code",
            attrs: r.attrs
          }
        ]
      };
    },
    hard_break() {
      return {
        singleTag: "br"
      };
    },
    heading(r) {
      return {
        tag: `h${r.attrs.level}`
      };
    },
    image(r) {
      return {
        singleTag: [
          {
            tag: "img",
            attrs: $(r.attrs, ["src", "alt", "title"])
          }
        ]
      };
    },
    list_item() {
      return {
        tag: "li"
      };
    },
    ordered_list() {
      return {
        tag: "ol"
      };
    },
    paragraph() {
      return {
        tag: "p"
      };
    }
  },
  marks: {
    bold() {
      return {
        tag: "b"
      };
    },
    strike() {
      return {
        tag: "strike"
      };
    },
    underline() {
      return {
        tag: "u"
      };
    },
    strong() {
      return {
        tag: "strong"
      };
    },
    code() {
      return {
        tag: "code"
      };
    },
    italic() {
      return {
        tag: "i"
      };
    },
    link(r) {
      const e = C({}, r.attrs), { linktype: t = "url" } = r.attrs;
      return O(t) && (e.href = `mailto:${e.href}`), e.anchor && (e.href = `${e.href}#${e.anchor}`, delete e.anchor), {
        tag: [
          {
            tag: "a",
            attrs: e
          }
        ]
      };
    },
    styled(r) {
      return {
        tag: [
          {
            tag: "span",
            attrs: r.attrs
          }
        ]
      };
    }
  }
};
const P = function(r) {
  const e = {
    "&": "&amp;",
    "<": "&lt;",
    ">": "&gt;",
    '"': "&quot;",
    "'": "&#39;"
  }, t = /[&<>"']/g, o = RegExp(t.source);
  return r && o.test(r) ? r.replace(t, (n) => e[n]) : r;
};
class L {
  constructor(e) {
    e || (e = N), this.marks = e.marks || [], this.nodes = e.nodes || [];
  }
  addNode(e, t) {
    this.nodes[e] = t;
  }
  addMark(e, t) {
    this.marks[e] = t;
  }
  render(e = {}) {
    if (e.content && Array.isArray(e.content)) {
      let t = "";
      return e.content.forEach((o) => {
        t += this.renderNode(o);
      }), t;
    }
    return console.warn("The render method must receive an object with a content field, which is an array"), "";
  }
  renderNode(e) {
    let t = [];
    e.marks && e.marks.forEach((n) => {
      const s = this.getMatchingMark(n);
      s && t.push(this.renderOpeningTag(s.tag));
    });
    const o = this.getMatchingNode(e);
    return o && o.tag && t.push(this.renderOpeningTag(o.tag)), e.content ? e.content.forEach((n) => {
      t.push(this.renderNode(n));
    }) : e.text ? t.push(P(e.text)) : o && o.singleTag ? t.push(this.renderTag(o.singleTag, " /")) : o && o.html && t.push(o.html), o && o.tag && t.push(this.renderClosingTag(o.tag)), e.marks && e.marks.slice(0).reverse().forEach((n) => {
      const s = this.getMatchingMark(n);
      s && t.push(this.renderClosingTag(s.tag));
    }), t.join("");
  }
  renderTag(e, t) {
    return e.constructor === String ? `<${e}${t}>` : e.map((n) => {
      if (n.constructor === String)
        return `<${n}${t}>`;
      {
        let s = `<${n.tag}`;
        if (n.attrs)
          for (let l in n.attrs) {
            let a = n.attrs[l];
            a !== null && (s += ` ${l}="${a}"`);
          }
        return `${s}${t}>`;
      }
    }).join("");
  }
  renderOpeningTag(e) {
    return this.renderTag(e, "");
  }
  renderClosingTag(e) {
    return e.constructor === String ? `</${e}>` : e.slice(0).reverse().map((o) => o.constructor === String ? `</${o}>` : `</${o.tag}>`).join("");
  }
  getMatchingNode(e) {
    if (typeof this.nodes[e.type] == "function")
      return this.nodes[e.type](e);
  }
  getMatchingMark(e) {
    if (typeof this.marks[e.type] == "function")
      return this.marks[e.type](e);
  }
}
var z = (r) => {
  if (typeof r != "object" || typeof r._editable > "u")
    return {};
  const e = JSON.parse(r._editable.replace(/^<!--#storyblok#/, "").replace(/-->$/, ""));
  return {
    "data-blok-c": JSON.stringify(e),
    "data-blok-uid": e.id + "-" + e.uid
  };
};
let j;
const U = (r, e) => {
  r.addNode("blok", (t) => {
    let o = "";
    return t.attrs.body.forEach((n) => {
      o += e(n.component, n);
    }), {
      html: o
    };
  });
}, B = (r, e, t) => {
  let o = t || j;
  if (!o) {
    console.error("Please initialize the Storyblok SDK before calling the renderRichText function");
    return;
  }
  return r === "" ? "" : r ? (e && (o = new L(e.schema), e.resolver && U(o, e.resolver)), o.render(r)) : (console.warn(`${r} is not a valid Richtext object. This might be because the value of the richtext field is empty.
    
  For more info about the richtext object check https://github.com/storyblok/storyblok-js#rendering-rich-text`), "");
};
function F() {
  return globalThis.storyblokApiInstance || console.error("storyblokApiInstance has not been initialized correctly"), storyblokApiInstance;
}
function V(r, e) {
  const t = globalThis.storyblokApiInstance.richTextResolver;
  if (!t) {
    console.error(
      "Please initialize the Storyblok SDK before calling the renderRichText function"
    );
    return;
  }
  return B(r, e, t);
}

const $$Astro$1o = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/global/Alert.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Alert = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$1o, $$props, $$slots);
  Astro2.self = $$Alert;
  const { blok = { type: "info" } } = Astro2.props;
  const { type } = blok;
  let inner;
  if (blok.inner_html) {
    inner = blok.inner_html;
  }
  return renderTemplate`${maybeRenderHead($$result)}<div${addAttribute(`alert ${type} astro-N6XFQBUY`, "class")}${spreadAttributes(z(blok))}>
	<div class="heading astro-N6XFQBUY">
		${renderComponent($$result, "Icon", $$Icon, { "class": `icon h-6 w-6 ${type} astro-N6XFQBUY`, "icon": type })}
	</div>
	<div class="text w-5/6 astro-N6XFQBUY">
		${renderSlot($$result, $$slots["default"], renderTemplate`
			<div class="astro-N6XFQBUY">${unescapeHTML(inner)}</div>
		`)}
	</div>
</div>

`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/global/Alert.astro");

const $$Astro$1n = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/global/CodeEmbed.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$CodeEmbed = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$1n, $$props, $$slots);
  Astro2.self = $$CodeEmbed;
  const { blok } = Astro2.props;
  const { height = "500", width = "100%", src, ...rest } = blok;
  return renderTemplate`${maybeRenderHead($$result)}<div class="code-embed w-full astro-A35YONKS"${spreadAttributes(rest)}${spreadAttributes(z(blok))}>
	<iframe class="aspect-video astro-A35YONKS"${addAttribute(height, "height")}${addAttribute(width, "width")}${addAttribute(src, "src")} loading="lazy"> </iframe>
</div>

`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/global/CodeEmbed.astro");

const $$Astro$1m = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/global/Panel.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Panel = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$1m, $$props, $$slots);
  Astro2.self = $$Panel;
  const { blok = { type: "info", fill: "fill-black", title: "Information" } } = Astro2.props;
  const { type, title, fill } = blok;
  let renderedRichText;
  if (blok.inner_html) {
    renderedRichText = V(blok.inner_html);
  }
  return renderTemplate`${maybeRenderHead($$result)}<div${addAttribute(`panel ${type} astro-LKN7R2SI`, "class")}${spreadAttributes(z(blok))}>
	<div class="heading astro-LKN7R2SI">
		<div class="p-2 astro-LKN7R2SI">
			${renderComponent($$result, "Icon", $$Icon, { "class": `icon h-6 w-6 ${type === "info" ? "fill-casper" : type === "warning" ? "fill-sunflower" : fill} astro-LKN7R2SI`, "icon": type })}
		</div>
		<div class="title astro-LKN7R2SI">
			${title}
		</div>
	</div>

	<div class="text astro-LKN7R2SI">
		${renderSlot($$result, $$slots["default"], renderTemplate`
			<div class="astro-LKN7R2SI">${unescapeHTML(renderedRichText)}</div>
		`)}
	</div>
</div>

`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/global/Panel.astro");

const $$Astro$1l = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/global/Table.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Table = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$1l, $$props, $$slots);
  Astro2.self = $$Table;
  const { blok } = Astro2.props;
  return renderTemplate`${maybeRenderHead($$result)}<table${spreadAttributes(z(blok))}>
	<thead>
		<tr>
			${blok.table.thead.map((th) => renderTemplate`<th>${th.value}</th>`)}
		</tr>
	</thead>
	<tbody>
		${blok.table.tbody.map((tr) => renderTemplate`<tr>
				${tr.body.map((td) => renderTemplate`<th>${td.value}</th>`)}
			</tr>`)}
	</tbody>
</table>`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/global/Table.astro");

async function getSluggedJson(path) {
  let files = /* #__PURE__ */ Object.assign({"../content/blog/authors/abdul-ajetunmobi.json": () => import('./chunks/abdul-ajetunmobi.68e10869.mjs'),"../content/blog/authors/adam-sypniewski.json": () => import('./chunks/adam-sypniewski.14c0c9ed.mjs'),"../content/blog/authors/aimie-ye.json": () => import('./chunks/aimie-ye.1fd72cba.mjs'),"../content/blog/authors/alexa-de-la-torre.json": () => import('./chunks/alexa-de-la-torre.4e76fe25.mjs'),"../content/blog/authors/bekah-hawrot-weigel.json": () => import('./chunks/bekah-hawrot-weigel.bd727c20.mjs'),"../content/blog/authors/brian-barrow.json": () => import('./chunks/brian-barrow.dc49539f.mjs'),"../content/blog/authors/call-tracking-metrics.json": () => import('./chunks/call-tracking-metrics.e3e73a8f.mjs'),"../content/blog/authors/chris-doty.json": () => import('./chunks/chris-doty.e1f90cdd.mjs'),"../content/blog/authors/claudia-ring.json": () => import('./chunks/claudia-ring.df407f56.mjs'),"../content/blog/authors/conner-goodrum.json": () => import('./chunks/conner-goodrum.61e9354b.mjs'),"../content/blog/authors/dan-shafer.json": () => import('./chunks/dan-shafer.771c71e9.mjs'),"../content/blog/authors/duygu-altinok.json": () => import('./chunks/duygu-altinok.75d19b23.mjs'),"../content/blog/authors/ehab-el-ali.json": () => import('./chunks/ehab-el-ali.768ce391.mjs'),"../content/blog/authors/evan-henry.json": () => import('./chunks/evan-henry.1745cf14.mjs'),"../content/blog/authors/greg-holmes.json": () => import('./chunks/greg-holmes.8bff212d.mjs'),"../content/blog/authors/julia-strout.json": () => import('./chunks/julia-strout.88ea0c94.mjs'),"../content/blog/authors/kate-weber.json": () => import('./chunks/kate-weber.1ab37f55.mjs'),"../content/blog/authors/katie-byrne.json": () => import('./chunks/katie-byrne.6ac44433.mjs'),"../content/blog/authors/keith-lam.json": () => import('./chunks/keith-lam.7b4bdfde.mjs'),"../content/blog/authors/kevin-lewis.json": () => import('./chunks/kevin-lewis.3d5c6faa.mjs'),"../content/blog/authors/luke-oliff.json": () => import('./chunks/luke-oliff.dd5920b2.mjs'),"../content/blog/authors/michael-jolley.json": () => import('./chunks/michael-jolley.8a0343c9.mjs'),"../content/blog/authors/morris-gevirtz.json": () => import('./chunks/morris-gevirtz.f5ad9d5a.mjs'),"../content/blog/authors/natalie-rutgers.json": () => import('./chunks/natalie-rutgers.1605de85.mjs'),"../content/blog/authors/nicole-ohanian.json": () => import('./chunks/nicole-ohanian.fab5aa9d.mjs'),"../content/blog/authors/nikola-whallon.json": () => import('./chunks/nikola-whallon.21c3bac8.mjs'),"../content/blog/authors/pankaj-trivedi.json": () => import('./chunks/pankaj-trivedi.ccc7ba5b.mjs'),"../content/blog/authors/ralphette-english.json": () => import('./chunks/ralphette-english.fbe9b6b2.mjs'),"../content/blog/authors/richard-stevenson.json": () => import('./chunks/richard-stevenson.ba324256.mjs'),"../content/blog/authors/ross-oconnell.json": () => import('./chunks/ross-oconnell.4d628528.mjs'),"../content/blog/authors/sam-zegas.json": () => import('./chunks/sam-zegas.ab344f64.mjs'),"../content/blog/authors/sandra-rodgers.json": () => import('./chunks/sandra-rodgers.daef0bac.mjs'),"../content/blog/authors/scott-stephenson.json": () => import('./chunks/scott-stephenson.b6eb7d80.mjs'),"../content/blog/authors/shadi-baqleh.json": () => import('./chunks/shadi-baqleh.f85c9758.mjs'),"../content/blog/authors/shae-burnette.json": () => import('./chunks/shae-burnette.28001891.mjs'),"../content/blog/authors/shir-goldberg.json": () => import('./chunks/shir-goldberg.edc6eebe.mjs'),"../content/blog/authors/tonya-sims.json": () => import('./chunks/tonya-sims.5536ea85.mjs'),"../content/blog/authors/yujian-tang.json": () => import('./chunks/yujian-tang.2b60b153.mjs'),"../content/blog/category/ai-and-engineering.json": () => import('./chunks/ai-and-engineering.7c931afe.mjs'),"../content/blog/category/announcement.json": () => import('./chunks/announcement.455318d5.mjs'),"../content/blog/category/best-practice.json": () => import('./chunks/best-practice.d6f8e90e.mjs'),"../content/blog/category/devlife.json": () => import('./chunks/devlife.1686061e.mjs'),"../content/blog/category/dg-insider.json": () => import('./chunks/dg-insider.67491215.mjs'),"../content/blog/category/identity-and-language.json": () => import('./chunks/identity-and-language.69202bd7.mjs'),"../content/blog/category/linguistics.json": () => import('./chunks/linguistics.82cb6b89.mjs'),"../content/blog/category/product-news.json": () => import('./chunks/product-news.1c007016.mjs'),"../content/blog/category/project-showcase.json": () => import('./chunks/project-showcase.38979593.mjs'),"../content/blog/category/speech-trends.json": () => import('./chunks/speech-trends.94e7f9c1.mjs'),"../content/blog/category/tutorial.json": () => import('./chunks/tutorial.5ecfce05.mjs'),"../content/blog/settings.json": () => import('./chunks/settings.e952b62e.mjs'),"../content/developers/api-specifications/openapi.json": () => import('./chunks/openapi.87ee39ab.mjs'),"../content/settings.json": () => Promise.resolve().then(() => settings$1),"../content/whitepapers/deepgram-whitepaper-how-deepgram-works.json": () => import('./chunks/deepgram-whitepaper-how-deepgram-works.15f2cbf7.mjs'),"../content/whitepapers/deepgram-whitepaper-make-application-voice-ready.json": () => import('./chunks/deepgram-whitepaper-make-application-voice-ready.f6bd3af3.mjs'),"../content/whitepapers/deepgram-whitepaper-newsletter.json": () => import('./chunks/deepgram-whitepaper-newsletter.a776801d.mjs'),"../content/whitepapers/deepgram-whitepaper-state-of-voice-2022.json": () => import('./chunks/deepgram-whitepaper-state-of-voice-2022.cd03fe6c.mjs')});
  files = Object.fromEntries(Object.entries(files).filter(([key]) => key.includes(`../content/${path}`)));
  let items = [];
  for (const path2 in files) {
    const item = await files[path2]();
    const slug = path2.split("/").pop().replace(".json", " ").trim();
    items.push({
      ...item,
      slug
    });
  }
  const dateable = items.every((item) => "date" in item);
  if (dateable) {
    items = items.sort((a, b) => {
      const aDate = new Date(b.date);
      const bDate = new Date(a.date);
      return aDate.getTime() - bDate.getTime();
    });
  }
  return items;
}
async function getUuid() {
  let uuid;
  try {
    const crypto = await import('node:crypto');
    uuid = crypto.randomUUID();
  } catch (err) {
    uuid = Math.floor(Math.random() * 100);
    console.log("crypto support is disabled! make sure you're on at least node 16.7");
  }
  return uuid;
}

var __freeze$4 = Object.freeze;
var __defProp$4 = Object.defineProperty;
var __template$4 = (cooked, raw) => __freeze$4(__defProp$4(cooked, "raw", { value: __freeze$4(raw || cooked.slice()) }));
var _a$4;
const $$Astro$1k = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/promos/AbstractPromo.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$AbstractPromo = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$1k, $$props, $$slots);
  Astro2.self = $$AbstractPromo;
  const { class: classes, strapline, title, backgroundImage, theme } = Astro2.props;
  let themeStyles;
  if (theme) {
    switch (theme) {
      case "black":
        themeStyles = "theme--black";
        break;
      case "white":
        themeStyles = "theme--white";
        break;
      case "red":
        themeStyles = "theme--red";
        break;
    }
  }
  const rawHTMLString = "&nbsp;";
  return renderTemplate(_a$4 || (_a$4 = __template$4(["", '<div class="promo-container astro-N2U5IHVT">\n	<div', "", '>\n		<header class="astro-N2U5IHVT">\n			', '\n			<h2 class="astro-N2U5IHVT">', '</h2>\n		</header>\n		<div class="astro-N2U5IHVT">\n			', "\n		</div>\n	</div>\n</div>\n\n<script>(function(){", '\n	if(strapline === "Newsletter"){\n		document.querySelector(".promo-container h6").remove()\n	}\n	\n})();<\/script>\n\n'])), maybeRenderHead($$result), addAttribute([["promo", classes ? classes : "", themeStyles], "astro-N2U5IHVT"], "class:list"), addAttribute(`background-image: url('${backgroundImage}');`, "style"), strapline ? renderTemplate`<h6 class="uppercase astro-N2U5IHVT">${strapline}</h6>` : renderTemplate`<h6 class="uppercase astro-N2U5IHVT">${unescapeHTML(rawHTMLString)}</h6>`, title, renderSlot($$result, $$slots["default"]), defineScriptVars({ strapline }));
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/promos/AbstractPromo.astro");

var __freeze$3 = Object.freeze;
var __defProp$3 = Object.defineProperty;
var __template$3 = (cooked, raw) => __freeze$3(__defProp$3(cooked, "raw", { value: __freeze$3(raw || cooked.slice()) }));
var _a$3;
const $$Astro$1j = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/global/WhitepaperPromo.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$WhitepaperPromo = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$1j, $$props, $$slots);
  Astro2.self = $$WhitepaperPromo;
  let whitepaper;
  const { whitepaper: whitepaperSlug, outsideBlog, ...rest } = Astro2.props;
  const allWhitepapers = await getSluggedJson("whitepapers");
  if (whitepaperSlug === "latest") {
    whitepaper = allWhitepapers.pop();
  } else {
    whitepaper = allWhitepapers.find((wp) => wp.slug === whitepaperSlug);
  }
  const { theme, title, image, script } = whitepaper;
  const { blok } = Astro2.props;
  return renderTemplate`${renderComponent($$result, "AbstractPromo", $$AbstractPromo, { ...z(blok), "strapline": "White Paper", "outsideBlog": outsideBlog, "backgroundImage": image, "theme": theme, "title": title, ...rest, "class": "astro-VPP6A52K" }, { "default": () => renderTemplate(_a$3 || (_a$3 = __template$3(["<script>", "<\/script>"])), unescapeHTML(script)) })}

`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/global/WhitepaperPromo.astro");

var __freeze$2 = Object.freeze;
var __defProp$2 = Object.defineProperty;
var __template$2 = (cooked, raw) => __freeze$2(__defProp$2(cooked, "raw", { value: __freeze$2(raw || cooked.slice()) }));
var _a$2;
const $$Astro$1i = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/global/WhitepaperPromoSB.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$WhitepaperPromoSB = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$1i, $$props, $$slots);
  Astro2.self = $$WhitepaperPromoSB;
  const { blok } = Astro2.props;
  const sbApi = F();
  const { data } = await sbApi.get(`cdn/stories/${blok.whitepaper.id}`, {
    find_by: "uuid"
  });
  const wp = data.story.content;
  return renderTemplate`${renderComponent($$result, "AbstractPromo", $$AbstractPromo, { ...z(blok), "strapline": wp.strapline ? wp.strapline : null, "backgroundImage": wp.image.filename, "theme": wp.theme, "title": wp.title, "class": "astro-KGGP7AV3" }, { "default": () => renderTemplate(_a$2 || (_a$2 = __template$2(["<script>", "<\/script>"])), unescapeHTML(wp.script)) })}

`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/global/WhitepaperPromoSB.astro");

const $$Astro$1h = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/global/YouTube.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$YouTube = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$1h, $$props, $$slots);
  Astro2.self = $$YouTube;
  const { blok } = Astro2.props;
  return renderTemplate`${maybeRenderHead($$result)}<div class="youtube-container w-full my-4"${spreadAttributes(z(blok))}>
	<iframe height="100%" width="100%" class="youtube aspect-video"${addAttribute(`https://www.youtube.com/embed/${blok.id}`, "src")} frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/global/YouTube.astro");

const { storyblokApi } = storyblokInit({
            accessToken: "DQ5PVW2Hi8ZeawcnLc3nvQtt",
            use: [apiPlugin],
            apiOptions: {"cache":{"clear":"auto","type":"memory"},"region":"us"},
          });
          const storyblokApiInstance$1 = storyblokApi;

global.Alert = $$Alert;
global.CodeEmbed = $$CodeEmbed;
global.Panel = $$Panel;
global.Table = $$Table;
global.WhitepaperPromo = $$WhitepaperPromo;
global.WhitepaperPromoSB = $$WhitepaperPromoSB;
global.YouTube = $$YouTube;
globalThis.storyblokApiInstance = storyblokApiInstance$1;

const __vite_glob_0_0$1 = "<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"422\" height=\"754\" fill=\"none\" viewBox=\"0 0 422 754\">\r\n  <path fill=\"url(#a)\" fill-rule=\"evenodd\" d=\"M213.229 56.668c-175.725 175.703-179.347 460.725.614 640.664L157.168 754C-54.451 542.408-50.115 206.642 156.553 0l56.676 56.668Zm103.858 104.399c-117.932 117.918-115.007 308.314 4.499 427.805l-56.675 56.669c-149.462-149.444-155.061-390.599-4.499-541.142l56.675 56.668Zm104.666 104.169c-60.781 60.774-60.763 162.963.247 223.965l-56.675 56.669c-92.217-92.206-92.424-245.138-.248-337.302l56.676 56.668Z\" clip-rule=\"evenodd\"/>\r\n  <defs>\r\n    <linearGradient id=\"a\" x1=\"393.662\" x2=\"51.194\" y1=\"522.827\" y2=\"421.086\" gradientUnits=\"userSpaceOnUse\">\r\n      <stop offset=\".141\" stop-color=\"#38EDAC\"/>\r\n      <stop offset=\".495\" stop-color=\"#5D6FD0\"/>\r\n      <stop offset=\".99\" stop-color=\"#E61A32\"/>\r\n    </linearGradient>\r\n  </defs>\r\n</svg>\r\n";

const __vite_glob_0_1$1 = "<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 320 512\"><!--! Font Awesome Pro 6.1.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license (Commercial License) Copyright 2022 Fonticons, Inc. --><path d=\"M23.19 32C28.86 32 34.34 34.08 38.59 37.86L312.6 281.4C317.3 285.6 320 291.6 320 297.9C320 310.1 310.1 320 297.9 320H179.8L236.6 433.7C244.5 449.5 238.1 468.7 222.3 476.6C206.5 484.5 187.3 478.1 179.4 462.3L121.2 346L38.58 440.5C34.4 445.3 28.36 448 22.01 448C9.855 448 0 438.1 0 425.1V55.18C0 42.38 10.38 32 23.18 32H23.19z\"/></svg>";

const __vite_glob_0_2$1 = "<svg role=\"img\"\r\n\txmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 227 28\" fill=\"currentColor\" stroke=\"currentColor\">\r\n\t<title>Deepgram</title>\r\n  <path d=\"M62.065 14h-7.668v14h7.668V14Z\"/>\r\n  <path d=\"M33.607 14v3.04c.274 7.299 4.493 10.948 12.656 10.948H58.16v-5.995H46.51a5.257 5.257 0 0 1-2.015-.228 5.193 5.193 0 0 1-1.77-.973 5.456 5.456 0 0 1-1.298-3.784 44.232 44.232 0 0 1-.087-2.998c0-1.222 0-2.21.087-2.955a5.105 5.105 0 0 1 .314-2.032 5.172 5.172 0 0 1 1.092-1.752 5.471 5.471 0 0 1 3.883-1.223h11.509V0H46.512c-4.013 0-7.14.935-9.357 2.817a10.55 10.55 0 0 0-2.653 3.673 10.386 10.386 0 0 0-.851 4.427c-.022.776-.044 1.839-.044 3.082ZM.833 14h7.669V.002H.833v14Z\"/>\r\n  <path d=\"M29.248 10.917c-.119-3.476-1.276-6.219-3.537-8.1C23.451.935 20.401 0 16.387 0H4.662v5.995h11.52a5.471 5.471 0 0 1 3.883 1.223 5.214 5.214 0 0 1 1.089 1.753c.241.65.349 1.34.317 2.03.054.745.087 1.723.087 2.956 0 1.233 0 2.222-.087 2.997a5.414 5.414 0 0 1-1.309 3.838 5.16 5.16 0 0 1-1.764.973 5.223 5.223 0 0 1-2.01.228H4.661v5.995H16.56c8.156 0 12.375-3.65 12.656-10.948V14c0-1.276.054-2.307.032-3.083Zm44.727 11.842a.615.615 0 0 1-.195-.447V5.71a.628.628 0 0 1 .184-.479.595.595 0 0 1 .422-.191h6.847a9.032 9.032 0 0 1 5.992 1.807 6.777 6.777 0 0 1 1.728 2.34c.397.894.594 1.862.576 2.837v3.922c-.173 4.67-2.873 7.005-8.102 7.005h-7.041a.632.632 0 0 1-.411-.191Zm7.301-3.625a3.365 3.365 0 0 0 2.423-.765 3.48 3.48 0 0 0 .833-2.424c0-.5.054-1.137.054-1.924 0-.787 0-1.414-.054-1.892a3.274 3.274 0 0 0-.2-1.3 3.318 3.318 0 0 0-.698-1.124 3.6 3.6 0 0 0-2.488-.776h-2.433v10.205h2.563Zm12.579 3.625a.617.617 0 0 1-.195-.447V5.708a.674.674 0 0 1 .412-.625.626.626 0 0 1 .259-.044h12.233a.65.65 0 0 1 .476.19.647.647 0 0 1 .206.48v2.53a.587.587 0 0 1-.206.456.676.676 0 0 1-.476.181h-8.231v3.253h7.658a.694.694 0 0 1 .487.191.632.632 0 0 1 .194.468v2.328a.638.638 0 0 1-.418.622.695.695 0 0 1-.263.047h-7.658v3.349h8.437a.677.677 0 0 1 .632.412c.034.081.051.17.049.257v2.52a.579.579 0 0 1-.194.457.651.651 0 0 1-.487.18H94.277a.653.653 0 0 1-.422-.201Zm17.879 0a.604.604 0 0 1-.195-.447V5.708a.676.676 0 0 1 .184-.478.603.603 0 0 1 .465-.191h12.288a.677.677 0 0 1 .632.411c.034.082.051.17.049.258v2.53a.579.579 0 0 1-.194.457.701.701 0 0 1-.487.18h-8.221v3.254h7.658a.67.67 0 0 1 .478.19.653.653 0 0 1 .193.469v2.328a.64.64 0 0 1-.195.478.651.651 0 0 1-.476.191h-7.658v3.349h8.437a.658.658 0 0 1 .671.67v2.519a.58.58 0 0 1-.195.457.65.65 0 0 1-.476.18h-12.45a.634.634 0 0 1-.508-.201Zm17.924-.001a.615.615 0 0 1-.195-.446V5.708a.672.672 0 0 1 .174-.479.632.632 0 0 1 .475-.19h7.367a8.376 8.376 0 0 1 5.31 1.509 5.305 5.305 0 0 1 1.489 1.987 5.23 5.23 0 0 1 .415 2.435 5.067 5.067 0 0 1-.419 2.415 5.169 5.169 0 0 1-1.506 1.953 8.647 8.647 0 0 1-5.311 1.446h-2.964v5.538a.641.641 0 0 1-.66.638h-3.742a.633.633 0 0 1-.433-.202Zm7.691-9.63a2.427 2.427 0 0 0 1.676-.553 2.11 2.11 0 0 0 .617-1.627 2.295 2.295 0 0 0-.563-1.615 2.22 2.22 0 0 0-.798-.488 2.253 2.253 0 0 0-.932-.118h-2.921v4.4h2.921Zm12.957 8.27a6.804 6.804 0 0 1-1.703-2.337 6.694 6.694 0 0 1-.569-2.818v-4.678a6.38 6.38 0 0 1 .575-2.752 6.471 6.471 0 0 1 1.708-2.254 9.075 9.075 0 0 1 5.895-1.775 11.07 11.07 0 0 1 4.326.797 6.936 6.936 0 0 1 2.834 2.03 4.4 4.4 0 0 1 1.028 2.54.53.53 0 0 1-.161.406.556.556 0 0 1-.413.158h-3.991a.723.723 0 0 1-.4-.085 1.07 1.07 0 0 1-.281-.266 3.047 3.047 0 0 0-.995-1.233 3.29 3.29 0 0 0-1.947-.51c-2.012 0-3.051 1.063-3.116 3.072v4.422c.065 2.126 1.082 3.253 3.17 3.253a3.613 3.613 0 0 0 2.401-.744 2.772 2.772 0 0 0 .887-2.253v-.426h-2.326a.662.662 0 0 1-.623-.41.622.622 0 0 1-.047-.26v-2.02a.639.639 0 0 1 .194-.478.65.65 0 0 1 .476-.191h6.62a.662.662 0 0 1 .671.67v2.965a6.676 6.676 0 0 1-1.082 3.753 6.56 6.56 0 0 1-2.899 2.402 10.78 10.78 0 0 1-4.326.83 8.844 8.844 0 0 1-5.906-1.808Zm18.487 1.36a.602.602 0 0 1-.195-.447V5.708a.672.672 0 0 1 .184-.479.61.61 0 0 1 .476-.19h7.16a8.502 8.502 0 0 1 5.409 1.53 5.33 5.33 0 0 1 1.484 1.94c.336.75.49 1.567.452 2.386a5.341 5.341 0 0 1-.844 3.05 5.652 5.652 0 0 1-2.369 2l3.516 6.239a.66.66 0 0 1 .086.287.547.547 0 0 1-.162.372.501.501 0 0 1-.39.16h-3.666a1.1 1.1 0 0 1-.658-.17 1.07 1.07 0 0 1-.424-.521l-2.866-5.55h-2.499v5.56a.571.571 0 0 1-.195.457.647.647 0 0 1-.487.18h-3.558a.658.658 0 0 1-.454-.201Zm7.571-9.811a2.384 2.384 0 0 0 1.709-.553 1.989 1.989 0 0 0 .584-1.52 2.12 2.12 0 0 0-.584-1.563 2.318 2.318 0 0 0-1.709-.585h-2.888v4.252l2.888-.031Zm10.826 9.843a.547.547 0 0 1-.162-.371.657.657 0 0 1 0-.234l6.068-16.402a.984.984 0 0 1 .399-.57c.199-.137.441-.199.682-.174h4.067a1.02 1.02 0 0 1 .683.173c.199.137.34.34.399.571l6.068 16.402a.614.614 0 0 1 0 .234.558.558 0 0 1-.173.372c-.1.1-.236.157-.378.16h-3.386a.9.9 0 0 1-.538-.138.875.875 0 0 1-.349-.426l-.941-2.477h-6.814l-.941 2.477a.88.88 0 0 1-.349.426.901.901 0 0 1-.538.137h-3.386a.551.551 0 0 1-.411-.16Zm6.685-6.717h4.629l-2.315-6.506-2.314 6.506Zm15.035 6.697a.58.58 0 0 1-.195-.457V5.709a.658.658 0 0 1 .419-.622.685.685 0 0 1 .263-.048h2.942c.196-.009.39.04.558.14a.97.97 0 0 1 .383.424l4.478 7.876 4.5-7.876a.97.97 0 0 1 .382-.424c.168-.1.363-.149.559-.14h2.942a.648.648 0 0 1 .476.191.638.638 0 0 1 .194.478v16.615a.578.578 0 0 1-.194.457.632.632 0 0 1-.476.181h-3.31a.678.678 0 0 1-.453-.193.65.65 0 0 1-.196-.445v-9.322l-2.856 5.07c-.084.17-.212.314-.372.419-.16.105-.345.166-.536.177h-1.406a1.096 1.096 0 0 1-.543-.175 1.067 1.067 0 0 1-.377-.42l-2.78-5.06v9.311a.578.578 0 0 1-.194.457.657.657 0 0 1-.487.181h-3.245a.66.66 0 0 1-.476-.191Z\"/>\r\n</svg>\r\n";

const __vite_glob_0_3$1 = "<svg role=\"img\"\r\n\txmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 80 37\" fill=\"currentColor\" stroke=\"currentColor\">\r\n\t<title>Deepgram</title>\r\n\t<g>\r\n\t\t<path d=\"M70,18.48V29.06H59.69a6.83,6.83,0,0,1-4.94-1.59c-1.08-1-1.63-2.74-1.71-5.07-.07-1-.11-2.34-.11-4s0-2.94.11-3.91a6.88,6.88,0,0,1,1.83-5A7.28,7.28,0,0,1,60,7.92H75V0H59.68c-5.23,0-9.31,1.25-12.22,3.73S43,9.79,42.87,14.44c0,1,0,2.38,0,4.07s0,3,0,4Q43.43,37,59.41,37H80V18.48Z\"></path>\r\n\t\t<path d=\"M32.54,3.72Q28.19,0,20.32,0H0V18.52H10V7.94h10a7.24,7.24,0,0,1,5.08,1.61,6.94,6.94,0,0,1,1.83,5c.06,1,.11,2.29.11,3.91s0,2.94-.11,4q-.11,3.48-1.7,5.07a6.84,6.84,0,0,1-4.95,1.59H5V37H20.58q16,0,16.54-14.47c0-1,.05-2.32.05-4s0-3.05-.05-4.06Q36.89,7.45,32.54,3.72Z\"></path>\r\n\t</g>\r\n</svg>";

const __vite_glob_0_4$1 = "<svg version=\"1.1\" role=\"img\" preserveAspectRatio=\"none\" xmlns=\"http://www.w3.org/2000/svg\"\r\n\txmlns:xlink=\"http://www.w3.org/1999/xlink\" viewBox=\"0 0 2452 150\">\r\n\t<path d=\"M2452,150c-58.4-10.7-116.4-22.5-173.9-35.3v0C1940.8,39.8,1588.5,0,1226,0C863.5,0,511.2,39.8,173.9,114.7\r\n\tC116.4,127.5,58.4,139.3,0,150H2452z\" />\r\n</svg>";

const __vite_glob_0_5$1 = "\r\n<svg viewBox=\"0 0 121 121\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path d=\"M85.2659 68.6383C85.7759 68.2415 86.3832 67.9894 87.0235 67.9107L120.832 63.705V57.1602L87.0235 52.9545C85.0639 52.711 83.6687 50.9176 83.9123 48.9594C83.9925 48.3191 84.2446 47.7104 84.6399 47.2018L105.572 20.3221L100.944 15.6938L74.0642 36.6261C72.5057 37.8394 70.2511 37.5586 69.0392 36.0016C68.6424 35.4916 68.3903 34.8828 68.3101 34.2425L64.1045 0.431152H57.5596L53.354 34.2397C53.2365 35.1894 52.7552 36.0359 52.0003 36.6232C51.2454 37.2105 50.3072 37.4698 49.3575 37.3509C48.7172 37.2707 48.1084 37.02 47.5998 36.6232L20.7201 15.691L16.0918 20.3192L37.0241 47.199C38.2374 48.7575 37.9566 51.0121 36.3981 52.224C35.8882 52.6208 35.2808 52.8729 34.6405 52.9517L0.832031 57.1587V63.7036L34.6405 67.9092C36.6001 68.1527 37.9953 69.9462 37.7518 71.9057C37.6716 72.546 37.4209 73.1548 37.0241 73.6633L16.0918 100.543L20.7201 105.171L47.5998 84.2391C48.3547 83.6518 49.2916 83.3925 50.2427 83.5114C51.1924 83.6288 52.039 84.1101 52.6263 84.865C53.0231 85.375 53.2752 85.9824 53.354 86.6227L57.5596 120.431H64.1045L68.3101 86.6227C68.5536 84.6645 70.347 83.2693 72.3052 83.5114C72.9469 83.5916 73.5543 83.8437 74.0642 84.2391L100.944 105.171L105.572 100.543L84.6399 73.6633C84.0541 72.9113 83.7962 71.9731 83.9151 71.0219C84.034 70.0708 84.5125 69.2242 85.2659 68.6383Z\"/>\r\n</svg>\r\n\r\n\r\n";

const __vite_glob_0_6$1 = "<svg width=\"336\"  viewBox=\"0 0 336 336\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path d=\"M29.5299 106.375L30.2624 105.176C58.186 56.7275 101.768 20.2476 152.397 0.715687C157.531 0.245283 162.736 0 167.995 0C171.191 0 174.366 0.094081 177.518 0.268803C118.966 16.2558 67.7895 54.9231 36.6805 108.926L36.6334 109.006L35.8438 110.3C35.7463 110.458 35.6254 110.669 35.4305 111.019L35.1281 111.556L35.104 111.6C34.961 111.861 34.8156 112.125 34.5165 112.743L34.4896 112.793C32.6482 116.449 31.3007 120.319 30.4808 124.301C25.4371 149.122 41.5058 173.445 66.3043 178.509C69.5133 179.16 72.7022 179.49 75.7835 179.49C92.8602 179.49 108.613 169.742 116.896 154.054L116.94 153.97C129.907 130.857 149.137 111.966 172.552 99.3394C192.784 88.3588 215.579 82.5527 238.463 82.5527C247.74 82.5527 257.219 83.5237 266.642 85.4322C289.071 90.0052 309.847 100.055 327.257 114.352C328.745 118.767 330.052 123.266 331.175 127.839C313.134 110.172 290.234 97.8307 265.16 92.7168C256.228 90.9057 247.246 89.9884 238.463 89.9884C216.816 89.9884 195.247 95.4821 176.09 105.878C153.922 117.833 135.72 135.705 123.445 157.565C113.868 175.676 95.6089 186.922 75.7835 186.922C72.2082 186.922 68.5221 186.542 64.8224 185.793C36.0051 179.91 17.3356 151.655 23.1992 122.809C24.1502 118.189 25.7093 113.707 27.8397 109.477C28.2396 108.65 28.458 108.257 28.6361 107.941L28.9937 107.301C29.2041 106.924 29.3526 106.658 29.5299 106.375Z\" fill=\"#4F6278\"/>\r\n<path d=\"M42.5237 112.685L41.2434 114.853L40.6923 115.921C39.1231 119.036 37.9773 122.315 37.2918 125.672C33.0041 146.756 46.6332 167.403 67.675 171.701C70.4371 172.259 73.1656 172.544 75.7833 172.544C90.2625 172.544 103.714 164.111 110.901 150.533C124.513 126.29 144.678 106.479 169.228 93.2408C190.488 81.7058 214.43 75.6073 238.462 75.6073C248.2 75.6073 258.153 76.6221 268.032 78.628C287.794 82.6567 306.332 90.7376 322.589 102.108C320.738 97.7668 318.705 93.5197 316.507 89.3768C302.058 80.896 286.201 74.7472 269.514 71.3468C259.145 69.2434 248.698 68.1783 238.462 68.1783C213.193 68.1783 188.025 74.5859 165.69 86.7089C139.893 100.619 118.7 121.445 104.396 146.941L104.352 147.021C98.4581 158.18 87.5138 165.112 75.7833 165.112C73.6596 165.112 71.425 164.88 69.1569 164.42C52.1339 160.942 41.1056 144.229 44.5768 127.164C45.1312 124.452 46.0586 121.794 47.3254 119.281L47.7018 118.538L48.9552 116.418C84.5535 54.6206 147.954 14.1691 217.723 7.48266C211.359 5.51368 204.83 3.9143 198.166 2.70469C133.579 14.5152 76.0958 54.4257 42.5237 112.685Z\" fill=\"#4F6278\"/>\r\n<path d=\"M258.96 127.114C252.488 125.813 245.979 125.155 239.602 125.155H239.393C221.742 125.195 204.423 130.104 189.309 139.354C174.406 148.47 162.273 161.399 154.218 176.745C145.992 191.475 133.714 203.561 118.707 211.702C105.874 218.705 91.4253 222.421 76.9293 222.454C71.0287 222.465 64.9467 221.856 58.8479 220.643C34.6071 215.758 13.3335 200.782 0.376406 179.5C0.725871 184.654 1.30719 189.745 2.11029 194.758C7.37242 201.145 13.4141 206.817 20.1816 211.696C31.3309 219.74 43.8478 225.2 57.3929 227.928C63.9789 229.242 70.5516 229.9 76.9494 229.884C92.6821 229.85 108.354 225.818 122.262 218.228C138.519 209.414 151.822 196.303 160.737 180.32L160.784 180.236C176.157 150.916 206.285 132.658 239.413 132.584H239.605C245.492 132.584 251.51 133.195 257.495 134.395C298.019 142.576 326.591 177.242 328.833 216.722C330.698 210.56 332.22 204.246 333.372 197.805C324.421 163.079 296.396 134.664 258.96 127.107V127.114Z\" fill=\"#4F6278\"/>\r\n<path d=\"M320.082 239.483C320.197 238.972 320.304 238.462 320.408 237.947C329.333 193.561 300.499 150.163 256.13 141.205C250.539 140.083 244.927 139.515 239.423 139.529C208.899 139.599 181.066 156.541 166.775 183.743C157.212 200.863 142.978 214.891 125.599 224.319C110.656 232.467 93.8377 236.795 76.9626 236.832H76.7442C69.9532 236.832 62.9773 236.126 56.0082 234.735C36.5827 230.821 18.807 221.342 4.71422 207.744C5.78949 212.176 7.04286 216.537 8.46088 220.821C21.8749 231.268 37.6277 238.609 54.5465 242.02C62.0802 243.525 69.6205 244.274 76.9794 244.261C95.0911 244.221 113.132 239.58 129.15 230.844C147.776 220.744 163.038 205.691 173.29 187.322L173.338 187.238C186.335 162.454 211.664 147.022 239.44 146.961C244.443 146.951 249.558 147.465 254.662 148.49C295.012 156.635 321.238 196.108 313.12 236.476C311.816 242.947 309.642 249.247 306.665 255.201L306.635 255.258C306.211 256.145 305.852 256.804 305.472 257.499L305.16 258.057L305.091 258.178C304.693 258.88 304.314 259.548 303.916 260.177L303.879 260.238C303.758 260.436 303.637 260.634 303.51 260.853C298.096 270.298 292.132 279.323 285.67 287.904C299.824 274.014 311.531 257.637 320.079 239.48L320.082 239.483Z\" fill=\"#4F6278\"/>\r\n<path d=\"M297.498 257.379L298.07 256.434C298.413 255.89 298.716 255.352 299.157 254.569L299.205 254.482L299.387 254.153C299.673 253.636 299.995 253.051 300.466 252.066C303.157 246.694 305.123 240.992 306.316 235.118C313.675 198.497 289.891 162.693 253.295 155.304C248.708 154.38 244.101 153.913 239.602 153.913H239.457C214.282 153.97 191.251 168.086 179.336 190.749C168.442 210.258 152.242 226.228 132.481 236.943C115.444 246.237 96.2573 251.173 76.9964 251.216C69.1772 251.233 61.1563 250.43 53.1657 248.834C38.6763 245.914 24.9968 240.279 12.7723 232.397C14.4356 236.402 16.2501 240.33 18.2058 244.174C28.6628 249.691 39.9297 253.75 51.704 256.122C60.0911 257.799 68.5152 258.649 76.7444 258.649H77.0133C97.5107 258.602 117.921 253.353 136.032 243.471C157.041 232.081 174.265 215.089 185.848 194.334L185.895 194.25C196.517 174.006 217.048 161.399 239.474 161.349C243.51 161.339 247.676 161.755 251.826 162.592C284.404 169.168 305.58 201.048 299.027 233.65C297.966 238.885 296.208 243.972 293.806 248.77L293.775 248.831C293.389 249.641 293.14 250.091 292.875 250.571L292.72 250.847L292.565 251.122L292.565 251.123C292.222 251.733 291.995 252.135 291.769 252.493L291.114 253.572L291.07 253.646C275.361 281.06 254.756 304.762 230.727 323.901C239.756 320.265 248.395 315.87 256.564 310.793C272.334 295.065 286.158 277.163 297.498 257.382V257.379Z\" fill=\"#4F6278\"/>\r\n<path d=\"M285.926 248.736L285.193 249.943L285.19 249.933C266.235 282.992 239.995 310.507 209.195 330.903C202.434 332.61 195.509 333.903 188.449 334.76C225.509 314.422 257.065 284.057 278.765 246.196L278.812 246.116L279.599 244.822C279.68 244.694 279.773 244.528 279.912 244.28L279.913 244.279L279.984 244.152L280.015 244.096L280.314 243.555L280.325 243.535C280.466 243.279 280.618 243.005 280.922 242.376L280.946 242.322C282.781 238.663 284.118 234.789 284.928 230.807C289.918 205.973 273.795 181.687 248.983 176.681C245.808 176.042 242.649 175.72 239.598 175.72H239.501C222.427 175.76 206.695 185.541 198.449 201.249L198.405 201.333C185.492 224.474 166.301 243.407 142.914 256.088C122.706 267.112 99.9267 272.969 77.0401 273.023H76.7411C67.5609 273.023 58.1825 272.075 48.8578 270.207C42.796 268.987 36.8517 267.365 31.062 265.365C28.5889 261.898 26.2468 258.326 24.0391 254.667C32.433 258.309 41.2368 261.091 50.3263 262.923C59.1637 264.693 68.0549 265.594 76.7444 265.594H77.0267C98.6767 265.547 120.233 260.003 139.366 249.563C161.506 237.561 179.668 219.649 191.893 197.758C201.429 179.627 219.662 168.338 239.487 168.294C243.069 168.287 246.752 168.657 250.455 169.399C279.283 175.219 298.016 203.43 292.22 232.289C291.279 236.909 289.73 241.398 287.609 245.632C287.222 246.439 287.004 246.833 286.831 247.146L286.823 247.161L286.42 247.883C286.231 248.221 286.091 248.473 285.926 248.736Z\" fill=\"#4F6278\"/>\r\n<path d=\"M242.444 222.878L241.688 224.158C210.778 278.093 155.72 313.478 95.1384 319.428C99.4899 321.525 103.949 323.44 108.502 325.164C166.409 315.601 218.18 280.116 248.127 227.878L248.936 226.507L249.074 226.248C249.356 225.67 249.733 224.773 249.924 223.795C251.043 218.234 247.478 212.804 241.977 211.692C241.131 211.521 240.334 211.434 239.605 211.434H239.585C236.786 211.44 232.774 212.72 229.568 218.772C213.345 247.819 189.306 271.558 160.035 287.427C134.604 301.301 105.935 308.67 77.1242 308.737H76.7479C76.5664 308.737 76.385 308.737 76.2035 308.733C80.051 311.247 84.006 313.609 88.0618 315.806C114.389 314.133 140.297 306.664 163.586 293.956C194.101 277.415 219.168 252.654 236.08 222.357L236.12 222.28C236.389 221.769 236.88 220.916 237.495 220.19C238.106 219.467 238.842 218.873 239.602 218.869H239.605C239.716 218.869 240.012 218.879 240.502 218.977C241.991 219.279 242.948 220.781 242.633 222.347C242.616 222.424 242.559 222.622 242.444 222.878Z\" fill=\"#4F6278\"/>\r\n<path d=\"M261.571 233.384L260.523 235.162C233.086 283.019 188.503 317.752 137.299 333.204C132.081 332.243 126.943 331.04 121.906 329.606C176.836 317.009 225.267 281.739 254.084 231.449L254.105 231.415L255.103 229.722L255.24 229.447C255.933 228.069 256.437 226.621 256.739 225.149C258.607 215.852 252.599 206.756 243.345 204.888C242.025 204.623 240.771 204.492 239.572 204.492C236.282 204.498 233.18 205.483 230.452 207.324C227.72 209.162 225.361 211.853 223.563 215.274L223.52 215.355C207.945 243.266 184.837 266.084 156.698 281.339C132.293 294.655 104.769 301.731 77.1109 301.791H76.7447C73.1459 301.791 69.5202 301.667 65.8844 301.429C62.373 298.734 58.9724 295.905 55.6861 292.951C62.7426 293.882 69.7924 294.356 76.748 294.356H77.0941C103.519 294.295 129.816 287.535 153.146 274.807C180.038 260.228 202.118 238.431 217.008 211.766C221.863 202.573 230.287 197.076 239.555 197.052H239.605C241.272 197.052 243.023 197.237 244.814 197.597C258.083 200.278 266.699 213.302 264.024 226.624C263.587 228.748 262.868 230.814 261.88 232.783L261.571 233.384Z\" fill=\"#4F6278\"/>\r\n<path d=\"M274.192 240.286L272.915 242.457V242.453C248.987 284.185 212.804 316.559 170.505 335.98C169.668 335.993 168.832 336 167.995 336C162.861 336 157.78 335.768 152.763 335.318C199.944 317.664 240.616 283.859 266.477 238.737L267.727 236.614L268.103 235.871C269.363 233.354 270.284 230.697 270.832 227.985C274.262 210.909 263.201 194.22 246.167 190.783C243.92 190.332 241.712 190.104 239.605 190.104H239.538C227.807 190.131 216.88 197.086 211.013 208.258L210.966 208.342C196.722 233.868 175.572 254.741 149.809 268.712C127.501 280.882 102.346 287.35 77.0771 287.407C67.1577 287.427 57.0064 286.443 46.9291 284.484C43.9485 281.389 41.0856 278.181 38.347 274.864C41.3612 275.677 44.4055 276.403 47.4835 277.021C57.3693 279.004 67.3324 279.998 77.0603 279.975C101.096 279.921 125.024 273.769 146.258 262.183C170.774 248.891 190.895 229.04 204.454 204.764C211.611 191.169 225.042 182.705 239.521 182.675H239.602C242.199 182.675 244.897 182.95 247.633 183.498C268.684 187.749 282.357 208.362 278.12 229.457C277.441 232.817 276.302 236.1 274.743 239.218L274.192 240.286Z\" fill=\"#4F6278\"/>\r\n<path d=\"M17.4028 98.6508L17.974 97.71C35.1986 67.8158 58.0851 42.2325 84.8192 22.0017C97.1277 14.9759 110.407 9.4587 124.413 5.70891C83.2936 27.1392 48.2497 60.0407 24.3988 101.456L24.3551 101.53L23.6999 102.606C23.4378 103.015 23.1757 103.479 22.7422 104.249L22.5843 104.534C22.3188 105.008 22.0701 105.458 21.6837 106.265L21.6535 106.325C19.2375 111.12 17.47 116.2 16.3981 121.435C9.77168 154.021 30.8739 185.948 63.438 192.597C67.5845 193.441 71.7411 193.871 75.7868 193.871C98.2131 193.871 118.771 181.308 129.44 161.087L129.487 161.003C141.117 140.275 158.378 123.32 179.413 111.977C197.545 102.139 217.972 96.9338 238.466 96.9338C246.783 96.9338 255.294 97.8041 263.772 99.5211C291.154 105.102 315.798 119.789 333.819 140.816C334.585 145.54 335.16 150.331 335.526 155.18C318.275 130.316 291.994 112.86 262.287 106.806C254.303 105.189 246.285 104.366 238.463 104.366C219.202 104.366 200.005 109.258 182.945 118.515C163.163 129.187 146.926 145.12 135.989 164.602C124.023 187.241 100.962 201.303 75.7835 201.303C71.2405 201.303 66.5865 200.826 61.9494 199.882C25.3699 192.413 1.66679 156.554 9.10971 119.947C10.3127 114.077 12.2952 108.381 14.9968 103.012C15.4673 102.034 15.7899 101.45 16.0755 100.936L16.2637 100.6C16.7307 99.7697 17.0432 99.2153 17.4028 98.6508Z\" fill=\"#4F6278\"/>\r\n<path d=\"M335.95 169.077C335.392 168.019 334.767 166.913 333.987 165.609C317.989 138.77 291.349 119.816 260.892 113.606C253.362 112.084 245.815 111.315 238.462 111.315C220.354 111.315 202.3 115.915 186.265 124.614C167.615 134.674 152.319 149.69 142.027 168.039L141.98 168.123C128.925 192.873 103.559 208.252 75.7834 208.252C70.7901 208.252 65.6691 207.724 60.5682 206.686C29.0089 200.241 6.14251 174.638 1.61963 144.471C0.645165 151.423 0.0974468 158.51 0 165.707C10.2622 189.657 31.6937 208.376 59.083 213.97C64.6711 215.106 70.2928 215.684 75.7834 215.684C106.311 215.684 134.181 198.807 148.532 171.634C158.133 154.535 172.4 140.54 189.8 131.152C204.759 123.038 221.587 118.747 238.462 118.747C245.324 118.747 252.374 119.469 259.414 120.891C287.818 126.683 312.673 144.371 327.606 169.423C328.339 170.649 328.917 171.668 329.417 172.625L329.535 172.837C331.343 175.811 333.335 178.65 335.483 181.348C335.798 177.323 335.973 173.26 336 169.161L335.95 169.08V169.077Z\" fill=\"#4F6278\"/>\r\n<path d=\"M60.3095 125.46L60.1717 125.736C59.4762 127.117 58.9688 128.561 58.663 130.03C56.7745 139.324 62.7625 148.433 72.0098 150.324C73.3304 150.593 74.6006 150.731 75.7834 150.731C82.3594 150.731 88.1994 146.813 91.815 139.982L91.8587 139.902C107.497 112.023 130.656 89.2592 158.832 74.0685C183.264 60.8064 210.804 53.794 238.462 53.794C249.652 53.794 261.067 54.9599 272.391 57.2548C282.411 59.2977 292.159 62.2478 301.527 66.0279C304.454 69.855 307.22 73.8097 309.814 77.8854C297.559 71.8307 284.498 67.3114 270.905 64.5394C260.075 62.3419 249.158 61.2264 238.462 61.2264C212.038 61.2264 185.727 67.9263 162.367 80.6071C135.441 95.1258 113.314 116.869 98.3642 143.5C93.4884 152.683 85.0509 158.163 75.7834 158.163C74.1032 158.163 72.3324 157.975 70.5246 157.609C57.2618 154.897 48.6764 141.857 51.3813 128.538C51.8215 126.414 52.544 124.348 53.5386 122.382L53.8511 121.784L54.8995 120.007C92.0772 55.4975 160.438 14.9722 234.061 13.4904C239.447 15.7954 244.689 18.3725 249.77 21.2084C245.922 20.99 242.078 20.8757 238.241 20.8757C165.673 20.8757 97.8836 60.2889 61.331 123.736L60.3095 125.46Z\" fill=\"#4F6278\"/>\r\n<path d=\"M72.9508 132.335L73.7102 131.055C107.753 71.9618 170.797 35.2568 238.241 35.2568C250.714 35.2568 263.271 36.5033 275.583 38.9595C271.524 35.5726 267.303 32.3739 262.935 29.3767C254.729 28.3418 246.467 27.821 238.244 27.821C168.16 27.821 102.652 65.9473 67.2854 127.322L66.4723 128.689L66.3345 128.948C66.0489 129.526 65.6725 130.423 65.4776 131.397C64.3452 136.958 67.9004 142.395 73.3977 143.517C74.2512 143.695 75.0543 143.783 75.7868 143.783C78.5893 143.783 82.6014 142.512 85.8205 136.468C102.111 107.457 126.204 83.7723 155.508 67.9701C180.972 54.1502 209.655 46.8455 238.466 46.8455C250.112 46.8455 261.998 48.0585 273.785 50.4508C280.217 51.7646 286.537 53.4346 292.727 55.4472C289.357 51.7142 285.818 48.1324 282.129 44.7153C279.854 44.1575 277.569 43.6401 275.267 43.173C262.989 40.6799 250.61 39.4165 238.466 39.4165C208.422 39.4165 178.509 47.0337 151.973 61.4348C121.422 77.909 96.2977 102.612 79.3184 132.872L79.2781 132.95C78.7337 133.971 77.3057 136.353 75.7868 136.353C75.6827 136.353 75.387 136.34 74.8896 136.239C73.4011 135.937 72.4468 134.432 72.766 132.866C72.7828 132.788 72.8399 132.594 72.9542 132.338L72.9508 132.335Z\" fill=\"#4F6278\"/>\r\n</svg>\r\n";

const __vite_glob_0_7$1 = "\r\n<svg viewBox=\"0 0 486 487\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M250.018 0.788818V6.24877C250.018 133.245 352.983 236.21 479.979 236.21H485.445V250.583H479.979C352.983 250.583 250.018 353.548 250.018 480.545V486.015H235.645V480.545C235.645 353.548 132.68 250.583 5.68377 250.583H0.21875V236.21H5.68377C132.68 236.21 235.645 133.245 235.645 6.24877V0.788818H250.018Z\"/>\r\n</svg>\r\n";

const __vite_glob_0_8$1 = "  <svg width=\"83\" height=\"62\" viewBox=\"0 0 83 62\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n    <path d=\"M0.832031 36.2744C0.832031 16.2423 17.0397 0 37.0291 0L36.9072 17.785C30.5809 17.785 24.9983 20.9985 21.7045 25.8828H36.9246V62H0.832031V36.2744Z\" fill=\"#FB3640\"/>\r\n    <path d=\"M46.6348 36.2744C46.6348 16.2423 62.8424 0 82.8319 0L82.7099 17.785C76.3837 17.785 70.801 20.9985 67.5072 25.8828H82.7273V62H46.6348V36.2744Z\" fill=\"#FB3640\"/>\r\n  </svg>";

const __vite_glob_0_9$1 = "<svg width=\"53\" height=\"53\" viewBox=\"0 0 53 53\" fill=\"currentColor\" stroke=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\">\r\n  <path d=\"M26.5924 18.2544V1.84814H26.0716V18.2544C26.0716 22.8565 22.3404 26.5877 17.7383 26.5877H1.33203V27.1086H17.7383C22.3404 27.1086 26.0716 30.8398 26.0716 35.4419V51.8482H26.5924V35.4419C26.5924 30.8398 30.3237 27.1086 34.9258 27.1086H51.332V26.5877H34.9258C30.3237 26.5877 26.5924 22.8565 26.5924 18.2544V18.2544Z\" stroke-width=\"2\"/>\r\n</svg>";

const __vite_glob_0_10$1 = "<svg width=\"41\" height=\"41\" viewBox=\"0 0 41 41\" fill=\"currentColor\" xmlns=\"http://www.w3.org/2000/svg\">\r\n  <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M21.0029 0.0483398V0.498546C21.0029 10.9676 29.491 19.4556 39.96 19.4556H40.4101V20.6404H39.96C29.491 20.6404 21.0029 29.1284 21.0029 39.5975V40.0483H19.8181V39.5975C19.8181 29.1284 11.3301 20.6404 0.861078 20.6404H0.410156V19.4556H0.861078C11.3301 19.4556 19.8181 10.9676 19.8181 0.498546V0.0483398H21.0029Z\" />\r\n</svg>";

const __vite_glob_0_11$1 = "<svg width=\"434\" height=\"219\" viewBox=\"0 0 434 219\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M-1.75921e-06 177.755L0 218.001L128.157 218.001C175.998 217.624 214.663 178.73 214.663 130.804C214.663 178.732 253.33 217.627 301.173 218.001L434 218.001L434 177.755L306.299 177.755L410.875 117.384L390.75 82.5303L268.751 152.96L336.731 35.2263L301.874 15.1033L234.79 131.285L234.79 -1.0263e-05L194.54 -8.50361e-06L194.54 123.197L132.126 15.1033L97.269 35.2263L165.073 152.655L43.6017 82.5303L23.4768 117.384L128.052 177.755L-1.75921e-06 177.755Z\" fill=\"url(#paint0_linear_1254_11035)\"/>\r\n<defs>\r\n<linearGradient id=\"paint0_linear_1254_11035\" x1=\"224.021\" y1=\"-7.12682e-05\" x2=\"224.021\" y2=\"251.5\" gradientUnits=\"userSpaceOnUse\">\r\n<stop offset=\"0.338542\" stop-color=\"#5D6FD0\"/>\r\n<stop offset=\"0.75\" stop-color=\"#FE5C5C\"/>\r\n</linearGradient>\r\n</defs>\r\n</svg>\r\n";

const __vite_glob_0_12$1 = "<svg width=\"288\" height=\"289\" viewBox=\"0 0 288 289\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n  <path fill-rule=\"evenodd\" clip-rule=\"evenodd\"\n    d=\"M163.226 1.82275C158.004 1.88471 153.013 3.9866 149.321 7.6792L5.85656 151.143C-1.95392 158.954 -1.95392 171.617 5.85656 179.428L109.026 282.597C116.836 290.407 129.5 290.407 137.31 282.597L280.733 139.174C284.45 135.457 286.554 130.426 286.59 125.169L287.304 20.7257C287.381 9.53392 278.259 0.457581 267.067 0.590395L163.226 1.82275ZM210.227 77.8978C218.582 86.2537 232.13 86.2536 240.486 77.8978C248.841 69.542 248.841 55.9945 240.486 47.6387C232.13 39.2829 218.582 39.2829 210.227 47.6387C201.871 55.9945 201.871 69.542 210.227 77.8978Z\"\n    fill=\"url(#paint0_linear_2508_20037)\" />\n  <defs>\n    <linearGradient id=\"paint0_linear_2508_20037\" x1=\"49.538\" y1=\"223.109\" x2=\"273.824\" y2=\"-1.17641\"\n      gradientUnits=\"userSpaceOnUse\">\n      <stop offset=\"0.203125\" stop-color=\"#38EDAC\" />\n      <stop offset=\"0.496524\" stop-color=\"#9EA9FF\" />\n      <stop offset=\"0.817708\" stop-color=\"#FE5C5C\" />\n    </linearGradient>\n  </defs>\n</svg>";

const __vite_glob_0_13$1 = "<svg width=\"40\" height=\"36\" viewBox=\"0 0 40 36\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path d=\"M10 19.875V3.625C10 1.90625 8.59375 0.5 6.875 0.5H3.125C1.32812 0.5 0 1.90625 0 3.625V19.875C0 21.6719 1.32812 23 3.125 23H6.875C8.59375 23 10 21.6719 10 19.875ZM7.5 19.875C7.5 20.2656 7.1875 20.5 6.875 20.5H3.125C2.73438 20.5 2.5 20.2656 2.5 19.875V3.625C2.5 3.3125 2.73438 3 3.125 3H6.875C7.1875 3 7.5 3.3125 7.5 3.625V19.875ZM37.2656 16.4375C37.5 15.9688 37.5781 15.3438 37.5781 14.7969C37.5781 13 36.5625 11.4375 35.0781 10.5781C35.1562 10.2656 35.2344 9.875 35.2344 9.48438C35.2344 7.76562 34.2969 6.20312 32.8125 5.34375C32.8906 2.6875 30.7031 0.5 28.0469 0.5H23.6719C17.6562 0.5 12.5 5.5 12.5 6.82812C12.5 7.45312 13.0469 8.07812 13.75 8.07812C15.2344 8 17.3438 3 23.6719 3H28.0469C29.2969 3 30.3906 4.09375 30.3906 5.34375C30.3906 5.73438 30.2344 5.8125 30.2344 6.125C30.2344 7.76562 32.7344 7.0625 32.7344 9.48438C32.7344 10.5 32.1875 10.6562 32.1875 11.3594C32.1875 12.0625 32.7344 12.4531 33.2031 12.5312C34.2969 12.7656 35.0781 13.7031 35.0781 14.7969C35.0781 16.2031 34.1406 16.2812 34.1406 17.2188C34.1406 17.8438 34.6875 18.3906 35.3125 18.4688C36.4844 18.5469 37.5 19.5625 37.5 20.7344C37.5 21.9844 36.4062 23 35.1562 23H25.1562C24.4531 23 23.9844 23.5469 23.9844 24.3281C23.9844 24.4844 23.9844 24.7188 24.1406 24.875C25.4688 27.375 26.1719 30.0312 26.1719 30.8125C26.0938 31.75 25.3906 33 23.75 33C22.5 33 22.2656 32.6875 21.5625 30.5C19.8438 25.2656 18.6719 25.0312 14.6094 20.8906C14.375 20.6562 14.0625 20.5 13.75 20.5C13.0469 20.5 12.5 21.0469 12.5 21.75C12.5 23.5469 16.9531 24.5625 19.1406 31.2031C19.8438 33.2344 20.5469 35.5 23.75 35.5C26.7969 35.5 28.5938 33.1562 28.5938 30.8125C28.5938 29.7188 28.125 27.6875 27.1875 25.5H35.1562C37.8125 25.5 40 23.3906 40 20.7344C40 18.8594 38.9062 17.2188 37.2656 16.4375Z\"/>\r\n</svg>\r\n";

const __vite_glob_0_14$1 = "<svg width=\"40\" height=\"36\" viewBox=\"0 0 40 36\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\r\n<path d=\"M6.875 13H3.125C1.32812 13 0 14.4062 0 16.125V32.375C0 34.1719 1.32812 35.5 3.125 35.5H6.875C8.59375 35.5 10 34.1719 10 32.375V16.125C10 14.4062 8.59375 13 6.875 13ZM7.5 32.375C7.5 32.7656 7.1875 33 6.875 33H3.125C2.73438 33 2.5 32.7656 2.5 32.375V16.125C2.5 15.8125 2.73438 15.5 3.125 15.5H6.875C7.1875 15.5 7.5 15.8125 7.5 16.125V32.375ZM40 15.3438C40 12.6875 37.8125 10.5 35.1562 10.5H27.1875C28.0469 8.39062 28.5938 6.35938 28.5938 5.26562C28.5938 2.92188 26.7969 0.5 23.75 0.5C20.5469 0.5 19.8438 2.84375 19.1406 4.79688C17.0312 11.4375 12.5 12.4531 12.5 14.25C12.5 15.0312 13.0469 15.5 13.75 15.5C14.0625 15.5 14.375 15.4219 14.6094 15.1875C18.6719 11.0469 19.8438 10.8125 21.5625 5.57812C22.2656 3.39062 22.5 3 23.75 3C25.3906 3 26.0938 4.32812 26.0938 5.26562C26.0938 6.04688 25.3906 8.70312 24.0625 11.2031C23.9062 11.3594 23.9062 11.5938 23.9062 11.75C23.9062 12.5312 24.5312 13 25.1562 13H35.1562C36.4062 13 37.5 14.0938 37.5 15.3438C37.5 16.5156 36.4844 17.5312 35.3125 17.6094C34.6875 17.6875 34.1406 18.2344 34.1406 18.8594C34.1406 19.7969 35.0781 19.875 35.0781 21.2812C35.0781 22.375 34.2969 23.3125 33.2031 23.5469C32.7344 23.625 32.1875 24.0156 32.1875 24.7188C32.1875 25.4219 32.7344 25.5781 32.7344 26.5938C32.7344 29.0156 30.2344 28.3125 30.2344 29.9531C30.2344 30.2656 30.3906 30.3438 30.3906 30.7344C30.3906 31.9844 29.2969 33 28.0469 33H23.6719C17.2656 33 15.2344 28 13.75 28C13.0469 28 12.5 28.625 12.5 29.25C12.4219 30.5781 17.5 35.5 23.6719 35.5H28.0469C30.7031 35.5 32.8906 33.3906 32.8906 30.7344C34.2969 29.875 35.2344 28.3125 35.2344 26.5938C35.2344 26.2031 35.1562 25.8125 35.0781 25.5C36.5625 24.6406 37.5781 23.0781 37.5781 21.2812C37.5781 20.7344 37.5 20.1094 37.2656 19.6406C38.9062 18.8594 40 17.2188 40 15.3438Z\" />\r\n</svg>\r\n";

const $$Astro$1g = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/general/Svg.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Svg = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$1g, $$props, $$slots);
  Astro2.self = $$Svg;
  function getSVG(name2) {
    const filepath = `/src/shared/assets/images/${name2}.svg`;
    const files = /* #__PURE__ */ Object.assign({"/src/shared/assets/images/amplify.svg": __vite_glob_0_0$1,"/src/shared/assets/images/arrow-pointer-solid.svg": __vite_glob_0_1$1,"/src/shared/assets/images/deepgram-long.svg": __vite_glob_0_2$1,"/src/shared/assets/images/deepgram.svg": __vite_glob_0_3$1,"/src/shared/assets/images/eclipse-divider.svg": __vite_glob_0_4$1,"/src/shared/assets/images/eight-point-star.svg": __vite_glob_0_5$1,"/src/shared/assets/images/fingerprint.svg": __vite_glob_0_6$1,"/src/shared/assets/images/four-point-star.svg": __vite_glob_0_7$1,"/src/shared/assets/images/quotation-mark.svg": __vite_glob_0_8$1,"/src/shared/assets/images/star-left.svg": __vite_glob_0_9$1,"/src/shared/assets/images/star-right.svg": __vite_glob_0_10$1,"/src/shared/assets/images/starburst-gradient.svg": __vite_glob_0_11$1,"/src/shared/assets/images/tag-gradient.svg": __vite_glob_0_12$1,"/src/shared/assets/images/thumbsdown.svg": __vite_glob_0_13$1,"/src/shared/assets/images/thumbsup.svg": __vite_glob_0_14$1

});
    if (!(filepath in files)) {
      throw new Error(`${filepath} not found`);
    }
    const root = parse(files[filepath]);
    const svg = root.querySelector("svg");
    const { attributes: attributes2, innerHTML: innerHTML2 } = svg;
    return {
      attributes: attributes2,
      innerHTML: innerHTML2
    };
  }
  const { name, ...attributes } = Astro2.props;
  const { attributes: baseAttributes, innerHTML } = getSVG(name);
  const svgAttributes = { ...baseAttributes, ...attributes };
  return renderTemplate`${maybeRenderHead($$result)}<svg${spreadAttributes(svgAttributes)}>${unescapeHTML(innerHTML)}</svg>`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/general/Svg.astro");

const $$Astro$1f = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/decoration/ContrastSection.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$ContrastSection = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$1f, $$props, $$slots);
  Astro2.self = $$ContrastSection;
  const { class: classes, background, contrast, topOverlay, bottomOverlay, topDivider, bottomDivider } = Astro2.props;
  return renderTemplate`${maybeRenderHead($$result)}<div${addAttribute([`bg-${contrast}`, { "mb-8 sm:mb-12 md:mb-16 lg:mb-20 xl:mb-24": bottomDivider && bottomOverlay }, classes ? classes : ""], "class:list")}>
	${topDivider && renderTemplate`<div${addAttribute([{ "-mb-4 sm:-mb-8 md:-mb-12 lg:-mb-16 xl:-mb-20": topOverlay }], "class:list")}>
				${renderComponent($$result, "Svg", $$Svg, { "name": topDivider, "class:list": ["block w-full", "h-4 sm:h-8 md:h-12 lg:h-16 xl:h-20", `bg-${background}`, `fill-${contrast}`] })}
			</div>`}
	<div${addAttribute(["relative contrast-content", { "z-[2]": bottomDivider && bottomOverlay }], "class:list")}>
		${renderSlot($$result, $$slots["default"])}
	</div>
	${bottomDivider && renderTemplate`<div${addAttribute([{ "z-[1] -mt-12 sm:-mt-20 md:-mt-28 lg:-mt-36 xl:-mt-60": bottomOverlay }], "class:list")}>
				${renderComponent($$result, "Svg", $$Svg, { "name": bottomDivider, "class:list": ["block w-full", "h-4 sm:h-8 md:h-12 lg:h-16 xl:h-20", `fill-${background}`] })}
			</div>`}
</div>`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/decoration/ContrastSection.astro");

const $$Astro$1e = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/general/InlineIcon.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$InlineIcon = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$1e, $$props, $$slots);
  Astro2.self = $$InlineIcon;
  const { icon, iconSuffix, iconStroke } = Astro2.props;
  const hasSlotContent = await Astro2.slots.has("default");
  return renderTemplate`${icon && !iconSuffix && renderTemplate`${renderComponent($$result, "Icon", $$Icon, { "icon": icon, "class:list": ["inline-icon", { "inline-icon--left": hasSlotContent }, { "stroke-current fill-transparent": iconStroke }, { "fill-current": !iconStroke }] })}`}${renderSlot($$result, $$slots["default"])}${icon && !!iconSuffix && renderTemplate`${renderComponent($$result, "Icon", $$Icon, { "icon": icon, "class:list": ["inline-icon", { "inline-icon--right": hasSlotContent }, { "stroke-current fill-transparent": iconStroke }, { "fill-current": !iconStroke }] })}`}
`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/general/InlineIcon.astro");

const $$Astro$1d = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/general/Button.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Button = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$1d, $$props, $$slots);
  Astro2.self = $$Button;
  const { icon, iconSuffix, iconStroke, type, ...rest } = Astro2.props;
  return renderTemplate`${maybeRenderHead($$result)}<button${addAttribute(type ? type : "button", "type")}${spreadAttributes(rest)}>
	${renderComponent($$result, "InlineIcon", $$InlineIcon, { "icon": icon, "iconSuffix": iconSuffix, "iconStroke": iconStroke }, { "default": () => renderTemplate`${renderSlot($$result, $$slots["default"])}` })}
</button>`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/general/Button.astro");

const $$Astro$1c = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/general/Link.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Link$1 = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$1c, $$props, $$slots);
  Astro2.self = $$Link$1;
  const { icon, iconSuffix, iconStroke, ...rest } = Astro2.props;
  return renderTemplate`${maybeRenderHead($$result)}<a${spreadAttributes(rest)}>
	${renderComponent($$result, "InlineIcon", $$InlineIcon, { "icon": icon, "iconSuffix": iconSuffix, "iconStroke": iconStroke }, { "default": () => renderTemplate`${renderSlot($$result, $$slots["default"])}` })}
</a>`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/general/Link.astro");

const $$Astro$1b = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/nav/MobileNavItem.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$MobileNavItem = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$1b, $$props, $$slots);
  Astro2.self = $$MobileNavItem;
  const { href } = Astro2.props;
  return renderTemplate`${maybeRenderHead($$result)}<div>
	<!-- Current: "bg-gray-100 text-gray-900", Default: "bg-white text-gray-600 hover:bg-gray-50 hover:text-gray-900" -->
	${renderComponent($$result, "Link", $$Link$1, { "href": href, "class": "group w-full flex items-center py-4 px-6 lg:px-10 text-white" }, { "default": () => renderTemplate`${renderSlot($$result, $$slots["default"])}` })}
	<div class="mx-6 lg:mx-10 border-b-2 border-steel"></div>
</div>`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/nav/MobileNavItem.astro");

const $$Astro$1a = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/nav/MobileNavMenu.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$MobileNavMenu = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$1a, $$props, $$slots);
  Astro2.self = $$MobileNavMenu;
  const { name } = Astro2.props;
  return renderTemplate`${maybeRenderHead($$result)}<div x-data="{ submenu: null }" x-on:click.outside="submenu = null">
	<!-- Current: "bg-gray-100 text-gray-900", Default: "bg-white text-gray-600 hover:bg-gray-50 hover:text-gray-900" -->
	<button${addAttribute(`submenu === name ? submenu = null : submenu = name`, "x-on:click")} type="button" class="group w-full flex items-center py-4 px-6 lg:px-10 text-left"${addAttribute(`mobile-submenu-${name}`, "aria-controls")} aria-expanded="submenu === name ? 'true' : 'false'">
		<span class="flex-1">${name}</span>
		<!-- Expanded: "text-gray-400 rotate-90", Collapsed: "text-gray-300" -->
		${renderComponent($$result, "Icon", $$Icon, { "icon": "angle-down", "class": "fill-gray-300 w-[1em] h-[1em]", "x-bind:class": "{ 'rotate-180' : submenu === name }" })}
	</button>
	<!-- Expandable link section, show/hide based on state. -->
	<div class="bg-rock"${addAttribute(`mobile-submenu-${name}`, "id")} x-bind:class="submenu === name ? 'block' : 'hidden'">
		${renderSlot($$result, $$slots["default"])}
	</div>
	<div class="mx-6 lg:mx-10 border-b-2 border-steel"></div>
</div>`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/nav/MobileNavMenu.astro");

const $$Astro$19 = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/nav/MobileNavMenuItem.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$MobileNavMenuItem = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$19, $$props, $$slots);
  Astro2.self = $$MobileNavMenuItem;
  const { icon, href } = Astro2.props;
  return renderTemplate`${renderComponent($$result, "Link", $$Link$1, { "href": href, "icon": icon, "class": "group w-full flex items-center py-4 px-6 lg:px-10 text-white" }, { "default": () => renderTemplate`${renderSlot($$result, $$slots["default"])}${renderComponent($$result, "Icon", $$Icon, { "icon": "arrow-right", "class": "w-[1em] h-[1em] fill-lightIris" })}` })}`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/nav/MobileNavMenuItem.astro");

const $$Astro$18 = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/nav/NavItem.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$NavItem = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$18, $$props, $$slots);
  Astro2.self = $$NavItem;
  const { class: classes, ...rest } = Astro2.props;
  return renderTemplate`${renderComponent($$result, "Link", $$Link$1, { "class": `button button--small button--main-nav button--nav-underline ${classes}`, ...rest }, { "default": () => renderTemplate`${renderSlot($$result, $$slots["default"])}` })}`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/nav/NavItem.astro");

const $$Astro$17 = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/nav/NavMenu.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$NavMenu = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$17, $$props, $$slots);
  Astro2.self = $$NavMenu;
  const { name, gridClass, primary } = Astro2.props;
  return renderTemplate`${maybeRenderHead($$result)}<div class="relative astro-I7BWTR4I"${addAttribute(`{ name: '${name}' }`, "x-data")}>
	${renderComponent($$result, "Button", $$Button, { "icon": "angle-down", "x-on:click": `menu === name ? menu = null : menu = name`, "x-bind:class": "{'button--active': menu === name }", "iconSuffix": true, "class": "button button--small button--main-nav button--nav-underline small-icon font-medium astro-I7BWTR4I" }, { "default": () => renderTemplate`${name}` })}

	<div class="absolute flex -ml-20 mt-[1.2rem] min-h-80 astro-I7BWTR4I" style="display: none;" x-show="menu === name">
		${primary && renderTemplate`<div class="bg-darkCharcoal min-w-[44.75rem] pt-4 sm:p-8 astro-I7BWTR4I">
				${renderSlot($$result, $$slots["primary"])}
			</div>`}
		<div${addAttribute(`bg-ink grid px-10 py-8 gap-8 ${gridClass ? gridClass : "lg:grid-cols-2"} astro-I7BWTR4I`, "class")}>
			${renderSlot($$result, $$slots["default"])}
		</div>
	</div>
</div>

`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/nav/NavMenu.astro");

const $$Astro$16 = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/nav/NavMenuItem.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$NavMenuItem = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$16, $$props, $$slots);
  Astro2.self = $$NavMenuItem;
  const { href, icon, class: classes } = Astro2.props;
  return renderTemplate`${renderComponent($$result, "Link", $$Link$1, { "href": href, "icon": icon, "class:list": [["fill-cloud container draw-underline decoration-lightIris flex text-white items-center gap-2 big-icon", classes], "astro-6HVRJNRW"] }, { "default": () => renderTemplate`${maybeRenderHead($$result)}<span class="astro-6HVRJNRW">${renderSlot($$result, $$slots["default"])}</span>${renderComponent($$result, "Icon", $$Icon, { "icon": "arrow-right", "class": "inner w-[1em] h-[1em] fill-lightIris astro-6HVRJNRW" })}` })}

`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/nav/NavMenuItem.astro");

const $$Astro$15 = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/nav/NavMenuPrimaryItem.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$NavMenuPrimaryItem = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$15, $$props, $$slots);
  Astro2.self = $$NavMenuPrimaryItem;
  const { href, description, icon } = Astro2.props;
  return renderTemplate`${maybeRenderHead($$result)}<h4 class="text-stone max-w-[21.125rem] mb-5 astro-B5FTI6FC">${description}</h4>
<div class="flex flex-col astro-B5FTI6FC">
	${renderComponent($$result, "Link", $$Link$1, { "href": href, "icon": icon, "iconSuffix": true, "class": "draw-underline nudge-icon nudge-icon--right flex text-lightIris decoration-lightIris font-normal text-xl items-center gap-2 big-icon astro-B5FTI6FC" }, { "default": () => renderTemplate`${renderComponent($$result, "Icon", $$Icon, { "icon": "brain-circuit", "class": "w-[1em] h-[1em] astro-B5FTI6FC" })}${renderSlot($$result, $$slots["default"])}` })}
	${renderSlot($$result, $$slots["emoji"])}
	<div class="flex gap-10 border-t-2 border-stone mt-7 pt-6 astro-B5FTI6FC">
		${renderSlot($$result, $$slots["item-one"])}
		${renderSlot($$result, $$slots["item-two"])}
	</div>
</div>

`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/nav/NavMenuPrimaryItem.astro");

const $$Astro$14 = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/layout/Header.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Header = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$14, $$props, $$slots);
  Astro2.self = $$Header;
  const { WWW_DOMAIN, DOCS_DOMAIN, BLOG_DOMAIN, CONSOLE_DOMAIN, STATUS_DOMAIN } = (Object.assign({"BASE_URL":"/","MODE":"production","DEV":false,"PROD":true},{_:process.env._,}));
  return renderTemplate`${maybeRenderHead($$result)}<header x-data="{ menu: null }" class="sticky top-0 z-10 astro-C4UE6NDF">
	<div class="absolute w-full h-screen bg-black/75 text-white astro-C4UE6NDF" style="display: none;" x-show="menu !== null" x-on:click="menu = null"></div>
	<nav class="absolute inset-x-0 bg-almostBlack/80 text-white bg-blur astro-C4UE6NDF">
		<div class="max-w-screen-2xl mx-auto px-6 lg:px-10 flex justify-between items-center xl:justify-start astro-C4UE6NDF">
			<div class="mr-[2.875rem] pl-4 astro-C4UE6NDF">
				${renderComponent($$result, "Link", $$Link$1, { "href": WWW_DOMAIN, "class": "flex astro-C4UE6NDF" }, { "default": () => renderTemplate`<span class="sr-only astro-C4UE6NDF">Deepgram</span>${renderComponent($$result, "Svg", $$Svg, { "name": "deepgram", "class": "my-[1.125rem] h-[1.82rem] md:h-[2.3125rem] w-auto text-fireEngine astro-C4UE6NDF" })}` })}
			</div>
			<div class="flex items-center space-x-5 xl:ml-12 pb-1 xl:hidden astro-C4UE6NDF">
				${renderComponent($$result, "Button", $$Button, { "class": "button button--mini sm:button--small button--secondary astro-C4UE6NDF", "x-on:click": "menu = 'Mobile'", "aria-label": "Mobile Menu" }, { "default": () => renderTemplate`Menu` })}
				${renderComponent($$result, "Link", $$Link$1, { "href": `${CONSOLE_DOMAIN}/signup`, "class": "button button--mini sm:button--small button--primary astro-C4UE6NDF" }, { "default": () => renderTemplate`Sign Up` })}
			</div>
			<div class="hidden xl:flex-1 xl:flex xl:items-center xl:justify-between astro-C4UE6NDF">
				<nav class="flex space-x-[2.1875rem] astro-C4UE6NDF" x-on:click.outside="menu = null">
					<!-- Product -->
					${renderComponent($$result, "NavMenu", $$NavMenu, { "name": "Product", "primary": true, "gridClass": "lg:grid-cols-1 min-w-[18.25rem]", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`<div class="h-[216px] grid gap-y-6 astro-C4UE6NDF">
							${renderComponent($$result, "NavMenuItem", $$NavMenuItem, { "class": "draw-underline decoration-lightIris astro-C4UE6NDF", "href": `${WWW_DOMAIN}/product/model-overview`, "icon": "model" }, { "default": () => renderTemplate`Model Overview` })}
							${renderComponent($$result, "NavMenuItem", $$NavMenuItem, { "href": `${WWW_DOMAIN}/product/languages`, "icon": "globe", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Languages` })}
							${renderComponent($$result, "NavMenuItem", $$NavMenuItem, { "href": `${WWW_DOMAIN}/why-deepgram`, "icon": "question", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Why Deepgram` })}
							${renderComponent($$result, "NavMenuItem", $$NavMenuItem, { "href": `${CONSOLE_DOMAIN}/signup?jump=keys`, "icon": "code", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Free API Key` })}
						</div>`, "primary": () => renderTemplate`${renderComponent($$result, "NavMenuPrimaryItem", $$NavMenuPrimaryItem, { "icon": "arrow-right", "slot": "primary", "href": `${WWW_DOMAIN}/product-overview`, "description": "Deepgram AI Speech Platform", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`<span class="text-white flex astro-C4UE6NDF">Product Overview</span>`, "item-one": () => renderTemplate`<a${addAttribute(`${WWW_DOMAIN}/product/transcription`, "href")} class="container flex flex-col draw-underline text-lightIris decoration-lightIris font-medium text-xl gap-2 big-icon astro-C4UE6NDF">
								<div class="flex astro-C4UE6NDF">
									${renderComponent($$result, "Icon", $$Icon, { "icon": "bullseye", "class": "w-[1em] h-[1em] mr-2 mt-1 astro-C4UE6NDF" })}
									<span class="flex items-center text-white text-xl font-normal astro-C4UE6NDF">Transcription</span>
									${renderComponent($$result, "Icon", $$Icon, { "icon": "arrow-right", "class": "inner w-[1em] ml-[.5em] fill-lightIris astro-C4UE6NDF" })}
								</div>
								<p class="text-casper text-base font-normal astro-C4UE6NDF">
									Automatically transcribe real-time or pre-recorded audio and video into text with AI, plus formatting features for better readability.
								</p>
							</a>`, "item-two": () => renderTemplate`<a${addAttribute(`${WWW_DOMAIN}/product/speech-understanding/`, "href")} class="container flex flex-col draw-underline text-lightIris decoration-lightIris font-medium text-xl gap-2 big-icon astro-C4UE6NDF">
								<div class="flex astro-C4UE6NDF">
									${renderComponent($$result, "Icon", $$Icon, { "icon": "chart", "class": "w-[1em] h-[1em] mr-2 mt-1 astro-C4UE6NDF" })}
									<span class="flex items-center text-white text-xl font-normal astro-C4UE6NDF">Understanding</span>
									${renderComponent($$result, "Icon", $$Icon, { "icon": "arrow-right", "class": "inner w-[1em] ml-[.5em] fill-lightIris astro-C4UE6NDF" })}
								</div>
								<p class="text-casper text-base font-normal astro-C4UE6NDF">
									Natural Language Understanding (NLU) for true voice intelligence. Get features like summarization, sentiment analysis, language detection, and more.
								</p>
							</a>` })}` })}
					<!-- Solutions -->
					${renderComponent($$result, "NavMenu", $$NavMenu, { "name": "Solutions", "gridClass": "lg:grid-cols-1 min-w-[20.375rem]", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`${renderComponent($$result, "NavMenuItem", $$NavMenuItem, { "href": `${WWW_DOMAIN}/built-with-deepgram`, "icon": "sparkles", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Built with Deepgram` })}${renderComponent($$result, "NavMenuItem", $$NavMenuItem, { "href": `${WWW_DOMAIN}/solutions/contact-centers`, "icon": "head-headphones", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Contact Centers` })}${renderComponent($$result, "NavMenuItem", $$NavMenuItem, { "href": `${WWW_DOMAIN}/solutions/speech-analytics`, "icon": "mag-glass-soundwave", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Speech Analytics` })}${renderComponent($$result, "NavMenuItem", $$NavMenuItem, { "href": `${WWW_DOMAIN}/solutions/voicebots`, "icon": "robot-head", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Conversational AI` })}${renderComponent($$result, "NavMenuItem", $$NavMenuItem, { "href": `${WWW_DOMAIN}/deepgram-for-podcast-transcription`, "icon": "microphone", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Podcast Transcription` })}` })}
					<!-- Compare -->
					${renderComponent($$result, "NavMenu", $$NavMenu, { "name": "Compare", "gridClass": "lg:grid-cols-1 min-w-[26.375rem]", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`${renderComponent($$result, "NavMenuItem", $$NavMenuItem, { "href": `${WWW_DOMAIN}/asr-comparison/`, "icon": "bar-chart", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`ASR Comparison Tool` })}${renderComponent($$result, "NavMenuItem", $$NavMenuItem, { "href": `https://offers.deepgram.com/whisper-benchmark-thank-you`, "icon": "scale", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Whisper vs. Deepgram Benchmark` })}${renderComponent($$result, "NavMenuItem", $$NavMenuItem, { "href": `${WWW_DOMAIN}/compare-google-stt-alternatives/`, "icon": "scale", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Compare to Google STT` })}${renderComponent($$result, "NavMenuItem", $$NavMenuItem, { "href": `${WWW_DOMAIN}/compare-amazon-transcribe-api-alternatives/`, "icon": "scale", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Compare to AWS Transcribe` })}${renderComponent($$result, "NavMenuItem", $$NavMenuItem, { "href": `${WWW_DOMAIN}/compare-microsoft-azure-stt-alternatives/`, "icon": "scale", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Compare to Microsoft STT` })}${renderComponent($$result, "NavMenuItem", $$NavMenuItem, { "href": `${WWW_DOMAIN}compare-nuance-dragon-speech-recognition-alternatives/`, "icon": "scale", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Compare to Nuance Dragon` })}${renderComponent($$result, "NavMenuItem", $$NavMenuItem, { "href": `${WWW_DOMAIN}/compare-assembly-ai-speech-to-text-api-alternatives/`, "icon": "scale", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Compare to Assembly AI` })}${renderComponent($$result, "NavMenuItem", $$NavMenuItem, { "href": `${WWW_DOMAIN}/compare-speechmatics-speech-to-text-api-alternatives/`, "icon": "scale", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Compare to Speechmatics` })}` })}
					<!-- Pricing -->
					${renderComponent($$result, "NavItem", $$NavItem, { "href": `${WWW_DOMAIN}/pricing`, "class": "text-white astro-C4UE6NDF" }, { "default": () => renderTemplate`Pricing` })}
					<!-- Docs -->
					${renderComponent($$result, "NavMenu", $$NavMenu, { "name": "Docs", "gridClass": "lg:grid-cols-2 min-w-[36.375rem]", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`${renderComponent($$result, "NavMenuItem", $$NavMenuItem, { "href": DOCS_DOMAIN, "icon": "file", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Documentation` })}${renderComponent($$result, "NavMenuItem", $$NavMenuItem, { "href": `${DOCS_DOMAIN}/on-prem/`, "icon": "server-icon", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`On-Prem Deployment` })}${renderComponent($$result, "NavMenuItem", $$NavMenuItem, { "href": `${DOCS_DOMAIN}/documentation/guides/`, "icon": "book", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Guides` })}${renderComponent($$result, "NavMenuItem", $$NavMenuItem, { "href": `${DOCS_DOMAIN}/api-reference/`, "icon": "code-laptop", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`API Reference` })}${renderComponent($$result, "NavMenuItem", $$NavMenuItem, { "href": `${DOCS_DOMAIN}/documentation/getting-started/`, "icon": "grad", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Tutorials` })}${renderComponent($$result, "NavMenuItem", $$NavMenuItem, { "href": STATUS_DOMAIN, "icon": "heartbeat", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Status` })}${renderComponent($$result, "NavMenuItem", $$NavMenuItem, { "href": `${DOCS_DOMAIN}/sdks-tools/`, "icon": "box", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`SDKs + Tools` })}${renderComponent($$result, "NavMenuItem", $$NavMenuItem, { "href": `${WWW_DOMAIN}/changelog`, "icon": "rotating-gear", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Changelog` })}${renderComponent($$result, "NavMenuItem", $$NavMenuItem, { "href": `https://github.com/orgs/deepgram/discussions`, "class": "github astro-C4UE6NDF", "icon": "github" }, { "default": () => renderTemplate`Community Forum` })}` })}
					<!-- Blog -->
					${renderComponent($$result, "NavItem", $$NavItem, { "href": BLOG_DOMAIN, "class": "text-white astro-C4UE6NDF" }, { "default": () => renderTemplate`Blog` })}
					<!-- About -->
					<!-- <NavMenu name="About" gridClass="lg:grid-cols-1 min-w-[18.25rem]">
						<NavMenuItem href={\`\${WWW_DOMAIN}/about\`} icon="deepgram-d">About Us</NavMenuItem>
						<NavMenuItem href="https://github.com/orgs/deepgram/discussions" icon="github">Community Forum</NavMenuItem>
						<NavMenuItem href={\`\${WWW_DOMAIN}/company/newsroom\`} icon="newspaper">Newsroom</NavMenuItem>
						<NavMenuItem href={\`\${WWW_DOMAIN}/company/careers\`} icon="open-door">Careers</NavMenuItem>
					</NavMenu> -->
				</nav>
				<div class="flex space-x-[1rem] items-center xl:ml-12 pr-4 astro-C4UE6NDF">
					${renderComponent($$result, "Button", $$Button, { "id": "docsearch-button", "icon": "search", "class": "button big-icon button--main-nav astro-C4UE6NDF", "aria-label": "Search" })}
					${renderComponent($$result, "Link", $$Link$1, { "href": `${CONSOLE_DOMAIN}/login`, "class": "button button--small button--main-nav text-white astro-C4UE6NDF" }, { "default": () => renderTemplate`Log In` })}
					${renderComponent($$result, "Link", $$Link$1, { "href": `${WWW_DOMAIN}/contact-us`, "class": "button button--secondary button--small w-[7rem] astro-C4UE6NDF" }, { "default": () => renderTemplate`Contact Us` })}
					${renderComponent($$result, "Link", $$Link$1, { "href": `${CONSOLE_DOMAIN}/signup`, "class": "button button--primary button--small w-[7rem] astro-C4UE6NDF" }, { "default": () => renderTemplate`Sign Up` })}
				</div>
			</div>
		</div>
		<!-- Mobile -->
		<div class="fixed flex flex-col h-screen inset-0 xl:hidden bg-black astro-C4UE6NDF" style="display: none;" x-show="menu === 'Mobile'">
			<div class="relative inset-x-0 bg-almostBlack text-white astro-C4UE6NDF">
				<div class="max-w-screen-2xl mx-auto px-6 flex justify-between items-center xl:justify-start astro-C4UE6NDF">
					<div class="mr-[2.875rem] astro-C4UE6NDF">
						${renderComponent($$result, "Link", $$Link$1, { "href": WWW_DOMAIN, "class": "flex astro-C4UE6NDF" }, { "default": () => renderTemplate`<span class="sr-only astro-C4UE6NDF">Deepgram</span>${renderComponent($$result, "Svg", $$Svg, { "name": "deepgram", "class": "my-[1.125rem] h-[1.82rem] w-auto text-dgRed astro-C4UE6NDF" })}` })}
					</div>
					<div class="flex items-center space-x-5 xl:ml-12 pb-1 xl:hidden astro-C4UE6NDF">
						${renderComponent($$result, "Button", $$Button, { "id": "docsearch-button-mobile", "icon": "search", "class": "button big-icon button--main-nav astro-C4UE6NDF", "aria-label": "Search" })}
						${renderComponent($$result, "Button", $$Button, { "icon": "xmark-large", "class": "button big-icon button--main-nav astro-C4UE6NDF", "x-on:click": "menu = null", "aria-label": "Close Menu" })}
					</div>
				</div>
			</div>
			<div class="grow flex flex-col justify-between overflow-y-scroll astro-C4UE6NDF">
				<div class="bg-darkCharcoal astro-C4UE6NDF">
					<nav class="grid font-bold astro-C4UE6NDF">
						<!-- Product -->
						${renderComponent($$result, "MobileNavMenu", $$MobileNavMenu, { "name": "Product", "primary": true, "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`${renderComponent($$result, "MobileNavMenuItem", $$MobileNavMenuItem, { "href": `${WWW_DOMAIN}/product`, "icon": "star", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`View Product` })}${renderComponent($$result, "MobileNavMenuItem", $$MobileNavMenuItem, { "href": `${WWW_DOMAIN}/product/model-overview`, "icon": "model", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Model Overview` })}${renderComponent($$result, "MobileNavMenuItem", $$MobileNavMenuItem, { "href": `${WWW_DOMAIN}/product/languages`, "icon": "globe", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Languages` })}${renderComponent($$result, "MobileNavMenuItem", $$MobileNavMenuItem, { "href": `${CONSOLE_DOMAIN}/signup?jump=keys`, "icon": "code", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Free API Key` })}${renderComponent($$result, "MobileNavMenuItem", $$MobileNavMenuItem, { "href": `${WWW_DOMAIN}/why-deepgram`, "icon": "question", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Why Deepgram` })}` })}
						<!-- Solutions -->
						${renderComponent($$result, "MobileNavMenu", $$MobileNavMenu, { "name": "Solutions", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`${renderComponent($$result, "MobileNavMenuItem", $$MobileNavMenuItem, { "href": `${WWW_DOMAIN}/built-with-deepgram`, "icon": "sparkles", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Built with Deepgram` })}${renderComponent($$result, "MobileNavMenuItem", $$MobileNavMenuItem, { "href": `${WWW_DOMAIN}/solutions/contact-centers`, "icon": "head-headphones", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Contact Centers` })}${renderComponent($$result, "MobileNavMenuItem", $$MobileNavMenuItem, { "href": `${WWW_DOMAIN}/solutions/speech-analytics`, "icon": "mag-glass-soundwave", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Speech Analytics` })}${renderComponent($$result, "MobileNavMenuItem", $$MobileNavMenuItem, { "href": `${WWW_DOMAIN}/solutions/voicebots`, "icon": "robot-head", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Convo AI` })}${renderComponent($$result, "MobileNavMenuItem", $$MobileNavMenuItem, { "href": `${WWW_DOMAIN}/deepgram-for-podcast-transcription`, "icon": "microphone", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Podcast Transcription` })}` })}
						<!-- Pricing -->
						${renderComponent($$result, "MobileNavItem", $$MobileNavItem, { "href": `${WWW_DOMAIN}/pricing`, "class": "text-white astro-C4UE6NDF" }, { "default": () => renderTemplate`Pricing` })}
						<!-- Docs -->
						${renderComponent($$result, "MobileNavMenu", $$MobileNavMenu, { "name": "Docs", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`${renderComponent($$result, "MobileNavMenuItem", $$MobileNavMenuItem, { "href": DOCS_DOMAIN, "icon": "file", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Documentation` })}${renderComponent($$result, "MobileNavMenuItem", $$MobileNavMenuItem, { "href": `${DOCS_DOMAIN}/on-prem/`, "icon": "server-icon", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`On-Prem Deployment` })}${renderComponent($$result, "MobileNavMenuItem", $$MobileNavMenuItem, { "href": `${DOCS_DOMAIN}/documentation/guides/`, "icon": "code-laptop", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Guides` })}${renderComponent($$result, "MobileNavMenuItem", $$MobileNavMenuItem, { "href": `${DOCS_DOMAIN}/api-reference/`, "icon": "grad", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`API Reference` })}${renderComponent($$result, "MobileNavMenuItem", $$MobileNavMenuItem, { "href": `${DOCS_DOMAIN}/tags/tutorial/`, "icon": "rotating-gear", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Tutorials` })}${renderComponent($$result, "MobileNavMenuItem", $$MobileNavMenuItem, { "href": STATUS_DOMAIN, "icon": "book", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Status` })}${renderComponent($$result, "MobileNavMenuItem", $$MobileNavMenuItem, { "href": `${DOCS_DOMAIN}/sdks-tools/`, "icon": "box", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`SDKs + Tools` })}${renderComponent($$result, "MobileNavMenuItem", $$MobileNavMenuItem, { "href": `${WWW_DOMAIN}/changelog`, "icon": "heartbeat", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Changelog` })}` })}
						<!-- Blog -->
						${renderComponent($$result, "MobileNavItem", $$MobileNavItem, { "href": BLOG_DOMAIN, "class": "text-white astro-C4UE6NDF" }, { "default": () => renderTemplate`Blog` })}
						<!-- About -->
						${renderComponent($$result, "MobileNavMenu", $$MobileNavMenu, { "name": "About", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`${renderComponent($$result, "MobileNavMenuItem", $$MobileNavMenuItem, { "href": `${WWW_DOMAIN}/about`, "icon": "deepgram-d", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`About Us` })}${renderComponent($$result, "MobileNavMenuItem", $$MobileNavMenuItem, { "href": "https://github.com/orgs/deepgram/discussions", "icon": "github", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Community Forum` })}${renderComponent($$result, "MobileNavMenuItem", $$MobileNavMenuItem, { "href": `${WWW_DOMAIN}/company/newsroom`, "icon": "newspaper", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Newsroom` })}${renderComponent($$result, "MobileNavMenuItem", $$MobileNavMenuItem, { "href": `${WWW_DOMAIN}/company/careers`, "icon": "open-door", "class": "astro-C4UE6NDF" }, { "default": () => renderTemplate`Careers` })}` })}
					</nav>
				</div>
			</div>
			<div class="bg-black grid grid-cols-2 grid-rows-2 gap-4 p-6 shadow-menu astro-C4UE6NDF">
				${renderComponent($$result, "Link", $$Link$1, { "href": `${CONSOLE_DOMAIN}/login`, "class": "button button--small button--secondary button--block astro-C4UE6NDF" }, { "default": () => renderTemplate`Log In` })}
				${renderComponent($$result, "Link", $$Link$1, { "href": `${CONSOLE_DOMAIN}/signup`, "class": "button button--small button--tertiary button--block astro-C4UE6NDF" }, { "default": () => renderTemplate`Sign Up` })}
				${renderComponent($$result, "Link", $$Link$1, { "href": `${WWW_DOMAIN}/contact-us`, "class": "col-span-2 button button--small button--primary button--block astro-C4UE6NDF" }, { "default": () => renderTemplate`Contact Us` })}
			</div>
		</div>
	</nav>
	${renderComponent($$result, "DocSearch", null, { "client:only": "preact", "client:component-hydration": "only", "class": "astro-C4UE6NDF", "client:component-path": "/Users/sandrarodgers/web-next/blog/src/shared/components/search/DocSearch", "client:component-export": "default" })}
</header>

`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/layout/Header.astro");

const $$Astro$13 = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/layout/PrimarySection.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$PrimarySection = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$13, $$props, $$slots);
  Astro2.self = $$PrimarySection;
  const { padding } = Astro2.props;
  return renderTemplate`${maybeRenderHead($$result)}<section${addAttribute(`${Astro2.props.class} max-w-screen-2xl mx-auto ${padding ? padding : "px-6"}`, "class")}>
	${renderSlot($$result, $$slots["default"])}
</section>`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/layout/PrimarySection.astro");

const $$Astro$12 = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/decoration/LeftDecoration.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$LeftDecoration = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$12, $$props, $$slots);
  Astro2.self = $$LeftDecoration;
  const { class: classes, icon } = Astro2.props;
  return renderTemplate`${renderComponent($$result, "Svg", $$Svg, { "name": icon, "class": `ml-[20%] mr-auto ${classes ? classes : ""}` })}`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/decoration/LeftDecoration.astro");

const $$Astro$11 = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/decoration/RightDecoration.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$RightDecoration = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$11, $$props, $$slots);
  Astro2.self = $$RightDecoration;
  const { class: classes, icon } = Astro2.props;
  return renderTemplate`${renderComponent($$result, "Svg", $$Svg, { "name": icon, "class": `mr-[5%] ml-auto ${classes ? classes : ""}` })}`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/decoration/RightDecoration.astro");

const $$Astro$10 = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/quotes/AmplifyQuote.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$AmplifyQuote = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$10, $$props, $$slots);
  Astro2.self = $$AmplifyQuote;
  const { theme, type, title, subtitle, class: classes } = Astro2.props;
  return renderTemplate`${maybeRenderHead($$result)}<section${addAttribute(`amplify-quote flex ${classes ? classes : ""} max-w-screen-2xl mx-auto px-0 lg:px-10 astro-T3TZDC2S`, "class")}>
	<div class="flex items-center astro-T3TZDC2S">
		${renderComponent($$result, "Svg", $$Svg, { "name": "amplify", "class": "h-[12rem] sm:h-[18rem] md:h-[24rem] lg:h-[36rem] xl:h-[48rem] w-auto astro-T3TZDC2S" })}
	</div>
	<div class="flex flex-col justify-center pl-2 sm:pl-9 astro-T3TZDC2S">
		${renderComponent($$result, "LeftDecoration", $$LeftDecoration, { "icon": "four-point-star", "class": `w-4 sm:w-5 md:w-6 lg:w-7 xl:w-8 mb-6 sm:mb-10 md:mb-20 lg:mb-30 xl:mb-40 ${theme === "light" ? "fill-black" : "fill-white"} astro-T3TZDC2S` })}

		<!-- {type === "standard" ? <h2>tl;dr</h2> : null} -->
		<h1 class="astro-T3TZDC2S">${title}</h1>
		${type === "standard" ? renderTemplate`<p class="text-base leading-6 sm:text-lg  astro-T3TZDC2S">${subtitle}</p>` : null}
		<div class="flex flex-col xl:flex-row mt-2 sm:mt-6 md:mt-10 items-start xl:items-end astro-T3TZDC2S">
			<div class="astro-T3TZDC2S">
				${renderComponent($$result, "Link", $$Link$1, { "href": "https://console.deepgram.com/signup", "class": `${theme === "light" ? "text-lightIris" : "text-lightIris"} text-xl sm:text-4xl font-bold leading-10 font-favorit underline astro-T3TZDC2S` }, { "default": () => renderTemplate`Start Free
				` })}<span class="text-xl sm:text-4xl astro-T3TZDC2S">💰</span>
			</div>

			${type === "standard" ? renderTemplate`<p class="ml-0 mt-0 flex flex-col sm:mt-2 sm:flex-row xl:mt-0 xl:ml-7 astro-T3TZDC2S">
						Got questions? ${renderComponent($$result, "Link", $$Link$1, { "class": `${theme === "light" ? "text-lightIris" : "text-lightIris"} ml-0 underline sm:ml-1 astro-T3TZDC2S` }, { "default": () => renderTemplate`Talk to an expert.` })}
					</p>` : null}
		</div>
		${renderComponent($$result, "RightDecoration", $$RightDecoration, { "icon": "eight-point-star", "class": "w-4 sm:w-5 md:w-6 lg:w-7 xl:w-8 mt-6 sm:mt-10 md:mt-20 lg:mt-30 xl:mt-40 fill-coral astro-T3TZDC2S" })}
	</div>
</section>

`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/quotes/AmplifyQuote.astro");

const $$Astro$$ = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/layout/Footer.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Footer = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$$, $$props, $$slots);
  Astro2.self = $$Footer;
  const { class: classes, ...rest } = Astro2.props;
  const { WWW_DOMAIN, DOCS_DOMAIN, BLOG_DOMAIN, CONSOLE_DOMAIN, STATUS_DOMAIN } = (Object.assign({"BASE_URL":"/","MODE":"production","DEV":false,"PROD":true},{_:process.env._,}));
  const formUuid = await getUuid();
  return renderTemplate`${maybeRenderHead($$result)}<footer x-data="{
  email: null,
  privacyAgree: false,
  newsletterSignup(e) {
    const form = new FormData(e.target)
    this.email = form.get('email')
    const body = {
      email: form.get('email'),
      page: window.location.href,
      hookId: form.get('hook'),
      honeypot: form.get('bot-field')
    }
      fetch('/.netlify/functions/forms', {
        method: 'POST',
        body: JSON.stringify(body),
      })
      .then(() => {
        this.email = null
      })
      .catch(() => {
        this.error = 'A problem occured submitting your rating.'
      })
    },
}"${addAttribute(`text-white text-base bg-black pt-40 ${classes ? classes : null} astro-3TDT7KK3`, "class")}>
	${renderComponent($$result, "PrimarySection", $$PrimarySection, { "class": "astro-3TDT7KK3" }, { "default": () => renderTemplate`${renderComponent($$result, "AmplifyQuote", $$AmplifyQuote, { "class": "bg-black text-white astro-3TDT7KK3", "type": "standard", "title": "Ready to see Deepgram in Action?", "subtitle": "Get your API key and unlock up to 12,000 minutes in free credit. No card required." })}` })}
	${renderComponent($$result, "PrimarySection", $$PrimarySection, { "class": "astro-3TDT7KK3" }, { "default": () => renderTemplate`<div class="grid grid-cols-1 w-full sm:w-2/3 lg:w-full lg:grid-cols-5 xl:grid-cols-6 gap-10 items-end py-14 astro-3TDT7KK3">
			<div class="lg:col-start-1 lg:col-end-3 flex flex-col space-y-6 astro-3TDT7KK3">
				${renderComponent($$result, "Svg", $$Svg, { "name": "deepgram-long", "class": "text-dgRed w-56 stroke-0 astro-3TDT7KK3" })}
				<p class="astro-3TDT7KK3">Every Voice. Heard and understood.</p>
				<ul class="grid grid-cols-4 sm:grid-cols-8 lg:place-items-center md:place-items-start gap-6 md:gap-12 astro-3TDT7KK3">
					<li class="astro-3TDT7KK3">
						${renderComponent($$result, "Link", $$Link$1, { "href": "https://www.twitch.tv/deepgramdevs", "rel": "noopener noreferrer nofollow", "target": "_blank", "class": "text-storm astro-3TDT7KK3" }, { "default": () => renderTemplate`${renderComponent($$result, "Icon", $$Icon, { "icon": "twitch", "class": "fill-current w-10 h-7 astro-3TDT7KK3" })}` })}
					</li>
					<li class="astro-3TDT7KK3">
						${renderComponent($$result, "Link", $$Link$1, { "href": "http://github.com/deepgram", "rel": "noopener noreferrer nofollow", "target": "_blank", "class": "text-storm astro-3TDT7KK3" }, { "default": () => renderTemplate`${renderComponent($$result, "Icon", $$Icon, { "icon": "github", "class": "fill-current w-10 h-7 astro-3TDT7KK3" })}` })}
					</li>
					<li class="astro-3TDT7KK3">
						${renderComponent($$result, "Link", $$Link$1, { "href": "http://twitter.com/DeepgramAI", "rel": "noopener noreferrer nofollow", "target": "_blank", "class": "text-storm astro-3TDT7KK3" }, { "default": () => renderTemplate`${renderComponent($$result, "Icon", $$Icon, { "icon": "twitter", "class": "fill-current w-10 h-7 astro-3TDT7KK3" })}` })}
					</li>
					<li class="astro-3TDT7KK3">
						${renderComponent($$result, "Link", $$Link$1, { "href": "http://www.linkedin.com/company/deepgram", "rel": "noopener noreferrer nofollow", "target": "_blank", "class": "text-storm astro-3TDT7KK3" }, { "default": () => renderTemplate`${renderComponent($$result, "Icon", $$Icon, { "icon": "linkedin", "class": "fill-current w-10 h-7 astro-3TDT7KK3" })}` })}
					</li>
					<li class="astro-3TDT7KK3">
						${renderComponent($$result, "Link", $$Link$1, { "href": "http://www.youtube.com/channel/UCZD2IkWTzZvajgiy9CjNfxA", "rel": "noopener noreferrer nofollow", "target": "_blank", "class": "text-storm astro-3TDT7KK3" }, { "default": () => renderTemplate`${renderComponent($$result, "Icon", $$Icon, { "icon": "youtube", "class": "fill-current w-10 h-7 astro-3TDT7KK3" })}` })}
					</li>
					<li class="astro-3TDT7KK3">
						${renderComponent($$result, "Link", $$Link$1, { "href": "http://www.facebook.com/deepgram/", "rel": "noopener noreferrer nofollow", "target": "_blank", "class": "text-storm astro-3TDT7KK3" }, { "default": () => renderTemplate`${renderComponent($$result, "Icon", $$Icon, { "icon": "facebook", "class": "fill-current w-10 h-7 astro-3TDT7KK3" })}` })}
					</li>
					<li class="astro-3TDT7KK3">
						${renderComponent($$result, "Link", $$Link$1, { "href": "http://dribbble.com/deepgram", "rel": "noopener noreferrer nofollow", "target": "_blank", "class": "text-storm astro-3TDT7KK3" }, { "default": () => renderTemplate`${renderComponent($$result, "Icon", $$Icon, { "icon": "dribbble", "class": "fill-current w-10 h-7 astro-3TDT7KK3" })}` })}
					</li>
				</ul>
			</div>
			<div class="lg:col-start-4 lg:col-end-6 xl:col-start-5 xl:col-end-7 space-y-2 astro-3TDT7KK3">
				<h5 class="astro-3TDT7KK3">Get news and product updates.</h5>
				<form name="newsletter" method="POST" @submit.prevent="newsletterSignup" class="astro-3TDT7KK3">
					<input class="hidden astro-3TDT7KK3" aria-label="Don't fill this in if you're a human." name="bot-field">
					<div class="flex flex-col space-y-4 astro-3TDT7KK3">
						<div class="input-group flex-row items-center space-x-1 astro-3TDT7KK3">
							<label class="flex items-center astro-3TDT7KK3"${addAttribute(`terms-${formUuid}`, "for")}>
								<input class="mr-1 astro-3TDT7KK3"${addAttribute(`terms-${formUuid}`, "id")} type="checkbox" x-model="privacyAgree"> I’m good with the</label>
							${renderComponent($$result, "Link", $$Link$1, { "href": "https://www.iubenda.com/privacy-policy/88905781", "target": "_blank", "class": "text-lightIris astro-3TDT7KK3" }, { "default": () => renderTemplate`Privacy Policy` })}
						</div>

						<div class="form--inline flex astro-3TDT7KK3">
							<div class="input-group astro-3TDT7KK3">
								<input type="text" name="email"${addAttribute(`email-${formUuid}`, "id")} x-model="email" placeholder="you@work-email.com" class="astro-3TDT7KK3">
								<input type="hidden" name="hook" value="blj5az4" class="astro-3TDT7KK3">
							</div>
							${renderComponent($$result, "Button", $$Button, { "type": "submit", ":disabled": "!privacyAgree", "value": "test", "class": "button button--secondary button--small astro-3TDT7KK3" }, { "default": () => renderTemplate`Submit` })}
						</div>
					</div>
				</form>
			</div>
		</div>` })}
	<div class="bg-darkCharcoal astro-3TDT7KK3">
		${renderComponent($$result, "PrimarySection", $$PrimarySection, { "class": "py-10 flex flex-col space-y-10 astro-3TDT7KK3" }, { "default": () => renderTemplate`<div class="grid grid-cols-2 lg:grid-cols-5 xl:grid-cols-6 gap-10 astro-3TDT7KK3">
				<ul class="space-y-3 leading-tight astro-3TDT7KK3">
					<li class="uppercase font-bold astro-3TDT7KK3">Product</li>
					<li class="astro-3TDT7KK3"><a${addAttribute(`${WWW_DOMAIN}/product`, "href")} class="astro-3TDT7KK3">Product</a></li>
					<li class="astro-3TDT7KK3"><a${addAttribute(`${WWW_DOMAIN}/product/model-overview`, "href")} class="astro-3TDT7KK3">Model Overview</a></li>
					<li class="astro-3TDT7KK3"><a${addAttribute(`${WWW_DOMAIN}/product/languages`, "href")} class="astro-3TDT7KK3">Languages</a></li>
					<li class="astro-3TDT7KK3"><a${addAttribute(`${WWW_DOMAIN}/why-deepgram`, "href")} class="astro-3TDT7KK3">Why Deepgram</a></li>
					<li class="astro-3TDT7KK3"><a${addAttribute(`${CONSOLE_DOMAIN}/signup?jump=keys`, "href")} class="astro-3TDT7KK3">Free API Key</a></li>
					<li class="astro-3TDT7KK3"><a${addAttribute(`${WWW_DOMAIN}/pricing`, "href")} class="astro-3TDT7KK3">Pricing</a></li>
				</ul>
				<ul class="space-y-3 leading-tight astro-3TDT7KK3">
					<li class="uppercase font-bold astro-3TDT7KK3">Solutions</li>
					<li class="astro-3TDT7KK3"><a${addAttribute(`${WWW_DOMAIN}/built-with-deepgram`, "href")} class="astro-3TDT7KK3">Built With Deepgram</a></li>
					<li class="astro-3TDT7KK3"><a${addAttribute(`${WWW_DOMAIN}/solutions/contact-centers`, "href")} class="astro-3TDT7KK3">Contact Centers</a></li>
					<li class="astro-3TDT7KK3"><a${addAttribute(`${WWW_DOMAIN}/solutions/speech-analytics`, "href")} class="astro-3TDT7KK3">Speech Analytics</a></li>
					<li class="astro-3TDT7KK3"><a${addAttribute(`${WWW_DOMAIN}/solutions/voicebots`, "href")} class="astro-3TDT7KK3">Conversational AI</a></li>
					<li class="astro-3TDT7KK3"><a${addAttribute(`${WWW_DOMAIN}/deepgram-for-podcast-transcription`, "href")} class="astro-3TDT7KK3">Podcast Transcription</a></li>
				</ul>
				<ul class="space-y-3 leading-tight astro-3TDT7KK3">
					<li class="uppercase font-bold astro-3TDT7KK3">Docs</li>
					<li class="astro-3TDT7KK3"><a${addAttribute(DOCS_DOMAIN, "href")} class="astro-3TDT7KK3">Documentation</a></li>
					<li class="astro-3TDT7KK3"><a${addAttribute(`${DOCS_DOMAIN}/documentation/guides`, "href")} class="astro-3TDT7KK3">Guides</a></li>
					<li class="astro-3TDT7KK3"><a${addAttribute(`${DOCS_DOMAIN}/documentation/getting-started`, "href")} class="astro-3TDT7KK3">Tutorials</a></li>
					<li class="astro-3TDT7KK3"><a${addAttribute(`${DOCS_DOMAIN}/api-reference`, "href")} class="astro-3TDT7KK3">API Reference</a></li>
					<li class="astro-3TDT7KK3"><a${addAttribute(`${DOCS_DOMAIN}/on-prem/`, "href")} class="astro-3TDT7KK3">On-Prem Deployment</a></li>
					<li class="astro-3TDT7KK3"><a${addAttribute(`${DOCS_DOMAIN}/sdks-tools`, "href")} class="astro-3TDT7KK3">SDKs + Tools</a></li>
					<li class="astro-3TDT7KK3"><a${addAttribute(STATUS_DOMAIN, "href")} class="astro-3TDT7KK3">Status</a></li>
					<li class="astro-3TDT7KK3"><a${addAttribute(`${WWW_DOMAIN}/changelog`, "href")} class="astro-3TDT7KK3">Changelog</a></li>
					<li class="astro-3TDT7KK3"><a${addAttribute(`${DOCS_DOMAIN}/support/`, "href")} class="astro-3TDT7KK3">Support</a></li>
				</ul>
				<ul class="space-y-3 leading-tight astro-3TDT7KK3">
					<li class="uppercase font-bold astro-3TDT7KK3">About</li>
					<li class="astro-3TDT7KK3"><a${addAttribute(`${WWW_DOMAIN}/about`, "href")} class="astro-3TDT7KK3">About</a></li>
					<li class="astro-3TDT7KK3"><a${addAttribute(BLOG_DOMAIN, "href")} class="astro-3TDT7KK3">Blog</a></li>
					<li class="astro-3TDT7KK3"><a href="https://github.com/orgs/deepgram/discussions" class="astro-3TDT7KK3">Community Forum</a></li>
					<li class="astro-3TDT7KK3"><a${addAttribute(`${WWW_DOMAIN}/company/newsroom`, "href")} class="astro-3TDT7KK3">Newsroom</a></li>
					<li class="astro-3TDT7KK3">
						<a${addAttribute(`${WWW_DOMAIN}/company/careers`, "href")} class="astro-3TDT7KK3">Careers</a><br class="astro-3TDT7KK3"><span class="text-lightIris font-bold astro-3TDT7KK3">We're Hiring! ✨</span>
					</li>
					<li class="astro-3TDT7KK3"><a${addAttribute(`${WWW_DOMAIN}/contact-us`, "href")} class="astro-3TDT7KK3">Contact Us</a></li>
					<li class="astro-3TDT7KK3"><a${addAttribute(`${WWW_DOMAIN}/data-security`, "href")} class="astro-3TDT7KK3">Security</a></li>
				</ul>
				<ul class="space-y-3 leading-tight astro-3TDT7KK3">
					<li class="uppercase font-bold astro-3TDT7KK3">Compare</li>
					<li class="astro-3TDT7KK3"><a${addAttribute(`${WWW_DOMAIN}/compare-google-stt-alternatives`, "href")} class="astro-3TDT7KK3">Google STT Alternative</a></li>
					<li class="astro-3TDT7KK3"><a${addAttribute(`https://offers.deepgram.com/whisper-benchmark-thank-you`, "href")} class="astro-3TDT7KK3">Whisper vs. Deepgram Benchmark</a></li>
					<li class="astro-3TDT7KK3"><a${addAttribute(`${WWW_DOMAIN}/compare-amazon-transcribe-api-alternatives`, "href")} class="astro-3TDT7KK3">Amazon Transcribe STT Alternative</a></li>
					<li class="astro-3TDT7KK3"><a${addAttribute(`${WWW_DOMAIN}/compare-microsoft-azure-stt-alternatives`, "href")} class="astro-3TDT7KK3">Microsoft Azure STT Alternative</a></li>
					<li class="astro-3TDT7KK3"><a${addAttribute(`${WWW_DOMAIN}/compare-nuance-dragon-speech-recognition-alternatives`, "href")} class="astro-3TDT7KK3">Nuance Dragon STT Alternative</a></li>
					<li class="astro-3TDT7KK3"><a${addAttribute(`${WWW_DOMAIN}/compare-assembly-ai-speech-to-text-api-alternatives`, "href")} class="astro-3TDT7KK3">AssemblyAI STT Alternative</a></li>
					<li class="astro-3TDT7KK3"><a${addAttribute(`${WWW_DOMAIN}/compare-speechmatics-speech-to-text-api-alternatives`, "href")} class="astro-3TDT7KK3">Speechmatics STT Alternative</a></li>
					<li class="astro-3TDT7KK3"><a${addAttribute(`${WWW_DOMAIN}/asr-comparison`, "href")} class="astro-3TDT7KK3">ASR Comparison Tool</a></li>
				</ul>
			</div><p class="text-stone astro-3TDT7KK3">
				<a class="text-stone astro-3TDT7KK3"${addAttribute(`${WWW_DOMAIN}/business/Business_TOS.pdf`, "href")}>Terms</a> | <a class="text-stone astro-3TDT7KK3"${addAttribute(`${WWW_DOMAIN}/privacy`, "href")}>Privacy</a> | Copyright &copy;2022
			</p>` })}
	</div>
</footer>

`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/layout/Footer.astro");

const $$Astro$_ = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/meta/Title.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Title = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$_, $$props, $$slots);
  Astro2.self = $$Title;
  const { title, slot: unused1, class: unused2, ...rest } = Astro2.props;
  return renderTemplate`<title${spreadAttributes(rest)}>${title}</title>
`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/meta/Title.astro");

const $$Astro$Z = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/meta/Meta.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Meta = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$Z, $$props, $$slots);
  Astro2.self = $$Meta;
  const { content, slot: unused1, class: unused2, ...rest } = Astro2.props;
  return renderTemplate`<meta${spreadAttributes(rest)}${addAttribute(content, "content")}>
`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/meta/Meta.astro");

const $$Astro$Y = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/meta/Description.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Description = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$Y, $$props, $$slots);
  Astro2.self = $$Description;
  const { content, slot: unused1, class: unused2, ...rest } = Astro2.props;
  return renderTemplate`${renderComponent($$result, "Meta", $$Meta, { "name": "description", "content": content, ...rest })}`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/meta/Description.astro");

var __freeze$1 = Object.freeze;
var __defProp$1 = Object.defineProperty;
var __template$1 = (cooked, raw) => __freeze$1(__defProp$1(cooked, "raw", { value: __freeze$1(raw || cooked.slice()) }));
var _a$1;
const $$Astro$X = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/meta/ScriptSrc.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$ScriptSrc = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$X, $$props, $$slots);
  Astro2.self = $$ScriptSrc;
  const { src, slot: unused1, class: unused2, ...rest } = Astro2.props;
  return renderTemplate(_a$1 || (_a$1 = __template$1(['<script type="text/javascript"', "", ">\n	// nothing\n<\/script>\n"])), addAttribute(src, "src"), spreadAttributes(rest));
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/meta/ScriptSrc.astro");

const $$Astro$W = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/meta/Link.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Link = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$W, $$props, $$slots);
  Astro2.self = $$Link;
  const { rel, href, slot: unused1, class: unused2, ...rest } = Astro2.props;
  return renderTemplate`<link${addAttribute(rel, "rel")}${addAttribute(href, "href")}${spreadAttributes(rest)}>
`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/meta/Link.astro");

const company = {
	name: "Deepgram",
	legalName: "Deepgram, Inc.",
	url: "http://www.deepgram.com",
	foundingDate: "2015"
};
const logo = {
	url: "https://github.com/deepgram/.github/raw/main/profile/dg-logo.png",
	width: "223",
	height: "28"
};
const founders = [
	"Scott Stephenson",
	"Noah Shutty"
];
const address = {
	streetAddress: "548 Market St. Suite 25104",
	addressLocality: "San Francisco",
	addressRegion: "California",
	postalCode: "94104",
	addressCountry: "USA"
};
const contacts = [
	{
		contactType: "Main",
		telephone: "+833-333-7472"
	},
	{
		contactType: "Careers",
		email: "people@deepgram.com"
	},
	{
		contactType: "Newsroom",
		email: "press@deepgram.com"
	},
	{
		contactType: "Website",
		url: "https://deepgram.com/contact-us/"
	}
];
const urls = [
	"https://www.facebook.com/deepgram",
	"https://www.linkedin.com/company/deepgram",
	"https://twitter.com/DeepgramAI",
	"https://www.ycombinator.com/companies/deepgram",
	"https://www.crunchbase.com/organization/deepgram",
	"https://github.com/deepgram",
	"https://www.cbinsights.com/company/lexika",
	"https://deepgram.com",
	"https://dpgr.am",
	"https://www.glassdoor.co.uk/Overview/Working-at-Deepgram-EI_IE1404455.11,19.htm",
	"https://jobs.lever.co/deepgram"
];
const settings = {
	company: company,
	logo: logo,
	founders: founders,
	address: address,
	contacts: contacts,
	urls: urls
};

const settings$1 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  company,
  logo,
  founders,
  address,
  contacts,
  urls,
  default: settings
}, Symbol.toStringTag, { value: 'Module' }));

var __freeze = Object.freeze;
var __defProp = Object.defineProperty;
var __template = (cooked, raw) => __freeze(__defProp(cooked, "raw", { value: __freeze(raw || cooked.slice()) }));
var _a;
const $$Astro$V = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/meta/JsonLD.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$JsonLD = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$V, $$props, $$slots);
  Astro2.self = $$JsonLD;
  const { company, logo, founders, address, contacts, urls } = settings;
  const siteSchema = [
    {
      "@context": "https://schema.org",
      "@type": "Organization",
      ...company,
      logo: {
        "@type": "ImageObject",
        ...logo
      },
      founders: founders.map((founder) => ({
        "@type": "Person",
        name: founder
      })),
      address: {
        "@type": "PostalAddress",
        ...address
      },
      contactPoint: contacts.map((contact) => ({
        "@type": "ContactPoint",
        ...contact
      })),
      sameAs: urls
    }
  ];
  const { schema } = Astro2.props;
  if (schema) {
    if (Array.isArray(schema)) {
      siteSchema.push(...schema);
    } else {
      siteSchema.push(schema);
    }
  }
  return renderTemplate(_a || (_a = __template(['<script type="application/ld+json">', "<\/script>\n"])), unescapeHTML(JSON.stringify(siteSchema)));
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/meta/JsonLD.astro");

const $$Astro$U = createAstro("/Users/sandrarodgers/web-next/blog/src/layouts/Default.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Default = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$U, $$props, $$slots);
  Astro2.self = $$Default;
  return renderTemplate`<html class="no-js" lang="en" dir="ltr">
	<head>
		<!-- Google Tag Manager -->
		${maybeRenderHead($$result)}
		<!-- End Google Tag Manager -->

		<meta charset="utf-8">

		${renderSlot($$result, $$slots["head:title"], renderTemplate`
			${renderComponent($$result, "Title", $$Title, { "title": "Deepgram Blog \u26A1\uFE0F" })}
		`)}
		${renderSlot($$result, $$slots["head:description"], renderTemplate`
			${renderComponent($$result, "Description", $$Description, { "name": "description", "content": "Deepgram Automatic Speech Recognition helps you build voice applications with better, faster, more economical transcription at scale." })}
		`)}
		${renderSlot($$result, $$slots["head:canonical"], renderTemplate`
			${renderComponent($$result, "Link", $$Link, { "rel": "canonical", "href": Astro2.url })}
		`)}

		${renderComponent($$result, "Link", $$Link, { "rel": "icon", "type": "image/ico", "href": "/favicon.ico" })}
		${renderComponent($$result, "Link", $$Link, { "rel": "sitemap", "href": "/sitemap-index.xml" })}
		${renderComponent($$result, "Link", $$Link, { "rel": "alternative", "type": "application/rss+xml", "href": "/rss.xml", "title": "RSS Feed" })}
		${renderSlot($$result, $$slots["rss"])}

		${renderSlot($$result, $$slots["og:locale"], renderTemplate`
			${renderComponent($$result, "Meta", $$Meta, { "property": "og:locale", "content": "en_US" })}
		`)}
		${renderSlot($$result, $$slots["og:type"], renderTemplate`
			${renderComponent($$result, "Meta", $$Meta, { "property": "og:type", "content": "website" })}
		`)}
		${renderSlot($$result, $$slots["og:title"], renderTemplate`
			${renderComponent($$result, "Meta", $$Meta, { "property": "og:title", "content": "Deepgram Blog \u26A1\uFE0F" })}
		`)}
		${renderSlot($$result, $$slots["og:description"], renderTemplate`
			${renderComponent($$result, "Meta", $$Meta, { "property": "og:description", "content": "Deepgram Automatic Speech Recognition helps you build voice applications with better, faster, more economical transcription at scale." })}
		`)}
		${renderSlot($$result, $$slots["og:url"], renderTemplate`
			${renderComponent($$result, "Meta", $$Meta, { "property": "og:url", "content": Astro2.url })}
		`)}
		${renderSlot($$result, $$slots["og:site_name"], renderTemplate`
			${renderComponent($$result, "Meta", $$Meta, { "property": "og:site_name", "content": "Deepgram" })}
		`)}
		${renderSlot($$result, $$slots["og:image"], renderTemplate`
			${renderComponent($$result, "Meta", $$Meta, { "property": "og:image", "itemprop": "image", "content": `${Astro2.url.origin}/build-something-great-with-voice.png` })}
		`)}
		${renderSlot($$result, $$slots["og:image:width"], renderTemplate`
			${renderComponent($$result, "Meta", $$Meta, { "property": "og:image:width", "content": "2400" })}
		`)}
		${renderSlot($$result, $$slots["og:image:height"], renderTemplate`
			${renderComponent($$result, "Meta", $$Meta, { "property": "og:image:height", "content": "1200" })}
		`)}
		${renderSlot($$result, $$slots["og:image:type"], renderTemplate`
			${renderComponent($$result, "Meta", $$Meta, { "property": "og:image:type", "content": "image/png" })}
		`)}
		${renderSlot($$result, $$slots["og:image:alt"], renderTemplate`
			${renderComponent($$result, "Meta", $$Meta, { "property": "og:image:alt", "content": "Deepgram - Build something great with voice. Faster, More Accurate Transcription Through AI Speech Recognition" })}
		`)}
		${renderSlot($$result, $$slots["twitter:card"], renderTemplate`
			${renderComponent($$result, "Meta", $$Meta, { "property": "twitter:card", "content": "summary" })}
		`)}
		${renderSlot($$result, $$slots["twitter:site"], renderTemplate`
			${renderComponent($$result, "Meta", $$Meta, { "property": "twitter:site", "content": "@deepgramai" })}
		`)}
		${renderSlot($$result, $$slots["head"])}

		${renderComponent($$result, "Meta", $$Meta, { "property": "DC.date.issued", "content": new Date().toISOString() })}
		${renderComponent($$result, "Meta", $$Meta, { "name": "theme-color", "content": "#0a121b" })}
		${renderComponent($$result, "Meta", $$Meta, { "name": "robots", "content": "noodp, noydir" })}
		${renderComponent($$result, "Meta", $$Meta, { "name": "viewport", "content": "width=device-width, initial-scale=1.0, viewport-fit=cover" })}
		${renderComponent($$result, "ScriptSrc", $$ScriptSrc, { "src": "//unpkg.com/alpinejs", "defer": true })}
		${renderComponent($$result, "ScriptSrc", $$ScriptSrc, { "charset": "utf-8", "type": "text/javascript", "src": "//js.hsforms.net/forms/v2.js?pre=1" })}

		${renderSlot($$result, $$slots["json:ld"], renderTemplate`
			${renderComponent($$result, "JsonLD", $$JsonLD, {})}
		`)}
	${renderHead($$result)}</head>

	<body>
		<!-- Google Tag Manager (noscript) -->
		<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PFZZ2KW" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
		<!-- End Google Tag Manager (noscript) -->

		${renderComponent($$result, "Header", $$Header, {})}

		<main class="bg-darkCharcoal">
			${renderSlot($$result, $$slots["default"])}
		</main>

		${renderComponent($$result, "Footer", $$Footer, { "class": "mt-40" })}
	</body></html>`;
}, "/Users/sandrarodgers/web-next/blog/src/layouts/Default.astro");

const $$Astro$T = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/strings/AuthorName.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$AuthorName = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$T, $$props, $$slots);
  Astro2.self = $$AuthorName;
  const { author } = Astro2.props;
  return renderTemplate`${author.content.title}
`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/strings/AuthorName.astro");

const $$Astro$S = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/strings/Authors.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Authors = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$S, $$props, $$slots);
  Astro2.self = $$Authors;
  const { authors, class: classes } = Astro2.props;
  const sbApi = F();
  const { data } = await sbApi.get("cdn/stories", {
    by_uuids: authors.toString()
  });
  const authorArray = data.stories;
  return renderTemplate`${authorArray && renderTemplate`${maybeRenderHead($$result)}<ul${addAttribute([["inline list-none p-0 text-xs font-bold uppercase text-cloud", classes], "astro-F2WZRE7V"], "class:list")}>
		${authorArray.map((author, index, array) => renderTemplate`<li class="inline-block p-0 astro-F2WZRE7V">
				${renderComponent($$result, "AuthorName", $$AuthorName, { "author": author, "class": "astro-F2WZRE7V" })}
			</li>`)}
	</ul>`}

`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/strings/Authors.astro");

const $$Astro$R = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/images/ImageSrcSet.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$ImageSrcSet = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$R, $$props, $$slots);
  Astro2.self = $$ImageSrcSet;
  const { filename, alt, class: classes } = Astro2.props;
  const srcs = ["240", "400", "800", "1200"];
  let image = filename;
  const ext = image.split(".").pop();
  let webp = void 0;
  let orig = void 0;
  if (image.indexOf("cloudinary") > -1) {
    webp = srcs.map((src) => {
      const url = image.replace("upload/", `upload/w_${src},c_scale/`).replace(`.${ext}`, ".webp");
      return `${url} ${src}w`;
    }).join(", ");
    orig = srcs.map((src) => {
      const url = image.replace("upload/", `upload/w_${src},c_scale/`);
      return `${url} ${src}w`;
    }).join(", ");
  }
  if (image.indexOf("storyblok.com") > -1) {
    let splitURL = image.split("/");
    splitURL[splitURL.indexOf("1001320") + 1].split("x")[0];
    webp = srcs.map((src, index) => {
      const url = image.concat(`/m/${src}x0`);
      return `${url} ${src}w`;
    }).join(", ");
    orig = srcs.map((src) => {
      const url = image.concat(`/m/${src}x0`);
      return `${url} ${src}w`;
    }).join(", ");
  }
  return renderTemplate`${maybeRenderHead($$result)}<picture${addAttribute(classes, "class")}>
	${webp && renderTemplate`<source${addAttribute(webp, "srcset")} type="image/webp" sizes="33.3vw">`}
	${orig && renderTemplate`<source${addAttribute(orig, "srcset")}${addAttribute(`image/${ext}`, "type")} sizes="33.3vw">`}
	<img loading="lazy"${addAttribute(classes, "class")}${addAttribute(image, "src")}${addAttribute(alt, "alt")}>
</picture>`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/images/ImageSrcSet.astro");

const $$Astro$Q = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/cards/GenericCard.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$GenericCard = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$Q, $$props, $$slots);
  Astro2.self = $$GenericCard;
  const { class: classes, href, ...rest } = Astro2.props;
  return renderTemplate`${renderComponent($$result, "Link", $$Link$1, { "href": href, "class": `block ${classes ? classes : ""}`, ...rest }, { "default": () => renderTemplate`${renderSlot($$result, $$slots["default"])}` })}`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/cards/GenericCard.astro");

const $$Astro$P = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/cards/GenericBlogCard.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$GenericBlogCard = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$P, $$props, $$slots);
  Astro2.self = $$GenericBlogCard;
  const { class: classes, post } = Astro2.props;
  const slug = post.slug;
  return renderTemplate`${renderComponent($$result, "GenericCard", $$GenericCard, { "class": `transparent-blur min-w-[10rem] bg-steel/50 ${classes ? classes : ""} astro-4B7ZWAZ4`, "href": `/${slug}` }, { "default": () => renderTemplate`${renderSlot($$result, $$slots["default"])}` })}

`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/cards/GenericBlogCard.astro");

const $$Astro$O = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/cards/BlogCardLarge.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$BlogCardLarge = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$O, $$props, $$slots);
  Astro2.self = $$BlogCardLarge;
  const { class: classes, post } = Astro2.props;
  let { title } = post.content;
  const { title: originalTitle, description, tags, authors, cover_image } = post.content;
  title = title.length > 80 ? `${title.substring(0, 77)}...` : title;
  return renderTemplate`${renderComponent($$result, "GenericBlogCard", $$GenericBlogCard, { "class": `w-full flex flex-col ${classes ? classes : ""}`, "post": post }, { "default": () => renderTemplate`${maybeRenderHead($$result)}<div class="overflow-hidden flex h-3/5 md:h-2/3 lg:h-3/5 place-content-center">
		${renderComponent($$result, "ImageSrcSet", $$ImageSrcSet, { "class": "object-cover w-full h-auto", "filename": cover_image.filename, "alt": `Blog title image for the blog post: ${originalTitle}` })}
	</div><div class="px-4 py-4 flex flex-col h-2/5 md:h-1/3 lg:h-2/5 justify-between text-white">
		<div class="h-full flex flex-col justify-start">
			<!-- <TagsList tags={tags} /> -->
			<h2${addAttribute(originalTitle, "title")} class="lg:text-h2-d md:text-h2-m text-h4-m py-2">${title}</h2>
			<p class="hidden lg:block text-cloud mt-2">${description}</p>
		</div>

		${renderComponent($$result, "Authors", $$Authors, { "authors": authors })}
	</div>` })}`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/cards/BlogCardLarge.astro");

const $$Astro$N = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/cards/BlogCardSmall.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$BlogCardSmall = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$N, $$props, $$slots);
  Astro2.self = $$BlogCardSmall;
  const { class: classes, post } = Astro2.props;
  let { title } = post.content;
  const { title: originalTitle, authors, cover_image } = post.content;
  title = title.length > 50 ? `${title.substring(0, 47)}...` : title;
  return renderTemplate`${renderComponent($$result, "GenericBlogCard", $$GenericBlogCard, { "class": `w-full flex justify-between ${classes ? classes : ""}`, "post": post }, { "default": () => renderTemplate`${maybeRenderHead($$result)}<div class="flex grow flex-col justify-between text-white pl-2 pb-4">
		<div class="max-h-18 overflow-hidden pt-2">
			<h5 class="text-base"${addAttribute(originalTitle, "title")}>${title}</h5>
		</div>
		${renderComponent($$result, "Authors", $$Authors, { "authors": authors })}
	</div><div class="shrink-0 w-[6rem] lg:w-[7rem] p-[0.5rem] flex justify-end place-items-start">
		${renderComponent($$result, "ImageSrcSet", $$ImageSrcSet, { "class": "h-24 w-24 object-cover", "filename": cover_image.filename, "alt": `Blog title image for the blog post: ${originalTitle}` })}
	</div>` })}`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/cards/BlogCardSmall.astro");

const $$Astro$M = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/cards/BlogCardHero.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$BlogCardHero = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$M, $$props, $$slots);
  Astro2.self = $$BlogCardHero;
  const { class: classes, post } = Astro2.props;
  let { title } = post.content;
  const { title: originalTitle, tags, authors, cover_image } = post.content;
  title = title.length > 60 ? `${title.substring(0, 57)}...` : title;
  return renderTemplate`${renderComponent($$result, "GenericBlogCard", $$GenericBlogCard, { "class": `w-full flex flex-col ${classes ? classes : ""}`, "post": post }, { "default": () => renderTemplate`${maybeRenderHead($$result)}<div class="overflow-hidden flex h-auto md:h-2/3 lg:h-[50%] place-content-center">
		${renderComponent($$result, "ImageSrcSet", $$ImageSrcSet, { "class": "object-cover w-full h-auto", "filename": cover_image.filename, "alt": `Blog title image for the blog post: ${originalTitle}` })}
	</div><div class="px-4 py-4 flex flex-col h-[50%] md:h-1/3 lg:h-[50%] justify-between text-white">
		<div class="h-full flex flex-col">
			<!-- <TagsList tags={tags} /> -->
			<h4 class="lg:text-h4-d md:text-h2-m text-h4-m py-0 lg:py-2">${title}</h4>
		</div>

		${renderComponent($$result, "Authors", $$Authors, { "authors": authors })}
	</div>` })}`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/cards/BlogCardHero.astro");

const $$Astro$L = createAstro("/Users/sandrarodgers/web-next/blog/src/components/homepage/Hero.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Hero$2 = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$L, $$props, $$slots);
  Astro2.self = $$Hero$2;
  const { title, featured, homepage, latest } = Astro2.props;
  return renderTemplate`${maybeRenderHead($$result)}<h1 class="mb-8 text-h1-small-m sm:text-h1-small-d">${title}</h1>
<div class="grid gap-4 lg:gap-6 grid-cols-1 sm:grid-cols-2 md:grid-cols-3 lg:grid-cols-3 xl:grid-cols-4">
	<div class="md:col-span-2 lg:col-span-2 xl:col-span-3 grid gap-4 lg:gap-6 xl:grid-cols-3">
		<div class="xl:order-last xl:col-span-2 flex">
			${renderComponent($$result, "BlogCardLarge", $$BlogCardLarge, { "post": featured })}
		</div>
		<div class="grid gap-4 lg:gap-6 lg:hidden xl:grid xl:grid-rows-2">
			${renderComponent($$result, "BlogCardHero", $$BlogCardHero, { "post": latest[0] })}
			${renderComponent($$result, "BlogCardHero", $$BlogCardHero, { "class": "sm:hidden lg:block", "post": latest[1] })}
		</div>
	</div>
	<div class="grid gap-4 lg:gap-6 grid-rows-5">
		${renderComponent($$result, "BlogCardSmall", $$BlogCardSmall, { "class": "hidden lg:flex xl:hidden", "post": latest[0] })}
		${renderComponent($$result, "BlogCardSmall", $$BlogCardSmall, { "class": "hidden md:flex xl:hidden", "post": latest[1] })}
		${renderComponent($$result, "BlogCardSmall", $$BlogCardSmall, { "post": homepage[0] })}
		${renderComponent($$result, "BlogCardSmall", $$BlogCardSmall, { "post": homepage[1] })}
		${renderComponent($$result, "BlogCardSmall", $$BlogCardSmall, { "post": homepage[2] })}
		${renderComponent($$result, "BlogCardSmall", $$BlogCardSmall, { "post": homepage[3], "class": "lg:hidden xl:flex" })}
		${renderComponent($$result, "BlogCardSmall", $$BlogCardSmall, { "post": homepage[4], "class": "md:hidden xl:flex" })}
	</div>
</div>`;
}, "/Users/sandrarodgers/web-next/blog/src/components/homepage/Hero.astro");

const $$Astro$K = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/layout/HasAside.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$HasAside = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$K, $$props, $$slots);
  Astro2.self = $$HasAside;
  const { class: classes, gridClass } = Astro2.props;
  return renderTemplate`${maybeRenderHead($$result)}<div${addAttribute(`relative ${classes ? classes : ""} ${gridClass ? gridClass : "grid grid-cols-1 gap-4 lg:gap-6 lg:grid-cols-6 xl:grid-cols-4"}`, "class")}>
	<div>${renderSlot($$result, $$slots["aside"])}</div>
	<div class="lg:col-span-5 xl:col-span-3">${renderSlot($$result, $$slots["default"])}</div>
</div>`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/layout/HasAside.astro");

const $$Astro$J = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/nav/CategoryList.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$CategoryList = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$J, $$props, $$slots);
  Astro2.self = $$CategoryList;
  const categories = await getSluggedJson("blog/category");
  const pathname = Astro2.url.pathname;
  return renderTemplate`${maybeRenderHead($$result)}<div class="hidden lg:block lg:sticky lg:top-20 mb-20">
	<h3 class="pb-8">Categories</h3>
	<ul>
		<li class="h-16">
			${renderComponent($$result, "Link", $$Link$1, { "href": "/posts", "class": "button button--secondary button--small", "iconSuffix": true, "icon": "arrow-right" }, { "default": () => renderTemplate`View All` })}
		</li>
		${categories && categories.map((category) => {
    const path = `/categories/${category.slug}/`;
    return renderTemplate`<li class="pb-4">
							${renderComponent($$result, "Link", $$Link$1, { "href": path, "class": `hover:text-lightIris font-semibold leading-5 ${pathname.endsWith(path) ? "text-casper" : "text-white"}` }, { "default": () => renderTemplate`${category.title}` })}
						</li>`;
  })}
	</ul>
</div>
<!-- mobile -->
<div x-data="{ open: false }" class="block bg-rock lg:hidden mb-10">
	${renderComponent($$result, "Link", $$Link$1, { "@click": "open = !open", "class": "button button--senary button--small button--block flex justify-between items-center text-white" }, { "default": () => renderTemplate`<h3 class="pb-0">Categories</h3>${renderComponent($$result, "Icon", $$Icon, { "icon": "angle-down", "class": "fill-gray-300 w-[1em] h-[1em]", "x-bind:class": "{ 'rotate-180' : open }" })}` })}

	<ul class="hidden absolute w-full z-20 bg-rock overflow-y-auto max-h-[36rem]" x-bind:class="{hidden: !open, block: open}">
		<hr class="sticky top-0 left-[3%] md:left-[2%] w-[97%] mb-4 bg-steel opacity-30">
		${categories && categories.map((category) => {
    return renderTemplate`<li class="pb-4">
							${renderComponent($$result, "Link", $$Link$1, { "class": "font-inter pl-4 font-normal text-white", "href": `/categories/${category.slug}` }, { "default": () => renderTemplate`${category.title}` })}
						</li>`;
  })}
	</ul>
</div>`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/nav/CategoryList.astro");

const $$Astro$I = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/lists/BlogCardList.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$BlogCardList = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$I, $$props, $$slots);
  Astro2.self = $$BlogCardList;
  const { class: classes, ...rest } = Astro2.props;
  return renderTemplate`${maybeRenderHead($$result)}<div${addAttribute(`w-full ${classes ? classes : ""}`, "class")}${spreadAttributes(rest)}>
	<div class="relative mx-auto">
		${renderSlot($$result, $$slots["title"])}
		<div class="grid grid-flow-row-dense grid-cols-1 md:grid-cols-2 xl:grid-cols-3 justify-items-center gap-4 lg:gap-6">
			${renderSlot($$result, $$slots["default"])}
		</div>
	</div>
</div>`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/lists/BlogCardList.astro");

const $$Astro$H = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/titles/LinkCardListTitle.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$LinkCardListTitle = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$H, $$props, $$slots);
  Astro2.self = $$LinkCardListTitle;
  const { title, class: classes, linkClass, ...rest } = Astro2.props;
  return renderTemplate`${maybeRenderHead($$result)}<div${addAttribute(`flex flex-col md:flex-row items-start md:items-end justify-between pb-6 md:pb-8 lg:pb-10 ${classes ? classes : ""}`, "class")}>
	${renderSlot($$result, $$slots["default"])}
	${renderComponent($$result, "Link", $$Link$1, { ...rest, "class": `text-lg font-semibold stroke-[3] ${linkClass ? linkClass : ""}`, "icon": "arrow-right", "iconSuffix": true }, { "default": () => renderTemplate`<span>View All ${title}</span>` })}
</div>`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/titles/LinkCardListTitle.astro");

const $$Astro$G = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/cards/BlogCardSB.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$BlogCardSB = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$G, $$props, $$slots);
  Astro2.self = $$BlogCardSB;
  const { class: classes, post } = Astro2.props;
  const { title: originalTitle, authors, cover_image } = post.content;
  let title = originalTitle.length > 60 ? `${originalTitle.substring(0, 57)}...` : originalTitle;
  return renderTemplate`${renderComponent($$result, "GenericBlogCard", $$GenericBlogCard, { "class": `w-full flex flex-col ${classes ? classes : ""}`, "post": post }, { "default": () => renderTemplate`${maybeRenderHead($$result)}<div class="overflow-hidden flex h-[55%] place-content-center">
		${renderComponent($$result, "ImageSrcSet", $$ImageSrcSet, { "class": "object-cover h-full w-full aspect-[1.6/1]", "filename": cover_image.filename, "alt": `Blog title image for the blog post: ${originalTitle}` })}
	</div><div class="px-4 pb-3 flex flex-col h-[45%] justify-between text-white">
		<div class="h-full flex flex-col justify-evenly">
			<!-- <TagsList tags={tag_list} /> -->
			<h4${addAttribute(originalTitle, "title")} class="lg:text-h4-d md:text-h2-m text-h4-m">${title}</h4>
		</div>

		${renderComponent($$result, "Authors", $$Authors, { "authors": authors })}
	</div>` })}`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/cards/BlogCardSB.astro");

const $$Astro$F = createAstro("/Users/sandrarodgers/web-next/blog/src/components/homepage/CategoryPostsSB.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$CategoryPostsSB = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$F, $$props, $$slots);
  Astro2.self = $$CategoryPostsSB;
  const { posts } = Astro2.props;
  return renderTemplate`${posts && posts.map((post) => renderTemplate`${renderComponent($$result, "BlogCard", $$BlogCardSB, { "post": post })}`)}
`;
}, "/Users/sandrarodgers/web-next/blog/src/components/homepage/CategoryPostsSB.astro");

const $$Astro$E = createAstro("/Users/sandrarodgers/web-next/blog/src/components/homepage/CategorySectionSB.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$CategorySectionSB = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$E, $$props, $$slots);
  Astro2.self = $$CategorySectionSB;
  const sbApi = F();
  const { category } = Astro2.props;
  const { data: allPosts } = await sbApi.get("cdn/stories", {
    by_slugs: "blog-posts/*",
    sort_by: "content.date:desc",
    per_page: 6,
    filter_query: {
      category: {
        in: category.uuid
      }
    }
  });
  const posts = allPosts.stories.sort((a, b) => {
    const aDate = new Date(b.content.date);
    const bDate = new Date(a.content.date);
    return aDate.getTime() - bDate.getTime();
  });
  return renderTemplate`${posts.length > 0 && renderTemplate`${renderComponent($$result, "Fragment", Fragment, {}, { "default": () => renderTemplate`${renderComponent($$result, "BlogCardList", $$BlogCardList, { "class": "mb-32" }, { "default": () => renderTemplate`${renderComponent($$result, "CategoryPosts", $$CategoryPostsSB, { "posts": posts })}`, "title": () => renderTemplate`${renderComponent($$result, "LinkCardListTitle", $$LinkCardListTitle, { "slot": "title", "href": `/categories/${category.slug}`, "title": category.content.plural, "linkClass": "draw-underline nudge-icon nudge-icon--right text-lightIris" }, { "default": () => renderTemplate`${maybeRenderHead($$result)}<h3>${category.content.title}</h3>` })}` })}` })}`}`;
}, "/Users/sandrarodgers/web-next/blog/src/components/homepage/CategorySectionSB.astro");

const $$Astro$D = createAstro("/Users/sandrarodgers/web-next/blog/src/pages/index.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Index$3 = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$D, $$props, $$slots);
  Astro2.self = $$Index$3;
  const sbApi = F();
  const { data: featured } = await sbApi.get("cdn/stories/4138");
  const featuredData = featured.story.content;
  const { data: featuredPost } = await sbApi.get(`cdn/stories/${featuredData.Featured.id}`, {
    find_by: "uuid"
  });
  const { data: homepagePosts } = await sbApi.get("cdn/stories", {
    by_uuids: featuredData.Homepage.join(","),
    sort_by: "content.date:desc"
  });
  const { data: allPosts } = await sbApi.get("cdn/stories", {
    by_slugs: "blog-posts/*",
    per_page: 8,
    sort_by: "content.date:desc"
  });
  const otherPosts = allPosts.stories.filter((post) => {
    return ![featuredPost, ...homepagePosts.stories].some((f) => {
      return f.uuid === post.uuid;
    });
  });
  const latestPosts = otherPosts.splice(0, 2);
  const { data } = await sbApi.get("cdn/stories", {
    by_slugs: "categories/*"
  });
  const categories = data.stories;
  const schema = {
    "@context": "https://schema.org",
    "@type": "BreadcrumbList",
    itemListElement: [
      {
        "@type": "ListItem",
        position: 1,
        name: "Deepgram Home",
        item: "https://deepgram.com"
      },
      {
        "@type": "ListItem",
        position: 2,
        name: "Blog"
      }
    ]
  };
  return renderTemplate`${renderComponent($$result, "Layout", $$Default, {}, { "default": () => renderTemplate`${renderComponent($$result, "ContrastSection", $$ContrastSection, { "contrast": "black", "background": "darkCharcoal", "class": "bg-sound-wave-cloud-dark", "bottomDivider": "eclipse-divider", "bottomOverlay": true }, { "default": () => renderTemplate`${renderComponent($$result, "PrimarySection", $$PrimarySection, { "class": "pt-[6rem] md:pt-[6.33203125rem] lg:pt-[6.6640625rem] xl:pt-[6.328125rem]" }, { "default": () => renderTemplate`${renderComponent($$result, "Hero", $$Hero$2, { "title": "Deepgram Blog \u26A1\uFE0F", "featured": featuredPost.story, "homepage": homepagePosts.stories, "latest": latestPosts })}` })}` })}${renderComponent($$result, "PrimarySection", $$PrimarySection, { "class": "mt-48 xl:mt-64" }, { "default": () => renderTemplate`${renderComponent($$result, "HasAside", $$HasAside, {}, { "aside": () => renderTemplate`${renderComponent($$result, "CategoryList", $$CategoryList, { "slot": "aside" })}`, "default": () => renderTemplate`${categories && categories.map((category, index) => renderTemplate`${renderComponent($$result, "Fragment", Fragment, {}, { "default": () => renderTemplate`${renderComponent($$result, "CategorySection", $$CategorySectionSB, { "category": category })}${index === 0 && renderTemplate`${maybeRenderHead($$result)}<div class="mb-32">
									${renderComponent($$result, "WhitepaperPromo", $$WhitepaperPromoSB, { "blok": { whitepaper: { id: "7727d114-4347-4fe8-88a7-642961b5aa33" } } })}
								</div>`}` })}`)}<center class="-mt-10 mb-20">
				${renderComponent($$result, "Link", $$Link$1, { "href": "/posts", "icon": "arrow-right", "iconSuffix": true, "class": "button button--secondary button--small" }, { "default": () => renderTemplate`View All` })}
			</center>` })}` })}`, "json:ld": () => renderTemplate`${renderComponent($$result, "JsonLD", $$JsonLD, { "slot": "json:ld", "schema": schema })}` })}`;
}, "/Users/sandrarodgers/web-next/blog/src/pages/index.astro");

const $$file$8 = "/Users/sandrarodgers/web-next/blog/src/pages/index.astro";
const $$url$8 = "";

const _page0 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  default: $$Index$3,
  file: $$file$8,
  url: $$url$8
}, Symbol.toStringTag, { value: 'Module' }));

const $$Astro$C = createAstro("/Users/sandrarodgers/web-next/blog/src/components/nav/Backlink.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Backlink = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$C, $$props, $$slots);
  Astro2.self = $$Backlink;
  const { href, linkClass, class: classes } = Astro2.props;
  return renderTemplate`${renderComponent($$result, "Link", $$Link$1, { "href": href, "class:list": ["text-[1.125rem] font-[600] leading-[1.5rem] inline-block nudge-icon nudge-icon--left", linkClass, classes], "icon": "arrow-left" }, { "default": () => renderTemplate`${renderSlot($$result, $$slots["default"])}` })}`;
}, "/Users/sandrarodgers/web-next/blog/src/components/nav/Backlink.astro");

const $$Astro$B = createAstro("/Users/sandrarodgers/web-next/blog/src/pages/categories/index.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Index$2 = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$B, $$props, $$slots);
  Astro2.self = $$Index$2;
  const sbApi = F();
  const categories = await sbApi.getAll("cdn/stories", {
    by_slugs: "categories/*"
  });
  const schema = {
    "@context": "https://schema.org",
    "@type": "BreadcrumbList",
    itemListElement: [
      {
        "@type": "ListItem",
        position: 1,
        name: "Deepgram Home",
        item: "https://deepgram.com"
      },
      {
        "@type": "ListItem",
        position: 2,
        name: "Blog",
        item: Astro2.url.origin
      },
      {
        "@type": "ListItem",
        position: 3,
        name: "Categories"
      }
    ]
  };
  return renderTemplate`${renderComponent($$result, "Layout", $$Default, {}, { "default": () => renderTemplate`${renderComponent($$result, "PrimarySection", $$PrimarySection, { "class": "pt-[6.75rem] md:pt-[7.03125rem] lg:pt-[7.3125rem] xl:pt-[7.875rem]" }, { "default": () => renderTemplate`${renderComponent($$result, "Backlink", $$Backlink, { "href": "/", "class": "mb-6 md:mb-8 lg:mb-10 xl:mb-12 text-lightIris" }, { "default": () => renderTemplate`Back to blog home` })}${maybeRenderHead($$result)}<h1 class="mb-8">Deepgram Blog ⚡️</h1>${renderComponent($$result, "HasAside", $$HasAside, {}, { "aside": () => renderTemplate`${renderComponent($$result, "CategoryList", $$CategoryList, { "slot": "aside" })}`, "default": () => renderTemplate`${categories && categories.map((category) => renderTemplate`${renderComponent($$result, "CategorySection", $$CategorySectionSB, { "category": category })}`)}<center class="-mt-10 mb-20">
				${renderComponent($$result, "Link", $$Link$1, { "href": "/posts", "icon": "arrow-right", "iconSuffix": true, "class": "button button--secondary button--small" }, { "default": () => renderTemplate`View All` })}
			</center>` })}` })}`, "head:description": () => renderTemplate`${renderComponent($$result, "Description", $$Description, { "slot": "head:description", "name": "description", "content": "All the categories on the Deepgram blog." })}`, "head:title": () => renderTemplate`${renderComponent($$result, "Title", $$Title, { "slot": "head:title", "title": "All categories - Deepgram Blog \u26A1\uFE0F" })}`, "json:ld": () => renderTemplate`${renderComponent($$result, "JsonLD", $$JsonLD, { "slot": "json:ld", "schema": schema })}`, "og:description": () => renderTemplate`${renderComponent($$result, "Meta", $$Meta, { "slot": "og:description", "property": "og:description", "content": "All the categories on the Deepgram blog." })}`, "og:title": () => renderTemplate`${renderComponent($$result, "Meta", $$Meta, { "slot": "og:title", "property": "og:title", "content": "All categories - Deepgram Blog \u26A1\uFE0F" })}` })}`;
}, "/Users/sandrarodgers/web-next/blog/src/pages/categories/index.astro");

const $$file$7 = "/Users/sandrarodgers/web-next/blog/src/pages/categories/index.astro";
const $$url$7 = "/categories";

const _page1 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  default: $$Index$2,
  file: $$file$7,
  url: $$url$7
}, Symbol.toStringTag, { value: 'Module' }));

const $$Astro$A = createAstro("/Users/sandrarodgers/web-next/blog/src/components/nav/Pagination.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Pagination = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$A, $$props, $$slots);
  Astro2.self = $$Pagination;
  const { page, slug } = Astro2.props;
  let pageStatuses = Array.from({ length: page.lastPage }, (v, i) => i + 1).map((i) => {
    var item = {
      pageNumber: i,
      isActive: page.currentPage == i,
      isDisabled: isDisabled(i)
    };
    return item;
  });
  function isDisabled(i) {
    if (page.totalPage <= 6)
      return false;
    if (i <= 2 || i >= page.totalPage - 1)
      return false;
    if (page.currentPage == i - 1 || page.currentPage == i || page.currentPage == i + 1)
      return false;
    return true;
  }
  pageStatuses[0].pageNumber = 0;
  pageStatuses[pageStatuses.length - 1].isDisabled = false;
  return renderTemplate`${maybeRenderHead($$result)}<nav class="flex w-full flex-col items-center my-28 astro-O4TK4JND">
	<ul class="flex items-center justify-center astro-O4TK4JND">
		<li class="astro-O4TK4JND">
			${renderComponent($$result, "Link", $$Link$1, { "icon": "chevron-left", "class": "stroke-2 text-cloud astro-O4TK4JND", "fillCurrent": true, "href": page.url.prev === void 0 ? null : `${page.url.prev}` })}
		</li>
		${pageStatuses.map(
    (pageStatus, index) => pageStatus.isActive ? renderTemplate`<li class="astro-O4TK4JND">
						${renderComponent($$result, "Link", $$Link$1, { "class": "page text-darkCharcoal border-darkCharcoal  font-bold astro-O4TK4JND" }, { "default": () => renderTemplate`${page.currentPage}` })}
					</li>` : pageStatus.isDisabled && !pageStatuses[index - 1].isDisabled ? renderTemplate`<li class="astro-O4TK4JND">
						${renderComponent($$result, "Link", $$Link$1, { "class": "page disabled:opacity-75 astro-O4TK4JND" }, { "default": () => renderTemplate`...` })}
					</li>` : !pageStatus.isDisabled && renderTemplate`<li class="astro-O4TK4JND">
							${renderComponent($$result, "Link", $$Link$1, { "class": "page astro-O4TK4JND", "href": `/${slug}/${pageStatus.pageNumber === 0 ? "" : pageStatus.pageNumber}` }, { "default": () => renderTemplate`${pageStatus.pageNumber === 0 ? 1 : pageStatus.pageNumber}` })}
						</li>`
  )}
		<li class="astro-O4TK4JND">
			${renderComponent($$result, "Link", $$Link$1, { "icon": "chevron-right", "class": "stroke-2 text-cloud astro-O4TK4JND", "fillCurrent": true, "href": page.url.next === void 0 ? null : `${page.url.next}` })}
		</li>
	</ul>
</nav>

`;
}, "/Users/sandrarodgers/web-next/blog/src/components/nav/Pagination.astro");

const $$Astro$z = createAstro("/Users/sandrarodgers/web-next/blog/src/components/posts/GeneralPostsPageSB.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$GeneralPostsPageSB = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$z, $$props, $$slots);
  Astro2.self = $$GeneralPostsPageSB;
  const { posts, page, slug, link, path, linkText, subtitle } = Astro2.props;
  return renderTemplate`${renderComponent($$result, "Backlink", $$Backlink, { "href": link, "class": "mb-6 md:mb-8 lg:mb-10 xl:mb-12 text-lightIris" }, { "default": () => renderTemplate`${linkText}` })}
${maybeRenderHead($$result)}<h1 class="mb-8">Deepgram Blog ⚡️</h1>
<h2 class="mb-8">${subtitle}</h2>
${renderComponent($$result, "HasAside", $$HasAside, {}, { "aside": () => renderTemplate`${renderComponent($$result, "CategoryList", $$CategoryList, { "slot": "aside" })}`, "default": () => renderTemplate`${renderComponent($$result, "BlogCardList", $$BlogCardList, {}, { "default": () => renderTemplate`${posts.map((post, index) => renderTemplate`${renderComponent($$result, "BlogCard", $$BlogCardSB, { "post": post })}`)}` })}${renderComponent($$result, "Pagination", $$Pagination, { "page": page, "path": path, "slug": slug })}` })}`;
}, "/Users/sandrarodgers/web-next/blog/src/components/posts/GeneralPostsPageSB.astro");

const $$Astro$y = createAstro("/Users/sandrarodgers/web-next/blog/src/pages/categories/[category]/[...page].astro", "", "file:///Users/sandrarodgers/web-next/blog/");
async function getStaticPaths$4({ paginate }) {
  const sbApi = F();
  const categories = await sbApi.getAll("cdn/stories", {
    by_slugs: "categories/*"
  });
  const allPosts = await sbApi.getAll("cdn/stories", {
    by_slugs: "blog-posts/*"
  });
  const sortedPosts = allPosts.sort((a, b) => {
    const aDate = new Date(b.content.date);
    const bDate = new Date(a.content.date);
    return aDate.getTime() - bDate.getTime();
  });
  return categories.map((category) => {
    const posts = sortedPosts.filter((post) => {
      return post.content.category === category.uuid;
    });
    return paginate(posts, {
      params: {
        category: category.slug
      },
      pageSize: 6,
      props: {
        category
      }
    });
  });
}
const $$$3 = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$y, $$props, $$slots);
  Astro2.self = $$$3;
  const { category, page } = Astro2.props;
  const schema = {
    "@context": "https://schema.org",
    "@type": "BreadcrumbList",
    itemListElement: [
      {
        "@type": "ListItem",
        position: 1,
        name: "Deepgram Home",
        item: "https://deepgram.com"
      },
      {
        "@type": "ListItem",
        position: 2,
        name: "Blog",
        item: Astro2.url.origin
      },
      {
        "@type": "ListItem",
        position: 3,
        name: "Categories",
        item: `${Astro2.url.origin}/categories`
      },
      {
        "@type": "ListItem",
        position: 4,
        name: category.title
      }
    ]
  };
  return renderTemplate`${renderComponent($$result, "Layout", $$Default, {}, { "default": () => renderTemplate`${renderComponent($$result, "PrimarySection", $$PrimarySection, { "class": "pt-[6.75rem] md:pt-[7.03125rem] lg:pt-[7.3125rem] xl:pt-[7.875rem]" }, { "default": () => renderTemplate`${renderComponent($$result, "GeneralPostsPage", $$GeneralPostsPageSB, { "slug": `categories/${category.slug}`, "page": page, "posts": page.data, "link": "/categories", "linkText": "All categories", "subtitle": `All ${category.content.plural}` })}` })}`, "head:description": () => renderTemplate`${renderComponent($$result, "Description", $$Description, { "slot": "head:description", "name": "description", "content": `All posts in the ${category.title} category on the Deepgram Blog.` })}`, "head:title": () => renderTemplate`${renderComponent($$result, "Title", $$Title, { "slot": "head:title", "title": `All posts in the ${category.title} category - Deepgram Blog \u26A1\uFE0F` })}`, "json:ld": () => renderTemplate`${renderComponent($$result, "JsonLD", $$JsonLD, { "slot": "json:ld", "schema": schema })}`, "og:description": () => renderTemplate`${renderComponent($$result, "Meta", $$Meta, { "slot": "og:description", "property": "og:description", "content": `All posts in the ${category.title} category on the Deepgram Blog.` })}`, "og:title": () => renderTemplate`${renderComponent($$result, "Meta", $$Meta, { "slot": "og:title", "property": "og:title", "content": `All posts in the ${category.title} category - Deepgram Blog \u26A1\uFE0F` })}` })}`;
}, "/Users/sandrarodgers/web-next/blog/src/pages/categories/[category]/[...page].astro");

const $$file$6 = "/Users/sandrarodgers/web-next/blog/src/pages/categories/[category]/[...page].astro";
const $$url$6 = "/categories/[category]/[...page]";

const _page2 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  getStaticPaths: getStaticPaths$4,
  default: $$$3,
  file: $$file$6,
  url: $$url$6
}, Symbol.toStringTag, { value: 'Module' }));

const $$Astro$x = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/lists/AuthorCardList.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$AuthorCardList = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$x, $$props, $$slots);
  Astro2.self = $$AuthorCardList;
  const { class: classes, ...rest } = Astro2.props;
  return renderTemplate`${maybeRenderHead($$result)}<div${addAttribute(`w-full ${classes ? classes : ""}`, "class")}${spreadAttributes(rest)}>
	<div class="relative mx-auto">
		${renderSlot($$result, $$slots["title"])}
		<div class="grid grid-flow-row-dense grid-cols-1 md:grid-cols-2 lg:grid-cols-3 justify-items-start gap-6 md:gap-8 lg:gap-10">
			${renderSlot($$result, $$slots["default"])}
		</div>
	</div>
</div>`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/lists/AuthorCardList.astro");

const $$Astro$w = createAstro("/Users/sandrarodgers/web-next/blog/src/components/authors/Hero.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Hero$1 = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$w, $$props, $$slots);
  Astro2.self = $$Hero$1;
  const { title, svg } = Astro2.props;
  return renderTemplate`${maybeRenderHead($$result)}<div class="grid grid-cols-1 xl:grid-cols-3">
	<h1 class="mb-20 text-white">${title}</h1>
	<div class="xl:order-last">
		${renderComponent($$result, "RightDecoration", $$RightDecoration, { "icon": "star-left", "class": "text-white w-6 lg:w-8 xl:-mt-10" })}
		${renderComponent($$result, "LeftDecoration", $$LeftDecoration, { "icon": "star-right", "class": "text-white w-5 lg:w-6 mt-4 xl:mt-20" })}
	</div>
	<div>${renderComponent($$result, "Svg", $$Svg, { "class": "opacity-0 -mb-10", "name": "starburst-gradient" })}</div>
</div>`;
}, "/Users/sandrarodgers/web-next/blog/src/components/authors/Hero.astro");

const $$Astro$v = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/titles/NoLinkCardListTitle.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$NoLinkCardListTitle = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$v, $$props, $$slots);
  Astro2.self = $$NoLinkCardListTitle;
  const { class: classes, ...rest } = Astro2.props;
  return renderTemplate`${maybeRenderHead($$result)}<div${spreadAttributes(rest)}${addAttribute(`flex flex-col md:flex-row items-start md:items-end justify-between pb-6 md:pb-8 lg:pb-10 ${classes ? classes : ""}`, "class")}>
	${renderSlot($$result, $$slots["default"])}
</div>`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/titles/NoLinkCardListTitle.astro");

const $$Astro$u = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/cards/Author.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Author = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$u, $$props, $$slots);
  Astro2.self = $$Author;
  const { class: classes, blok, slug } = Astro2.props;
  if (blok.picture.filename.indexOf("cloudinary") > -1) {
    if (blok.picture.filename.indexOf("w_") === -1) {
      blok.picture.filename = blok.picture.filename.replace("upload/", "upload/w_200/");
    }
    if (blok.picture.filename.indexOf("c_") === -1) {
      blok.picture.filename = blok.picture.filename.replace("upload/", "upload/c_crop/");
    }
  }
  return renderTemplate`${maybeRenderHead($$result)}<a${addAttribute(`block text-white ${classes ? classes : ""}`, "class")}${addAttribute(`/authors/${slug}`, "href")}${spreadAttributes(z(blok))}>
	<div class="flex">
		<div class="shrink-0 w-[6rem] md:w-[8rem]">
			<img class="h-[6rem] md:h-[8rem] w-[6rem] md:w-[8rem] object-cover rounded-full"${addAttribute(blok.picture.filename, "src")}${addAttribute(`Author image for ${blok.title}`, "alt")}>
		</div>
		<div class="flex flex-col ml-4 pt-2 md:pt-8">
			<h4>${blok.title}</h4>
			<h5 class="text-mist">${blok.jobtitle}</h5>
		</div>
	</div>
</a>`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/cards/Author.astro");

const pick = function (attrs, allowed) {
  if (!attrs) {
    return null;
  }
  let h = {};
  for (let key in attrs) {
    let value = attrs[key];
    if (allowed.indexOf(key) > -1 && value !== null) {
      h[key] = value;
    }
  }
  if (attrs.id) ;
  return h;
};
const isEmailLinkType = type => type === "email";
const schema = {
  nodes: {
    horizontal_rule() {
      return {
        singleTag: "hr"
      };
    },
    blockquote() {
      return {
        tag: "blockquote"
      };
    },
    bullet_list() {
      return {
        tag: "ul"
      };
    },
    code_block(node) {
      return {
        tag: ["pre", {
          tag: "code",
          attrs: node.attrs
        }]
      };
    },
    hard_break() {
      return {
        singleTag: "br"
      };
    },
    heading(node) {
      const newAttrs = {
        ...node.attrs,
        id: slugify(node.content[0].text, {
          lower: true
        }),
        style: "scroll-margin-top: 4.6rem"
      };
      return {
        tag: [{
          tag: `h${node.attrs.level}`,
          attrs: pick(newAttrs, ["id", "style"])
        }]
      };
    },
    image(node) {
      return {
        singleTag: [{
          tag: "img",
          attrs: pick(node.attrs, ["src", "alt", "title"])
        }]
      };
    },
    list_item() {
      return {
        tag: "li"
      };
    },
    ordered_list() {
      return {
        tag: "ol"
      };
    },
    paragraph() {
      return {
        tag: "p"
      };
    }
  },
  marks: {
    bold() {
      return {
        tag: "b"
      };
    },
    strike() {
      return {
        tag: "strike"
      };
    },
    underline() {
      return {
        tag: "u"
      };
    },
    strong() {
      return {
        tag: "strong"
      };
    },
    code() {
      return {
        tag: "code"
      };
    },
    italic() {
      return {
        tag: "i"
      };
    },
    link(node) {
      const attrs = {
        ...node.attrs
      };
      const {
        linktype = "url"
      } = node.attrs;
      if (isEmailLinkType(linktype)) {
        attrs.href = `mailto:${attrs.href}`;
      }
      if (attrs.anchor) {
        attrs.href = `${attrs.href}#${attrs.anchor}`;
        delete attrs.anchor;
      }
      return {
        tag: [{
          tag: "a",
          attrs: attrs
        }]
      };
    },
    styled(node) {
      return {
        tag: [{
          tag: "span",
          attrs: node.attrs
        }]
      };
    }
  }
};

const $$Astro$t = createAstro("/Users/sandrarodgers/web-next/blog/src/components/storyblok/RichText.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$RichText = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$t, $$props, $$slots);
  Astro2.self = $$RichText;
  const clonedSchema = cloneDeep(schema);
  const { blok } = Astro2.props;
  const renderedRichText = V(blok.content, { schema: clonedSchema });
  return renderTemplate`${maybeRenderHead($$result)}<div${spreadAttributes(z(blok))}>
  <div>${unescapeHTML(renderedRichText)}</div>
</div>`;
}, "/Users/sandrarodgers/web-next/blog/src/components/storyblok/RichText.astro");

const $$Astro$s = createAstro("/Users/sandrarodgers/web-next/blog/src/components/storyblok/Iframe.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Iframe = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$s, $$props, $$slots);
  Astro2.self = $$Iframe;
  const { blok } = Astro2.props;
  return renderTemplate`${maybeRenderHead($$result)}<iframe${spreadAttributes(z(blok))}${spreadAttributes(blok)}></iframe>`;
}, "/Users/sandrarodgers/web-next/blog/src/components/storyblok/Iframe.astro");

const $$Astro$r = createAstro("/Users/sandrarodgers/web-next/blog/src/components/storyblok/Image.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Image = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$r, $$props, $$slots);
  Astro2.self = $$Image;
  const { blok } = Astro2.props;
  return renderTemplate`${maybeRenderHead($$result)}<img${spreadAttributes(z(blok))}${spreadAttributes(blok)}>`;
}, "/Users/sandrarodgers/web-next/blog/src/components/storyblok/Image.astro");

const themes = {
	'css-variables': () => import('./chunks/css-variables.fec89dfe.mjs').then((mod) => mod.default),
	'dark-plus': () => import('./chunks/dark-plus.c64d8286.mjs').then((mod) => mod.default),
	'dracula-soft': () => import('./chunks/dracula-soft.97887887.mjs').then((mod) => mod.default),
	dracula: () => import('./chunks/dracula.16037935.mjs').then((mod) => mod.default),
	'github-dark-dimmed': () =>
		import('./chunks/github-dark-dimmed.887421d6.mjs').then((mod) => mod.default),
	'github-dark': () => import('./chunks/github-dark.a141d561.mjs').then((mod) => mod.default),
	'github-light': () => import('./chunks/github-light.4ab896e1.mjs').then((mod) => mod.default),
	hc_light: () => import('./chunks/hc_light.cffab4a5.mjs').then((mod) => mod.default),
	'light-plus': () => import('./chunks/light-plus.82aac543.mjs').then((mod) => mod.default),
	'material-darker': () => import('./chunks/material-darker.7d15313f.mjs').then((mod) => mod.default),
	'material-default': () => import('./chunks/material-default.42b48278.mjs').then((mod) => mod.default),
	'material-lighter': () => import('./chunks/material-lighter.3e844679.mjs').then((mod) => mod.default),
	'material-ocean': () => import('./chunks/material-ocean.f900a915.mjs').then((mod) => mod.default),
	'material-palenight': () =>
		import('./chunks/material-palenight.2b604358.mjs').then((mod) => mod.default),
	'min-dark': () => import('./chunks/min-dark.caa582f0.mjs').then((mod) => mod.default),
	'min-light': () => import('./chunks/min-light.39619116.mjs').then((mod) => mod.default),
	monokai: () => import('./chunks/monokai.3f5e5246.mjs').then((mod) => mod.default),
	nord: () => import('./chunks/nord.c4e6234a.mjs').then((mod) => mod.default),
	'one-dark-pro': () => import('./chunks/one-dark-pro.508ab27a.mjs').then((mod) => mod.default),
	poimandres: () => import('./chunks/poimandres.9d3a0da2.mjs').then((mod) => mod.default),
	'rose-pine-dawn': () => import('./chunks/rose-pine-dawn.a1ac07b2.mjs').then((mod) => mod.default),
	'rose-pine-moon': () => import('./chunks/rose-pine-moon.3502176d.mjs').then((mod) => mod.default),
	'rose-pine': () => import('./chunks/rose-pine.4260fdab.mjs').then((mod) => mod.default),
	'slack-dark': () => import('./chunks/slack-dark.ca95ebf9.mjs').then((mod) => mod.default),
	'slack-ochin': () => import('./chunks/slack-ochin.92b84326.mjs').then((mod) => mod.default),
	'solarized-dark': () => import('./chunks/solarized-dark.8c233c43.mjs').then((mod) => mod.default),
	'solarized-light': () => import('./chunks/solarized-light.c6bb6780.mjs').then((mod) => mod.default),
	'vitesse-dark': () => import('./chunks/vitesse-dark.3f9b485b.mjs').then((mod) => mod.default),
	'vitesse-light': () => import('./chunks/vitesse-light.2a8df4bf.mjs').then((mod) => mod.default),
};

const languages = {
	abap: () =>
		import('./chunks/abap.tmLanguage.90f43bd3.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'abap');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	'actionscript-3': () =>
		import('./chunks/actionscript-3.tmLanguage.22c45571.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'actionscript-3');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	ada: () =>
		import('./chunks/ada.tmLanguage.bbada208.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'ada');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	apache: () =>
		import('./chunks/apache.tmLanguage.14bf6d31.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'apache');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	apex: () =>
		import('./chunks/apex.tmLanguage.53e45f77.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'apex');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	apl: () =>
		import('./chunks/apl.tmLanguage.c2079fda.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'apl');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	applescript: () =>
		import('./chunks/applescript.tmLanguage.3fd248fb.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'applescript');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	asm: () =>
		import('./chunks/asm.tmLanguage.15765988.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'asm');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	astro: () =>
		import('./chunks/astro.tmLanguage.48a2894d.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'astro');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	awk: () =>
		import('./chunks/awk.tmLanguage.2f62c203.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'awk');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	ballerina: () =>
		import('./chunks/ballerina.tmLanguage.6024f645.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'ballerina');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	bat: () =>
		import('./chunks/bat.tmLanguage.cded4316.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'bat');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	berry: () =>
		import('./chunks/berry.tmLanguage.c980beee.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'berry');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	bibtex: () =>
		import('./chunks/bibtex.tmLanguage.fc9af179.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'bibtex');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	bicep: () =>
		import('./chunks/bicep.tmLanguage.63286f93.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'bicep');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	blade: () =>
		import('./chunks/blade.tmLanguage.5ab4f623.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'blade');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	c: () =>
		import('./chunks/c.tmLanguage.cb7fbdd5.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'c');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	cadence: () =>
		import('./chunks/cadence.tmLanguage.582679a9.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'cadence');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	clarity: () =>
		import('./chunks/clarity.tmLanguage.c2023279.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'clarity');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	clojure: () =>
		import('./chunks/clojure.tmLanguage.552833f9.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'clojure');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	cmake: () =>
		import('./chunks/cmake.tmLanguage.837f2f7d.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'cmake');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	cobol: () =>
		import('./chunks/cobol.tmLanguage.1d855399.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'cobol');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	codeql: () =>
		import('./chunks/codeql.tmLanguage.ac48a804.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'codeql');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	coffee: () =>
		import('./chunks/coffee.tmLanguage.8d9f5e98.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'coffee');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	'cpp-macro': () =>
		import('./chunks/cpp-macro.tmLanguage.56442b58.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'cpp-macro');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	cpp: () =>
		import('./chunks/cpp.tmLanguage.0d62ebb3.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'cpp');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	crystal: () =>
		import('./chunks/crystal.tmLanguage.5dba6dad.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'crystal');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	csharp: () =>
		import('./chunks/csharp.tmLanguage.e843df0c.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'csharp');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	css: () =>
		import('./chunks/css.tmLanguage.aa42be68.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'css');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	cue: () =>
		import('./chunks/cue.tmLanguage.659a5c98.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'cue');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	d: () =>
		import('./chunks/d.tmLanguage.2f917db0.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'd');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	dart: () =>
		import('./chunks/dart.tmLanguage.eeb4c7f9.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'dart');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	diff: () =>
		import('./chunks/diff.tmLanguage.d31ff9b6.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'diff');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	docker: () =>
		import('./chunks/docker.tmLanguage.d111d5ed.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'docker');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	'dream-maker': () =>
		import('./chunks/dream-maker.tmLanguage.0ffb65b4.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'dream-maker');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	elixir: () =>
		import('./chunks/elixir.tmLanguage.6d46e86b.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'elixir');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	elm: () =>
		import('./chunks/elm.tmLanguage.103aa363.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'elm');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	erb: () =>
		import('./chunks/erb.tmLanguage.0fa47828.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'erb');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	erlang: () =>
		import('./chunks/erlang.tmLanguage.cf3fb91b.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'erlang');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	fish: () =>
		import('./chunks/fish.tmLanguage.f1f0521b.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'fish');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	fsharp: () =>
		import('./chunks/fsharp.tmLanguage.680ee921.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'fsharp');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	gherkin: () =>
		import('./chunks/gherkin.tmLanguage.c3b24f9b.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'gherkin');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	'git-commit': () =>
		import('./chunks/git-commit.tmLanguage.14bba07b.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'git-commit');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	'git-rebase': () =>
		import('./chunks/git-rebase.tmLanguage.93cbf6c5.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'git-rebase');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	glsl: () =>
		import('./chunks/glsl.tmLanguage.f2313b4d.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'glsl');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	gnuplot: () =>
		import('./chunks/gnuplot.tmLanguage.b75af28f.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'gnuplot');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	go: () =>
		import('./chunks/go.tmLanguage.c2a5df11.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'go');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	graphql: () =>
		import('./chunks/graphql.tmLanguage.3f9696f4.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'graphql');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	groovy: () =>
		import('./chunks/groovy.tmLanguage.4caa187d.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'groovy');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	hack: () =>
		import('./chunks/hack.tmLanguage.4b437c29.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'hack');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	haml: () =>
		import('./chunks/haml.tmLanguage.1856c417.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'haml');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	handlebars: () =>
		import('./chunks/handlebars.tmLanguage.2efba05a.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'handlebars');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	haskell: () =>
		import('./chunks/haskell.tmLanguage.4c61530d.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'haskell');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	hcl: () =>
		import('./chunks/hcl.tmLanguage.5d6b7314.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'hcl');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	hlsl: () =>
		import('./chunks/hlsl.tmLanguage.613dd968.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'hlsl');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	html: () =>
		import('./chunks/html.tmLanguage.6fb5149b.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'html');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	ini: () =>
		import('./chunks/ini.tmLanguage.0cbf1d05.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'ini');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	java: () =>
		import('./chunks/java.tmLanguage.c39bfaf6.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'java');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	javascript: () =>
		import('./chunks/javascript.tmLanguage.61e966ee.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'javascript');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	'jinja-html': () =>
		import('./chunks/jinja-html.tmLanguage.39b5f211.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'jinja-html');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	jinja: () =>
		import('./chunks/jinja.tmLanguage.fa903c4e.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'jinja');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	json: () =>
		import('./chunks/json.tmLanguage.74144e27.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'json');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	jsonc: () =>
		import('./chunks/jsonc.tmLanguage.1a3b9109.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'jsonc');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	jsonnet: () =>
		import('./chunks/jsonnet.tmLanguage.8d48996b.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'jsonnet');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	jssm: () =>
		import('./chunks/jssm.tmLanguage.6c1b9158.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'jssm');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	jsx: () =>
		import('./chunks/jsx.tmLanguage.583f1a16.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'jsx');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	julia: () =>
		import('./chunks/julia.tmLanguage.0a8b94e7.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'julia');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	kotlin: () =>
		import('./chunks/kotlin.tmLanguage.e8a6b7a6.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'kotlin');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	latex: () =>
		import('./chunks/latex.tmLanguage.ecadecb8.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'latex');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	less: () =>
		import('./chunks/less.tmLanguage.bd7b8d56.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'less');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	liquid: () =>
		import('./chunks/liquid.tmLanguage.7754d6e3.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'liquid');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	lisp: () =>
		import('./chunks/lisp.tmLanguage.5f9b63b2.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'lisp');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	logo: () =>
		import('./chunks/logo.tmLanguage.b0c8f7e2.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'logo');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	lua: () =>
		import('./chunks/lua.tmLanguage.8d5fb6ef.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'lua');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	make: () =>
		import('./chunks/make.tmLanguage.c2039eb5.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'make');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	markdown: () =>
		import('./chunks/markdown.tmLanguage.a4092da8.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'markdown');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	marko: () =>
		import('./chunks/marko.tmLanguage.ba870c0b.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'marko');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	matlab: () =>
		import('./chunks/matlab.tmLanguage.59d7a9f2.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'matlab');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	mdx: () =>
		import('./chunks/mdx.tmLanguage.55107cd9.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'mdx');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	mermaid: () =>
		import('./chunks/mermaid.tmLanguage.4de46447.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'mermaid');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	nginx: () =>
		import('./chunks/nginx.tmLanguage.b75b10ef.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'nginx');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	nim: () =>
		import('./chunks/nim.tmLanguage.c387f2c9.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'nim');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	nix: () =>
		import('./chunks/nix.tmLanguage.59e90ede.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'nix');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	'objective-c': () =>
		import('./chunks/objective-c.tmLanguage.20751fe3.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'objective-c');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	'objective-cpp': () =>
		import('./chunks/objective-cpp.tmLanguage.35952028.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'objective-cpp');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	ocaml: () =>
		import('./chunks/ocaml.tmLanguage.0143759c.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'ocaml');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	pascal: () =>
		import('./chunks/pascal.tmLanguage.24002509.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'pascal');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	perl: () =>
		import('./chunks/perl.tmLanguage.95aaa323.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'perl');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	'php-html': () =>
		import('./chunks/php-html.tmLanguage.9bf25695.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'php-html');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	php: () =>
		import('./chunks/php.tmLanguage.208cc284.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'php');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	plsql: () =>
		import('./chunks/plsql.tmLanguage.2f001168.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'plsql');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	postcss: () =>
		import('./chunks/postcss.tmLanguage.df844470.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'postcss');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	powershell: () =>
		import('./chunks/powershell.tmLanguage.557fecb1.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'powershell');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	prisma: () =>
		import('./chunks/prisma.tmLanguage.3d2ecd3f.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'prisma');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	prolog: () =>
		import('./chunks/prolog.tmLanguage.6aaa58fd.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'prolog');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	pug: () =>
		import('./chunks/pug.tmLanguage.923cd00e.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'pug');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	puppet: () =>
		import('./chunks/puppet.tmLanguage.7c62b6f0.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'puppet');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	purescript: () =>
		import('./chunks/purescript.tmLanguage.a1fbe8e9.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'purescript');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	python: () =>
		import('./chunks/python.tmLanguage.255784a7.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'python');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	r: () =>
		import('./chunks/r.tmLanguage.27744799.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'r');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	raku: () =>
		import('./chunks/raku.tmLanguage.3eec78ae.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'raku');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	razor: () =>
		import('./chunks/razor.tmLanguage.423995f0.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'razor');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	rel: () =>
		import('./chunks/rel.tmLanguage.8d9faf37.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'rel');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	riscv: () =>
		import('./chunks/riscv.tmLanguage.86c81d11.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'riscv');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	rst: () =>
		import('./chunks/rst.tmLanguage.3203d5d2.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'rst');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	ruby: () =>
		import('./chunks/ruby.tmLanguage.5878ff9e.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'ruby');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	rust: () =>
		import('./chunks/rust.tmLanguage.ca198b9a.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'rust');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	sas: () =>
		import('./chunks/sas.tmLanguage.96dffcab.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'sas');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	sass: () =>
		import('./chunks/sass.tmLanguage.69993358.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'sass');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	scala: () =>
		import('./chunks/scala.tmLanguage.f0618f94.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'scala');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	scheme: () =>
		import('./chunks/scheme.tmLanguage.43867c45.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'scheme');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	scss: () =>
		import('./chunks/scss.tmLanguage.34ac990b.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'scss');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	shaderlab: () =>
		import('./chunks/shaderlab.tmLanguage.32cc3af0.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'shaderlab');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	shellscript: () =>
		import('./chunks/shellscript.tmLanguage.709c69f9.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'shellscript');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	smalltalk: () =>
		import('./chunks/smalltalk.tmLanguage.bed30313.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'smalltalk');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	solidity: () =>
		import('./chunks/solidity.tmLanguage.f49e6b87.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'solidity');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	sparql: () =>
		import('./chunks/sparql.tmLanguage.cca7e4fb.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'sparql');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	sql: () =>
		import('./chunks/sql.tmLanguage.53f84ea8.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'sql');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	'ssh-config': () =>
		import('./chunks/ssh-config.tmLanguage.88607f95.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'ssh-config');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	stata: () =>
		import('./chunks/stata.tmLanguage.7941b321.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'stata');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	stylus: () =>
		import('./chunks/stylus.tmLanguage.aae41083.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'stylus');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	svelte: () =>
		import('./chunks/svelte.tmLanguage.51e3e183.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'svelte');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	swift: () =>
		import('./chunks/swift.tmLanguage.1758b78f.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'swift');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	'system-verilog': () =>
		import('./chunks/system-verilog.tmLanguage.98c0822c.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'system-verilog');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	tasl: () =>
		import('./chunks/tasl.tmLanguage.f048ca02.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'tasl');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	tcl: () =>
		import('./chunks/tcl.tmLanguage.331e619d.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'tcl');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	tex: () =>
		import('./chunks/tex.tmLanguage.378e91de.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'tex');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	toml: () =>
		import('./chunks/toml.tmLanguage.ac48c2b1.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'toml');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	tsx: () =>
		import('./chunks/tsx.tmLanguage.8c2c7b1b.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'tsx');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	turtle: () =>
		import('./chunks/turtle.tmLanguage.81eec047.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'turtle');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	twig: () =>
		import('./chunks/twig.tmLanguage.cdc9b736.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'twig');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	typescript: () =>
		import('./chunks/typescript.tmLanguage.e7dbfd15.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'typescript');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	vb: () =>
		import('./chunks/vb.tmLanguage.b376ae92.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'vb');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	verilog: () =>
		import('./chunks/verilog.tmLanguage.6c2eff21.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'verilog');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	vhdl: () =>
		import('./chunks/vhdl.tmLanguage.336d9759.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'vhdl');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	viml: () =>
		import('./chunks/viml.tmLanguage.bf2daa01.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'viml');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	'vue-html': () =>
		import('./chunks/vue-html.tmLanguage.3a2e7543.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'vue-html');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	vue: () =>
		import('./chunks/vue.tmLanguage.c77b2cf0.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'vue');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	wasm: () =>
		import('./chunks/wasm.tmLanguage.b7f5d22e.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'wasm');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	wenyan: () =>
		import('./chunks/wenyan.tmLanguage.5d7089b7.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'wenyan');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	xml: () =>
		import('./chunks/xml.tmLanguage.f76daefd.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'xml');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	xsl: () =>
		import('./chunks/xsl.tmLanguage.ab9f8922.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'xsl');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	yaml: () =>
		import('./chunks/yaml.tmLanguage.2e704356.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'yaml');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
	zenscript: () =>
		import('./chunks/zenscript.tmLanguage.7f56cf0d.mjs')
			.then((mod) => mod.default)
			.then((grammar) => {
				const lang = BUNDLED_LANGUAGES.find((l) => l.id === 'zenscript');
				if (lang) {
					return {
						...lang,
						grammar,
					};
				} else {
					return undefined;
				}
			}),
};

// Caches Promise<Highlighter> for reuse when the same theme and langs are provided
const _resolvedHighlighters = new Map();

/** @type {Promise<any>} */
let _allLanguages;

function stringify(opts) {
	// Always sort keys before stringifying to make sure objects match regardless of parameter ordering
	return JSON.stringify(opts, Object.keys(opts).sort());
}

/**
 * @param {import('shiki').HighlighterOptions} opts
 * @returns {Promise<import('shiki').Highlighter>}
 */
async function resolveHighlighter(opts) {
	const resolvedThemes = [];
	if (opts.theme && opts.theme in themes) {
		resolvedThemes.push(await themes[opts.theme]());
	}

	let resolvedLanguages;
	if (opts.langs) {
		resolvedLanguages = opts.langs;
	} else {
		if (!_allLanguages) {
			_allLanguages = (await Promise.all(Object.values(languages).map((fn) => fn()))).filter(
				Boolean
			);
		}
		resolvedLanguages = await _allLanguages;
	}

	/** @type {import('shiki').HighlighterOptions} */
	const highlighterOptions = {
		...opts,
		themes: resolvedThemes,
		langs: resolvedLanguages,
	};

	// Do not pass through the theme as that will attempt to load it, even if it's included in themes
	delete highlighterOptions['theme'];

	// Start the async getHighlighter call and cache the Promise
	const highlighter = getHighlighter$1(highlighterOptions).then((hl) => {
		hl.setColorReplacements({
			'#000001': 'var(--astro-code-color-text)',
			'#000002': 'var(--astro-code-color-background)',
			'#000004': 'var(--astro-code-token-constant)',
			'#000005': 'var(--astro-code-token-string)',
			'#000006': 'var(--astro-code-token-comment)',
			'#000007': 'var(--astro-code-token-keyword)',
			'#000008': 'var(--astro-code-token-parameter)',
			'#000009': 'var(--astro-code-token-function)',
			'#000010': 'var(--astro-code-token-string-expression)',
			'#000011': 'var(--astro-code-token-punctuation)',
			'#000012': 'var(--astro-code-token-link)',
		});
		return hl;
	});

	return highlighter;
}

/**
 * @param {import('shiki').HighlighterOptions} opts
 * @returns {Promise<import('shiki').Highlighter>}
 */
function getHighlighter(opts) {
	const key = stringify(opts);

	// Highlighter has already been requested, reuse the same instance
	if (_resolvedHighlighters.has(key)) {
		return _resolvedHighlighters.get(key);
	}

	const highlighter = resolveHighlighter(opts);
	_resolvedHighlighters.set(key, highlighter);

	return highlighter;
}

const $$Astro$q = createAstro("/Users/sandrarodgers/web-next/blog/node_modules/astro/components/Code.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Code = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$q, $$props, $$slots);
  Astro2.self = $$Code;
  const { code, lang = "plaintext", theme = "github-dark", wrap = false } = Astro2.props;
  function repairShikiTheme(html2) {
    html2 = html2.replace('<pre class="shiki"', '<pre class="astro-code"');
    if (wrap === false) {
      html2 = html2.replace(/style="(.*?)"/, 'style="$1; overflow-x: auto;"');
    } else if (wrap === true) {
      html2 = html2.replace(
        /style="(.*?)"/,
        'style="$1; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;"'
      );
    }
    return html2;
  }
  const highlighter = await getHighlighter({
    theme,
    langs: typeof lang !== "string" ? [lang] : void 0
  });
  const _html = highlighter.codeToHtml(code, {
    lang: typeof lang === "string" ? lang : lang.id,
    theme
  });
  const html = repairShikiTheme(_html);
  return renderTemplate`${renderComponent($$result, "Fragment", Fragment, {}, { "default": () => renderTemplate`${unescapeHTML(html)}` })}
`;
}, "/Users/sandrarodgers/web-next/blog/node_modules/astro/components/Code.astro");

const $$Astro$p = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/code/CodeBlockSB.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$CodeBlockSB = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$p, $$props, $$slots);
  Astro2.self = $$CodeBlockSB;
  const { blok, index } = Astro2.props;
  let lang = blok.code.content[0].attrs.class.replace("language-", "");
  return renderTemplate`${maybeRenderHead($$result)}<div${addAttribute(`lang == ${index}`, "x-show")}${addAttribute(`code-block astro-YI24S3CN`, "class")}${addAttribute(`{'block':lang === ${index}}`, "x-bind:class")}>
	${renderComponent($$result, "Code", $$Code, { "code": blok.code.content[0].content[0].text, "lang": lang, "class": "astro-YI24S3CN" })}
</div>

`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/code/CodeBlockSB.astro");

const $$Astro$o = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/code/CodeGroupSB.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$CodeGroupSB = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$o, $$props, $$slots);
  Astro2.self = $$CodeGroupSB;
  const { blok } = Astro2.props;
  return renderTemplate`${maybeRenderHead($$result)}<div class="code-group astro-V7JYVM3X"${addAttribute(`{ lang: 0, 
  change(e) {
    console.log('lang before', this.lang)
    console.log('e', e)
    this.lang = e.target.value
    console.log('lang after', this.lang)
  } 
}`, "x-data")}>
	<header class="astro-V7JYVM3X">
		<p class="text-sm tracking-wide flex items-center astro-V7JYVM3X" style="padding-bottom: 0">${blok.title}</p>

		${blok.code_blocks && blok.code_blocks.length > 1 && renderTemplate`<select x-on:change="change" class="font-medium astro-V7JYVM3X">
				${blok.code_blocks.map((cb, index) => {
    return renderTemplate`<option${addAttribute(index, "value")} class="astro-V7JYVM3X">${cb.tab_title}</option>`;
  })}
			</select>`}
	</header>

	${blok.code_blocks && blok.code_blocks.length > 0 && blok.code_blocks.map((cb, index) => {
    return renderTemplate`${renderComponent($$result, "CodeBlok", $$CodeBlockSB, { "blok": cb, "index": index, "class": "astro-V7JYVM3X" })}`;
  })}
</div>

`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/code/CodeGroupSB.astro");

const components = {author: $$Author,richTextSection: $$RichText,iframe: $$Iframe,image: $$Image,whitepaperLink: $$WhitepaperPromoSB,panel: $$Panel,youTube: $$YouTube,alert: $$Alert,codeEmbed: $$CodeEmbed,table: $$Table,codeGroup: $$CodeGroupSB};

const $$Astro$n = createAstro("/Users/sandrarodgers/web-next/blog/node_modules/@storyblok/astro/StoryblokComponent.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$StoryblokComponent = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$n, $$props, $$slots);
  Astro2.self = $$StoryblokComponent;
  const { blok, ...props } = Astro2.props;
  const key = camelcase(blok.component);
  if (!(key in components)) {
    throw new Error(
      `Component could not be found for blok "${blok.component}"! Is it defined in astro.config.mjs?`
    );
  }
  const Component = components[key];
  return renderTemplate`${renderComponent($$result, "Component", Component, { "blok": blok, ...props })}`;
}, "/Users/sandrarodgers/web-next/blog/node_modules/@storyblok/astro/StoryblokComponent.astro");

const $$Astro$m = createAstro("/Users/sandrarodgers/web-next/blog/src/pages/authors/index.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Index$1 = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$m, $$props, $$slots);
  Astro2.self = $$Index$1;
  const sbApi = F();
  const authors = await sbApi.getAll("cdn/stories", {
    by_slugs: "authors/*",
    sort_by: "content.title:asc"
  });
  const team = authors.filter((a) => a.content.team === "team");
  const alumni = authors.filter((a) => a.content.team === "alum");
  const guests = authors.filter((a) => a.content.team === "guest");
  const schema = {
    "@context": "https://schema.org",
    "@type": "BreadcrumbList",
    itemListElement: [
      {
        "@type": "ListItem",
        position: 1,
        name: "Deepgram Home",
        item: "https://deepgram.com"
      },
      {
        "@type": "ListItem",
        position: 2,
        name: "Blog",
        item: Astro2.url.origin
      },
      {
        "@type": "ListItem",
        position: 3,
        name: "Authors"
      }
    ]
  };
  return renderTemplate`${renderComponent($$result, "Layout", $$Default, {}, { "default": () => renderTemplate`${renderComponent($$result, "ContrastSection", $$ContrastSection, { "contrast": "black", "class": "bg-starburst-gradient bg-responsive-svg", "background": "darkCharcoal", "bottomDivider": "eclipse-divider" }, { "default": () => renderTemplate`${renderComponent($$result, "PrimarySection", $$PrimarySection, { "class": "pt-[6.75rem] md:pt-[7.03125rem] lg:pt-[7.3125rem] xl:pt-[7.875rem]" }, { "default": () => renderTemplate`${renderComponent($$result, "Backlink", $$Backlink, { "href": "/posts", "class": "mb-6 md:mb-8 lg:mb-10 xl:mb-12 text-lightIris" }, { "default": () => renderTemplate`All posts` })}${renderComponent($$result, "Hero", $$Hero$1, { "title": "Authors" })}` })}` })}${renderComponent($$result, "PrimarySection", $$PrimarySection, {}, { "default": () => renderTemplate`${renderComponent($$result, "AuthorCardList", $$AuthorCardList, { "class": "mt-10 lg:mt-16 xl:mt-20" }, { "default": () => renderTemplate`${team.map((blok) => renderTemplate`${renderComponent($$result, "StoryblokComponent", $$StoryblokComponent, { "blok": blok.content, "slug": blok.slug })}`)}`, "title": () => renderTemplate`${renderComponent($$result, "NoLinkCardListTitle", $$NoLinkCardListTitle, { "slot": "title" }, { "default": () => renderTemplate`${maybeRenderHead($$result)}<h2>Deepgram Team</h2>` })}` })}${renderComponent($$result, "AuthorCardList", $$AuthorCardList, { "class": "mt-10 lg:mt-16 xl:mt-20" }, { "default": () => renderTemplate`${guests.map((blok) => renderTemplate`${renderComponent($$result, "StoryblokComponent", $$StoryblokComponent, { "blok": blok.content, "slug": blok.slug })}`)}`, "title": () => renderTemplate`${renderComponent($$result, "NoLinkCardListTitle", $$NoLinkCardListTitle, { "slot": "title" }, { "default": () => renderTemplate`<h2>Guest Authors</h2>` })}` })}${renderComponent($$result, "AuthorCardList", $$AuthorCardList, { "class": "mt-10 lg:mt-16 xl:mt-20 mb-6" }, { "default": () => renderTemplate`${alumni.map((blok) => renderTemplate`${renderComponent($$result, "StoryblokComponent", $$StoryblokComponent, { "blok": blok.content, "slug": blok.slug })}`)}`, "title": () => renderTemplate`${renderComponent($$result, "NoLinkCardListTitle", $$NoLinkCardListTitle, { "slot": "title" }, { "default": () => renderTemplate`<h2>Deepgram ${alumni.length > 1 ? "Alumni" : "Alum"}</h2>` })}` })}` })}`, "head:description": () => renderTemplate`${renderComponent($$result, "Description", $$Description, { "slot": "head:description", "name": "description", "content": "All the authors writing content on the Deepgram blog." })}`, "head:title": () => renderTemplate`${renderComponent($$result, "Title", $$Title, { "slot": "head:title", "title": "All authors - Deepgram Blog \u26A1\uFE0F" })}`, "json:ld": () => renderTemplate`${renderComponent($$result, "JsonLD", $$JsonLD, { "slot": "json:ld", "schema": schema })}`, "og:description": () => renderTemplate`${renderComponent($$result, "Meta", $$Meta, { "slot": "og:description", "property": "og:description", "content": "All the authors writing content on the Deepgram blog." })}`, "og:title": () => renderTemplate`${renderComponent($$result, "Meta", $$Meta, { "slot": "og:title", "property": "og:title", "content": "All authors - Deepgram Blog \u26A1\uFE0F" })}` })}`;
}, "/Users/sandrarodgers/web-next/blog/src/pages/authors/index.astro");

const $$file$5 = "/Users/sandrarodgers/web-next/blog/src/pages/authors/index.astro";
const $$url$5 = "/authors";

const _page3 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  default: $$Index$1,
  file: $$file$5,
  url: $$url$5
}, Symbol.toStringTag, { value: 'Module' }));

const $$Astro$l = createAstro("/Users/sandrarodgers/web-next/blog/src/components/authors/AuthorPostsPage.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$AuthorPostsPage = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$l, $$props, $$slots);
  Astro2.self = $$AuthorPostsPage;
  const { posts, page, slug, path } = Astro2.props;
  return renderTemplate`${maybeRenderHead($$result)}<div class="w-full">
	<div class="relative mx-auto">
		<div class="grid grid-flow-row-dense grid-cols-1 md:grid-cols-2 lg:grid-cols-3 xl:grid-cols-4 justify-items-center gap-4 lg:gap-6">
			${posts.map((post, index) => renderTemplate`${renderComponent($$result, "BlogCard", $$BlogCardSB, { "post": post })}`)}
		</div>
	</div>
</div>

${renderComponent($$result, "Pagination", $$Pagination, { "page": page, "path": path, "slug": slug })}`;
}, "/Users/sandrarodgers/web-next/blog/src/components/authors/AuthorPostsPage.astro");

const $$Astro$k = createAstro("/Users/sandrarodgers/web-next/blog/src/components/authors/AuthorHero.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$AuthorHero = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$k, $$props, $$slots);
  Astro2.self = $$AuthorHero;
  const { author } = Astro2.props;
  return renderTemplate`${maybeRenderHead($$result)}<div class="author-hero mb-20 astro-A7KCTYQB">
	<div class="flex-1 author-info astro-A7KCTYQB">
		<h1 class="name astro-A7KCTYQB">${author.content.title}</h1>
		<h2 class="title astro-A7KCTYQB">${author.content.jobtitle}</h2>
		<ul class="socials astro-A7KCTYQB">
			${author.content.twitter && renderTemplate`<li class="astro-A7KCTYQB">
					${renderComponent($$result, "Link", $$Link$1, { "href": `https://twitter.com/${author.content.twitter}`, "target": "_blank", "rel": "nofollow", "class": "astro-A7KCTYQB" }, { "default": () => renderTemplate`${renderComponent($$result, "Icon", $$Icon, { "icon": "twitter", "class": "astro-A7KCTYQB" })}` })}
				</li>`}

			${author.content.linkedin && renderTemplate`<li class="astro-A7KCTYQB">
					${renderComponent($$result, "Link", $$Link$1, { "href": author.content.linkedin, "target": "_blank", "rel": "nofollow", "class": "astro-A7KCTYQB" }, { "default": () => renderTemplate`${renderComponent($$result, "Icon", $$Icon, { "icon": "linkedin", "class": "astro-A7KCTYQB" })}` })}
				</li>`}
			${author.content.github && renderTemplate`<li class="astro-A7KCTYQB">
					${renderComponent($$result, "Link", $$Link$1, { "href": author.content.github, "target": "_blank", "rel": "nofollow", "class": "astro-A7KCTYQB" }, { "default": () => renderTemplate`${renderComponent($$result, "Icon", $$Icon, { "icon": "github", "class": "astro-A7KCTYQB" })}` })}
				</li>`}
			${author.content.twitch && renderTemplate`<li class="astro-A7KCTYQB">
					${renderComponent($$result, "Link", $$Link$1, { "href": author.content.twitch, "target": "_blank", "rel": "nofollow", "class": "astro-A7KCTYQB" }, { "default": () => renderTemplate`${renderComponent($$result, "Icon", $$Icon, { "icon": "twitch", "class": "astro-A7KCTYQB" })}` })}
				</li>`}
		</ul>
		<p class="bio text-cloud font-normal astro-A7KCTYQB">${author.content.bio}</p>
	</div>
	<div class="picture-section astro-A7KCTYQB">
		${renderComponent($$result, "Svg", $$Svg, { "class": "fingerprint astro-A7KCTYQB", "name": "fingerprint" })}
		<img class="picture astro-A7KCTYQB"${addAttribute(author.content.picture.filename, "src")}${addAttribute(author.content.title, "alt")}>
	</div>
</div>

`;
}, "/Users/sandrarodgers/web-next/blog/src/components/authors/AuthorHero.astro");

const $$Astro$j = createAstro("/Users/sandrarodgers/web-next/blog/src/pages/authors/[author]/[...page].astro", "", "file:///Users/sandrarodgers/web-next/blog/");
async function getStaticPaths$3({ paginate }) {
  const sbApi = F();
  const allPosts = await sbApi.getAll("cdn/stories", {
    by_slugs: "blog-posts/*"
  });
  const authors = await sbApi.getAll("cdn/stories", {
    by_slugs: "authors/*"
  });
  const sortedPosts = allPosts.sort((a, b) => {
    const aDate = new Date(b.content.date);
    const bDate = new Date(a.content.date);
    return aDate.getTime() - bDate.getTime();
  });
  return authors.map((author) => {
    const posts = sortedPosts.filter((post) => {
      if (post.content.authors) {
        return post.content.authors.includes(author.uuid);
      }
      return false;
    });
    return paginate(posts, {
      params: {
        author: author.slug
      },
      pageSize: 20,
      props: {
        author
      }
    });
  });
}
const $$$2 = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$j, $$props, $$slots);
  Astro2.self = $$$2;
  const { page, author } = Astro2.props;
  const name = author.content.title.split(" ");
  const firstName = name.pop();
  const lastName = name.join("");
  if (author.content.picture.filename.indexOf("cloudinary") > -1) {
    if (author.content.picture.filename.indexOf("w_") === -1) {
      author.content.picture.filename = author.content.picture.filename.replace("upload/", "upload/w_320/");
    }
    if (author.content.picture.filename.indexOf("c_") === -1) {
      author.content.picture.filename = author.content.picture.filename.replace("upload/", "upload/c_crop/");
    }
  }
  const socials = [];
  if (author.content.twitter) {
    socials.push(`https://twitter.com/${author.content.twitter}`);
  }
  if (author.content.linkedin) {
    socials.push(author.content.linkedin);
  }
  if (author.content.github) {
    socials.push(author.content.github);
  }
  if (author.content.twitch) {
    socials.push(author.content.twitch);
  }
  if (author.content.website) {
    socials.push(author.content.website);
  }
  if (author.content.youtube) {
    socials.push(author.content.youtube);
  }
  const schema = [
    {
      "@context": "https://schema.org",
      "@type": "Person",
      image: author.content.picture ? author.content.picture : null,
      name: author.content.title,
      jobTitle: author.content.team ? author.content.jobtitle ? author.content.jobtitle : "Deepgram Team" : author.content.alumni ? "Deepgram Alumni" : "Guest Author",
      url: Astro2.url,
      sameAs: socials
    },
    {
      "@context": "https://schema.org",
      "@type": "BreadcrumbList",
      itemListElement: [
        {
          "@type": "ListItem",
          position: 1,
          name: "Deepgram Home",
          item: "https://deepgram.com"
        },
        {
          "@type": "ListItem",
          position: 2,
          name: "Blog",
          item: Astro2.url.origin
        },
        {
          "@type": "ListItem",
          position: 3,
          name: "Authors",
          item: `${Astro2.url.origin}/authors`
        },
        {
          "@type": "ListItem",
          position: 4,
          name: author.content.title
        }
      ]
    }
  ];
  return renderTemplate`${renderComponent($$result, "Layout", $$Default, {}, { "default": () => renderTemplate`${renderComponent($$result, "ContrastSection", $$ContrastSection, { "contrast": "black", "background": "darkCharcoal", "bottomDivider": "eclipse-divider", "class": "-mb-32" }, { "default": () => renderTemplate`${renderComponent($$result, "PrimarySection", $$PrimarySection, { "class": "bg-black pt-[6.75rem] md:pt-[7.03125rem] lg:pt-[7.3125rem] xl:pt-[7.875rem]" }, { "default": () => renderTemplate`${renderComponent($$result, "Backlink", $$Backlink, { "href": "/authors", "class": "text-lightIris mb-6 md:mb-8 lg:mb-10 xl:mb-12" }, { "default": () => renderTemplate`All authors` })}${renderComponent($$result, "AuthorHero", $$AuthorHero, { "author": author })}` })}` })}${renderComponent($$result, "PrimarySection", $$PrimarySection, {}, { "default": () => renderTemplate`${renderComponent($$result, "AuthorPostsPage", $$AuthorPostsPage, { "slug": `authors/${author.slug}`, "page": page, "posts": page.data, "link": "/authors", "linkText": "All authors", "subtitle": `All ${author.content.title} posts` })}` })}`, "head": () => renderTemplate`${renderComponent($$result, "Meta", $$Meta, { "slot": "head", "property": "profile:username", "content": author.slug })}${renderComponent($$result, "Meta", $$Meta, { "slot": "head", "property": "profile:first_name", "content": firstName })}${renderComponent($$result, "Meta", $$Meta, { "slot": "head", "property": "profile:last_name", "content": lastName })}${author.content.twitter && renderTemplate`${renderComponent($$result, "Meta", $$Meta, { "slot": "head", "property": "twitter:creator", "content": `@${author.content.twitter}` })}`}`, "head:description": () => renderTemplate`${renderComponent($$result, "Description", $$Description, { "slot": "head:description", "name": "description", "content": author.content.bio ? author.content.bio : `${author.content.title} is a ${author.content.jobtitle} writing content for the Deepgram blog.` })}`, "head:title": () => renderTemplate`${renderComponent($$result, "Title", $$Title, { "slot": "head:title", "title": `${author.content.title} - ${author.content.jobtitle} - Deepgram Blog \u26A1\uFE0F` })}`, "json:ld": () => renderTemplate`${renderComponent($$result, "JsonLD", $$JsonLD, { "slot": "json:ld", "schema": schema })}`, "og:description": () => renderTemplate`${renderComponent($$result, "Meta", $$Meta, { "slot": "og:description", "property": "og:description", "content": author.content.bio ? author.content.bio : `${author.content.title} is a ${author.content.jobtitle} writing content for the Deepgram blog.` })}`, "og:image": () => renderTemplate`${author.content.picture.filename && renderTemplate`${renderComponent($$result, "Meta", $$Meta, { "slot": "og:image", "property": "og:image", "content": author.content.picture.filename })}`}`, "og:image:alt": () => renderTemplate`${author.content.picture.filename && renderTemplate`${renderComponent($$result, "Meta", $$Meta, { "slot": "og:image:alt", "property": "og:image:alt", "content": author.content.bio ? `Profile pic for ${author.content.title}. ${author.content.bio}` : `Profile pic for ${author.content.title}, a ${author.content.jobtitle} writing content for the Deepgram blog.` })}`}`, "og:title": () => renderTemplate`${renderComponent($$result, "Meta", $$Meta, { "slot": "og:title", "property": "og:title", "content": `${author.content.title} - ${author.content.jobtitle} - Deepgram Blog \u26A1\uFE0F` })}`, "og:type": () => renderTemplate`${renderComponent($$result, "Meta", $$Meta, { "slot": "og:type", "property": "og:type", "content": "profile" })}` })}`;
}, "/Users/sandrarodgers/web-next/blog/src/pages/authors/[author]/[...page].astro");

const $$file$4 = "/Users/sandrarodgers/web-next/blog/src/pages/authors/[author]/[...page].astro";
const $$url$4 = "/authors/[author]/[...page]";

const _page4 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  getStaticPaths: getStaticPaths$3,
  default: $$$2,
  file: $$file$4,
  url: $$url$4
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$4k = {"title":"2021 State of Automatic Speech Recognition Infographic","description":"The 2021 State of Automatic Speech Recognition (ASR) report details where enterprises are currently using this technology and where they see the most value. Find out why 85% of enterprises say that ASR is important or very important to their future strategy.","date":"2021-03-03T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981248/blog/2021-state-of-automatic-speech-recognition-infographic/2021-state-of-asr-infogfx%402x.jpg","authors":["keith-lam"],"category":"speech-trends","tags":["education"],"seo":{"title":"2021 State of Automatic Speech Recognition Infographic","description":"The 2021 State of Automatic Speech Recognition (ASR) report details where enterprises are currently using this technology and where they see the most value. Find out why 85% of enterprises say that ASR is important or very important to their future strategy."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981248/blog/2021-state-of-automatic-speech-recognition-infographic/2021-state-of-asr-infogfx%402x.jpg"},"shorturls":{"share":"https://dpgr.am/51a2e3b","twitter":"https://dpgr.am/8a7f914","linkedin":"https://dpgr.am/f727a45","reddit":"https://dpgr.am/c1c5957","facebook":"https://dpgr.am/adbc253"}};
						const file$4k = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/2021-state-of-automatic-speech-recognition-infographic/index.md";
						const url$4k = undefined;
						function rawContent$4k() {
							return "The [2021 State of Automatic Speech Recognition (ASR) report](https://deepgram.com/state-of-asr-report/) details where enterprises are currently using this technology and where they see the most value.  Find out why 85% of enterprises say that ASR is important or very important to their future strategy. \n\n[![](https://res.cloudinary.com/deepgram/image/upload/v1661976835/blog/2021-state-of-automatic-speech-recognition-infographic/state-of-asr-infogfx-full%402x.png)](https://deepgram.com/state-of-asr-report/)";
						}
						async function compiledContent$4k() {
							return load$4k().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$4k() {
							return (await import('./chunks/index.d424c8ac.mjs'));
						}
						function Content$4k(...args) {
							return load$4k().then((m) => m.default(...args));
						}
						Content$4k.isAstroComponentFactory = true;
						function getHeadings$4k() {
							return load$4k().then((m) => m.metadata.headings);
						}
						function getHeaders$4k() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$4k().then((m) => m.metadata.headings);
						}

const __vite_glob_0_0 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$4k,
  file: file$4k,
  url: url$4k,
  rawContent: rawContent$4k,
  compiledContent: compiledContent$4k,
  default: load$4k,
  Content: Content$4k,
  getHeadings: getHeadings$4k,
  getHeaders: getHeaders$4k
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$4j = {"title":"5 Ways to Understand the Voice of the Customer with Voice Technology","description":"Surveys and interviews are old—use voice technology to understand the voice of your customers better than ever before.","date":"2022-05-18T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981421/blog/5-ways-understand-voice-of-the-customer-voice-technology/harness-voice-customer-thumb-554x220%402x.png","authors":["chris-doty"],"category":"speech-trends","tags":["call-analytics","contact-center","sales-enablement"],"seo":{"title":"5 Ways to Understand the Voice of the Customer with Voice Technology","description":"Surveys and interviews are old—use voice technology to understand the voice of your customers better than ever before."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981421/blog/5-ways-understand-voice-of-the-customer-voice-technology/harness-voice-customer-thumb-554x220%402x.png"},"shorturls":{"share":"https://dpgr.am/83269b5","twitter":"https://dpgr.am/804d622","linkedin":"https://dpgr.am/8ab7271","reddit":"https://dpgr.am/6ce738c","facebook":"https://dpgr.am/2112978"}};
						const file$4j = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/5-ways-understand-voice-of-the-customer-voice-technology/index.md";
						const url$4j = undefined;
						function rawContent$4j() {
							return "\r\nThere's no doubt that customer feedback is important for businesses. It can help you understand what you're doing well and where you need to make improvements. But in the past, capturing and understanding the voice of the customer was a difficult process-it often involved surveys or focus groups that were time-consuming and expensive to conduct. But now, with the advent of voice technology, businesses have a new way to collect customer feedback and harness the voice of the customer. Voice technology is already being used in a number of customer-facing applications, such as customer service and call center operations. But it's also starting to be used in other areas, such as market research and product development. Voice technology can help businesses collect customer feedback in a more natural and efficient way, and can provide insights that traditional surveys and focus groups may not be able to provide. Before we dive into 5 ways that voice technology is being used to capture the voice of the customer, let's discuss what exactly the voice of the customer is.\r\n\r\n<WhitepaperPromo whitepaper=\"latest\"></WhitepaperPromo>\r\n\r\n## What is the Voice of the Customer?\r\n\r\nVoice of the customer (or VoC, also occasionally called \"customer voice\") is the customer's opinion about a company, including general impressions as well as specific feedback about a company's products and services. It can be positive or negative. Customer voice is important for businesses because it can help them understand what customers think about their business and where they need to make improvements. There are a number of ways to collect the voice of the customer, such as surveys, customer interviews, and focus groups, but there are problems with each of these methods. Surveys, for example, often only get 2% or 3% response rates, and those responses are typically only from people who are either very happy or very upset. Additionally, although surveys can be sent quickly after transactions, the other methods-like customer interviews-come with a time lag, which can lead to lost information as people forget how they felt about their experiences.\r\n\r\n## 5 Key Uses for Voice Technology in the Voice of the Customer\r\n\r\nWith the advent of voice technology, however, businesses now have a way to collect customer feedback that solves the problems discussed above-simply by listening to what their customers are saying. Here are 5 of the top use cases for voice technology in capturing the voice of the customer.\r\n\r\n### 1\\. Customer Service\r\n\r\nCustomer service is one of the most obvious applications for voice technology. Call center agents can use it to collect customer feedback after a call. This feedback can be used to improve the customer service experience and make sure that customers are getting the help they need. What's more, [searchable audio](https://blog.deepgram.com/search-through-sound-finding-phrases-in-audio/) can help managers and marketers quickly pull out customer comments about specific things based on information that already exists-no focus groups needed.\r\n\r\n### 2\\. Market Research\r\n\r\nMarket research is another area where voice technology is starting to make an impact. Businesses can use voice technology to collect customer feedback about new products or services. This feedback can help businesses make better decisions about what to develop and how to market their products. For example, voice technology can help businesses understand how customers feel about their products and services. This is because it can capture the customer's tone of voice, which [can provide valuable insights into their emotional state](https://blog.deepgram.com/sentiment-analysis-emotion-regulation-difference/). Voice technology can also help businesses track customer satisfaction over time and across different channels.\r\n\r\n### 3\\. Product Development\r\n\r\nProduct development is another area where voice technology can be used to collect customer feedback. By collecting customer feedback throughout the product development process, businesses can make sure that they're developing products that meet customer needs and expectations. It's not uncommon for this process to happen even without voice technology-but extracting insights from customer interviews can be challenging and time consuming without the ability to search audio or easily create searchable transcripts, for example.\r\n\r\n### 4\\. Social Media Monitoring\r\n\r\nSocial media monitoring can help businesses track customer sentiment about their products and services. This information can be used to make improvements or take corrective action if necessary. Social media monitoring sometimes makes use of [sentiment analysis](https://blog.deepgram.com/sentiment-analysis-emotion-regulation-difference/), which is a process that analyzes customer feedback to understand the customer's overall attitude towards a product or service. This information can be used to improve customer satisfaction or take other actions to improve the customer experience.\r\n\r\n### 5\\. Sales and Marketing\r\n\r\nSales and marketing has been using surveys to understand what messages, sales pitch, or ads resonate with their customers. But with voice technology, agents could simply ask where a customer saw the business or how they felt about the message, sales pitch, or ad and get immediate emotional feedback-audio that can be stored for later analysis, or quickly turned into a transcript. In the near future, using [emotion detection](https://blog.deepgram.com/sentiment-analysis-emotion-regulation-difference/), the company could also determine how the customer truly felt about the messages that they'd seen.\r\n\r\n## Wrapping up\r\n\r\nVoice technology is changing the way businesses collect the voice of the customer. By making it easier to collect VOC data, businesses can make sure that they're constantly improving their products and services to meet customer needs, coaching their customer-facing employees, and determining what sales or marketing messages resonate better. This will help them stay ahead of the competition and continue to grow. Overall, voice technology is providing a new way for businesses to collect customer feedback and harness the voice of the customer. It's efficient, natural, and provides insights that traditional methods may not be able to provide. So if you're looking for a new way to collect customer feedback, consider using voice technology to unlock even more insights.\r\n";
						}
						async function compiledContent$4j() {
							return load$4j().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$4j() {
							return (await import('./chunks/index.dc74e25d.mjs'));
						}
						function Content$4j(...args) {
							return load$4j().then((m) => m.default(...args));
						}
						Content$4j.isAstroComponentFactory = true;
						function getHeadings$4j() {
							return load$4j().then((m) => m.metadata.headings);
						}
						function getHeaders$4j() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$4j().then((m) => m.metadata.headings);
						}

const __vite_glob_0_1 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$4j,
  file: file$4j,
  url: url$4j,
  rawContent: rawContent$4j,
  compiledContent: compiledContent$4j,
  default: load$4j,
  Content: Content$4j,
  getHeadings: getHeadings$4j,
  getHeaders: getHeaders$4j
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$4i = {"title":"6 Biggest Challenges of Automatic Speech Recognition (ASR) for Hindi","description":"Speech-to-text systems for Hindi can encounter some unique challenges.  Here are 6 of the biggest ones that tend to crop up.","date":"2022-03-17T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981411/blog/6-challenges-asr-hindi/6-biggest-challenges-of-ASR-for-Hindi-thumb-554x22.png","authors":["dan-shafer"],"category":"linguistics","tags":["deep-learning","language"],"seo":{"title":"6 Biggest Challenges of Automatic Speech Recognition (ASR) for Hindi","description":"Speech-to-text systems for Hindi can encounter some unique challenges. Here are 6 of the biggest ones that tend to crop up."},"shorturls":{"share":"https://dpgr.am/1fb276e","twitter":"https://dpgr.am/f7bd5e2","linkedin":"https://dpgr.am/eb4f800","reddit":"https://dpgr.am/ee4763e","facebook":"https://dpgr.am/a927f55"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981411/blog/6-challenges-asr-hindi/6-biggest-challenges-of-ASR-for-Hindi-thumb-554x22.png"}};
						const file$4i = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/6-challenges-asr-hindi/index.md";
						const url$4i = undefined;
						function rawContent$4i() {
							return "\r\nAs Deepgram expands [the number of languages that we offer automatic speech recognition (ASR) for](https://deepgram.com/product/languages/), we're bound to run into languages that present different challenges than we encountered for English. In this blog post, we'll review six of the biggest challenges that are present for people looking to create a Hindi speech-to-text model. Before we dive into the specifics, let's take a look at what Hindi is and where it's spoken.\r\n\r\n## What is Hindi?\r\n\r\n[Hindi](https://en.wikipedia.org/wiki/Hindi) is an Indo-European language spoken in the northern part of India, in the so-called [Hindi Belt](https://en.wikipedia.org/wiki/Hindi_Belt), and is one of the two official languages of the government of India. With some 322 million native speakers with an additional 270 second-language users, Hindi is one of the most widely-spoken languages in the world.\r\n\r\n## 6 Biggest Challenges for Hindi ASR\r\n\r\nWith that background out of the way, let's look at six ways that Hindi can create challenges for speech-to-text systems.\r\n\r\n### 1\\. Limited Resources\r\n\r\nPerhaps the first challenge that arises when trying to build an ASR model for Hindi is that the language is what's sometimes called a low-resource language. This means that there isn't as much data available for training ASR models as there is for languages like English. For example, the open source [Common Voice](https://commonvoice.mozilla.org/en) project, which releases crowd-sourced and crowd-validated utterances for dozens of languages, released a Hindi dataset for the first time at the end of 2020, with a mere half an hour of labeled (validated) audio. That number has since grown to 11 hours. Compare that with 217 validated hours for Tamil (another Indian language) or 2186 for English. Training a robust supervised ASR model typically requires several thousand hours of labeled audio, so the lack of available audio can create real challenges.\r\n\r\n### 2\\. Multilingualism and Dialects\r\n\r\nBecause Hindi is a lingua franca in India, maybe people speak it as a second (or even third or fourth) language. This means that, even in conversations that are in Hindi, speakers may be switching between it and other languages, a phenomenon called [code switching](https://en.wikipedia.org/wiki/Code-switching). This can make it difficult for an ASR model to track what's being said. Even if speakers from other languages aren't code switching, Hindi has a lot of loanwords from other languages. This can make it difficult for ASR to correctly identify the words, since the pronunciation may not follow the usual rules of Hindi. Add on to that the fact that Hindi has several different dialects, and this can again make it difficult for ASR to correctly recognize words, since the same word can be pronounced differently in different dialects.\r\n\r\n<WhitepaperPromo whitepaper=\"latest\"></WhitepaperPromo>\r\n\r\n### 3\\. Data Quality Problems\r\n\r\nTo address the lack of available data, teams will sometimes purchase labeled audio data from data vendors to help make their model more robust-a common practice when trying to bump up the amount of data they have to train a model. However, purchased data can be of mixed quality, which means you need an internal quality control process to make sure the data will actually improve the accuracy of your model instead of harming it. And even if you get good data, it might not be labeled and annotated in the same way as your model expects, creating an additional hurdle for using purchased data. And data labeling itself can also be a challenge, as we are about to see.\r\n\r\n### 4\\. Data Labeling\r\n\r\nBecause of code switching, and the fact that English words often get used in Hindi-along with the fact that some customers will send Indian-accented English to the Hindi model-you need to have a model that can handle this variety of the language, often called [Hinglish](https://en.wikipedia.org/wiki/Hinglish). In order to handle this, your model needs to be trained on all of the possibilities, from solely English on one end of the scale, to solely Hindi on the other end. This means that the data must be labeled based on what is there-English transcription for the English elements, and Hindi transcriptions in Devanagari for the Hindi elements.\r\n\r\n### 5\\. Inflection\r\n\r\nOne additional ASR challenge for Hindi is that it is an [inflected language](https://en.wikipedia.org/wiki/Inflection), meaning its words change depending on factors like their role in the sentence, the tense of the verb, and so forth. This means that there can be a lot of variation in forms of words. For ASR to work well, it needs to be able to handle this kind of variability and be able to recognize different forms of the same word. This is doubly challenging due to the mixture of English; you need a very large dictionary to not only contain all of the variations of Hindi words, but also all of the English words that might get mixed into the speech of those communicating in Hindi-of even just Indian-accented English that gets sent to the Hindi model.\r\n\r\n### 6\\. Model Building\r\n\r\nThe final challenge that we'll talk about is actually producing an [end-to-end deep-learning model](https://blog.deepgram.com/deep-learning-speech-recognition/) that successfully addresses the other challenges mentioned above. To tackle differences in labels and labeling conventions, you need a pipeline to normalize the transcripts from different sources, and you need to have a dictionary that can address issues of code switching and borrowings. And even with all that, you still have to deal with not having as much data as you would for, say, English. One of the best ways to address the last point is with transfer learning. Transfer learning allows you to take a model that you already have and is performing well, and train it on new data. This obviously isn't a perfect solution-you can't just take an English model, toss in Hindi data, and expect it to work as well as your English model-but it does give you a place to start, and can help make up for some of the data issues that we discussed above.\r\n\r\n## Wrapping Up\r\n\r\nASR technology has come a long way in recent years, and our data operations and curation team, along with our in-house linguists, allow us to effectively find and use real-world audio data to train our models, including our Hindi model. We're excited to be able to continue to expand the number of [languages we offer models for](https://deepgram.com/product/languages/). We currently have both a Hindi model and an Indian English model that can handle code switching, borrowings, and the other issues discussed above-and we're confident that our current model has better accuracy than other Hindi speech-to-text systems available because we've tested it against them. If you'd like to give our Hindi model a try, you can sign up for a [free API key](https://console.deepgram.com/) or [reach out to us](https://deepgram.com/contact-us/) to chat about your needs and how we can help.\r\n";
						}
						async function compiledContent$4i() {
							return load$4i().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$4i() {
							return (await import('./chunks/index.2fafce21.mjs'));
						}
						function Content$4i(...args) {
							return load$4i().then((m) => m.default(...args));
						}
						Content$4i.isAstroComponentFactory = true;
						function getHeadings$4i() {
							return load$4i().then((m) => m.metadata.headings);
						}
						function getHeaders$4i() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$4i().then((m) => m.metadata.headings);
						}

const __vite_glob_0_2 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$4i,
  file: file$4i,
  url: url$4i,
  rawContent: rawContent$4i,
  compiledContent: compiledContent$4i,
  default: load$4i,
  Content: Content$4i,
  getHeadings: getHeadings$4i,
  getHeaders: getHeaders$4i
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$4h = {"title":"A Conversation with Asian American & Pacific Islander Deepgrammers","description":"AAPI Deepgramers talk about their heritage and experiences and show the culture what they have brought to the American Experiment.","date":"2021-05-26T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981370/blog/a-conversation-with-asian-american-pacific-islander-deepgrammers/convo-w-AAPI-dgers%402x.jpg","authors":["sam-zegas"],"category":"identity-and-language","tags":["aapi","education"],"seo":{"title":"A Conversation with Asian American & Pacific Islander Deepgrammers","description":"AAPI Deepgramers talk about their heritage and experiences and show the culture what they have brought to the American Experiment."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981370/blog/a-conversation-with-asian-american-pacific-islander-deepgrammers/convo-w-AAPI-dgers%402x.jpg"},"shorturls":{"share":"https://dpgr.am/4abe7e3","twitter":"https://dpgr.am/0a8e838","linkedin":"https://dpgr.am/cc3a2f1","reddit":"https://dpgr.am/4e448b1","facebook":"https://dpgr.am/2aee0b5"}};
						const file$4h = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/a-conversation-with-asian-american-pacific-islander-deepgrammers/index.md";
						const url$4h = undefined;
						function rawContent$4h() {
							return "A core part of why we show up to work at Deepgram every day is that we believe every voice should be heard and understood. We know that the way we speak and express ourselves is deeply connected to the cultural ties we share with friends and family - and we're here to celebrate that. This year, we're marking Asian American & Pacific Islander Heritage Month by talking with six Asian American & Pacific Islander Deepgrammers about what it means to be connected to their roots. Join us on this exploration of what it means to have an AAPI voice at Deepgram! \n\n## Let's meet the group\n\n![](https://res.cloudinary.com/deepgram/image/upload/v1661976842/blog/a-conversation-with-asian-american-pacific-islander-deepgrammers/Natalie-photo.png)\n\n**Natalie**, Head of Product — I'm half Chinese, from my mom's side. I'm actually second-generation though because my Gung Gung (grandpa) came over when he was 8 years old and grew up in Utah working in restaurants. He was a [paper son](https://www.immigrant-voices.aiisf.org/stories-by-author/737-my-father-was-a-paper-son/), someone who immigrated to the U.S. with fraudulent documentation to get around the near-total exclusion of Chinese immigration following the [Chinese Exclusion Act](https://en.wikipedia.org/wiki/Chinese_Exclusion_Act) of 1882. He later got his citizenship when he served in the Korean war. I never met my grandma because she passed before I was born, but my grandpa brought her to the U.S. after his time in the Marines. I don't know much about her other than that she fled from the Japanese during WWII and was quite educated for her time and gender. \n\n![](https://res.cloudinary.com/deepgram/image/upload/v1661976843/blog/a-conversation-with-asian-american-pacific-islander-deepgrammers/Keith-photo.png)\n\n**Keith**, Head of Product Marketing — I was born in Hong Kong and came to the San Francisco Bay Area when I was 2 years old. Growing up, my dad owned an arts and crafts store in San Francisco's Chinatown and I spent a lot of time in the San Francisco and Oakland Chinatowns. I still have relatives in Hong Kong but the San Francisco Bay Area is the only place I've ever thought of as home.\n\n![](https://res.cloudinary.com/deepgram/image/upload/v1661976844/blog/a-conversation-with-asian-american-pacific-islander-deepgrammers/Tammy-photo.jpg)\n\n**Tammy (Anh Thu)**, Head of Talent — I am full Vietnamese. My mom is from Saigon and my dad is from Hanoi. My mom escaped the Vietnam War by a boat and spent a year in Thailand in a refugee camp. The refugee boats did not guarantee safe passage and only had limited food and water. My mom was separated from her parents and unfortunately, my grandparents lost their lives at the hands of pirates. My dad emigrated here when he was 16 and studied at the University of San Francisco. He dedicated his life to sponsoring his siblings and their families to come to the U.S.  \n\n![](https://res.cloudinary.com/deepgram/image/upload/v1661976845/blog/a-conversation-with-asian-american-pacific-islander-deepgrammers/Marshall-photo.png)\n\n**Marshall**, Customer Success Manager — I was born in Hong Kong but came to the U.S. when I was 2. Because I immigrated at such a young age, I never fully integrated with my Asian heritage and mostly assimilated into American culture. My parents dedicated themselves to bringing me and my brother to the U.S. to avoid the CCP. Growing up, my parents were already well-traveled due to their fashion export business and didn't have issues migrating to California.\n\n![](https://res.cloudinary.com/deepgram/image/upload/v1661976846/blog/a-conversation-with-asian-american-pacific-islander-deepgrammers/Minh-photo.png)\n\n**Minh**, Solutions Engineer — My family and I were born in Vietnam. In Vietnam, it is common for family members to live together under one roof and visit the homes of friends and family without prior appointments. This culture lived on in the Vietnamese-American community when our family immigrated to the U.S. As a child, I enjoyed the spontaneous visits to our home, sitting on the laps of dear friends and family, and exchanging stories over green tea and dried fruit snacks (though I wouldn't fully appreciate imbibing green tea until much later in life!)\n\n![](https://res.cloudinary.com/deepgram/image/upload/v1661976847/blog/a-conversation-with-asian-american-pacific-islander-deepgrammers/Sofia-photo.png)\n\n**Sofia**, Sales Development Representative — I was born and raised in the Bay Area, but both my parents grew up in Manila, Philippines. They both speak Tagalog fluently, but they only speak English to each other and the rest of my family if we're visiting the Philippines. We are extremely bonded with our Filipino culture and go back to visit my grandparents at least every couple of years and attend our massive (500+ people) family reunion in Cebu every 5 years, where you will not be caught alive without a full plate of food in hand. After taking an ancestry test a couple of years ago, I found out that I am ~20% genetically Filipino, which made me think a lot about what it means to \"belong\" to a culture since I've always been so closely tied to that part of my heritage.\n\n## What personal values do you draw from your Asian American roots?\n\n**Minh:** For me, it can be tough to firmly grasp a sense of identity and pride for a country I departed from as a refugee when I was less than 2 years of age. My father was drafted into the Army of the Republic of Vietnam (South Vietnam) and climbed to the rank of officer. After suffering several shrapnel wounds in combat, the medics patched him up and sent him back to the front lines. Following the Fall of Saigon, my father was submitted to years in the re-education camps before rejoining society. While the men served in the war, the women took up the mantle of managing the household and the crops. The war wasn't easy for non-combatants either, as many were displaced from their homes. My family sought refuge in the United States so that my brothers and I would never know the same tragedies of loss. To me, the history of my family forms my roots - and being Vietnamese is to carry out the same duties as our ancestors: to persevere in the face of improbable odds, to look out for our fellow brothers and sisters, to shoulder collective responsibility, and to share the fruits of our labor.\n\n**Marshall:** For me, it's about the value of strong family bonds to closer and more distant relatives.\n\n**Tammy:** Both my parents came here with no money, no family, and no knowledge of English, and were able to establish a family and home. They've worked their entire lives to get to where they are today and to provide for three children. They've taught me that sacrifice, discipline, and hard work are the key to success and that we are very fortunate to be in a land of opportunity.\n\n**Keith:** When my parents arrived here, my dad luckily had a job lined up with his uncle, who ran a butcher shop in the Mission District of San Francisco. At first, we lived in an apartment near Oakland Chinatown while we saved enough money to buy an apartment building, then later moved to a single-family home. Growing up, we had one old car, walked to Chinatown for groceries, and never ate out. We always seemed to be trying to save money but we were never without. Our fun was getting together with relatives and playing with my cousins. This upbringing taught me a dedication to family, to always strive to be better, to save money, and to always look for discounts or wait for a sale.\n\n**Natalie:** For me, it's the importance of family, and of making your family, team, or group proud through your accomplishments and effort. It's also about demonstrating your love for someone through your actions more than your words.\n\n**Sofia:** For me, I learned about the importance of bonding with family. Growing up, I was always introduced to second, third, and fourth cousins! To this day, my mom is still a family tree wizard and always considered even my most distant relatives to be a \"tita/o\" or a cousin.\n\n## What languages are spoken in your family? If you speak that language, who do you speak it with, and what feels most natural to talk about in that language?\n\n**Natalie:** Some of my relatives can still speak Cantonese, but that wasn't passed on to me. My grandparents didn't speak in Cantonese much with my mom or her brothers because they saw themselves as American. At the time, \"being American\" meant speaking primarily in English. Looking at it today, that severance has actually made it harder for my mom, myself, and my sister to have ties to the community. Whereas at one point my family may have been proud of integrating into American society, today we fail to be \"Asian enough\" because we can't speak the language.\n\n**Marshall:** My family speaks Cantonese. I barely speak the language but am able to speak broken Cantonese with my distant relatives. There is a language barrier with my grandmother, who is still in Hong Kong. My mother has to help with translation when I speak with my grandmother. My parents, uncles and aunts are located all across the world, speak fluent English, and speak to each other in a mix of Cantonese and English. My mother and father still speak to me only in Cantonese to help me practice my language skills. **Sofia:** My parents speak Tagalog, but I probably only understand 10% of what they say. We primarily speak English at home, but I love using Tagalog words to describe situations or feelings that don't exist in the English dictionary.\n\n**Minh:** When our family arrived in the US, none of us spoke English. But regardless of age, each of us studied English and furthered our education in the States. As a result, in my family, we speak with each other in a mix of Vietnamese and English. Several of my non-Vietnamese-speaking friends have commented on how the Vietnamese language can sound very intense or harsh. As a Vietnamese-American, I can understand that perspective, and as an English/Vietnamese/Spanish speaker, I could be persuaded to believe English sounds more \"neutral.\" It would be difficult to generalize about the Vietnamese language, but in my experience, spoken poetry and music in Vietnamese can deliver more powerful and stronger sentiments than works in English.\n\n**Tammy:** As a first-generation Vietnamese-American, I don't speak Vietnamese very well but can understand when my mom speaks to me. My mom's primary language is still Vietnamese. Growing up, my parents focused on assimilating to American culture, so I was never enrolled in Vietnamese classes. This has led to a large language barrier, as I only know words that come up regularly in conversations with my mom. In hindsight, I understand why they chose that path for me, but I do wish I could communicate better with my mom and Vietnamese-speaking relatives.\n\n**Keith:** I speak broken Cantonese, mostly with my mom and relatives. I understand 90% of what is said in Cantonese but because I spend most of my time speaking English, I sometimes have trouble responding with the right Cantonese words. Over time, forgetting Cantonese words makes it harder to communicate with my mom and relatives. When I do speak Cantonese though, we generally talk about family and how everyone else is doing, so I can cover those conversation topics well.\n\n## What message about being AAPI in America today do you want people to hear?\n\n**Marshall:** Asian Americans identify strongly with American culture, just as much as with our heritage culture. We have also suffered from racism and discrimination, labor exploitation, and other forms of oppression in America. Like all peoples, our parents worked and sacrificed to give their children a better life. We identify with this land and our neighbors as much as any other Americans, and we take great care in respecting each other. It's been difficult to see how Asians have recently been unjustly targeted for causing a pandemic that could have been lessened if we - as a whole society - had all just behaved in each other's interest.\n\n**Keith:** \"American\" is not an ethnicity, it is a feeling, a belief. \"American\" describes a belief in democracy, the freedom to be who you are, and the freedom to succeed. Asian Americans don't feel any different than other Americans. We have the same goals, needs, wants, and future plans. We want the opportunity to better ourselves and create better lives for our children. Just because we are the descendants of people from another country doesn't mean we agree with what that country is doing today. For example, I'm afraid to go back to Hong Kong because democracy is being suppressed there and I don't trust the Chinese government.\n\n**Tammy:** Some people stereotype Asians as a \"model minority.\" The model minority myth perpetuates the idea that all Asians are smart or musically talented, that they have Tiger Moms who force them to study all day and effeminate fathers who work in STEM. Even though some aspects of these stereotypes are positive, all stereotypes work against the struggle for racial justice. Asian Americans are not what the model minority myth represents them to be. Furthermore, we should recognize the struggles that Asian Americans overcame to make a home in the U.S. They endured racism and a long arduous journey to America, they worked hard for their families - but today, they have been made the scapegoat of a global pandemic. Not every Asian is the same, but every group went through its own hardship on the path to America: from [Japanese internment camps](https://www.archives.gov/education/lessons/japanese-relocation) and the Chinese Exclusion Act to the Vietnam War. We should recognize and honor each kind of experience for its unique contribution to what it means to be Asian American.\n\n**Minh:** Many people think of Vietnam as having one homogenous culture, but I've always thought of Vietnam as a melting pot. Formed from the early peoples of South China (sharing similar legends and mythology), colonized as French Indochina (Vietnamese crepes and sandwiches served on baguette - yes please!), and scattered across the globe following the Vietnam War, pockets of Vietnamese culture are a bit \"here and there,\" and have assimilated into countless other cultures around the world. Finally, we all know Vietnamese restaurants serve delicious Vietnamese food. But for authentic, genuine Vietnamese cuisine, befriend your local Vietnamese-American and get yourselves invited into their home and kitchen!\n\n## A mirror to American society: what AAPI voices teach us\n\nFrom these conversations, a few themes about AAPI identities shine through. First, AAPI is a broad category of ethnicities tied together by common experiences on the journey to becoming American. The Asia-Pacific region includes hundreds - if not thousands - of distinct communities and cultures, many of which are represented in the United States. From these diverse origins, the common thread in the AAPI experience is that many immigrants endured significant hardships before, during, and after their journey to America. The experience of immigrating and making a new life in the U.S. is a foundational part of American identity that ties together hundreds of millions of Americans with immigrant heritage.\n\nAcross America's history of immigration and assimilation, AAPI experiences stand out for their perseverance through wars, overtly racist immigration policies, and struggles to assimilate - all to form the uniquely vibrant communities in the U.S. that we know today. Second, AAPI Deepgrammers talk about common themes in their cultural values: the importance of family ties, the value of hard work and achievement, and a respect for personal sacrifice. These values, too, are fundamentally connected to the experience of immigrants, not just in AAPI communities but among all people who leave home to make a new life in a far-away place. These values are deeply American. They represent the traits that are necessary to survive and thrive after making the momentous decision to start a new life in the United States.\n\nFinally, Asian Americans & Pacific Islanders aren't a minority set apart from other Americans. They have been part of this American experiment alongside other groups for hundreds of years, and their experiences today contribute to the core definition of what it means to be American. The wave of anti-Asian discrimination that grew out of the coronavirus pandemic may have tested the ties that hold us together, but we will overcome it - and emerge stronger. We hope you enjoyed these stories from Asian American Deepgrammers as much as we do, and whatever your heritage, we hope you find ways to connect with AAPI history this month!";
						}
						async function compiledContent$4h() {
							return load$4h().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$4h() {
							return (await import('./chunks/index.1e829fa0.mjs'));
						}
						function Content$4h(...args) {
							return load$4h().then((m) => m.default(...args));
						}
						Content$4h.isAstroComponentFactory = true;
						function getHeadings$4h() {
							return load$4h().then((m) => m.metadata.headings);
						}
						function getHeaders$4h() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$4h().then((m) => m.metadata.headings);
						}

const __vite_glob_0_3 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$4h,
  file: file$4h,
  url: url$4h,
  rawContent: rawContent$4h,
  compiledContent: compiledContent$4h,
  default: load$4h,
  Content: Content$4h,
  getHeadings: getHeadings$4h,
  getHeaders: getHeaders$4h
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$4g = {"title":"A Note to our Customers: OpenAI Whisper's Entrance into Voice","description":"A Note to our Customers: OpenAI Whisper's Entrance into Voice","date":"2022-10-14T20:32:06.850Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1665448168/blog/a-note-to-our-customers-openai-whispers-entrance-to-voice/2210-Whispers-Entrance-to-Voice-featured-1200x630_2x_rp7bnz.png","authors":["scott-stephenson"],"category":"product-news","tags":["whisper"],"shorturls":{"share":"https://dpgr.am/c019570","twitter":"https://dpgr.am/2aa83bd","linkedin":"https://dpgr.am/1afb8fe","reddit":"https://dpgr.am/4920fcf","facebook":"https://dpgr.am/197f389"}};
						const file$4g = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/a-note-to-our-customers-openai-whispers-entrance-into-voice/index.md";
						const url$4g = undefined;
						function rawContent$4g() {
							return "\nLast month was big for speech intelligence as OpenAI released Whisper, a general-purpose speech recognition model. We've gotten several questions about what this means for the future of Voice AI, so we wanted to share our thoughts with you.\n\nOpenAI [Whisper](https://openai.com/blog/whisper/) is an open source speech-to-text tool built using end-to-end deep learning. **In OpenAI's own words, Whisper is designed for \"AI researchers studying robustness, generalization, capabilities, biases and constraints of the current model.\"** This use case stands in contrast to Deepgram's speech-to-text API, which is designed for software developers to build highly scalable, production-quality products using voice. The consequences of these differing design objectives are evident in the offerings of both products, as will be described throughout this whitepaper.\n\nOpenAI offers Whisper in five model sizes, where larger models provide higher accuracies at the tradeoff of increased processing speed and compute cost. At Deepgram, we know a good speech model when we see one and are impressed with the accuracy of some of Whisper's larger models. However, we also know that there are many important factors beyond accuracy that make the difference between a research-oriented tool and a tool built for highly scalable products.\n\nOverall, our assessment of Whisper depends on your intended use case. **Whisper is a great tool if you want to quickly create a demo product or conduct research on AI speech recognition. On the other hand, if your use case involves making a reliable, scalable, cost-efficient product, Whisper may not be a good fit.** Its higher-accuracy models are big, slow, and expensive to run. It has a limited feature set and therefore requires its users to divert significant resources to building and maintaining additional functionality. It does not come with dedicated support options. These barriers make the decision to build with Whisper a significant commitment. For those interested in playing with the Whisper model, we've made it available through the Deepgram API — further instructions are available on our [blog](https://blog.deepgram.com/use-openai-whisper-speech-recognition-with-the-deepgram-api/).\n\nDue to the bare-bones, take-it-or-leave it nature of the offering, companies that build with Whisper must essentially commit to rebuilding the wheel that speech processing companies like Deepgram have been refining for years as their sole mission. For more complex use cases, making Whisper work in production can come at a high cost of diverting engineering, research, and product resources away from the primary mission or product. Deepgram allows users to avoid these disruptions and go straight to production with reliable, accurate, cost-efficient, fast, and feature-rich speech processing tools.\n\nIn the rest of this note, we'll provide some data and commentary on Whisper in three areas: **Accuracy**, **Speed and Latency**, and **Functionality**.\n\n## Accuracy\n\nWe know that accuracy is top of mind for our users. Deepgram's Enhanced model beats the highest-performing Whisper model by a wide margin — over 20%, relative — when tested on English data. Furthermore, our models achieve these accuracies at significantly faster processing speeds: on the order of 25x faster than the Whisper Medium model, which is the basis for the accuracy comparison mentioned above. We built our product to these performance standards because we know the combination of accuracy and speed is what unlocks scalability for our users.\n\nIn its documentation, the Whisper team claims that their model \"approaches human level robustness and accuracy on English speech recognition.\" While we are impressed by their work, we encourage users to adopt healthy skepticism toward claims of human-level accuracy in speech recognition. Whisper's highest self-published accuracy statistics are below human levels of accuracy (benchmarked at 95-97%) and seem to be the result of testing on unrealistically easy audio. Real-world audio is messy: audio quality varies, background noise interrupts the dialog, speakers talk quickly with diverse accents, industry jargon and branded terms are common, etc. These natural complexities of speech should be taken into account when assessing the robustness of a speech recognition solution, but seem to be inadequately represented in Whisper's testing data set.\n\nTo give you a better sense of how Whisper and Deepgram accuracies compare on real-world audio, we conducted a side-by-side Word Error Rate (WER). For this analysis, we submitted 254 test files to Whisper and the Deepgram Enhanced model. The audio in these files feature noise, speaker accents, and long tail vocabulary typical of real-world audio data from phone calls and meetings. The files contain a range of audio durations and a range of topics, which is important when benchmarking ASR capabilities of a general model. Bear in mind that human-level accuracy can be benchmarked in the 3-5% WER range.\n\n<table>\r\n    <tr>\r\n        <th></th>\r\n        <th>Model Size/Tier and Relative Speed</th>\r\n        <th>English Word Error Rate</th>\r\n        <th>Multilingual Word Error Rate</th>\r\n    </tr>\r\n    <tr>\r\n        <th rowspan=\"5\">OpenAI Whisper</th>\r\n        <td>Tiny</td>\r\n        <td>15.3%</td>\r\n        <td>16.2%</td>\r\n    </tr>\r\n    <tr>\r\n        <td>Base (.9x faster than Tiny)</td>\r\n        <td>13.5%</td>\r\n        <td>14.0%</td>\r\n    </tr>\r\n    <tr>\r\n        <td>Small (3x slower than Tiny)</td>\r\n        <td>13.1%</td>\r\n        <td>12.5%</td>\r\n    </tr>\r\n    <tr>\r\n        <td>Medium (4x slower than Tiny)</td>\r\n        <td>13.2%</td>\r\n        <td>12.4%</td>\r\n    </tr>\r\n    <tr>\r\n        <td>Large (10x slower than Tiny)</td>\r\n        <td>N/A: Large model only available for Multilingual</td>\r\n        <td>13.2%</td>\r\n    </tr>\r\n    <tr>\r\n        <th rowspan=\"3\">Deepgram</th>\r\n        <td>Base (82x faster than Large; 2.6x faster than Tiny)</td>\r\n        <td>12.8%</td>\r\n        <td>Unique to each language offering</td>\r\n    </tr>\r\n    <tr>\r\n        <td>Enhanced (66x faster than Large; 7x faster than Tiny)</td>\r\n        <td>10.6%</td>\r\n        <td>Unique to each language offering</td>\r\n    </tr>\r\n    <tr>\r\n        <td>Custom Trained Enhanced (66x faster than Large; 2.6x faster than Tiny)</td>\r\n        <td>4.0%</td>\r\n        <td>Unique to each language offering</td>\r\n    </tr>\r\n</table>\n\n![](https://res.cloudinary.com/deepgram/image/upload/v1665439308/blog/a-note-to-our-customers-openai-whispers-entrance-to-voice/194953371-601609cf-51ec-4d7b-90cd-c06aa77bcabf_yah89r.png \"Deepgram's Enhanced model outperform's Whisper on real-word data.\")\n\nYou may have noticed that accuracy gains level off with Whisper's Medium and Large models, which might seem counterintuitive at first glance. There are several possible interpretations of this result, but the most salient is that while model size may be loosely correlated with accuracy, the correlation can be offset by a variety of factors. Our extensive research on speech processing through end-to-end deep learning suggests that when transformer models like Whisper encounter data that they aren't sure how to interpret (e.g., noisy phonecall data), they tend to generate outputs verbosely rather than being quiet. A larger model has more capacity to memorize text and be inventive, and so will tend to be more verbose in its errors, driving accuracy down.\n\n### Cost\n\nOpenAI Whisper is not offered as a service. They only provide example code that has to be integrated into your application. It can run on either CPU or GPU resources, but CPUs are very slow for this type of workload — roughly 20x slower —  so you will probably need to run your application on hosts equipped with GPU acceleration. These hosts are much more expensive and most of your application will not be able to take advantage of the GPU, meaning it will likely be very lightly utilized.\n\nThere are five different versions of the OpenAI model that trade quality vs speed. The best performing version has 32 layers and 1.5B parameters. This is a big model. It is not fast. It runs slower than real time on a typical Google Cloud GPU and costs ~$2/hr to process, even if running flat out with 100% utilization.\n\nBeyond hosting, there are people costs associated with managing in-house speech recognition technology like Whisper. Maintaining speech recognition in-house typically demands that dedicated engineering and research teams integrate the model and regularly optimize for accuracy. Foregoing these investments can severely impact the user experience of your product or the accuracy of a downstream model. As a result, a Whisper deployment will come with $150-250k of additional cost per technical employee added to the team.\n\n## Speed and Latency\n\nOne of the major differences between Whisper and Deepgram relates to speed and latency.\n\nDeepgram offers batch processing for pre-recorded audio as well as real-time processing for streaming audio. OpenAI Whisper only offers batch processing for pre-recorded audio. The table below compares processing times for OpenAI Whisper on Google Cloud GPUs and Deepgram on Deepgram's GPUs:\n\n<table>\r\n    <tr>\r\n        <th></th>\r\n        <th>OpenAI Whisper</th>\r\n        <th>Deepgram Enhanced</th>\r\n    </tr>\r\n    <tr>\r\n        <th>1 Hour of Pre-Recorded Speech (Batch Processing)</th>\r\n        <td>9 minutes \r\n        (Large Model)</td>\r\n        <td>8s</td>\r\n    </tr>\r\n    <tr>\r\n        <th>Latency (Live Streaming)</th>\r\n        <td>N/A - not available for live streaming</td>\r\n        <td>Under 300ms</td>\r\n    </tr>\r\n</table>\n\n![](https://res.cloudinary.com/deepgram/image/upload/v1665439308/blog/a-note-to-our-customers-openai-whispers-entrance-to-voice/194954878-c05182bb-e637-4132-9706-6cae1e4a6434_jdae8u.png)\n\n## Functionality and Features\n\nWe know our users need more than just raw transcripts, so we've built rich features to help everyone get the most out of their audio. Below is a comparison of functionality and features offered by Deepgram and Whisper.\n\n<table>\r\n    <thead>\r\n        <tr>\r\n            <th>Functionality  Category</th>\r\n            <th>Capability</th>\r\n            <th>OpenAI Whisper</th>\r\n            <th>Deepgram Enhanced</th>\r\n        </tr>\r\n    </thead>\r\n    <tbody>\r\n        <tr>\r\n            <th>Software Type</th>\r\n            <td>Closed or open source</td>\r\n            <td>Open Source</td>\r\n            <td>Closed</td>\r\n        </tr>\r\n        <tr>\r\n            <th rowspan=\"2\">User Interface</th>\r\n            <td>Application Programming Interface (API)</td>\r\n            <td></td>\r\n            <td>✅</td>\r\n        </tr>\r\n        <tr>\r\n            <td>Graphical User Interface (GUI)</td>\r\n            <td></td>\r\n            <td>✅</td>\r\n        </tr>\r\n        <tr>\r\n            <th rowspan=\"17\">Transcription</th>\r\n            <td>Pre-Recorded</td>\r\n            <td>✅</td>\r\n            <td>✅</td>\r\n        </tr>\r\n        <tr>\r\n            <td>Live Streaming</td>\r\n            <td></td>\r\n            <td>✅</td>\r\n        </tr>\r\n        <tr>\r\n            <td>Language Support*</td>\r\n            <td>38*</td>\r\n            <td>30</td>\r\n        </tr>\r\n        <tr>\r\n            <td>Use Case Models</td>\r\n            <td>1 - General</td>\r\n            <td>7</td>\r\n        </tr>\r\n        <tr>\r\n            <td>Model Training</td>\r\n            <td></td>\r\n            <td>✅</td>\r\n        </tr>\r\n        <tr>\r\n            <td>Word Level Timestamps</td>\r\n            <td></td>\r\n            <td>✅</td>\r\n        </tr>\r\n        <tr>\r\n            <td>Punctuation</td>\r\n            <td>✅</td>\r\n            <td>✅</td>\r\n        </tr>\r\n        <tr>\r\n            <td>Numeral Formatting</td>\r\n            <td>✅</td>\r\n            <td>✅</td>\r\n        </tr>\r\n        <tr>\r\n            <td>Paragraphs</td>\r\n            <td></td>\r\n            <td>✅</td>\r\n        </tr>\r\n        <tr>\r\n            <td>Utterances</td>\r\n            <td></td>\r\n            <td>✅</td>\r\n        </tr>\r\n        <tr>\r\n            <td>Find & Replace</td>\r\n            <td></td>\r\n            <td>✅</td>\r\n        </tr>\r\n        <tr>\r\n            <td>Profanity Filtering</td>\r\n            <td>✅**</td>\r\n            <td>✅</td>\r\n        </tr>\r\n        <tr>\r\n            <td>Deep Search</td>\r\n            <td></td>\r\n            <td>✅</td>\r\n        </tr>\r\n        <tr>\r\n            <td>Keyword Boosting</td>\r\n            <td></td>\r\n            <td>✅</td>\r\n        </tr>\r\n        <tr>\r\n            <td>Interim Results</td>\r\n            <td></td>\r\n            <td>✅</td>\r\n        </tr>\r\n        <tr>\r\n            <td>Voice Activity Detection</td>\r\n            <td></td>\r\n            <td>✅</td>\r\n        </tr>\r\n        <tr>\r\n            <td>Request Tagging</td>\r\n            <td></td>\r\n            <td>✅</td>\r\n        </tr>\r\n        <tr>\r\n            <th rowspan=\"9\">Speech Understanding</th>\r\n            <td>Speaker Diarization</td>\r\n            <td></td>\r\n            <td>✅</td>\r\n        </tr>\r\n        <tr>\r\n            <td>Language Detection</td>\r\n            <td>✅</td>\r\n            <td>✅</td>\r\n        </tr>\r\n        <tr>\r\n            <td>Entity Detection</td>\r\n            <td></td>\r\n            <td>✅</td>\r\n        </tr>\r\n        <tr>\r\n            <td>Redaction</td>\r\n            <td></td>\r\n            <td>✅</td>\r\n        </tr>\r\n        <tr>\r\n            <td>Summarization</td>\r\n            <td></td>\r\n            <td>✅</td>\r\n        </tr>\r\n        <tr>\r\n            <td>Topic Detection</td>\r\n            <td></td>\r\n            <td>✅</td>\r\n        </tr>\r\n        <tr>\r\n            <td>Sentiment Analysis</td>\r\n            <td></td>\r\n            <td>✅</td>\r\n        </tr>\r\n        <tr>\r\n            <td>Language Translation</td>\r\n            <td>✅</td>\r\n            <td>✅</td>\r\n        </tr>\r\n        <tr>\r\n            <td>Speaker ID</td>\r\n            <td></td>\r\n            <td>✅</td>\r\n        </tr>\r\n        <tr>\r\n            <th rowspan=\"3\">Enterprise Support</th>\r\n            <td>Hosted or Virtual Private Cloud</td>\r\n            <td></td>\r\n            <td>✅</td>\r\n        </tr>\r\n        <tr>\r\n            <td>On-Prem Deployment</td>\r\n            <td></td>\r\n            <td>✅</td>\r\n        </tr>\r\n        <tr>\r\n            <td>Dedicated Customer Support</td>\r\n            <td></td>\r\n            <td>✅</td>\r\n        </tr>\r\n        <tr>\r\n            <th rowspan=\"2\">Partner Integrations</th>\r\n            <td>UniMRCP</td>\r\n            <td></td>\r\n            <td>✅</td>\r\n        </tr>\r\n        <tr>\r\n            <td>Twilio</td>\r\n            <td></td>\r\n            <td>✅</td>\r\n        </tr>\r\n    </tbody>\r\n</table>\n\n> \\* Whisper has 38 language models available at WER of 30% or less, as assessed on \"easy\" audio files. We consider 30% WER to be the outer limit of transcript usability, and consider WER of 15% to be the threshold of a high quality model. We therefore exclude their language models for which OpenAI's own studies indicate a WER of greater than 30%. Whisper's documentation states it achieves \"strong ASR results in ~10 languages.\"\n\n> \\*\\* Profanity Filtering is listed as a Whisper feature for English models but our testing of Whisper indicates that the filter has inconsistent performance. We recommend further testing of this feature before adoption in production settings.\n\n## The Bottom Line\n\nFor simple use cases, Whisper can be a great choice. OpenAI introduced Whisper as best suited for \"AI Researchers interested in evaluating the performance of the Whisper model.\" It may also be a good tool for building a speech-enabled demo product, as long as the use case doesn't require streaming, advanced functionality, or large scale — and assuming that the user has GPU hosts available. Developers and companies that want to build scalable products with voice will likely gravitate toward a solution that is designed for that purpose. These differences in design become clear when you compare Whisper to Deepgram, as Deepgram offers higher accuracy, richer features, lower operating costs, faster processing speeds, convenient deployment options, model customization, dedicated support, and more.\n\n**If you have any questions about Whisper, please reach out to your Customer Success team member to discuss.**\n\n";
						}
						async function compiledContent$4g() {
							return load$4g().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$4g() {
							return (await import('./chunks/index.63d15a25.mjs'));
						}
						function Content$4g(...args) {
							return load$4g().then((m) => m.default(...args));
						}
						Content$4g.isAstroComponentFactory = true;
						function getHeadings$4g() {
							return load$4g().then((m) => m.metadata.headings);
						}
						function getHeaders$4g() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$4g().then((m) => m.metadata.headings);
						}

const __vite_glob_0_4 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$4g,
  file: file$4g,
  url: url$4g,
  rawContent: rawContent$4g,
  compiledContent: compiledContent$4g,
  default: load$4g,
  Content: Content$4g,
  getHeadings: getHeadings$4g,
  getHeaders: getHeaders$4g
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$4f = {"title":"A Voice Destin-ation: Project Voice X 2021","description":"Project Voice X was a voice destination for all things conversational AI, NLP and the next big things in speech recognition.","date":"2021-12-09T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981402/blog/a-voice-destin-ation-project-voice-x-2021/proj-voice-x-session-overview-blog-thumb-554x220%402.png","authors":["claudia-ring"],"category":"speech-trends","tags":["project-voice-x"],"seo":{"title":"A Voice Destin-ation: Project Voice X 2021","description":"Project Voice X was a voice destination for all things conversational AI, NLP and the next big things in speech recognition."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981402/blog/a-voice-destin-ation-project-voice-x-2021/proj-voice-x-session-overview-blog-thumb-554x220%402.png"},"shorturls":{"share":"https://dpgr.am/763ff32","twitter":"https://dpgr.am/b165811","linkedin":"https://dpgr.am/e14d7a4","reddit":"https://dpgr.am/79b90a5","facebook":"https://dpgr.am/075af13"}};
						const file$4f = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/a-voice-destin-ation-project-voice-x-2021/index.md";
						const url$4f = undefined;
						function rawContent$4f() {
							return "Face-to-face events are (kind of) back - and we decided to rejoin the fray for Project Voice X held in Destin, Florida.  It was great to meet and listen to the innovators and futurists in the panel sessions which ran the gamut from why voice is the next interface to diagnosing mental declines in patients.  Voice technology has moved from command and response smart speakers to true business, retail and medical uses that will change how we interface with machines.  How about a multi-channel NLU based, voicebot, SMS, and email interface AI that can provide customers solutions in the method that works best for them?  Or supplement doctors' regular in person visits with voicebots calls that can determine if the patient is doing well between visits.  Or analyzing a speaker to determine psychographic information on their predilection to certain topics, their overall sentiment, and voice personality. \n\n![](https://res.cloudinary.com/deepgram/image/upload/v1661976852/blog/a-voice-destin-ation-project-voice-x-2021/Project-Voice-X-Scott-Keynote-1024x496.jpg)\n\n*Scott Stephenson, Deepgram CEO, presenting \"Building the Future of Voice\" at Project Voice X 2021.*\n\nIn partnership with Cyrano.ai and Project Voice, we recorded a sample of the sessions from Day 1 to transcribe.  [Cyrano.ai](https://www.cyrano.ai/) ran an analysis of Bradley's opening session, which you can see in the third transcription post for the session #3 by Scott Sandland below. Listen, learn, enjoy and comment!\n\n1. [Welcome / Opening Keynote](https://blog.deepgram.com/opening-keynote-bradley-metrock-ceo-project-voice-project-voice-x/) - Bradley Metrock (CEO, Project Voice)\n2. [Amazon Keynote](https://blog.deepgram.com/opening-keynote-jeff-blankenberg-principal-technical-evangelist-amazon-alexa-project-voice-x/) - Jeff Blankenburg (Principal Technical Evangelist, Amazon)\n3. [Say What You Mean: Navigating Critical Conversations](https://blog.deepgram.com/say-what-you-mean-navigating-critical-conversations-scott-sandland-ceo-cyrano-ai-project-voice-x/) - Scott Sandland (CEO, Cyrano.AI)\n4. [Building The Future Of Voice](https://blog.deepgram.com/building-the-future-of-voice-scott-stephenson-ceo-deepgram-project-voice-x/) - Scott Stephenson (CEO, Deepgram)\n5. [The New Age Of Voice Commerce](https://blog.deepgram.com/the-new-age-of-voice-commerce-mike-zagorsek-coo-soundhound-project-voice-x/) - Mike Zagorsek (Chief Operations Officer, SoundHound)\n6. [Sparking The Future Of Conversation Design](https://blog.deepgram.com/sparking-the-future-of-conversation-design-braden-ream-ceo-voiceflow-project-voice-x/) - Braden Ream (CEO, Voiceflow)\n7. [NLP On The Edge: Voice, AI, and Hardware](https://blog.deepgram.com/nlp-on-the-edge-voice-ai-and-hardware-robert-daigle-and-andi-huels-lenovo-project-voice-x/) - Robert Daigle (Global AI and Innovation Leader, Lenovo) and Andi Huels (Head of AI, North America, Lenovo)\n8. [What's Next For AI in the Contact Center](https://blog.deepgram.com/deepgram-projectvoicex-transcription-aicontactcenter-artcoombs/) - Art Coombs (CEO, KomBea)\n9. [Voice in Healthcare Part I](https://blog.deepgram.com/voice-in-healthcare-dr-yared-alemu-ceo-tqintelligence-project-voice-x/) - Dr. Yared Alemu (CEO, TQIntelligence)\n10. [Voice in Healthcare Part II](https://blog.deepgram.com/voice-in-healthcare-henry-oconnell-ceo-canary-speech-project-voice-x/) - Henry O'Connell (CEO, Canary Speech)\n11. S[onic Branding in the Enterprise](https://blog.deepgram.com/sonic-branding-in-the-enterprise-audrey-arbeeny-ceo-audiobrain-project-voice-x/) - Audrey Arbeeny (CEO, Audiobrain)\n12. [Retail, Restaurants, and Travel](https://blog.deepgram.com/retail-restaurants-and-travel-shilp-agarwal-ceo-blutag-project-voice-x/) - Shilp Agarwal (CEO, Blutag)\n13. T[he Evolution Of Conversational AI In The Car And Beyond](https://blog.deepgram.com/the-evolution-of-conversational-ai-in-the-car-and-beyond-shyamala-prayaga-sr-software-product-manager-ford-project-voice-x/) - Shyamala Prayaga (Product Manager, Conversational AI, Ford Motor Company)\n14. [The Importance of Testing with Voice Experiences and Conversational AI](https://blog.deepgram.com/the-importance-of-testing-with-voice-experiences-and-conversational-ai-john-kelvie-ceo-bespoken-project-voice-x/) - John Kelvie (CEO, Bespoken)\n15. [Accuracy Matters: Improving Speech Recognition Through Data Processes](https://blog.deepgram.com/accuracy-matters-improving-speech-recognition-through-data-processes-esteban-gorupicz-ceo-atexto-project-voice-x/) - Esteban Gorupicz (CEO, Atexto)\n\nTo learn more about how Deepgram can help you, [contact us](https://deepgram.com/contact-us/).";
						}
						async function compiledContent$4f() {
							return load$4f().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$4f() {
							return (await import('./chunks/index.32fed3f0.mjs'));
						}
						function Content$4f(...args) {
							return load$4f().then((m) => m.default(...args));
						}
						Content$4f.isAstroComponentFactory = true;
						function getHeadings$4f() {
							return load$4f().then((m) => m.metadata.headings);
						}
						function getHeaders$4f() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$4f().then((m) => m.metadata.headings);
						}

const __vite_glob_0_5 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$4f,
  file: file$4f,
  url: url$4f,
  rawContent: rawContent$4f,
  compiledContent: compiledContent$4f,
  default: load$4f,
  Content: Content$4f,
  getHeadings: getHeadings$4f,
  getHeaders: getHeaders$4f
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$4e = {"title":"Accuracy Matters: Improving Speech Recognition Through Data Processes - Esteban Gorupicz, CEO, Atexto- Project Voice X","description":"Accuracy Matters: Improving Speech Recognition Through Data Processes presented by Esteban Gorupicz, CEO of Atexto, presented on day one of Project Voice X. ","date":"2021-12-09T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981396/blog/accuracy-matters-improving-speech-recognition-through-data-processes-esteban-gorupicz-ceo-atexto-project-voice-x/proj-voice-x-session-esteban-gorupicz-blog-thumb-5.png","authors":["claudia-ring"],"category":"speech-trends","tags":["accuracy","project-voice-x"],"seo":{"title":"Accuracy Matters: Improving Speech Recognition Through Data Processes - Esteban Gorupicz, CEO, Atexto- Project Voice X","description":"Accuracy Matters: Improving Speech Recognition Through Data Processes presented by Esteban Gorupicz, CEO of Atexto, presented on day one of Project Voice X. "},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981396/blog/accuracy-matters-improving-speech-recognition-through-data-processes-esteban-gorupicz-ceo-atexto-project-voice-x/proj-voice-x-session-esteban-gorupicz-blog-thumb-5.png"},"shorturls":{"share":"https://dpgr.am/8ecf923","twitter":"https://dpgr.am/2b2ddce","linkedin":"https://dpgr.am/55a483e","reddit":"https://dpgr.am/220264b","facebook":"https://dpgr.am/a79aaba"}};
						const file$4e = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/accuracy-matters-improving-speech-recognition-through-data-processes-esteban-gorupicz-ceo-atexto-project-voice-x/index.md";
						const url$4e = undefined;
						function rawContent$4e() {
							return "*This is the transcript for “Accuracy Matters: Improving Speech Recognition Through Data Processes,” presented by Esteban Gorupicz, CEO at Atexto, presented on day one of Project Voice X.* \n\n*The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was **94% accurate.**  Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram’s API.  If you want to see if Deepgram is right for your use case, [contact us](https://deepgram.com/contact-us/).*\n\n\\[Esteban Gorupicz:] Hi, everyone. I am Esteban Gorupicz, founder and CEO of Atexto. Let me introduce myself. I started to work with speech processing technologies fifteen years ago and with crowd sourcing technologies, and I founded Atexto four years ago to help companies solve the main problem related to voice technologies adoption worldwide. That is the problem of accuracy… or better say, the lack of accuracy. And when I speak about accuracy, I’m not only speaking about the word error rate.\n\nI’m talking about bias or fairness and also talking about language support. And these kind of problems are the ones our company is trying to solve, and to do that, we built a software platform that is code free, tool website for machine learning teams, for data science teams, to help them visualize, label, and collect speech training data faster. For example, related to labeling speech, we are the only platform that allows data managers to label, not only text, but also sounds in recordings. And these teams can do it by themself.\n\nBut, also, we have a crowdsourcing facility, a crowdsourcing platform fully automated with more than one point five million users… or rather registered from more than fifty countries to perform micro-tasks related to annotation speech, annotation text, audio transcription to build curated datasets to train machine learning models and improve the accuracy of these kind of products. Also, through our data manager platform, data managers can run voice data collection projects, and they can, for example, select the country of residents of the people they need to repeat… allow different training phrases, pronouncing brands, pronouncing product names, and and this kind of information. And they can select their mother tongue, their country of residents, the gender distribution, also the kind of frequency they need for the recordings.\n\nFor example, as you know, if you need data to train a speech recognition model for a call center, you need recordings from from the telephone. That is not only related to an eight kilohertz frequency. It’s related also to to the distance to the microphone. It’s very important to to better train the model, but they can select a collection project utilizing a a desktop computer where you can utilize another kind of microphone, and it’s useful for for voice assistance in the car, for example. The the cool thing about our platform is our clients don’t need to define… to design a UI to perform this kind of task. They don’t need to design a workflow. They don’t need to set up golden questions, consensus algorithms to curate the information, to curate the the data we are collecting. They only need to provide the prompt. Our users will pronounce, and our software will curate… will filtrate the correct utterances, and we’ll send to to our clients only the best ones. Also, we have another module that is a ASR benchmark module where you can run experiments to measure the word error rate, but also token error rate related to punctuation, related to capitalization, inverse text normalization. And you can come… can compare different brands of your own speech recognition engine against the main vendors that is Amazon Transcribe, IBM Watson, Google, and and all all of them, not only by recording, but also by gender, by age, by ethnic origin. So our platform are helping big companies to measure the fairness of the the voice-based products they are releasing to production environments in a way to be… fairness with all of their clients, and that that is a a very important thing, I think. The last, I I will be very, very brief because I… I’m the last one on the stage today, and brevity is a… is the soul of wit. We are helping our clients, and this is very important for us, to feel their own long-term defensibilities around data. We ambition a future where every company will be a voice-based company. And in this future world, we not only accelerate, we… not only we’ll accelerate the voice technology adoption, we also democratize this kind of technology for every company. This is our vision. This is Atexto. Thank you for hearing me. Thank you.";
						}
						async function compiledContent$4e() {
							return load$4e().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$4e() {
							return (await import('./chunks/index.c768d039.mjs'));
						}
						function Content$4e(...args) {
							return load$4e().then((m) => m.default(...args));
						}
						Content$4e.isAstroComponentFactory = true;
						function getHeadings$4e() {
							return load$4e().then((m) => m.metadata.headings);
						}
						function getHeaders$4e() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$4e().then((m) => m.metadata.headings);
						}

const __vite_glob_0_6 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$4e,
  file: file$4e,
  url: url$4e,
  rawContent: rawContent$4e,
  compiledContent: compiledContent$4e,
  default: load$4e,
  Content: Content$4e,
  getHeadings: getHeadings$4e,
  getHeaders: getHeaders$4e
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$4d = {"title":"How to Add Captions and Subtitles to HTML5 Videos","description":"Generating accurate transcripts is often just the start of a journey. Learn how to use Deepgram's best-in-class transcriptions in HTML Video elements.","date":"2022-07-28T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1658998154/blog/2022/07/adding-subtitles-to-html-video-element/post-cover.png","authors":["kevin-lewis"],"category":"tutorial","tags":["beginner","javascript"],"seo":{"title":"How to Add Captions and Subtitles to HTML5 Videos","description":"Generating accurate transcripts is often just the start of a journey. Learn how to use Deepgram's best-in-class transcriptions in HTML Video elements."},"shorturls":{"share":"https://dpgr.am/0126109","twitter":"https://dpgr.am/3778ec9","linkedin":"https://dpgr.am/4f6726f","reddit":"https://dpgr.am/bbb4eb8","facebook":"https://dpgr.am/86e87ae"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454104/blog/adding-subtitles-to-html-video-element/ograph.png"}};
						const file$4d = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/adding-subtitles-to-html-video-element/index.md";
						const url$4d = undefined;
						function rawContent$4d() {
							return "\r\nGenerating accurate transcripts is often just the start of a journey. Learn how to use Deepgram's best-in-class transcriptions in HTML `<video>` elements.\r\n\r\n## Generating Transcriptions\r\n\r\nTo add subtitles to a HTML `<video>` element requires a WebVTT file. We previously wrote about [generating WebVTT captions with Node.js](https://blog.deepgram.com/generate-webvtt-srt-captions-nodejs/). Assuming you have an MP4 video to transcribe, you can use this snippet to generate a subtitles file:\r\n\r\n```js\r\n// npm install @deepgram/sdk\r\n\r\nconst fs = require('fs')\r\nconst { Deepgram } = require('@deepgram/sdk')\r\nconst deepgram = new Deepgram('YOUR_DEEPGRAM_API_KEY')\r\n\r\nconst fileSource = {\r\n    stream: fs.createReadStream('./video.mp4'),\r\n    mimetype: 'video/mp4',\r\n}\r\n\r\ndeepgram.transcription.preRecorded(fileSource, {\r\n    punctuate: true,\r\n    utterances: true\r\n}).then(result => {\r\n    fs.writeFileSync('captions-en.vtt', result.toWebVTT());\r\n})\r\n```\r\n\r\nYou will need to replace `YOUR_DEEPGRAM_API_KEY` with a valid key which you can get [for free here](https://console.deepgram.com).\r\n\r\n## Set Up a Video Player\r\n\r\nCreate an `index.html` page:\r\n\r\n```html\r\n<!DOCTYPE html>\r\n<html>\r\n    <body>\r\n        <video controls width=\"500px\">\r\n            <source type=\"video/mp4\" src=\"video.mp4\" >\r\n       </video>\r\n    </body>\r\n</html>\r\n```\r\n\r\nLoad the webpage, and you should see a video player.\r\n\r\n![A webpage with only a video player visible](https://res.cloudinary.com/deepgram/image/upload/v1657806576/blog/2022/07/adding-subtitles-to-html-video-element/video.png)\r\n\r\n## Add Subtitles\r\n\r\nInside of the `<video>` tag add a `<track>` element to represent the caption file:\r\n\r\n```html\r\n<track src=\"captions-en.vtt\" label=\"English\" kind=\"subtitles\" srclang=\"en\" default>\r\n```\r\n\r\n*   The `src` attribute is a filepath. This assumes the file is in the same directory as the HTML file.\r\n*   `label` is shown to the user when selecting which subtitles they want to see.\r\n*   `kind` specifies the type of track. We're choosing `subtitles` as these generally just contain spoken words, while `captions` include other important background sounds.\r\n*   `srclang` specifies the language of the file.\r\n*   `default` is honored by Safari, while Chromium-based browsers will try and select a captions file based on the browser's language setting.\r\n\r\n![A webpage with only a video player visible. The video player has subtitles.](https://res.cloudinary.com/deepgram/image/upload/v1657806575/blog/2022/07/adding-subtitles-to-html-video-element/subtitles.png)\r\n\r\nYou can add as many subtitle tracks as you want, but only one can have the `default` attribute. For example:\r\n\r\n```html\r\n<video controls width=\"500px\">\r\n    <source type=\"video/mp4\" src=\"video.mp4\" >\r\n    <track src=\"captions-en.vtt\" label=\"English\" kind=\"subtitles\" srclang=\"en\" default >\r\n    <track src=\"captions-fr.vtt\" label=\"French\" kind=\"subtitles\" srclang=\"fr\" >\r\n</video>\r\n```\r\n\r\nAs a final note, if you've not seen [Scott's Chili Pepper video](https://blog.deepgram.com/chili-pepper/) which is in the header image for this post - you should. It's hilarious. If you have any questions, please feel free to reach out on Twitter - we're [@DeepgramDevs](https://twitter.com/DeepgramDevs).\r\n\r\n        ";
						}
						async function compiledContent$4d() {
							return load$4d().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$4d() {
							return (await import('./chunks/index.0c16eb41.mjs'));
						}
						function Content$4d(...args) {
							return load$4d().then((m) => m.default(...args));
						}
						Content$4d.isAstroComponentFactory = true;
						function getHeadings$4d() {
							return load$4d().then((m) => m.metadata.headings);
						}
						function getHeaders$4d() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$4d().then((m) => m.metadata.headings);
						}

const __vite_glob_0_7 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$4d,
  file: file$4d,
  url: url$4d,
  rawContent: rawContent$4d,
  compiledContent: compiledContent$4d,
  default: load$4d,
  Content: Content$4d,
  getHeadings: getHeadings$4d,
  getHeaders: getHeaders$4d
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$4c = {"title":"What Role Does Bias Have in Machine Learning? — AI Show","description":"Welcome to the AI Show. Today we ask the question: What role does bias have in machine learning?","date":"2018-10-19T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981322/blog/ai-show-bias-in-machine-learning/what-role-does-bias-have-in-ml-blog-thumb%402x.jpg","authors":["morris-gevirtz"],"category":"ai-and-engineering","tags":["bias","machine-learning"],"seo":{"title":"What role does bias have in machine learning? — AI Show","description":"Welcome to the AI Show. On the AI Show, we talk about all things AI. Today we ask this question:What role does bias have in machine learning?"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981322/blog/ai-show-bias-in-machine-learning/what-role-does-bias-have-in-ml-blog-thumb%402x.jpg"},"shorturls":{"share":"https://dpgr.am/1df2844","twitter":"https://dpgr.am/06306ca","linkedin":"https://dpgr.am/634934f","reddit":"https://dpgr.am/6b81f61","facebook":"https://dpgr.am/e26011d"}};
						const file$4c = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/ai-show-bias-in-machine-learning/index.md";
						const url$4c = undefined;
						function rawContent$4c() {
							return " <iframe src=\"https://www.youtube.com/embed/mvvbPmYSHcY\" width=\"600\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\n**Scott:** Welcome to the AI Show. On the AI Show, we talk about all things AI. Today we ask this question:\n\n## What role does bias have in machine learning?\n\n**Susan:** Whoa. Bias. That's a really cool one.\n\n**Scott:** It is a pretty good one.\n\n**Susan:** I love bias. I'm predisposed towards it at least.\n\n**Scott:** You might be biased.\n\n**Susan:** I might be biased.\n\n**Scott:** You might be biased but you like bias.\n\n**Susan:** I like bias. I think bias is pretty much everywhere.\n\n## How do we define bias in machine learning?\n\n**Susan:** It's the predisposed nature of your model.\n\n**Scott:** What's a good example?\n\n**Susan:** A good example would be digit prediction. Do you think seven happens more often than one?\n\n![alt](https://images.unsplash.com/photo-1522069213448-443a614da9b6?ixlib=rb-0.3.5&ixid=eyJhcHBfaWQiOjEyMDd9&s=3cbcf6bd6971590052f6c18355f0aa40&auto=format&fit=crop&w=800&q=60)\n\n**Scott:** I don't know. It depends on the context.\n\n**Susan:** It does depend on the context, but if you were to just randomly pick up a bunch of numbers and measure the frequency.\n\n**Scott:** So like for area codes on phone numbers or something?\n\n**Susan:** Yes.\n\n**Scott:** Would zero happen a lot? Zero is never the first number.\n\n**Susan:** There's a good set of laws in numbers here about which ones happen first and which one's the relative magnitude to frequency here. That's an example of not necessarily bias, but what the natural tendency of a distribution is.\n\n**Scott:** For example, pick a random name, there's probably like the top 10 names in English speaking world, and we're likely to get one of those.\n\n**Susan:** Our models are predisposed towards certain answers. This is their prior probability here.\n\n* When they're biased, what does that mean?\n* Why is it biased?\n\nWe're biased because we're predisposed towards one thing, but we're picking against a different distribution. In other words, we train the model one way and we are biased against that training.\n\n## What's a good example?\n\n**Scott:** You have an image recognition program, and you want it to tell the difference between cats and dogs, and all you do is give it cats and dogs, fine. But all of the cats that you give it are brown, and all the dogs that you give it are black. But, then you show it a black cat, and now it guesses that it's a dog. And you're like, \"What is that about?\" It's because all of the cats that you gave it, or all the dogs you gave it were black, and all the cats were brown, or whatever. It learns these, it doesn't necessarily always learn that, \"Hey, that's a cat.\" How a person would think of it, they think like, \"Oh, it's a brown cat.\" But anything that's brown, I think of it as a cat or something like that.\n\n![alt](https://res.cloudinary.com/deepgram/image/upload/v1661976754/blog/ai-show-bias-in-machine-learning/Havana_Brown_-_choco.jpg)\n\n*Brown cats are actually rather rare. However, the Havana Brown Cat just such a cat.*\n\n## Where is bias introduced?\n\n**Susan:** I mean, we were just talking about a little bit, obviously, you can have A, specifically bias but how does that happen?\n\n**Scott:**\n\n* Is it the model that is biased?\n* Is it the data that's biased?\n* Is it the way that you're training the model or something like that?\n\nWell, it's usually not the model that's biased. But it's usually the data. So that might be the way you get a whole bunch of different data, and the way that you clean the data. Maybe you just cut out a whole section because you don't know how to deal with it. Maybe you can't tell whether it's a cat or a dog, and so you just never include that. That would be your biasing just the input dataset. Just the way that you can collect pictures of cats and dogs. Maybe it's more difficult to collect pictures of dogs, because people don't take as many pictures of them compared to their cats or something like that. So you have this overwhelming majority of pictures of cats or something like that.\n\n**Susan:** Cats demand pictures.\n\n**Scott:** You have to have them. Facebook, et cetera.\n\n**Susan:** Dogs love you no matter what, but cats, cats are like, \"You need to do stuff to earn my love.\"\n\n**Scott:** Some data is easier to get, some data is easier to label, some pictures are more clear, audio is more clear. So the short answer is that\n\n\"The data is usually the thing that's biased. Your model might be biased, but usually it's biased in just the fact that maybe it can't learn that type of thing at all, or it can, and that's just what sort of function does it have. But the data is what makes it biased, kind of like a human.\"\n\nHumans aren't necessarily growing up biased in the way that we think about it, but their experience sort of teaches them that.\n\n**Susan:** It's true. It's very hard to build bias into this structure of a model when you're talking about deep learning networks. I mean, it is possible, like you said. Hey I'm only going to accept these frequencies or something like that. Then you've codified a bias into there, but there you go. There's some fairly famous examples of bias in machine learning popping up lately.\n\n**Scott:** [Amazon is a big one in the news recently](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G).\n\n**Susan:** Yeah, what do you think about that?\n\n**Scott:** Yeah, the details behind that are essentially, they had a recruiting machine learning model that would automatically analyze resumes and then try to give a score for how likely that model would think it is to hire that person essentially. If you look at tech companies across the world, you see that they're predominantly male, and the males might tend to describe themselves in different ways. There's a really interesting example in there that was like, if you had just the word \"women,\" or \"women's,\" sort of anywhere in your resume - it doesn't say, \"I'm a female\" or anything like that. For example: \"When I was in college I focused on women's studies.\" or something like that. Then it was \"Negative, we don't think we should hire you.\" But it's not assessing you as a *person* thinking that they shouldn't hire you, it's just that all of the examples of the people that they've hired didn't have that background. So it was biased against you.\n\n**Susan:** That really points out a pretty significant thing about training data. You really need to know what you're targeting. In this case, it targeted being like the hiring process. It didn't focus on the result.\n\n**Scott:** How would you train a model like that?\n\n**Susan:** What is the correct positive question there? It's not necessarily, \"Would that person have been hired?\" It's how well they did.\n\n\"Focusing on how well a person did is a very hard thing to gather data on, so they had to gather data on the secondary thing, which is: \"Would the old system have hired them?\" The old system clearly showed bias, and they hired in a gender non-neutral way.\"\n\n**Scott:** And in some ways, you want your system to be biased, right? You want good people working at your company. You don't want people that are going to just blow everything up. You don't want people that are going to drive things into the ground, you want people to be good. The only example that they have of that is looking back in the past and saying, \"Here are all the people that we've hired, and here's the pile of them. Then we're going to train a model based on that.\" But, it's a little bit of a naïve approach. It might be the only way they can do it, but still it's naïve.\n\n**Susan:** Yeah, it's a really hard problem. In general,\n\n\"This is a hard problem to deal with, because first of all, you have to recognize you have a bias and not everybody recognizes that. Then that can permeate every last step of gathering your data and then cleaning your data, and then selecting every single stage, or you're biasing the results.\"\n\n## How do you avoid bias?\n\n**Susan:** That's a really hard thing. The best way to is large data that you try to be sure that when you go through each step, you actively are thinking, \"Is this biasing my training set? Is this doing something that'll have a systemic error, introducing a systemic error in there?\"\n\n**Scott:** In particular, biasing in a way that I don't want it to? You might want it to be biased toward good people!\n\n**Susan:** Or perhaps you're biasing it towards a specific population, not of people, but of things. It's like, \"Hey, I know that for a fact this visual recognition system is only going to be applied on cars, so I'm not going to give it a bunch of underwater scenes.\" Probably not going to have many cars that are underwater, and I don't need it to recognize fish. It's a class of image data that you might want to get rid of. But those types of decisions are really, really hard. The more you include in the more noise your model may have to parse through to find the signal. ![Alt](https://cdn-images-1.medium.com/max/1600/1*oKycB6NgLgPJG31fiGwpUA.png) *Photo courtesy of the [Udacity self-driving dataset.](https://github.com/udacity/self-driving-car)*\n\n**Scott:** Yeah, it's true. It's like if you trained an autonomous driving car in fair weather California in the city streets, and then you take it to snowy country roads and say, \"Drive.\" It's like, \"I've never seen this before.\" It's very biased.\n\n## Bias and regularization\n\n**Susan:** In its essence, bias is sort of a [regularization](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a) question. We're trying to make a durable model that is able to predict in a wide field accurately. It's not 100% the exact same thing, I don't want to say fixing bias is all about fixing regularization, it's not totally true.\n\n**Scott:** What do you mean by regularization?\n\n**Susan:** Regularization of a model. So this is the whole overfit, underfit. You know, the overfit question.\n\n**Scott:** And this is related though, bias is related to those things in a meaningful way.\n\n**Susan:** It's definitely related. You can develop a model that overfits, it learns to recognize just the training set. Which very much what just happened with Amazon, they recognized the training set, right?\n\n**Scott:** Or like it thinks all brown things are cats.\n\n**Susan:** Yeah, all brown things are cats. I've never seen a black cat, have you ever seen a black cat?\n\n**Scott:** Well, the model wouldn't in that context.\n\n**Susan:** If the model says, \"It's not cat, it can't be.\" One of the techniques for regularization is a diverse set of data. That is to say diversity in your data will help reduce bias. It's a tool, it's not the only tool.\n\n**Scott:** Yeah, you want to cover the different categories, and you want to have those categories show up in different situations as well, like in the context of images again. Maybe it's a picture of a cat, but you want a picture of a cat outside, in a car, in a bus, with low light, lots of light, lots of glare, cloudy day. All these different things essentially to give you a different feel for that object, but in all these different settings.\n\n**Susan:** Another important thing is just paying attention. You do have to pay attention to the frequency of the data that you're training on. If, for instance, you are trying to recognize common pets, and you give it exactly 50% training examples of pigs, and 50% training examples of cats, then it's going to have a bias, this preconceived notion on training data.\n\n**Scott:** Like, pigs are as popular as cats.\n\n**Susan:** Pigs are pretty darn likely. It's not impossible to see one, but that will likely give your model a bias saying, \"That portly cat that you've been feeding too much may have turned into a pig.\"\n\n**Scott:** A fat cat, calls it a pig. Yeah. But we see bias in other areas as well in these models that you're training. Like a credit transaction doing fraud detection or something, right? Again, may be good, may be bad. But when you're making your weird online purchase, and then it gets denied, it's like, \"Hey, I'm a good person. Why did that get denied?\" It's like, \"Shady characters out there sometimes buy from this site,\" or something like that. It's not so nefarious, right? But it contributes to that.\n\n**Susan:** Yeah, it's a big challenge. We've talked about sources of bias in the data, and locations of bias throughout the training pipeline. What are your thoughts on the results of bias and the social implications of this in models?\n\n## Is this going to be a bigger problem in the future?\n\n**Scott:** Nothing is going to be perfect tomorrow.\n\nYou can't snap your fingers and fix bias. Humans always fall victim to bias all the time. It's a product of the environment that you grow up in as a human.\n\nHowever, it's also as a model, the product of the environment that you grow up in, et cetera. One of the most important things that you should keep in mind, though, it's like, \"Where do I have good predictive power that is not biased? Then, where do I make predictions but I don't have good predictive power and it is biased over here? What should I listen to and what shouldn't I listen to essentially? That's the hard part, really, what are the features that are being sent into your model you should disregard. Although, your model thinks that these are really important. It's like, \"You shouldn't be using those as a marker of whether they're good or not, use other things as a marker of whether they're good or not.\" That's a challenge for AI scientists and engineers to build the different datasets, but also to kind of pick and prune and use your human brain a little bit, and then bootstrap into the future. Like, \"Whoa, what did we do wrong in the past?\" Let that inform the dataset that we procure in the future. Then also, how we train our model in the future and the frequency that we show the data to our model. Things like that. It's not going to be done tomorrow. It's not, \"Oh, oops, we were biased. Now it's going to be solved.\" You have to evolve into it through multiple processes.\n\n**Susan:** That's a common theme that I've run into, you've run into. The very first time you try to build something, you're going to get it wrong more likely, but you learn so much. The second time you maybe get something that appears to be useful, but that has some crazy, weird behaviors.\n\n**Scott:** Sure, it's probably 80% useful. It's a lot of the way there, but it's got some weird stuff going on.\n\n**Susan:** In general, we're at that point where we're getting really complex problems that subtleties like gender bias are creeping into it.\n\nPeople are setting their opinions about machine learning right now based off these initial stories. But the real where-are-we-at and what-will-the-trends-be are going to come in the next couple years as the Amazons of the world, the Facebooks of the world start realizing that \"Oh, that first one was biased.\"\n\nThey start building in processes that correct that for the next time. They do a complex human modeling kind of task. They start realizing, \"Oh, gender bias was a problem. What did we do to fix that in this other type of problem? Or some other type of bias.\" It's going to be a challenge over the next couple years.\n\n**Scott:** It's not going to be easy. You might look at the first version of the model that you created that it's actually good and you're say, \"Wow, such a triumph, look at us, pat on the back, this is so great. World leading, never been done before.\" Et cetera. Then say, \"Well, we're going to have to do five times as much work in order to make it not as biased as it is.\" Yeah, it's useful now, but it's also hurting us in a lot of ways. To get that extra 10% or 20% or something and sort of take care of those problems is going to be even more work.\n\n**Susan:** It gets well-rounded.\n\n**Scott:** Yeah, it's well-rounded, exactly.\n\n**Susan:** That's really what comes down to. Technology becomes well-rounded as we run into those problems over time. Predicting the problems is very, very hard. No one can say: \"Hey, this model is going to have this problem, of course, hindsight it's 2020.\" You look back and, \"Of course you shouldn't have done that.\" At the time, we didn't know that.\n\n**Scott:** Yeah, it's a lot like a child learning growing up, and becoming more experienced and senior at things. In the very beginning you're like, \"Whoa, they're actually pretty good. They get a lot of what's going on here, but do they know the broader context of the entire world?\" Do they see one thing and now they've got a hammer and everything becomes a nail? But as you grow up, you start to think like, \"Hmm, maybe there's other nuances to take into account here.\" I think that maturity has to happen with the models as well.\n\n## Going forward, where do you see bias' biggest problem?\n\n**Susan:** Everything's going to get bias, but where are the types of tasks will present the biggest problems?\n\n**Scott:** I'm not sure what the biggest is going to be, but for me, something that comes right to mind are financial decisions, like giving people bank loans or something like that. The economic achievement of a country could be this much, but it's actually this little, because the financial system and distribution of money, or other resources isn't as good as it should be essentially because of bias. This is something that naturally happens in countries now. Who should get federal aid for schools? Or, which company should be saved by the government? Which tax breaks? But, as those things start to turn into a more informed machine, as you take some of the human emotion out of it, I think you're going to get a whole bunch of value from it. But, you probably still aren't going to see the full value.\n\nThere's a lot of bias now, you can only remove some of it. If you remove all of it, you probably can extract a ton of value.\n\nI think that's the thing that's going to have the most impact on the world if you just sort of look at the quality of life of people.\n\n**Susan:** That's a great point. We look at machine learning and the bias that it has, but in this exact instance, the Amazon instance, it exposed the bias that was there.\n\n**Scott:** Correct, bias was already there.\n\n**Susan:** This really helped expose it.\n\nSo going forward there's a chance that machine learning could really help us understand the biases we have, because when people make these models, they're really critically thinking about the data that's going into it, and there's a very good chance that they're considering, \"Am I making this biased?\"\n\n**Scott:** We just keep pulling ourselves up by our bootstraps, but over and over. Limp yourself into a better area.\n\n**Susan:** Yeah, hope for the future, I like hope for the future. I'm a positive guy. You may not have known this.\n\n**Scott:** Oh yeah, well, you know. I suspected it. I think another area to think about is\n\n**Whom do you give raises to? How do you keep a company functioning?** In the Amazon example they only had your resume to start with. But then once you're inside a company, and they have all of this other data about you, and some of the tech companies are going to have a lot of data about you because they build their internal systems to know everything that you're doing. Like how often do you email, how long are your emails, do you complain to HR, do you whatever, all these different things. Now, as they're building their internal machine learning algorithms to help figure out how to make their companies more efficient, they're probably going to have another bias. It's definitely going to be crammed in there like a whole other second level of bias.\n\n## Reinforcement and bias\n\n**Susan:** That's a really interesting thing. Sort of that reinforcement issue we spoke about. This thing maybe hires people that fit into this little pigeonhole that then causes them to hire more people that fit into this pigeonhole. It starts learning to narrow it down more and more and more. That's a really dangerous path, because you need diversity and everything to get those new ideas.\n\n**Scott:** And you end up with the [Facebook feed phenomenon](https://www.wired.com/2016/11/facebook-echo-chamber/). Where it's echo chamber. That's biased right there. It's bias that feels really good to people. Search results.\n\n**Susan:** Everyone thinks like me, Scott, everyone.\n\n**Scott:** Yeah, they always like my post.\n\n**Susan:** It's amazing. Every single thing in my feed is like, \"I totally agree. I knew that. Wow, that's awesome.\"\n\n**Scott:** Yeah. You never see an opposing viewpoint.\n\n**Susan:** I always remove those.\n\n**Scott:** Yeah, block.\n\n**Susan:** Block for 30 days, and then put in on constant block for 30 days.\n\n**Scott:** You're feeding the algorithm. Search results are another big one.\n\n**Susan:** Man, yeah. Probably it's paid by us.\n\n**Scott:** Yeah, that's a good point. Ads, yeah. But also the ranking, right?\n\n**Susan:** That's another great one. How can adversaries take advantage of bias? You start learning a little bit about the underpinnings of an algorithm, maybe get ahold of a training set, and you start realizing there's this bias I can take advantage of.\n\n**Scott:** It's a [search engine optimization](https://en.wikipedia.org/wiki/Search_engine_optimization) 3.0!\n\n**Susan:** I'm going to build that perfect resume that just says, \"My name is this.\" And then has a just awesome data dump of words right below. Suddenly, you're at the top of every single resume list. It's just got name is this, and just gibberish below it because you figured out the bias of the algorithm.\n\n**Scott:** You figured out what gave you the highest signal. How would you figure that out? How would you build an adversary to figure that out? What's your first attempt?\n\n**Susan:** My first attempt, well, just brute force running through... The problem is the feedback, how do you know? How do you do it in a black box way? You set up about 50 fake people? You change a couple little things, and you submit those 50 resumes. You try to figure out some pattern.\n\n**Scott:** Then submit 100 different resumes to the same company, but you sort of randomize some of the results and see which one surface to the top.\n\n**Susan:** At least which one gets called back, right? Which does this. I've seen some studies that have done this. They write a resume, and tried their best to keep gender pronouns out, but throw one little thing in there to see how that affects it. Or the same resume and they'll have either a male or a female answer the phone on the call back and see what the result is after that. That type of trying to see if the system is biased. But that's not the machine learning set, that's we'll table that discussion for a different area.\n\n**Scott:** Yeah.\n\n**Susan** Let's see. We've covered quite a bit, the future of bias to a certain degree, what do we think of that, it's social implications.\n\n## What's the solution to bias in machine learning?\n\n**Scott:** We talked about overfitting, how does underfitting fit into this? Is that the solution to bias? Make worse models?\n\n**Susan:** Make worse models. Wow.\n\n**Scott:** That's kind of what Amazon did. They said, \"You know, we're going to pull back on this whole model. We're only going to use it to pick out obvious people that are submitting duplicate resumes and things like that.\"\n\n**Susan:** A solution to bias. I think one of the crucial ones we briefly mentioned before is pay attention to the actual thing you're going for. If you try to gather data based off what you did before, and not actually look at the end result, how good are the employees that you're hiring, and go for effective employee, which is, like we mentioned, really hard to figure out. It's really, really hard to figure out. The approach looks something like this: You hire these X people, you've got these performance reviews. This is the best person that had the best review, and then you kind of [backpropagate](https://en.wikipedia.org/wiki/Backpropagation) through the whole hiring chain to say, \"This is what the resume looked like.\" That's a long set of stuff that led to there. But, that's the point. It's really hard to pay attention to the question - to the thing that actually really, really mattered. That's why we have these proxies that lead to bias.\n\n**Scott:** Generally, a [data sparsity problem](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.144.3180&rep=rep1&type=pdf), right? If you had a whole bunch of examples of all the different categories, but the people still had really good results, then your model wouldn't be that biased probably. But, there are only so many top performers in a company, or so many different examples of the good thing that you want. Then that's a lot of how bias creeps in.\n\n**Susan:** But definitely, you need to admit to yourself when you're [proxying the question](https://en.wikipedia.org/wiki/Proxy_(statistics)), or when you're proxying your cost, I should say, your cost function.\n\n**Scott:** Something that I think correlates with it.\n\n**Susan:**I think it correlates to it, because I can't get to the root thing here. At least, when you admit it, you do two things. First of all, you critically analyze it a lot more. Second of all, you're constantly looking for a way to directly get that direct information. Maybe you'll change your system, your hiring system in some way that allows you to start trickling in that data to be able to build a model better in the future.\n\n**Scott:** You brought up a really good point that I think it can't be hammered enough in the age of AI: what's going to happen? Is it going to replace all humans? Or is it something like that? It's like,\n\n\"No, no, no. There should and will probably always be a human in the loop.\" Use the human intellect. Use the creativity of a person. Use the ability to use common sense as best, you know, in the places that make sense. Essentially, in the ambiguous cases.\n\nRight? There'll always be a human in the loop. The same way that there aren't manufacturing facilities where there's just literally zero people in the building. There are people there. They're making sure the machines are running the way they're supposed to be and they're watching over it.\n\n**Susan:** I did a tour of a brewery, which was phenomenal. There they had people constantly running around checking how things are going, because you know, that one little can could throw the whole thing off.\n\n**Scott** If they're not putting the labels on, they're just making sure that the labels are put on the right way.\n\n**Susan** When you get 4,000 cases of beer that are with an upside down label, what are you going to do with that? Maybe... \n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976755/blog/ai-show-bias-in-machine-learning/Bottles-beer-fill--food-drink..jpg)\n\n*Filling machines have automatically filled, capped and labeled beer and other drinks since the 19th century. As machine learning has improved, humans have had to oversee less and less of the process, focusing on more creative aspects of the process, such as creating labels and making tastier beer.\"*\n\n**Scott:** Ship them to the Deepgram office.\n\n**Susan** Yeah, please.\n\n**Scott** Any parting thoughts?\n\n**Susan:** Man, what's my favorite bias? My favorite bias, I'm biased against raw onions on pizza. Sorry.\n\n**Scott:** Huge. Yeah, me too. Onions, tomatoes, yeah, get them out of here.\n\n**Susan:** Yeah.\n\n**Scott:** I know you won't agree on the tomatoes.\n\n**Susan:** Yeah, I mean, tomatoes are pretty amazing.";
						}
						async function compiledContent$4c() {
							return load$4c().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$4c() {
							return (await import('./chunks/index.8d9c18b4.mjs'));
						}
						function Content$4c(...args) {
							return load$4c().then((m) => m.default(...args));
						}
						Content$4c.isAstroComponentFactory = true;
						function getHeadings$4c() {
							return load$4c().then((m) => m.metadata.headings);
						}
						function getHeaders$4c() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$4c().then((m) => m.metadata.headings);
						}

const __vite_glob_0_8 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$4c,
  file: file$4c,
  url: url$4c,
  rawContent: rawContent$4c,
  compiledContent: compiledContent$4c,
  default: load$4c,
  Content: Content$4c,
  getHeadings: getHeadings$4c,
  getHeaders: getHeaders$4c
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$4b = {"title":"What are the different types of machine learning? - AI Show","description":"Learn about the different kinds of machine learning in this episode of the AI Show.","date":"2018-09-15T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981314/blog/ai-show-different-types-of-machine-learning/what-are-the-different-types-of-machine-learning-b.jpg","authors":["scott-stephenson"],"category":"ai-and-engineering","tags":["deep-learning"],"seo":{"title":"What are the different types of machine learning? - AI Show","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981314/blog/ai-show-different-types-of-machine-learning/what-are-the-different-types-of-machine-learning-b.jpg"},"shorturls":{"share":"https://dpgr.am/5c9136c","twitter":"https://dpgr.am/cd3415f","linkedin":"https://dpgr.am/b61ac44","reddit":"https://dpgr.am/af0a17a","facebook":"https://dpgr.am/4bfcd8a"}};
						const file$4b = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/ai-show-different-types-of-machine-learning/index.md";
						const url$4b = undefined;
						function rawContent$4b() {
							return "\r\n<iframe width=\"600\" height=\"315\" src=\"https://www.youtube.com/embed/7VMio8Tk2so\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe>\r\n\r\n**Scott**: Welcome to the AI show. Today we're asking a couple big questions. What are those big questions?\r\n\r\n*   What are the different types of [machine learning](https://en.wikipedia.org/wiki/Machine_learning)?\r\n*   What are the different types of data?\r\n\r\nUsually people think about three different types, like reinforcement learning, unsupervised, or supervised.\r\n\r\n## What is Reinforcement Learning?\r\n\r\n**Susan**: [Reinforcement learning](https://en.wikipedia.org/wiki/Reinforcement_learning) is learning from a series of actions where you get a series of choices and rewards along away. So, a classic one that's been in the news is [AlphaGo](https://en.wikipedia.org/wiki/AlphaGo). A large chunk of reinforcement learning techniques are used in there, specifically Monte Carlo tree research techniques.\r\n\r\n**Scott**: So, people play the game Go. AlphaGo is a machine playing Go, being very good at it, and beating the world's top Go players.\r\n\r\n [![alt](https://res.cloudinary.com/deepgram/image/upload/v1661976374/blog/ai-show-different-types-of-machine-learning/alphago.jpg)](https://www.independent.co.uk/life-style/gadgets-and-tech/news/google-alphago-computer-beats-professional-at-worlds-most-complex-board-game-go-a6837506.html)\r\n\r\n**Susan**: Another one that's really fun and has popped up the last couple of years is based off of Atari games. Atari specifically has a great tool kit if you're into this world. If you want to learn more, dig up the [Atari learning environment](http://yavar.naddaf.name/ale/) and start going through a lot of go of code associated with that. It's a phenomenal toolkit for all sorts of stuff, but specifically it's been helping reinforcement learning because these games are a series actions. They lead to reward, a score.\r\n\r\n**Scott**: So, reinforcement learning is like, \"Hey, I'm taking some actions. I can do any number of things, but what I'm trying to do is make some number go up - like my score or my happiness.\"\r\n\r\n**Susan**: Exactly. Classically, there isn't just one reward at the end. You can get rewards along the way and the question is, \"How do I maximize that in the infinite game? How do I get the biggest payoff throughout time?\" It's great because it allows you not only to explore classic reinforcement learning, but also get into things like image recognition:\r\n\r\n*   How do I look at the screen?\r\n*   What does it mean to do that?\r\n*   What parts of that screen are important?\r\n*   What do I focus on?\r\n\r\nThere's a lot of great techniques that can draw you in there. You can do simple stuff or really complex stuff. It is just a fun, awesome environment to explore nothing but a bunch of great technologies.\r\n\r\n## What is Reinforcement Learning good at?\r\n\r\n**Scott**: What kind of problem would you throw at reinforcement learning, say, \"Go\" and expect it to work?\r\n\r\n**Susan**: So reinforcement learning has a lot of problems that fit into graphs. So a series actions that lead to other choices, actions that lead to other choices, actions lead to other choices. So,\r\n\r\n*   Trying to find a good path on a map.\r\n*   Trying to win one of the thousand Atari games out there.\r\n*   Trying to play a game like checkers or chess.\r\n\r\nYou know, the classic, \"I take a move, my opponent takes a move, I take a move, my opponent takes a move.\"\r\n\r\n**Scott**: So where things are pretty rigid and the actions that you take are known. An act of God isn't going to come in and change something. It's, \"Here's your environment, play in it.\"\r\n\r\n**Susan**: Basically where there's some finite set states, they transition in some way that we can model, and they have actions that affect those transitions.\r\n\r\nSo, say I have four options in front of me. I choose A and I'm in state _n_ right now. There's some probability based off of the action I choose for what state I'll end up in after that. Reinforcement learning allows me to understand those probabilities, these transitions, and also what's called the best policy. In other words, given all this information, \"What is the best action I should take to get some sort of reward in the end?\" But, it sounds dry and academic when you start talking that way. It's a lot better when you say, \"Hey, I'm gonna play Pole Position. Do I move the car left? Do I move it right? Do I tell it to speed up? Do I tell it to slow down?\" It's the same thing.\r\n\r\n [![alt](https://res.cloudinary.com/deepgram/image/upload/v1661976374/blog/ai-show-different-types-of-machine-learning/poleposition.png)](https://en.wikipedia.org/wiki/Pole_Position)\r\n\r\n**Scott**: Yeah, it might have access to the image - it can look around, it could look on the screen, it can see if a car has passed it or not - but it has to figure all those things out on its own. It's getting the input from the screen, it knows what position it's in, and it's trying to make that position number one or make its lap time faster in the game.\r\n\r\n**Susan**: Exactly, it's great. And going back to the Atari learning environment, when you're trying to frame this as a reinforcement learning problem, you've got to figure out how to identify the state you're in and what actions are available and do those different things.\r\n\r\n## What is supervised learning?\r\n\r\n**Susan**: So, when we talk about unsupervised, supervised, semi-supervised the classic definitions are all around data. So [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning), you've got a bunch of labeled, training data. Labeled meaning like \"I've got a picture of a cat and I've got right next to it `cat`. Every single piece of information I've got has the answer of what it is right next to it.\r\n\r\nOne of the most famous examples that people start off with is the [MNIST](http://yann.lecun.com/exdb/mnist/) handwriting digit recognition.\r\n\r\n [![alt](https://res.cloudinary.com/deepgram/image/upload/v1661976375/blog/ai-show-different-types-of-machine-learning/mnist.png)](https://en.wikipedia.org/wiki/MNIST_database)\r\n\r\n**Scott**: These are images where people have handwritten numbers on a piece of paper. That means 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\\. Many different people have written many different digits, but they've all been labeled. So somebody has gone through and said, \"A one is a 1 and a five is a 5, and an eight is an 8.\" It's collections of 28x28 pixels. This was done in the late nineties, but it's a large number, I think sixty thousand images. It's just black and white images of 28x28 pixels, but you can read it. You can look at it as a human and be like, \"Yep. That's a two.\"\r\n\r\n**Susan**: It's great as an academic set and it was also very useful when it came out for practical things like reading the mail.\r\n\r\n**Scott**: I think this was actually implemented in the post office in the late nineties.\r\n\r\n**Susan**: It's awesome because it's small off enough from a data perspective that pretty much anybody with a home computer can do real networks against it, you can see real results, apply some basic stuff, and have a good time learning a lot of stuff about it. It's also very easy for you to say, \"Oh, that's a one. That's a two.\" or whatever and kind of see where your model messes up.\r\n\r\n**Scott**: Yeah you only have to pick from ten. It's not that hard, but there are many different objects in the world for other datasets.\r\n\r\n## Let's talk datasets\r\n\r\n**Susan**: [ImageNet](https://en.wikipedia.org/wiki/ImageNet) or [CIFAR](https://www.cs.toronto.edu/~kriz/cifar.html) is one of my favorites. 10 and 100 there. That one's actually great because you can start off with the simpler world, the 10, and then go to 100.\r\n\r\n**Scott**: It's a large number of color images. It's the same ones, but they're labeled differently. They're either labeled into ten categories or a hundred categories.\r\n\r\n [![alt](https://res.cloudinary.com/deepgram/image/upload/v1661976376/blog/ai-show-different-types-of-machine-learning/cifar.png)](https://www.researchgate.net/figure/Heterogeneousness-and-diversity-of-the-CIFAR-10-entries-in-their-10-image-categories-The_fig1_322148855)\r\n\r\n**Susan**: Yeah, same number of pixels and colors and all that stuff so you can use basically the same model.\r\n\r\n**Scott**: Yeah so you can take the same data and say like, \"Okay, I'm going to try for the simple thing: just picking from one of ten, like `airplane`, `car`, `animal`. Things like that.\" And then maybe the animal would have ten different ones where it's like `dog`, `cat`,`lizard`.\r\n\r\n**Susan**: That's another great kind of step up because you've got more data to play with, harder classification task to deal. The simple model, the simple two layer feed forward network that would have worked on MNIST will not do nearly as well on CIFAR. So it allows you to step up your game that bit. But, both of these sets which are fully supervised, you've got the answer sitting in front of you. They're not super realistic in the real world. They're well cropped, things are well-sized, everything's exactly the same dimensions. It's great for learning, but the real world has a lot more problems than that.\r\n\r\n**Scott**: Yeah, not all images are the same size or zoomed in to the same leveling.\r\n\r\n## What is semi-supervised learning?\r\n\r\n**Susan**: [Semi-supervised](https://en.wikipedia.org/wiki/Semi-supervised_learning) says, \"I have some of the labels and I'm going to use unlabeled data to help me boost my results in some way.\" Think of audio normalization as an example - I've got a bunch of audio and I'm going to use that to help me normalize background, but I'm really only learning on the subset that I have.\r\n\r\n**Scott**: You mean like the volume?\r\n\r\n**Susan**: Yeah, like the volume or some simple stuff like that. Or, I could use it in more complex ways like using it to compress down to try to rebuild the exact same audio stream.\r\n\r\n**Scott**: Yeah, to expand on that some more, it's like \"Hey, if I gave a network or some machine learning algorithm the original image and then I forced that network to squish down into a really small space, meaning it can only hang on to a few numbers, it then has to rebuild that and try to come up with the original image.\r\n\r\nIf you squeeze that down into one number, it can't reconstruct any image. It could probably do like how bright or dark it was, but you expand that out to a few numbers, at least now it can start to construct what color the image was, maybe some shapes in it, and expanded it a little further. Now it kind of looks like the original image, but it's been compressed way down so it's not the same size as the original image.\r\n\r\nThis is a traditional [unsupervised](https://en.wikipedia.org/wiki/Unsupervised_learning) technique. Then, what you do is you take that model that tried to squish it down and you use that later in a supervised way.\r\n\r\n**Susan**: Basically, what that last little bit is, that little squished down version, is the essential information behind that image.\r\n\r\n**Scott**: Yeah, so \"I see circles here. It's red. I see some texture here.\" Maybe it's feathers or something. It doesn't even know the idea of feathers, but it knows that it's sort of grouped together. You can use that information later.\r\n\r\n**Susan**: With every single problem, you're going to find some different way of using unlabeled data to help you out. There isn't some \"Oh this is the set way.\" of doing semi-supervised learning. It's \"I found this great semi-supervised technique to help me out in problem _x_.\" In general, that's the way it goes.\r\n\r\n\"Supervised world, there's a lot more canned answers. Semi-supervised, a lot less canned answers. Unsupervised, it's pretty hard to find answers.\"\r\n\r\n## Machine learning in business\r\n\r\n**Scott**: We started out with the more academic side - reinforcement learning. Is that super useful in real world business? Not so much, right?\r\n\r\nYou could dream up ways, but is that what people are running right now in order to save money? That's not what they're doing in their business. They're probably using some sort of supervised learning technique.\r\n\r\n**Susan**: Yeah, the majority of what we're thinking about in the machine learning world is supervised, likely a [deep learning](https://en.wikipedia.org/wiki/Deep_learning) world.\r\n\r\n**Scott**: Like translation. You know, going from English to French or doing speech recognition, going from an audio recording to text that was spoken. Those are all trained. They might be augmented a little bit with some unsupervised technique, but it's almost a hundred percent supervised.\r\n\r\n**Susan**: Yeah, currently.\r\n\r\n\"As we move forward in this world, the real future is semi-supervised, unsupervised as much as possible because that's where most of our data lies.\"\r\n\r\n**Scott**: That's how a human learns.\r\n\r\n**Susan**: Exactly. Humans, we bootstrap.\r\n\r\n**Scott**: We test the world, we poke.\r\n\r\n**Susan**: Exactly. Mom and Dad start off by saying, \"Apple. Apple. Apple.\" Later on, _you_ start testing the world by saying, \"Apple?\"\r\n\r\n**Scott**: But then you hear somebody with an accent that says \"apple\" and you can kinda work out from context - _They must mean apple, I'm gonna adjust my [acoustic model](https://en.wikipedia.org/wiki/Acoustic_model)._\r\n\r\n**Susan**: We start learning more and more and more just by going through the world. That's really an amazing thing that we need to learn a lot more in the machine learning world. The more we can get those techniques and rip them out of our own head, the more we'll be able to take advantage of huge data sources that are out there.\r\n\r\n**Scott**: I still think from a pragmatic perspective, if you're a business and you want to tackle some problem using machine learning, you don't start with unsupervised, you don't start with reinforcement learning, you start with supervised learning. You go and label some data, you go gather data, then you label some of it, and then you train a model and you see how well it works. You probably don't even start with deep learning. You start with some tree-based method or something like that.\r\n\r\n**Susan**: But you definitely have to have data. If you don't understand your problem enough to have a dataset of examples and answers, then you definitely don't understand the problem well enough to train something to figure it out.\r\n\r\n## What are the different types of data? What do you do with it?\r\n\r\n**Susan**: Image and audio.\r\n\r\n**Scott**: Video.\r\n\r\n**Susan**: Text. Sequences in general.\r\n\r\n**Scott**: It might be like, \"I turned right down this street, I drove this long, I turned left down that street, and Google's trying to figure out what your intentions are. Are you going to a restaurant? Should I pop up a gas station?\" Things like that.\r\n\r\n**Susan**: Which, by the way, are great examples of where reinforcement learning is winning big. But, again, on the big classes data we're definitely blurring what it means to say classes of data anymore. Sure, it's easy to say this is an image, this is video, this is audio, this is whatever.\r\n\r\nNow we're trying to fuse together different sources of data to help us answer the question at hand: Make the money. What information sources do you need? Is it clicks? Is it pictures? Is it audio? Is it text? Is it bank accounts? Is it all these different sources of information?\r\n\r\n**Scott**: Usually called multi-channel in a business setting. Somebody emails you, they send you some chats, they also call you on the phone and you're trying to fuse all that information together, build a model, and predict something about them: Are they pissed off? Are they happy?\r\n\r\n**Susan**: That's a big challenge on two fronts.\r\n\r\nFirst of all, we've got a lot of great techniques that are sifting signal from noise, but the more noise you give it the harder it is to work and sometimes adding more data actually hurts you. So, if you can filter out a lot of bad sources, you're going to probably make your model better.\r\n\r\n\"Focus on useful data that has predictive power. There's this common misconception: If I just put enough layers, enough neurons together and enough data sources and then run it for a long enough time on a big GPU, magic and the right answer happens on the other end.\"\r\n\r\n**Scott**: Not necessarily the case. There are other constraints that come into play.\r\n\r\n## Dealing with data\r\n\r\n**Susan**: Just purely dealing with that data becomes a problem. More data means a longer time before your model converges, before it starts being able to get predictive power. Sometimes it's so far that it just never gets there.\r\n\r\n**Scott**: You're saying don't get data?\r\n\r\n**Susan**: I'm not saying don't get data. I'm saying, try to figure out how useful your data is. If you can pre-process it, you might make a huge difference on your model in some way.\r\n\r\nWe'll talk audio for a second. If you do absolutely nothing to your audio and send it to a model, you probably will get better results for whatever you're doing to that audio.\r\n\r\nIt's all about getting to a baseline because, in general, a lot of our techniques are really good at saying, \"Hey given that something's at this baseline, I can tell you to nudge it up this way or nudge it down that way.\" But, if the data's coming in down here all the time, it's working really hard just to predict up to the baseline and go above it. It's going to be biased in some way.\r\n\r\nSo, if you can get it through some simple statistical technique or whatever to a reasonable baseline, it's a lot easier for your model to nudge it in the right direction and get the right answer.\r\n\r\n## What's considered a large dataset?\r\n\r\n**Scott**: MNIST was sixty thousand images. That's not generally considered a large image dataset. It's pretty big. It's good for what it's trying to do. It can tell handwritten digits pretty well, but the large datasets in the world like ImageNet, which is actually not ten or a hundred like CIFAR, but a thousand categories. I think it's nineteen million images labeled into those categories. That's a pretty big dataset, but is it the biggest dataset in the world? No!\r\n\r\n**Susan**: It's also purely academic right now. When you look at the self-driving car world at the visual information that they're sucking in and the terabytes worth of data that they're processing through their models, that makes nineteen million images seem quaint. They're sucking down large portions of YouTube.\r\n\r\n**Scott**: They also need data that's segmented. It's not just, \"Is there a cat in this image?\" It's, \"There are three people in this image, I've drawn the outline of the person, I've drawn the shape of the road, I've drawn the traffic light,\" and things like that. It knows exactly where it is.\r\n\r\n## Thinking about your task\r\n\r\n**Scott**: So, you kind have to think about your task, right? If you're choosing from one of ten categories, then maybe like sixty thousand or ten thousand or a hundred thousand. Somewhere in that range is the number of labeled pieces of data that you would need.\r\n\r\nIf you're choosing from a hundred categories, maybe it's five or ten times that. If you're choosing from a thousand categories, maybe it's another five or ten times that in order to get up to the amount of data that you need to tell the differentiation between them. But in the speech world, we're not talking images anymore. We're talking about hours or seconds of labeled audio.\r\n\r\n**Susan**: That's an interesting one. We were talking about this earlier: How long does it take a human to learn? How many hours of audio does it take for a human to learn a language? Even from infancy.\r\n\r\n**Scott**: To get a really good grasp on your one language that you learn from birth it takes probably ten years.\r\n\r\n**Susan**: Yeah, when you think about the amount of audio that was heard in that, it was really not a tremendous amount.\r\n\r\n**Scott**: Maybe tens of thousands of hours of speech that you've heard and in a semi-supervised way.\r\n\r\n**Susan**: Exactly, a very semi-supervised way. And the crazy thing is, training a world class speech model generally takes a lot more than that.\r\n\r\n**Scott**: You're talking tens to thousands of hours or maybe ten times that. It's all purely supervised as well.\r\n\r\nSo a human is semi-supervised - at about ten thousand hours it can master language. But, a machine at ten thousand hours, even supervised, hasn't really mastered the language. It's doing pretty well, but it's not mastered. Maybe 10x that, now you're getting to the territory where it feels like it mastered the language. Still, it was a supervised way, not a semi-supervised way.\r\n\r\n## What mechanisms do humans have that machines don't?\r\n\r\n**Susan**: What are the mechanisms that allow a human to do that that we you don't have available to us? Is it how we're representing the information? Is it the structure of the model inside? Are we asking the question the right way?\r\n\r\nThis is where speech is such a fun area because there's clearly examples where there are learning algorithms in the world. What's in your head beat the living daylights of what's available on the computer side and that means there's a lot of great, fun play to figure stuff out.\r\n\r\n**Scott**: Some of it's like tone inflection, things like that, but it's also that you understand the world around you.\r\n\r\n\"I couldn't put the pineapple in the suitcase because it was too small.\" What does \"it\" refer to? Is it that the pineapple was too small or that the suitcase was too small? We can figure that out really quickly.\r\n\r\n_We_ know, but the machine is kind of like, \"Huh?\"\r\n\r\n**Susan**: Yeah, \"I saw Grandma.\" With a handsaw or a power one?\r\n\r\n**Scott**: Yeah, what was the \"saw\" here? There's a lot of common sense here that humans are able to apply really easily. Another really large dataset is text though.\r\n\r\n## Let's talk about text\r\n\r\n**Scott**: Text is huge.\r\n\r\n**Susan**: It's huge and it's awesome.\r\n\r\n**Scott**: The web exists. There's tons of textual data all over the place and titles and bodies of blogs are just everywhere.\r\n\r\n**Susan**: The great thing about text is you can learn so much in so many different fields. It's not just \"Oh, we're going to only apply this to translation or only to transcription.\" You can use text to start learning about some subject matter and help your model in interesting ways with that.\r\n\r\nText is a great secondary source to so many problems out there if you can figure out how to work it in just because there's so much and it can be compactly represented comparatively to things.\r\n\r\n**Scott**: Text is the biggest, richest data source that people have right now just because the internet exists and everything is kind of in text. Video's kind of growing, audio's growing, images are going, but text is massive.\r\n\r\n**Susan**: Yeah you know, \"a picture's worth a thousand words.\" A thousand words may be worth more than single picture in all honesty in how much meaning it conveys. There's a lot of great stuff in text, huge things. The synthetic world, just as a big, broad category, synthetic versus truly labeled, supervised data versus unsupervised.\r\n\r\n## How do you label data?\r\n\r\n**Scott**: Humans do it! You have a bunch of humans that look at images and say, \"That is a cat\" or they circle the person and say, \"That's a person\" or they listen to audio and they type out what was said. It's very, very tough work.\r\n\r\n**Susan**: You know,\r\n\r\n\"There's a lot of people in companies that think, \"AI can do that\" and maybe you can build the model, maybe you can buy the GPUs, maybe you can set up the servers, maybe you can dedicate some people, but just go out and find the data and either pay the millions of dollars to get it or spend the tens of thousands of man hours.\"\r\n\r\n**Scott**: Right now there isn't just a, \"Here's a ten million dollar check\" and get a dataset labeled. It's more like, you're going to be waiting two years or a year or half a year, and you're going to be orchestrating the effort. You have to do it.\r\n\r\n**Susan**: Just think about the data it takes to go out and build a map in the world, physically getting people out there. That data will be useful not only today and for the problem you've got, but probably ten, twenty, fifty years from now.\r\n";
						}
						async function compiledContent$4b() {
							return load$4b().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$4b() {
							return (await import('./chunks/index.e3d6f8e8.mjs'));
						}
						function Content$4b(...args) {
							return load$4b().then((m) => m.default(...args));
						}
						Content$4b.isAstroComponentFactory = true;
						function getHeadings$4b() {
							return load$4b().then((m) => m.metadata.headings);
						}
						function getHeaders$4b() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$4b().then((m) => m.metadata.headings);
						}

const __vite_glob_0_9 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$4b,
  file: file$4b,
  url: url$4b,
  rawContent: rawContent$4b,
  compiledContent: compiledContent$4b,
  default: load$4b,
  Content: Content$4b,
  getHeadings: getHeadings$4b,
  getHeaders: getHeaders$4b
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$4a = {"title":"How Do You Use a Neural Network in Your Business?—AI Show","description":"What's a neural network, and how can it be used in your business? Find out here!","date":"2018-11-20T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981332/blog/ai-show-how-do-you-use-a-neural-network-in-your-business/how-use-neural-network-in-business-blog-thumb%402x.jpg","authors":["morris-gevirtz"],"category":"ai-and-engineering","tags":["deep-learning"],"seo":{"title":"How Do You Use a Neural Network in Your Business?—AI Show","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981332/blog/ai-show-how-do-you-use-a-neural-network-in-your-business/how-use-neural-network-in-business-blog-thumb%402x.jpg"},"shorturls":{"share":"https://dpgr.am/87430c3","twitter":"https://dpgr.am/a4ed9b7","linkedin":"https://dpgr.am/1304f26","reddit":"https://dpgr.am/dbc998d","facebook":"https://dpgr.am/69b5994"}};
						const file$4a = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/ai-show-how-do-you-use-a-neural-network-in-your-business/index.md";
						const url$4a = undefined;
						function rawContent$4a() {
							return "\r\n<iframe src=\"https://www.youtube.com/embed/L3qudM5xCv0\" width=\"600\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\n**Scott:** Welcome to the AI Show. Today we're asking the question: How do you use a neural network in your business?\n\n**Susan:** Well let's just talk about what people think of neural networks, simple ones. There's sort of the classic one, the very first thing you ever build when you're learning how to deal with this stuff, the MNIST digit predictory. You're familiar with this?\n\n**Scott:** Yes, MNIST.\n\n**Susan:** It's like Modified National Institute of Standards and Technology.\n\n**Scott:** Handwritten digits. 28 by 28 pixels. Gray scale.\n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976776/blog/ai-show-how-do-you-use-a-neural-network-in-your-business/MnistExamples.png)\n\n**Susan:** Gray scale, basically they're handed to you on a silver platter and centered. Absolutely useless without a massive ecosystem around it to feed those digits into you in the perfect way.\n\n**Scott:** What do you mean?\n\n**Susan:** Well, you don't just take a picture of a letter and suddenly everything works.\n\n**Scott:** Not that simple, right?\n\n**Susan:** It's definitely not that simple.\r\n\r\n*   How do you take something like a task of digit recognition?\r\n*   How do you break it down?\r\n*   How can you use deep learning to actually make an effective, useful model?\r\n*   How do you create a tool that you can use in some meaningful way?\r\n\r\n## Breaking down tasks\r\n\r\n**Scott:** In the real world, you have a task and you want to do something with a neural network. In this case it's like, I have a camera and I want to take a picture of something and I want it to figure out what is written on some letter. You have handwritten digits, just the digits - zero, one, two, three, four, five, six, seven, eight, nine - and tell me what the digits are. Simple task, right? A human can tell you right off the bat, they can just read them right off.\n\n**Susan:** This is sort of the difference between accuracy in the machine learning world and utility. You can have the most accurate classifier in the world but it's completely useless because you can't feed it that data in the real world.\r\n\r\n**Scott:** You want to send letters to the right place at the post office or something and you want it to be mechanized. But, people have handwritten everything, so hey, a hard problem, you used to do it with humans, now you want to do it with a machine.\n\n**Susan:** That's the core idea for today, how do we actually create a machine learning model or use neural networks in a real world situation there? We've got a great example there, digital recognition on a letter or something like that-however in the news they're talking about [license plate scanners](https://en.wikipedia.org/wiki/Automatic_number-plate_recognition). What would it take to actually build something like that? How do you actually take an idea and use deep learning in there? What are your big ideas of what you should be thinking about?\r\n\r\n## Think: What's my data set?\r\n\r\n**Scott:** You have to think: \"What's my data set?\" That data set has to be at least kind of close to the task that you're trying to accomplish. Is it pictures of handwritten digits that are centered and perfect or is it pictures of license plates on cars that are driving down roads at oblique angles with lots of light on them or smoke? If you have those pictures fine, but do you have them labeled by a human and are they properly labeled? Are they centered or not, are they all blown out and all white, are they too dark, do they have a big glare in them, et cetera?\n\n**Susan:** Just that first step we've talked about data so many times.\n\n**Scott:** Very important.\n\n**Susan:** It's important not only just to have data, but data that represents the production environment that you're going to be in. It's all well and good to have, say for instance, license plate data. But, if it's not taken in a meaningful way, if it's staged with professional cameras, is that going to be as good of a dataset as actually taking footage from the real world and from the equipment that I expect to use and dealing with it that way? Not the version that it is pristine, but the version that's already gone through whatever Kodak's have had their hands on.\n\n**Scott:** The kind that has been compressed.\n\n**Susan:** It's been compressed, it's been mangled. By the way, why is it whenever you see video like that it's just absolutely the worst?\n\n**Scott:** It's always crappy, like it's been shot by a potato.\n\n**Susan:** It's like Big Foot. Magically, whenever Big Foot shows up, it's on the worst video equipment ever, but you got to think you're about to take pictures of Big Foot and you need to recognize you're going to have that quality.\n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976777/blog/ai-show-how-do-you-use-a-neural-network-in-your-business/bigfoot-roger-patterson-1_h-1.jpg)\n\n**Scott:** But, you also have to be careful. Don't try to boil the ocean. You don't have to get every single angle. Is it snowing, is it raining, is it whatever? Okay, get verticalized, get one thing working pretty well first and then you'll start to see the problems crop up. But, maybe 80% or 90% of your solution is already there and then you tackle those problems later. You don't have to do everything all at once.\n\n**Susan:** Yeah. That's also another key thing here is be prepared for: iterations on everything.\n\n**Scott:** Iterate.\n\n**Susan:** Iterate.\n\n**Scott:** ... iterate, iterate, iterate.\n\n**Susan:** Get your first hour of data just so you understand formats and how you might be processing and dealing with it before you spend tens of thousands of hours and umpteen millions dollars collecting data that you then find out is not quite right. That's really heartbreaking.\n\n**Scott:** Yeah, you can't just guess the answer from the outset, it's too hard.\n\n**Susan:** It's [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent), right? You assess, take a step, assess, take a step, assess, take a step, it's pretty classic.\r\n\r\n## What is gradient descent?\r\n\r\n**Scott:** What do you mean by gradient descent?\n\n**Susan:** It's the basic algorithm that a huge chunk of machine learning uses to train neural networks. Just like I said, I was giving the example there for assessing, taking a step, assessing, taking a step. The assessment stage is using what's called the gradient and that points in a direction that might be a good way to go for your [weights](https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html).\n\n![Alt](https://cdn-images-1.medium.com/max/1600/1*f9a162GhpMbiTVTAua_lLQ.png)\n\n**Scott:** As an example, if you're walking around in a hilly terrain and it's your job to find water you might want to start walking downhill. What do you do? You look around at your feet and you think, \"Oh, well the hill is sloping that way, I should go that way.\" Does that necessarily mean that water's going to be that way? Not necessarily. If you keep going downhill, if there's water around, it's going to be around there. That gets into the discussion of maybe you're in a local optimum, meaning a local minimum in this case and you might need to go over the next ridge and then find an even lower hole somewhere. Still, this is gradient descent. You're looking at the slope and you're moving along that path.\n\n**Susan:** Gradient descent then applies to machine learning, but it also applies to life. It's a great process/technique that really works well. We were talking about gathering data and the idea behind it is: don't jump in whole hog right away.\r\n\r\nWhen you're designing a system that's going to be useful, you're really actually thinking: how am I going to use this thing. You've got to think about the data you're going to feed it and the data it needs to be fed in order to predicate some answer that you can then use to do something with.\r\n\r\nJust, very briefly, think about the MNIST style digit classifier there, data inputs are a 28 by 28 gray scale, centered.\n\n**Scott:** The pixels are white and black, and gray.\n\n**Susan:** How do you build that? You've got a whole ecosystem surrounding that, which it's kind to find where the digits are at, it's got to parse them out and do all these different things. When you're thinking about a production environment: I've got these cameras, how am I going to get to the classifier itself or to the network itself, the data into the shape and form that it was trained on in order to make the prediction that I'm going to then use back out there. If you find that the task of doing that is a lot harder than the model itself, you're probably right. The real world is not well normalized.\n\n**Scott:** If you get your data set right, you get your tools right, you pick your model architecture correct, you get your input and output set, correctly the training's actually pretty easy, you just say, \"Go\".\n\n**Susan** Encapsulate the problem, that's really what we're talking about here.\n\n**Scott:** Define the problem well.\n\n**Susan:** You need to define that problem. Going back to the iterative idea here, you'll find that you started collecting some data and then you started designing inputs, and outputs, and a model behind it and you realize maybe those inputs, and outputs, and that model can't work with that data so you need to adjust.\r\n\r\nYou go through this iterative system, but you always have to have an eye on the idea: \"I can't do anything that the real world doesn't support.\"\r\n\r\nThat's what a lot of people lose sight of when they're learning how to use these tools the first time.\n\n**Scott:** It has to work for real in a real setting.\n\n**Susan:** Yeah, they're given these pristine data sets that have well encapsulated some simple problem, or even a complex problem. I've personally spent two weeks working on one dataset just to whip it into shape to be usable.\n\n**It's a really hard task to get the real world to be bent and shaped into something that's usable by your particular model.** Keep that in mind when you're thinking about a usable model.\r\n\r\n## Designing a simple system\r\n\r\n**Scott:** Let's go from beginning to end for a simple system. You have a dash cam in your car and you want it to detect license plate numbers and display them on a display in your car. So, you're like a police officer or something, right?\n\n**Susan:** Officer Susan.\n\n**Scott:** Officer Susan reporting for duty. You're driving around in your car, you have a dash cam and you want to get a text or display on your screen. Using all the license plate numbers around you, how do you build that system? Okay, you have the camera, then what?\n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976778/blog/ai-show-how-do-you-use-a-neural-network-in-your-business/car-collage.jpg)\n\nThe camera what is it doing? It's looking at an optical signal in the world. As a lens it's taking in light and it's digitizing it, so that's really important. You have to be able to digitize the thing that you're actually trying to measure. It's pretty hard to measure people's thoughts.\n\n**Susan:** Like you said, you've got to digitize that, you got to be able to put it in some sort of portable processing system if you're doing this real time.\n\n**Scott:** So maybe it's hooked up to a USB. That dash cam is hooked up to a USB cable that goes to a computer and that computer is just saving an image, 30 of them every second, just saving all of them and just building up tons of data.\n\n**Susan:** Relevant to the the question about inputs and outputs, we'll just take a base one here. Are you going to try to figure out something over time or you treat each image individually?\n\n**Scott:** You have 100 pictures of the same license plate. Do you want 100 notifications of that license plate or just 1?\n\n**Susan:** The image classification world has gone light years, just massive leaps forward since the original work on MNIST and what everybody's familiar with, making a simple multi-layer network to recognize digits. In general, you're going to have to find some way of taking that image that you've digitized which you've been able to feed into some engineering solution that takes a picture n seconds or as fast as it can be processed and then looks for the next one. It takes all that, feeds it in to something that's going to probably normalize the image for light and do some techniques for basic image processing to take care of a whole lot of stuff.\n\n**Scott:** Try to make it not too dark, not too light.\n\n**Susan:** The more you can normalize your data, the less your neural network is going to have to work, which is a great thing because the accuracy is going to go up there.\r\n\r\n**Scott:** Sure, but you have a camera and it's got a pretty big view, and the license plate could be anywhere inside.\n\n**Susan:** You probably have to go into something that's going to detect where license plates are at.\n\n**Scott:** You probably have two systems.\n\n**Susan:** At least.\n\n**Scott:** One that's a license plate detector. It just says, \"I think a license plate is here\" but that's looking at the entire image. It's looking for the whole thing and then saying, \"Oh, I think a license plate is here\". Then you have another one that says, \"I'm going to snip out only that section and then I'm going to try and read the digits\".\n\n**Susan:** It's going to scale it next. It's going to snip out, scale it. You're going to make certain assumptions, because you know what license plates look like, about how to scale it. It's actually probably a nice problem because of that.\n\n**Scott:** A fun problem.\n\n**Susan:** Then finally, you can send it off to your classifier after you've scaled, and sliced, and diced. Now you've got something that might be able to output possible answers that you then display to the person driving. Hopefully they're not texting while they're doing it.\n\n**Scott:** To build the data set for that, if you're starting out it's like, I want to build a license plate reader for that dash cam but I have no data. What do you start doing, strapping cameras to the front of cars and driving around, right? Then you send it off to crowd source the data labeling or you do it yourself and you sit down and look at images, and you draw a box around the license plates. There's the box around the license plates and you use those boxes, the pixel numbers for those boxes, to say in here, \"There was a license plate.\" That's to get the data to build your first model. That just tells you where the license plate is. Once you've gone through and made all those boxes, now those are just images that are for your next data set. Then you go in and say, \"Can I read these or not,\" or \"Can a person read them or not?\" Then, type in what that license plate is, the numbers or the letters. Now you actually have a labeled data set at that point and that's how you train the models that we're talking about. Identify where the license plate is, then also what is it, what are the numbers.\n\n**Susan:** Keep in mind, this is all a very simplified version of this problem.\n\n**Scott:** We don't have to make it more complicated though. This is a simplified version and it's already really complicated.\r\n\r\n## The real world has kinks and curves\r\n\r\n**Susan:** This is a real world use case. The real world is going to throw all sorts of kinks and curves at you. For instance, having multiple cars. You start detecting multiple license plates. What happens when a motorcycle splits lanes right next to you? What are you going to look at there? Those kinds of things, shadows hitting you, those people that put the shields over their license plates to make him hard to see, which, I don't know the legality of that.\n\n**Scott:** A typical system that would identify either where a license plate is or what the numbers are. That would be just a typical [CNN network](https://en.wikipedia.org/wiki/Convolutional_neural_network), a convolutional neural network. These work really well, but those things have been done to death. Many academic papers written about them. You can figure out how deep should it be, how wide should it be, which kernels should I use, all these different settings. You just go download one like Pytorch, TensorFlow, and there it is for you. Now, it might not be trained for exactly what your task is, but you don't have to pick the model architecture and you don't have to go through that whole design process to figure out what's going to work or not. You can pretty much take it off the shelf and hit train, and maybe adjust a few parameters. But, you spent an hour, five hours on that section, maybe a day, and then you spent two months on the other stuff.\n\n**Susan:** That's a great point because there's a lot off the shelf stuff that didn't exist before, especially in the image recognition world. If you're playing in that world, I don't get a good chance to go back there too often, but every time I look there's just more and more amazing tools, especially when it comes to anything on the road. For obvious reasons due to the [autonomous driving revolution](https://www.iotforall.com/iot-and-autonomous-vehicles/) that's happening. Those tools are just getting a tremendous amount of attention and there's a lot of great work that's out there. If you're thinking about building some of these things look for off-the-shelf solutions first.\n\n**Scott:** There's won't be an end to end, everything you need to do, but there will be parts of it that you can save a considerable amount of time.\r\n\r\n## Using an off-the-shelf system\r\n\r\n**Susan:** If you go with some off-the-shelf system, that might dictate some hardware that you don't have access to. This model here is using these tools and these tools you either have to delve into them or figure out how to build something that can mimic them in some way, shape, or form. That becomes a real concern, especially for something in the real world where you don't have a lot of processing power available to do this task.\n\n**This comes back to the difference between accuracy and usability. If you have to have a rack of servers sitting in a car to be able to do the task, that's probably not usable, even if it's accurate.**\n\n**Scott:** Maybe a first proof of concept, but this isn't going to be a real product that you ship.\n\n**Susan:** Driving around with all the fans whirring behind you.\n\n**Scott:** With $100,000 worth of computers in the back of your car.\n\n**Susan:** It's great, I can read those license plates now, although probably don't need that much compute for that task.\n\n**Scott:** We talked about data, super important, we talked about inputs and outputs, loss function.\r\n\r\n## How do you define what your error is?\r\n\r\n**Susan:** This is really determining more the type of problem you're doing. When we think about a loss function, what we're talking about is the thing that takes truth versus prediction and says how close are they.\n\n**Scott:** What's my error? How do I define what my error is?\n\n**Susan:** That loss function has to be crafted in such a way that it can work with auto-differentiation, this ability to what we call [backpropagate](https://en.wikipedia.org/wiki/Backpropagation) the error all the way through the model if you're talking about deep neural networks.\n\n**Scott:** What does that do though, this backpropagation?\n\n**Susan:** When we talk about a model and model structure the structure is the physical way that the math is laid out. In other words, this equation leads to that equation, which leads to that equation. This is the layers. But, these layers, those equations, have a whole bunch of parameters. It's the simple slope formula y=mx+b.\n\n**Scott:** Just numbers.\n\n**Susan:** They're just numbers. If you can get those M and Bs just right then you can fit the curve.\n\n**Scott:** And there's just millions of them though.\n\n**Susan:** There's millions and millions and millions of them. Those things we call parameters.\n\n**Scott:** So, all these dials in the network they need to be turned. ![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976779/blog/ai-show-how-do-you-use-a-neural-network-in-your-business/knobs-1.jpg)\n\n**Susan:** That's actually one of my favorite images is a person sitting in front of a switch board with 10,000 knobs between 0 and 11\\. Every single one of those knobs affects every other knob. You've got inputs over here and you've got outputs over there. If you could just twist those knobs right here to the right step-\n\n**Scott:** There is a correct one that minimizes your error.\n\n**Susan:** There is a great setting of them, but finding that out is hard. So what do you do? This is where back propagation, gradient descent and all these things come into play. You send something through that model and you let it make a guess.\n\n**Scott:** Leave the settings where they are and let it just go.\n\n**Susan:** You look at the outputs that came in there, and you look at the truth, and you have your loss function.\n\n**Scott:** You know the answer to the input that you gave it.\r\n\r\n## How far away is the output from the model compared to what the actual truth is?\r\n\r\n**Susan:** You've got your loss function that's going to show you that. Now, from that loss function I can take that error and I can propagate it backwards through that network.\n\n**Scott:** Essentially there's a recipe that says if we have this error down here then what you need to do is go back and turn all of these knobs this much, but it's only a little bit, each of them. It doesn't say, \"Put this knob in this position.\" It says, \"Move this one a little bit that way, move that one a little bit that way.\"\n\n**Susan:** In every single example that goes through that it's going to say, \"Hey, the knob should have been here\" and \"The knob's going to be there.\" When you've got a bunch of these examples, you take the average of a bunch of examples at once, this is what we call a batch, and now the average says, \"In general, this knob should have gone over here.\" You do this a whole lot and eventually you get the settings for those knobs.\n\n**Scott:** You don't do this once or 10 times, you do this millions of times.\n\n**Susan:** In the end, it comes up with a great setting for those knobs and now the outputs are getting you pretty close to what you want.\n\n**Scott:** At first there's a lot of movement, the knobs are moving all over the place and then there's slow refinement as the model starts to get trained.\n\n**Susan:** There is the occasional time where it trips over and a whole bunch of them start going off into their local optimum.\n\n**Scott:** Yeah because they all affect each one of them. It has to make up for that change. That's generally backpropagation.\r\n\r\nOne of the key skills is coming up with 1001 ways of thinking of that. The more ways you start thinking about how this works, the better you understand intuitively what's going on. That can help you design these things in the future.\r\n\r\n**Scott:** Constraints help a lot with this:\r\n\r\n*   How much money do you have?\r\n*   What computing resources do you have?\r\n*   What talent do you have?\r\n\r\nYou can go on many, many goose chases here, a lot of rabbit holes. You could spend the next 15 years working on a problem and never come up with something that's actually valuable. There's still many good things that you're learning along the way.\r\n\r\nYou have to learn to cut off certain things and be like, \"Good enough, good enough, good enough\". That's kind of the way that machine learning is now at least. You have to have some restraint in order to get a real product out the door.\r\n\r\n**Susan:** We've talked a bit about designing something, but I think a lot of what people don't realize is that not only is building of them a challenge, but the world isn't static. Maintaining a deep neural network is actually a really big challenge.\r\n\r\n## Maintaining a deep neural network\r\n\r\n**Susan:** Even just consider the license plate problem: every single year there's 100s of new license plates. Someone goes right to their state representatives and says, \"Hey, I think the state should have this picture of my favorite cartoon character from 1983\" and they get enough signatures and suddenly there's a brand new license plate in the world. Car designs change, vehicle designs change, all sorts of things change.\n\n**Scott:** In California, the first digit kind of just incrementally goes up. There's a new first digit just because it's later on. It wasn't likely before, but now it's likely. The idea that you put all this time and effort and it stops, maybe there are problems out there like that, but it's pretty hard to imagine. We'll just go back to handwriting, the digit recognition. I can guarantee you that the average penmanship has changed considerably in the last 15-20 years. So, if you think that handwriting is stagnant, you're not banking on a good bet.\r\n\r\nYou need to have some way of keeping your model, keeping your environment up-to-date, and swapping it out, and keeping it trained, and topped off.\r\n\r\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976779/blog/ai-show-how-do-you-use-a-neural-network-in-your-business/Early_morning_cobwebs_-2859589264-.jpg)\n\n_Keep the cobwebs off your model, keep your environment up-to-date._\n\n**Scott:** Also presumably, you've got a model in production and now what do you do with your time? You try to make it better. Hey, I have some ideas, maybe I could do this or maybe I could do that and that will make it better. Great, but the other stuff is still operating over there. What if you make a small improvement? Okay, now there's this version change, history, and now you get into [version control of your models](https://www.lpalmieri.com/posts/2018-09-14-machine-learning-version-control-is-all-you-need/) which is not really a thing yet. People don't quite think in that way, but they certainly will have to think in that way in the future.\n\n**Susan:** Well that's a really huge challenge. There's some decent articles and blog posts on this fairly recently, talking about version control and the machine learning world. That's just a big challenge. A lot of times you're talking about not just a few bytes of data here but 100s of megs, which doesn't sound like a lot but you version that 15 times a day and suddenly you're talking real data.\n\n**Scott:** A gig or so. You're filling up your hard drive pretty fast.\n\n**Susan:** The reproducibility of these things is a slight challenge because you can take the same exact model, structure, the same training data, the exact same training pipeline, and most of them incorporate randomness into them for very good reasons, [so you'll get a different result if you train a model twice](https://blog.dominodatalab.com/machine-learning-reproducibility-crisis/) with what you thought was all the same stuff. What does it really mean to version control something is a big challenge.\n\n**Scott:** The question will have to be answered in the next few years probably.\n\n**Susan:** Yeah, we're seeing the evolution of the industry.\n\n**Scott:** This is a long timescale thing. Hey, we're at the beginning. It's like electricity in the 1900s or something, that's AI now.\r\n\r\n## What's the key point?\r\n\r\n**Susan:** The key point is you got to take real world data somehow. Don't get stuck in some training world.\n\n**Scott:** You got to use real world data.\n\n**Susan:** Real world data. If you can't take real world data, you don't have a useful model, useful structure. How about yourself?\n\n**Scott:** Scott's key point is: try to get water through the pipes. Just get something working, anything working, and then you can iterate.\n\n**Susan:** Iterate.\n\n**Scott:** Iterate.\n\n**Susan:** Iterate.\n\n**Scott:** Then you can iterate. Hey, maybe it doesn't work very well at first, that happens for sure, but then you tweak some things, now it starts to work again. Is it all worth it to go through that?\n\n**Susan:** Depends on the problem, depends on the money.\n\n**Scott:** Some problems are too hard, just don't do it right now. Some problems are really easily solved with something you don't need to use a neural network for. But there's a region in between where it's like, yep, this makes a lot of sense.\n\n**Susan:** If you can get 95% of the way with a simple tool, maybe you should be doing that first.\r\n\r\n**Scott:** Might keep doing that, unless it's something you make billions of dollars from then hey maybe we can do something with neural networks.\r\n";
						}
						async function compiledContent$4a() {
							return load$4a().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$4a() {
							return (await import('./chunks/index.c7a3eb62.mjs'));
						}
						function Content$4a(...args) {
							return load$4a().then((m) => m.default(...args));
						}
						Content$4a.isAstroComponentFactory = true;
						function getHeadings$4a() {
							return load$4a().then((m) => m.metadata.headings);
						}
						function getHeaders$4a() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$4a().then((m) => m.metadata.headings);
						}

const __vite_glob_0_10 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$4a,
  file: file$4a,
  url: url$4a,
  rawContent: rawContent$4a,
  compiledContent: compiledContent$4a,
  default: load$4a,
  Content: Content$4a,
  getHeadings: getHeadings$4a,
  getHeaders: getHeaders$4a
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$49 = {"title":"How Will Data Influence the Future of Machine Learning? — AI Show","description":"Curious about what the future of data looks like for machine learning? Check out this episode of the AI Show!","date":"2018-10-26T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981323/blog/ai-show-how-will-data-influence-the-future-of-machine-learning/how-will-data-influence-future-ML%402x.jpg","authors":["natalie-rutgers"],"category":"ai-and-engineering","tags":["machine-learning"],"seo":{"title":"How will data influence the future of machine learning? — AI Show","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981323/blog/ai-show-how-will-data-influence-the-future-of-machine-learning/how-will-data-influence-future-ML%402x.jpg"},"shorturls":{"share":"https://dpgr.am/dcc16aa","twitter":"https://dpgr.am/fccf518","linkedin":"https://dpgr.am/7f19b8b","reddit":"https://dpgr.am/b9410a7","facebook":"https://dpgr.am/2b63b3d"}};
						const file$49 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/ai-show-how-will-data-influence-the-future-of-machine-learning/index.md";
						const url$49 = undefined;
						function rawContent$49() {
							return "<iframe width=\"600\" height=\"315\" src=\"https://www.youtube.com/embed/HqVoulU4uRA\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe>\n\n**Scott:** Welcome to the AI Show. On the AI Show, we talk about all things AI. Today we have the question, our big question today:\n\n## How will data influence the future of machine learning?\n\n**Susan:** You know, it's a pretty key thing, as we've discussed many times before.\n\n**Scott:** Well, you have to have the trifecta, right?\n\n* You've got to have computing power,\n* You've got to have data,\n* You've got to have good models.\n\n**Susan:** You know, I think we're starting to see some trends here.\n\n## How do normal machine learning problems go?\n\n**Scott:** Well, if you have a small amount of data, then you use some simplistic models and things like that. If you have a large amount of data you probably will have to use a lot of computational power. But, also it can shape your model and make it more intricate, more nuanced. That's usually how that goes. But, it isn't necessarily that the bigger the data, the better your model is going to be, right?\n\n**Susan:** No, definitely not. Data plays a key role, and the size of data definitely helps if you've got more, but there's a lot more that goes into it. But, we're starting to see a lot of these problems evolve along a standard path. We've seen this a couple times now.\n\n**Scott:** How do they evolve?\n\n**Susan:** It seems like there's a couple key points in the life of a machine-learning problem. At least, I've noticed this. Have you noticed this?\n\n**Scott:** I, probably. I don't know. I'm not sure what you're thinking.\n\n**Susan:** Well, here's what I'm thinking, I'll just lay it all out on the table right here. In general, we see a problem emerge, that people finally recognize as a problem. And I've noticed, and a lot of people have noticed, that pretty soon someone publishes a dataset. It becomes the dataset that everybody works their magic against to try to attack this problem the first time. Like, the classic - handwriting digits or image recognition. There's a lot of classic datasets that, once those were published, people could try different algorithms and compare them.\n\n**Scott:** Yep. So if you're drawing \"0, 1, 2, 3, 4, 5, 6, 7, 8, 9,\" just writing it. \"Hey, can you recognize those?\" That type of dataset. Maybe another simple one like, \"What category does this fall into? Is it a human? Is it an airplane? Is it a cat?\"\n\n![](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)\n\n*[The MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database)*\n\n**Susan:** Yeah, we've talked about [CIFAR dataset](https://en.wikipedia.org/wiki/CIFAR-10) and stuff like that. But, those standardized public datasets really help frame the problem. Then following that, you get some tools that will generally come out. We're seeing [Tensorflow](https://www.tensorflow.org/), [PyTorch](https://pytorch.org/), all these standardized tools kind of follow along. But, people start standardizing: [ImageNet](http://www.image-net.org/) and stuff like that. Like, \"Hey, here's eight ways to attack this problem.\"\n\n> \"We see the evolution of this problem go from understanding there's a problem, a classic dataset's released, a bunch of people and academic papers get published, some standardized solutions start coming out, and you see the polish on the solutions start evolving over time.\"\n\n**Scott:** And the progress is rapid. You can look back and be like, \"Wow, how did so much get accomplished?\" But, it also still takes place over the span of decades. Like, [MNIST](https://en.wikipedia.org/wiki/MNIST_database), the hand written digits, was 20 years ago.\n\n**Susan:** Maybe more.\n\n**Scott:** Maybe a little longer. But yeah, CIFAR, probably a similar age, at least close, not quite as old. ImageNet, 10 years roughly, et cetera.\n\n**Susan:** It's pretty cool to see that we've had enough of these problems that we can see those arcs going through them. From that we can see new problems and gauge where they're at. It's like charting a star: \"This is how old it is based on how it looks.\"\n\n**Scott:** That's a good point. That's sort of based on\n\n* How hard is the problem?\n* How easy is it to get the data?\n* How scintillating is the data?\n* Is it something that you can find for free easily on the Internet, nicely labeled?\n\nImages are like that a lot. It's easy to find a lot of images that are labeled. Not super easy, but you can search for a \"tool\" and you'll find tons of pictures of tools. Okay, that's pretty easy. But, there isn't an easy way for speech recognition to say the word, \"tool.\" \"Give me all the examples of everybody saying that word.\" That's a harder problem.\n\n![Alt](https://1.bp.blogspot.com/-2WefVFMGytE/VL_k4Wh-R-I/AAAAAAAAGFE/DNkTHbE4Bx4/s1600/Tool%2BBelt%2BLabeled.jpg)\n\n**Susan:** Well, by the time that becomes easy it probably means that the problem has been\n\n**Scott:** Problem has been solved!\n\n**Susan:** Which is the other cool thing. But it does bring up, as these problems have gotten tougher, they're still kind of following that arc.\n\n**Scott:** Well, it's not trivial. The reason that you could search for a tool and get pictures of a tool is because people would label them.\n\n**Susan:** Yes.\n\n**Scott:** Here's the caption: \"A picture of a tool.\"\n\n\"Oh, okay. So that probably means that's what the image is.\" But, that's not how it goes in audio. If you just recorded your meetings, you're not going to sit down and label all the moments.\n\n**Susan:** No.\n\n**Scott:** Not generally. Usually not.\n\n**Susan:** There are more and more academic sources that are helping to do that. And, the problem is growing over time. But yeah, that is just hard.\n\nTo the viewers at home, if you want to appreciate how hard speech is, just say something in your own voice for five minutes and record it. Then transcribe what you said and see how long it takes.\n\n**Scott:** Sit down, try to get it exactly. You already know what you're talking about. You already know all the vocabulary words.\n\n**Susan:** You're the one who said it!\n\n**Scott:** Yeah! You know your voice, right? And then you're like, \"Wow, this takes a lot longer than I would expect.\"\n\n### How datasets come to be\n\n**Scott:**\n\n* Is the data fairly easy to get?\n* Is it pretty freely available?\n* Is it all that hard to label?\n* Is it a problem that's worth solving or interesting to solve? You have to have all of those things, and then the datasets pop up.\n\n**Susan:** There's one more angle here that's been popping up more and more lately. That, in the early datasets, we really just didn't concern ourselves with. That is the privacy angle of the dataset. As these tools are getting better and better, as we're putting more and more attention to them, and as the amount of data grows, even what you might think are trivial datasets become big privacy concerns.\n\nYou thought you were anonymizing your history here, and suddenly now everybody knows what movies you've been watching. Remember [the Netflix prize](https://en.wikipedia.org/wiki/Netflix_Prize)?\n\n**Scott:** Yeah, maybe eight years ago now? It was a machine-learning prize. It was, \"Get a million dollars if you're able to make a recommendation that's better than 90% accurate,\" or something like that. Recommend movies to people. Then, if that matches the taste of something that they would like, that's how you gauge your accuracy. They ran that and a bunch of different academics and companies and whatnot went after that problem.\n\n![Alt](https://cdn.vox-cdn.com/thumbor/afYE-AVV0fNqW4eSdBRWVV24O4I=/0x0:1100x825/1820x1213/filters:focal(0x0:1100x825):format(webp)/cdn.vox-cdn.com/uploads/chorus_image/image/49520055/netflix-prize1.0.jpg) *Winners of the Netflix prize; photo: dannypeled.com*\n\n**Susan:** It kind of did follow the same curve we were talking about earlier:\n\n1. A big dataset was put out there\n2. A bunch of different people, academics, started throwing a lot of different answers to it,\n3. They finally got to an acceptable solution.\n\nNow, from what I understand, the solution that won wasn't what they actually implemented, because it was a fairly complex, heavyweight thing and they wanted a more stripped down version of it. But, it does show that arc, and it also shows how privacy really came into this, because afterwards the security researchers got a hold of this dataset and they started linking real people back to these anonymized movie records.\n\n**Scott:** It was like, can you guess, even with 30% accuracy or 10% accuracy, who this person is? Based on just the movies that they like.\n\n### The challenging link between data and privacy\n\n**Susan:** This is a growing trend in these datasets and it also is a growing challenge. Because, like we said, when you publish those datasets, it helps frame the problem. But, if you're getting challenges publishing it because you've got privacy concerns, that could put the brakes on a lot of problems that might be solved. There's tradeoffs here. I'm not advocating throwing out privacy, let me be very clear about this.\n\n**Scott:** In order to make AI work well, you need data. And, how do you get data? You get it from people. People say things, or people do things, or they take pictures of things, or they write things, or whatever. So laws surround the use of this data and it's been fairly free up until recent times. [GDPR](https://eugdpr.org/) happened in Europe, and so that means that you have to very explicitly give permission to use your data, rather than it defaulting to being able to use the data. The US is still pretty free about this.\n\nPeople are probably going to have to choose, in the future or now, in the next ensuing years, what do we want that to look like? Do I have to give you my data in order to use a very useful service, yes or no? Is there some other protection in there? How does that work? But the way that it'll probably work is that if you say, \"No,\" then you probably are giving up some functionality there, because now it can't learn from you.\n\n**Susan:** So one thing you and I have talked a lot about is a symbiotic evolution of these things. What you see is huge privacy concerns through big data breaches and things like that, you get a swing on the other side, privacy laws start coming in, which shapes the next set of concerns, which shapes the next set of laws.\n\nThis is what you see everywhere and honestly, we're kind of at the beginning of this. We're starting to really see big legal entities come in and move and start doing this, and they think they've got enough information to start building laws and stuff like that. This is the beginning of a process that's going to take decades to shake out. So it's a huge, huge, huge thing that you have to pay attention to when dealing with this.\n\n**Scott:** This has been in the [news recently with Apple, Tim Cook. Apple CEO Tim Cook at a data privacy conference](https://techcrunch.com/2018/10/24/apples-tim-cook-makes-blistering-attack-on-the-data-industrial-complex/) giving a keynote there and sort of lampooning a lot of the other tech companies, saying, \"Hey, you're stepping on people's data rights.\"\n\n> We believe that privacy is a fundamental human right. No matter what country you live in, that right should be protected in keeping with four essential principles:\n>\n> * Tim Cook (@tim_cook) [October 24, 2018](https://twitter.com/tim_cook/status/1055035539915718656?ref_src=twsrc%5Etfw)\n\n**Susan:** Yeah, it's interesting to see a very large company, especially one with access to so much personal data.\n\n**Scott:** With tons of data. With machine learning groups.\n\n**Susan:** And moving into huge fields like personal health monitoring and stuff like that which they do a huge, huge amount of that.\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/wFTmQ27S7OQ?start=1206\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\"></iframe>\n\n**Scott:** They've always planted a flag in the ground and said, \"We are security conscious. We are data privacy conscious, et cetera,\" Apple has been.\n\nFamously saying, \"Hey, we're not going to give you the keys to unlock somebody who's been arrested's iPhone.\" Things like that.\n\n**Susan:** I think they just shut down a major tool, with the last iOS 12 and above upgrade. So, they've been doing this but (this is the jaded side of me) I'm going to have to ...\n\n**Scott:** You don't really believe it?\n\n**Susan:** Well, I mean, it's kind of hard to see big companies saying that they're altruistic. I'm not saying Apple's evil or anything like that. I'm just saying you always have to take a grain of salt with a big company movement. That's a well thought out movement and they have, I'm sure, some under the hood ideas that they're not telling everybody.\n\n**Scott:** And there's a competitive landscape that you're in as well. Hey, you're a company, other people are getting data, and maybe you are taking a stance so that you don't. Okay, now you have a competitive disadvantage.\n\nMaybe this is why he's saying those things, so that competitors like Google, Facebook, et cetera, have more heat put on them from the government.\n\n**Susan:** Also, [if you come out early as an advocate, knowing that it's going to go down the route of tighter and stricter laws, maybe you have more influence](https://www.cnet.com/news/us-privacy-law-is-on-the-horizon-heres-how-tech-companies-want-to-shape-it/). Maybe you can shape those laws more in your favor. Apple definitely would want to shape those laws in their favor, any big company is going to say, \"Hey, make it work for me, as best as I can.\" So you come out waving the flag of privacy first, and you get a bit more of a voice and that's inevitable.\n\n**Scott:** Sure.\n You want to build a company, or you want to build products, that people aren't going to hate, they're going to like, they're going to want to keep using, and they provide value. You just have to find that balance. Apple will have to find that balance. Every company will have to find that balance. With data and the models that they build.\n\n**Susan:** It's true. But it is interesting, again, going back to the general theme here, seeing the arc of these problems, and seeing that simple arc is getting more and more complex by the day. But that complexity is also growing.\n\n**Scott:** Understandably.\n\n**Susan:** Understandably. And the complexity is growing with the complexity of the problems we're tackling. When we first wanted to figure out, is this a 1 or a 7? Is this a horse or a pig? When they first came out, they were incredibly hard problems to solve. Now it's like, \"Yeah, of course that's easy.\" We're tackling really deep, hard problems now.\n\nAnother [great dataset that just came out, Twitter](https://boingboing.net/2018/10/17/twitter-publishes-tweet-archiv.html).\n\n**Scott:** Text datasets, Twitter.\n\n**Susan:** That's a big one. It hits all of these fronts. This is talking about really complex social issues and really complex technological issues all wrapped up in a big brand-new dataset. We'll see if it becomes adopted as a cornerstone for figuring out this problem - the problem of basically weaponized social media. How do you fight that?\n\nMaybe we have a first dataset on there. But there's a lot of concerns with that.\n\n### What data policies do we need? What exists?\n\n**Scott:** So what do you think our people should do from a policy perspective? What exists? What should exist in the future in order to enable an ideal landscape for AI to flourish but privacy to still be a thing?\n\n**Susan:** What's the balance? Man, this is so early. Like, throwing a dart about what the right balance is, is really, really, hard.\n\n**Scott:** It's like 1900 trying to talk about electricity.\n\n**Susan:** Exactly. I mean, I think first of all, without talking about restricting what is and isn't there. Susan's personal take is, openness is the number one thing.\n Being open about what data's being collected and what it's being used for. I'm not saying restrictions anything like that, but that will help. That'll help frame the conversation, and help educate consumers and individuals and companies. That way we can go into an informed future and make more informed decisions. I can guarantee you that whatever is being hidden right now, eventually will come out. That's the nature of the digital age. If you're more open right about now, it'll go a lot easier when the harder privacy laws inevitably start hitting.\n\n**Scott:** If you're asking me to make a prediction, \"Hey, how are people going to feel about this in the future or what's going to happen?\" I think if you just look back into the past - year 2000, Internet hits the world in a big way and everybody's afraid of it. They're like, \"I don't know if I'm going to go on there. Is this thing watching me? I have a webcam. Oh, no, it can see my entire life. Should I put my credit card in here? I'm not going to trust anything. How could I get anything through the mail, through eBay and trust that?\"\n\nThere's a lot of things that have to be figured out. But, they get figured out. So, a similar thing with AI. A lot of things have to be figured out. Do you have to fear everything? No. Are people going to fear everything at some point in time? Yes. Are they going to be resolved? For the most part, yeah.\n\n**Susan:**\n Fear is not a good way to approach the future, no matter the problem.It should be a motivator to understand, but not to stop you from going into the future. Because no matter what, there's only really one guarantee, the future is going to happen. So you can either be part of shaping it, or you can hide in the corner.\n\n**Scott:** In the back and watch what happens.\n\nBut to answer our big question, how is data going to influence the future of machine learning? Okay, so we've got to worry about privacy, you've got the different types of machine learning. We talked a bit about text, audio, images, other stuff.\n\n### What's the future going to hold?\n\n**Susan:** I think that we're going to see more and more of these big data dumps trying be the cornerstone of solving a problem.\n\n**Scott:** Like a jumping off point?\n\n**Susan:** Yeah, a jumping off point. Especially, honestly, from bigger companies, because now, just the fact that Twitter released it. I'm already calling it The Twitter Dataset! Their name is out there and they're looking like they're doing something about this problem. So it's good PR, although it's really challenging and it can be a disaster if you do it wrong. But, we're going to see more of those cornerstone datasets come out there, help define these problems with data.\n\nData defining the problems. That's a big piece of what's going to shape what the machine learning world looks like five, ten years down the road.\n\n**Scott:**\n This is a really interesting part of AI, in that the data that is collected, labeled, used to train models, has transferred very heavily from being small academic datasets to very large datasets that are captured by companies and used to build models. So this is why you see talent moving from academia to big companies. That's not really going to end because that's where all the data is. The big companies have the big data and in order to do AI well, you need big data.\n\n**Susan:** And, people that want to solve problems want to solve problems where the problems are interesting. Early on it's interesting in the public sector and later on it becomes interesting in the academic world. It gets its seeds in academia, flourishes in public, and then goes back to academia.\n\n**Scott:** Yeah, I think so. Because, a lot of the baseline problems will be solved, people will be tired of it, it'll be very common place: \"Yeah, yeah, yeah, the AI system, and whatever. Yeah we got our data flywheel and we're collecting what we're doing, and doing all the things.\" But, what's the new good stuff, general AI, or whatever, where's that going to be seeded? It may be in the companies, but probably more like there'll be some kind of data partnership with academic institutions or something like that.\n\n**Susan:** You know, an interesting kind of area that data, this is pure speculation here, but where data could really start playing a different role is government-supplied datasets that you must conform to if you're going to release something.\n\n**Scott:** Oh, yeah, like it has to fit these rules.\n\n**Susan:** Yeah. Or, \"You must have learned from this dataset, and we're going to test you against your ability to deal with these datasets.\"\n\n**Scott:** Kind of an adversarial, like \"If you can't deal with this, then you're not a good enough model?\"\n\n**Susan:** Well, think about judging. We'll make up a hypothetical doctor program. They have their test set of diagnostic cases that you must pass.\n\n**Scott:** Answer yes or no.\n\n**Susan:** Yeah, this becomes how to board-certify an algorithm.\n\n**Scott:** A really good point. Machine learning models in the future will probably be tested a lot like humans are now. There's a curriculum and tests that you should pass and maybe you specialize in certain areas, but if you go work for one company the things that you learn there will probably also be transferred to other companies. Maybe it's not trade secrets or something like that, but sort of underlying ways that humans work. It's what happens for humans now. You go work for one company, fine. You leave, you go work for another company. Did you forget everything that happened for those years that you worked for that one company? No. So you bring those along with you. People will start to think of models that way.\n\n**Susan:** Yeah, it's a really interesting world, just on the data side. Just so much that goes into curating a really good dataset, publicizing it and getting it accepted and all the areas.\n\n### What are the future datasets going to be?\n\n**Scott:** I think it's going to be interesting. I think we are just in the very beginning stages. It's like the first railroad was built across the US or something, now there's going to be 3,000 railroads. It's a similar type of thing. The first telephones were a long time ago, but it took a very long time after that until everybody had a telephone. It's going to work it's way into every part of life and, at least from my perspective, people shouldn't be too afraid of it because it's going to make your life so much easier.\n\nSo as long as the path is taken in a way that isn't crazy, which, companies are pretty non-crazy now. They don't want to scare you off from being a customer. Then it's like, \"Hey, this is going to evolve. It's going to make your life way easier. Things are going to be more efficient. Then, you'll just be very glad.\"\n\nSimilarly to text messaging or Facebook or something like that. Hey, you're giving up all this data and some privacy and things like that, but your life is so much better now that you can connect to your network that is spread across the world, essentially. Right?\n\n**Susan:** Yeah. I mean, as much as we are definitely challenged by the privacy, where it is today, no one is talking about giving up their connectivity. Well, I should say, I definitely know people who say they're going to drop off social media, they're going to drop off and you never hear from them again.\n\n**Scott:** But, are you going to stop using Google just because they used your search terms to help train their models to serve you better search terms? Probably not, right?\n\nI think data obviously plays a big part now, but it's going to play a very big part in the future. You'll get your baselines set over the next few years, but then there'll be all these offshoots that the rest of the world and life just starts to become easier because you get that labeling, flywheel going. \"Hey, here's some data. Hey, we labeled it. Hey we trained a model and then it did this task.\"Then people will become used to it and they go, \"Well, this is awesome. I don't have to do all these menial tasks anymore.\"\n\n**Susan:** It gives you that bootstrap. So one more final interesting aspect of all this is people don't realize it, but the first hour is the hardest hour. The ten thousandth hour is way easier than the first hour. Even if the first hour isn't as targeted as you'd like it to be, just having someone to have kicked out that first hour for you saves you so much work and effort, mentally, because now it gives you a structure- what the ten thousand hours might look like.\n\n**Scott:** It's something to build off of. If you just have an example, you have a template, and it's like, \"Okay, maybe I'll add one hour from my time to make that.\" Now it's a two hour dataset, and then it starts to build a community around it.\n\n**Susan:** Yeah, these things are like little seeds. These little datasets are just seeds.\n\n**Scott:** You've got to form it in the right way and then it grows.";
						}
						async function compiledContent$49() {
							return load$49().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$49() {
							return (await import('./chunks/index.32419930.mjs'));
						}
						function Content$49(...args) {
							return load$49().then((m) => m.default(...args));
						}
						Content$49.isAstroComponentFactory = true;
						function getHeadings$49() {
							return load$49().then((m) => m.metadata.headings);
						}
						function getHeaders$49() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$49().then((m) => m.metadata.headings);
						}

const __vite_glob_0_11 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$49,
  file: file$49,
  url: url$49,
  rawContent: rawContent$49,
  compiledContent: compiledContent$49,
  default: load$49,
  Content: Content$49,
  getHeadings: getHeadings$49,
  getHeaders: getHeaders$49
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$48 = {"title":"What does an AI transformation look like? — AI Show","description":"How is AI transforming the world? Have a listen to this episode of the AI Show and see what we think.","date":"2018-10-12T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981319/blog/ai-show-what-does-an-ai-tranformation-look-like/what-does-ai-transformation-look-like-blog-thumb%402.jpg","authors":["morris-gevirtz"],"category":"ai-and-engineering","tags":["voice-tech"],"seo":{"title":"What does an AI transformation look like? — AI Show","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981319/blog/ai-show-what-does-an-ai-tranformation-look-like/what-does-ai-transformation-look-like-blog-thumb%402.jpg"},"shorturls":{"share":"https://dpgr.am/1e57b96","twitter":"https://dpgr.am/8f8b6c6","linkedin":"https://dpgr.am/8680425","reddit":"https://dpgr.am/0590832","facebook":"https://dpgr.am/971b8a2"}};
						const file$48 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/ai-show-what-does-an-ai-tranformation-look-like/index.md";
						const url$48 = undefined;
						function rawContent$48() {
							return "\n<iframe src=\"https://www.youtube.com/embed/naC8T4FG0Nc\" width=\"600\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\n**Scott:** Welcome to the AI Show. On the AI Show we talk about all things AI. Today we're asking the question, \"What does an AI transformation look like?\"\n\n## What does an AI transformation look like?\n\n**Susan:** So much of this world is being rapidly transformed by AI. There's so many great things to think about.\n\n**Scott:** What's already been transformed?\n\n**Susan:** What hasn't been already transformed. My favorite is spam. Your inbox is finally cleaned out a little bit.\n\n**Scott:** This is late '90s.\n\n**Susan:** This is a long time ago. Before that you couldn't use it and it really enabled us to have that medium. Without that no one would use email, not that people use email anymore. Do you still use email, Scott?\n\n**Scott** Definitely.\n\n**Susan:** I checked it last week, honest.\n\n**Scott:** I don't believe you now.\n\n**Susan:** You don't believe me?\n\n**Scott:** Yeah.\n\n**Susan:** No, there's some great ones. One of my favorites is the up and coming world.\n\n**Scott:** What's that?\n\n## Agriculture is about to be massively transformed.\n\n**Scott:** That's true, agriculture is getting pretty automated. You've got big tractors that are [listening to a GPS](https://www.gps.gov/applications/agriculture/), figuring out where they are, and going and doing some pre-programmed thing. I think AI applies a lot there.\n\n![alt](https://res.cloudinary.com/deepgram/image/upload/v1661976384/blog/ai-show-what-does-an-ai-tranformation-look-like/joao-marcelo-marques-791496-unsplash_V2.jpg)\n\n**Susan:** There is a tremendous amount of this world that is rapidly changing due to AI. It's mainly making those small decisions. A person would learn to make those decisions pretty quickly and come off the cuff and say, \"Okay, we're going to make this, we're going to make that.\" By automating that, you now free up those people to think about the bigger stuff.\n\n**Scott:** It might even be an easy decision. For agriculture, if you see an area over here, the plants are smaller, and over here they're bigger. There's something different about that. Maybe it doesn't have enough water. Maybe it doesn't have enough nutrients. It would be easy for a human to come to that, but they would have to just go around, and look at every square foot, but that's a pretty menial task.\n\n**Susan:** On the note of agriculture, I'm going to make a little confession here. If I could jump to another field other than speech, it would probably be the Ag world and applying [machine learning](https://en.wikipedia.org/wiki/Machine_learning).\n\n**Susan:** Obviously, it's something that every human on planet deals with every single day, the results of agriculture. You need food, but it is just incredibly ripe for a disruptive change from AI.\n\n### What makes agriculture ripe for disruption?\n\n**Scott:** Is this because it's simplistic or why? Take the savings that simple things could do.\n\n**Susan:** A simple thing, for instance, as you mentioned and what's going on right now, monitoring your fields to say \"This area over here needs a little bit more water, or it needs a little bit more fertilizer.\" Being predictive and being proactive about managing a field of whatever resource.\n\n**Susan:** But that's a small thing, and those are fairly easy things. You can detect through all sorts of fairly cheap sensors, water levels and all that.\n\n**Scott:** Like what? What would a cheap sensor be?\n\n**Susan:** Well, relatively cheap sensor, you could do infrared.\n\n**Scott:** From what though? Like a farmer standing with an infrared gun?\n\n**Susan:** No, there's a couple big sources that have been used traditionally. Satellites and light aircraft, but [very lately, a lot of UAV's](https://en.wikipedia.org/wiki/Agricultural_drone) (un-manned aerial vehicles) are starting to take control.\n\n**Scott:** Aerial imagery of some sort, right?\n\n**Susan:** Aerial imagery, and that's a fun one, because not only the products that they return back, can you analyze with [different machine learning techniques](https://blog.deepgram.com/ai-show-different-types-of-machine-learning/), but also the automation of flying around an area is inevitably going to be automated with machine learning. Right now a lot of it, the ones I've been looking at, is done by hand.\n\n**Scott:** People flying planes?\n\n**Susan:** People flying, or UAV's. That type of stuff. It's pretty easy to imagine a world where you got the tractor of the air, your UAV, taking over.\n\n**Scott:** A UAV that takes off from your barn.\n\n![alt](https://res.cloudinary.com/deepgram/image/upload/v1661976385/blog/ai-show-what-does-an-ai-tranformation-look-like/david-henrichs-399195-unsplash_v2.jpg)\n\n**Susan:** Sure, it does a lap every two hours, and comes back to the charging station, and just keeps doing that.\n\n**Scott:** What would it do with that data, though? It looks around, it gets infrared, it gets visual?\n\n**Susan:** You could do a hundred different tasks. Again, the field one, I think everybody gets intuitively, \"Sure, I monitor my crops, and I can do that,\" but again, go to say, cattle ranching. \"Where are my cattle right now?\" That's a pretty big question that you probably want to know. Have they left your property?\n\n**Scott:** \"Do they have enough grass?\"\n\n**Susan:** \"Are they in a good area? Maybe I could rotate them on a different field?\"\n\n**Scott:** \"Have they eaten up too much?\"\n\n**Susan:** \"Has a new calf been born? Are my fences secure?\" All these things are where a combination of this cheap sensor world and machine learning can come into play. You set up something that can fly around your area, it monitors fences, and knows how to look for a fence that is damaged or a gate that is open. It knows how to look for your cattle, and can recognize them and track them for you. And then you get all sorts of much deeper products later on like, being able to track say, the growth rate of a cow over time, as opposed to bringing them back and weighing them, and poking and prodding them for a half a day, which takes a lot of your time, and takes that cow out of the grass, where they're basically turning grass into money.\n\n**Scott:** So you'll be able to personally identify the cow?\n\n**Scott:** Facial recognition, but not facial recognition?\n\n**Susan:** It's sort of like Cow-book. You know, you open up page, and it's got little like, \"Oh-\"\n\n**Scott:** How much does it weigh? What is it into? It likes to go over here or over there.\n\n**Susan:** \"I've been kind of favoring this hoof over here for the last couple days. Maybe you should check this out.\" Or, \"Ever since we've been in that field over there, we've been not as active. Maybe there's something in that field that's not helping us out.\"\n\n**Scott:** Do you think there's anything down the line? Meaning, further along the pipeline? Okay, you have some grain, now what? Like shipping it, or storing it?\n\n## Automate The Simple Stuff\n\n**Susan:** It's pretty sci-fi still, to say completely automated farm...\n\n> \"But the deal is, automate the simple stuff. The stuff that takes those very simple decisions.\" Then, the bigger things-coordinating schedules, coordinating big muscle movements-that kind of stuff now becomes to purview of the farmer, the rancher, the whatever, allows them to think more strategically instead of tactically.\n\n**Susan:** This is a tool that takes you from kind of that course grained, everything must be uniform. \"My goal on this farm is to make it as uniform as possible, so I don't have to think about the differences between here and there. I can do everything all the same to take advantage of that small detail over here. This part of my land is better for this type of crop, and this one's better for that type of crop.\" These tools can alleviate the detailed work that before, you just couldn't have the time to focus mentally, or even physically, on specializing for that.\n\n**Scott:** So instead of trying to make everything the same so that you can get an average yield from it, now you can take advantage of the uniqueness?\n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976386/blog/ai-show-what-does-an-ai-tranformation-look-like/markus-spiske-632419-unsplash_v2.jpg)\n\n**Susan:** Talking about disruptions, that's a very disruptive idea. So you flip the script. Now it's not about the big farm, it's about the small farm, and finding how much you can get the optimal yield out of a smaller area, whereas making a uniform big area actually reduces profits, because you've gotten rid of the potential for an optimal yield.\n\n**Scott:** There are a lot of AI startups sort of cropping up around this area. Why do you think that is?\n\n**Susan:** Well, it's trillions of dollars.\n\n**Scott:** Huge market?\n\n**Susan:** Yeah, with a _T_.\n\n**Scott:** A huge market.\n\n## Is the Ag AI problem easy to solve?\n\n**Scott:** Do you think it's easy though? Like saying: \"Hey, this is going to be an easy problem to solve\"?\n\n**Susan:** Nothing's easy. _Nothing_ is easy.\n\n**Scott:** Okay, is this going to be easier than something else?\n\n**Susan:** I think there's certain low hanging fruit that's easy in this market, and you're starting to see that more on the sensor side and the simple data analytic side. For example, analytics telling you, \"This is how hydrated an area is,\" and stuff like that. [These are well established things](https://en.wikipedia.org/wiki/Precision_agriculture) that have been there for a while, but giving that on a smaller scale to farmers is starting to happen on ... The smaller scale happening on a larger scale, if that makes any sense whatsoever. But the harder, deeper questions for instance, trying to figure out the best crop rotation for this exact field and those types of problems are going to be a bit hard. They will be solved. Could I give you a timeframe? No.\n\n**Scott:** Presumably, this is going to make food cheaper again. Like, food has already become pretty cheap, at least in the U.S., and most developed regions of the world. What do you think it means to the economy if food becomes even cheaper? It's even easier to make food?\n\n**Susan:** I think the biggest change will be in distribution. If this vision comes to fruition, the idea of local is a lot easier. Not only, as you mentioned, on the transportation/shipping side, so if you have a small farm with a lot of specialized stuff, now machine learning could help manage a bunch of those small farms, and get the distribution chain together to give the volume necessary that big companies would really pay attention to.\n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976387/blog/ai-show-what-does-an-ai-tranformation-look-like/nigel-tadyanehondo-239555-unsplash_v2.jpg)\n\n**Susan:** Say you've got a hundred acres. That sounds like a lot, but that's not enough to keep grocery story chain X happy with you. But if you got several hundred of these farms and they're all kind of coordinated together, you might have a real change in distribution. Again, you'd need something that would take a lot of small decisions. This would be something that machine learning type of algorithms could help out with. But this is, to me, a big thing. More diversity, more locality. Those are really important things, at least to me, and what I look for at a grocery store.\n\n## Are fully automated, AI farms possible?\n\n**Susan:** Tractor's already, they're basically driving themselves around.\n\n**Scott:** Sure, but think like, Interstellar. There's an AI brain inside the device.\n\n**Susan:** This is definitely well beyond. What I've deeply investigated, the Jetson's make me want to say \"yes.\"\n\n**Scott:** It'd be like a Roomba.\n\n**Susan:** Yeah, the Roomba. I mean, the inevitability is there. The question is, what's the time scale?\n\n**Susan:** Is this next year? This five years? Is this 10 years? Is this 100 years? And what does it mean to be truly automated? We're seeing the growth of the autonomous vehicle go from nothing to \"We're really starting to see them in the public sphere this year,\" and the next five years, it's likely that you're going to have been in one of them.\n\n**Scott:** Yeah, and farms aren't all that complicated. You know, you don't have to worry about the lady with the stroller, pushing out in front of the tractor as much.\n\n**Susan:** Going back to the drone thing, that's what makes, potentially, automated drones around the farm a real possibility. Flying drones like, okay. The first or two you're flying it around, you mark off, \"Don't fly here. There are trees.\"\n\n**Scott:** So is Ag going to be the only transformation we see? Agriculture?\n\n**Susan:** We should probably look for things that aren't going to be transformed.\n\n### What won't be touched by AI?\n\n**Scott:** Electricity will still be a thing.\n\n**Susan:** Hopefully. Maybe when we unify all the forces and physics, we'll figure out that we can do something else.\n\n**Scott:** You'll still probably sleep in beds for a while.\n\n**Susan:** For me, it goes back to people have the wrong view of machine learning, and where its real power is at. The right view, to me, this is again, just Susan's view here, but\n\n> \"Get rid of the smaller decisions, those tactical decisions, so you can focus on a higher strategy. Once you start doing that, you realize, there's always something above that.\"\n\n**Scott:** Focus your creative energy somewhere else.\n\n**Susan:** It's like, why spend 90% of your time down here doing this? We need to stay creative. We need these things to offload those decisions that really all should be offloaded. But we need to be intelligent about what which ones we keep, even if it kind of makes better decisions. By automating it, you're getting rid of a lot of chances for creativity.\n\n**Scott:** I'm pretty sure though, at least in my life, if I had some AI bot that was analyzing my patterns, it would be more creative than me. I go to the same places, I eat the same things, I do whatever. It could definitely diversify my social interactions, just by forcing me to go to new places.\n\n**Susan:** That's true. You need to have more exploration, and less exploitation in your life.\n\n**Scott:** Exactly. So you just have to find a balance with the AI there. Make sure it has exploration and exploitation built in.\n\n**Susan:** There's an author I've read, David Brin, for anyone that is a Brin fan, he wrote a book. One of his characters was forced by a program to view things that they did not want to view. Just a random percentage of the time, \"You're gonna view this stuff, just to shake it up.\"\n\n**Scott:** I'm liking it already. Where's it go? If a book was written about it, a fictional book I presume, then it's gotta go in really good places, right?\n\n**Susan:** Oh, the rest of this book, it goes really good- It's Earth, by David Brin. That was just one character showing the quirks of this particular character, and how she forced herself to stay creative. It does come down to the disruptive side of the house, we do have to manage that. We're seeing so many industries that are led by the change, staying ahead of the change is pretty hard. Making those intelligent decisions is really important. Intelligent decisions to say, \"I'm gonna offload this. I'm gonna be creative over here.\"\n\n## How does this transform the industry?\n\n**Susan:** We talked about little bits and pieces, but how does it actually affect things in the long run? Like I said, this is, especially in Ag, it's a big change that flips the economy as a scale. So now, a smaller farm potentially is the more productive one.\n\n**Scott:** This is because it has local knowledge? Why wouldn't a large farm be able to take care of, like pixelize it's areas, and take care of it really well?\n\n![alt](https://res.cloudinary.com/deepgram/image/upload/v1661976388/blog/ai-show-what-does-an-ai-tranformation-look-like/al-x-407225-unsplash_v2.jpg)\n\n**Susan:** You're right, they can. But I think it now allows a small area to be productive. So, the goal is to focus on the small area. Big farms will have a natural tendency to still just make everything the same. As soon as you say, \"I'm gonna go large,\" why did you go large?\n\n**Scott:** Right, bigger farms are easier easier to manage with 20th century tech.\n\n**Susan:** You went large because you wanted to make things more uniform. And there will always be a place for that, but this is an economic shift, I predict, that will enable smaller farms in a way that we haven't seen for quite a while.\n\n**Scott:** Do you think that data is a big differentiator there, in the Ag case?\n\n**Susan:** Yes, and no. I'm going to say, in the lead up to these transformative times, winning, so to speak, solving the problem takes a tremendous amount of data, and legwork, and hard, hard work. The area we know about speech, it is hard work to get the data you need. But eventually, speech will be solved. I'm not saying it's tomorrow, and I'm not saying you'll be able- But you'll be able to just get a model that someone has put a whole of time and effort-\n\n**Scott:** A lot of effort into it.\n\n**Susan:** Into, and the data that went into it is-\n\n**Scott:** Less and less important.\n\n**Susan:** Is still down to this thing you just download and use. But, the goal on that small scale is then to take it, and specialize it, based off of your local data. And this hugely as important. Again, to bring up the speech world,\n\n> \"You can take a generic world, generic speech model, and just give it a few hours of some specialized knowledge, and suddenly you just see massive improvements.\"\n\nThat's the same on the small farm size. So maybe they solve some of these big problems.\n\n**Scott:** It learns general themes, yeah?\n\n**Susan:** Yes, but then you specialize it with just a year or two worth of your own data, and suddenly it's really good at your farm, and really good at making whatever decisions this particular thing was built to solve. Whether it's trying to track the growth rate of your animals, or trying to learn your land, and what it's best at.\n\n## What are the key components of these transformations?\n\n**Susan:** Well, you have to have the environment for the transformation to happen. Generally that means, something that's been there for a while.\n\n**Scott:** Like social environment? Like people kinda know how it already goes?\n\n**Scott:** It's already been figured out pretty well?\n\n**Susan:** Yeah, I mean, Ag's been around, I've been told, more than a few years.\n\n**Scott:** I've heard that, too.\n\n**Susan:** And the longer an industry is around, without massive change, the more likely that there's a good chance for massive change to occur.\n\n**Scott:** Well, yeah, you run up against the problems you're going to run against, and over the years, \"Okay,\" things become established, and some things are hard, and you're not going to do them, and some things are not and you are going to do them.\n\n**Susan:** And people get in the mindset before of, \"Well, that didn't work 10 years ago, so why do I want to try it now?\" Well, that was 10 years ago. That vision that said, \"These ideas didn't work,\" now may work today, because of changes in technology.\n\n**Susan:** It's someone coming in from the outside, with a different vision, generally powered by seeing what a technology can do today, that maybe it couldn't do before.\n\n**Scott:** Or someone from the inside finding another...\n\n**Susan:** Yeah, finding that notch out of the wilderness, and suddenly getting a glimpse of vision, and realizing, \"We're gonna change.\"\n\n**Susan:** Another great transformation right now, that's happening, is space. I mean, I'm a SpaceX fan. I'm not paid in any way, shape, or form.\n\n**Scott:** This isn't really an AI transformation. Maybe in how they developed some of their parts or something.\n\n**Susan:** Yeah, it just shows this general transformation where, a new vision came into place-\n\n**Scott:** It was one way for a long time.\n\n**Susan:** Rapidly replacing the old system.\n\n**Scott:** Let's take a decade or so to like, work on some stuff, and then...\n\n**Susan:** Now the big companies that were in space are probably very worried about their technology.\n\n**Scott:** SpaceX will be making all the money in the future.\n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976388/blog/ai-show-what-does-an-ai-tranformation-look-like/spacex-549326-unsplash_V2.jpg)\n\n**Susan:** It's great, but that's how these transformations generally happen. You know, an industry that's been established for a while, it has players that are established for a while, they're big. They've been sitting there.\n\n**Scott:** They've become pretty complacent.\n\n**Susan:** Not necessarily complacent, but they just don't see the way out. Everywhere they look, looks a little worse than where they're at, because they're stuck in a rut. This new entity comes in, and sees a new way, and they just go to a different optimum. That new optimum just crushes the old one.\n\n**Susan:** And a lot of transformations just happen in that way.\n\n**Scott:** This is the classic, great story recipe, the world was one way, then something happened. Now it's a different way.\n\n**Susan:** And suddenly the dust of those old companies, no one really remembers them anymore.\n\n**Scott:** Yeah, nobody cares anymore.\n\n**Susan:** And you're seeing a lot of big, old, massive companies that people aren't hearing about anymore, you know? Where's GE going lately? Not to pick on them-\n\n**Scott:** I'm sure they have some data science and machine learning people there, doing something.\n\n**Susan:** Yeah, they're being forced to rapidly innovate, that's for sure. That, or be left behind.\n\n**Susan:** You need to jump on board with these tools. It's painful, and there's going to be some wrong alleys.\n\n**Scott:** Any final thoughts you have for people that are thinking about the AI transformation, what's going to happen?\n\n**Susan:** Yeah. I would get smart on it, get your shots, get inoculated.\n\n**Scott:** There you go. Get used to this idea.\n\n**Susan:** Yeah. It doesn't mean that you're going to be doing it yourself. It doesn't mean that you won't be impacted by it, but do some due diligence right now. Start looking into it, and keep an eye towards those things that used to not work. Think about, with some sort of machine learning powering them, maybe they could have worked. It took too many people, and too many decisions 10 years ago, but maybe it's now possible.\n\n**Scott:** I would say,\n\n**AI is not going to change literally everything, but just like electricity didn't literally change everything, and the internet didn't literally change everything.**\n\n**Susan:** It didn't?\n\n**Scott:** No, it didn't.\n\n**Susan:** I brush my teeth with an electric toothbrush that can communicate on the internet.\n\n**Scott:** Yeah, but these transformations are usually good in certain areas, but they're like, really, really good in those certain areas, and make a big difference. You'll see that, when there's something along those lines of:\n\n> \"What's a mundane task for a human to do, but still takes human intelligence to do at least right now?\" Okay, that's probably a really good spot to be thinking about \"How is AI going to change that, and extract value?\"\n";
						}
						async function compiledContent$48() {
							return load$48().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$48() {
							return (await import('./chunks/index.c738848c.mjs'));
						}
						function Content$48(...args) {
							return load$48().then((m) => m.default(...args));
						}
						Content$48.isAstroComponentFactory = true;
						function getHeadings$48() {
							return load$48().then((m) => m.metadata.headings);
						}
						function getHeaders$48() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$48().then((m) => m.metadata.headings);
						}

const __vite_glob_0_12 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$48,
  file: file$48,
  url: url$48,
  rawContent: rawContent$48,
  compiledContent: compiledContent$48,
  default: load$48,
  Content: Content$48,
  getHeadings: getHeadings$48,
  getHeaders: getHeaders$48
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$47 = {"title":"What Does it Mean for a Machine to Learn? — AI Show","description":"In this AI Show episode, we talk about what it means for a machine to learn.","date":"2018-11-02T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981324/blog/ai-show-what-does-it-mean-for-a-machine-to-learn/what-does-it-mean-for-machine-to-learn-blog-thumb%40.jpg","authors":["morris-gevirtz"],"category":"ai-and-engineering","tags":["machine-learning"],"seo":{"title":"What Does it Mean for a Machine to Learn? — AI Show","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981324/blog/ai-show-what-does-it-mean-for-a-machine-to-learn/what-does-it-mean-for-machine-to-learn-blog-thumb%40.jpg"},"shorturls":{"share":"https://dpgr.am/3edfeb5","twitter":"https://dpgr.am/3448424","linkedin":"https://dpgr.am/647bb8d","reddit":"https://dpgr.am/020286a","facebook":"https://dpgr.am/44a48c4"}};
						const file$47 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/ai-show-what-does-it-mean-for-a-machine-to-learn/index.md";
						const url$47 = undefined;
						function rawContent$47() {
							return "\n<iframe src=\"https://www.youtube.com/embed/ZWIi3Ah-VHY\" width=\"600\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\n**Scott:** Welcome to the AI Show, I'm Scott Stephenson, co-founder of Deepgram. With me is Jeff Ward, aka Susan. On the AI show, we talk about all things AI. Today we're asking the very big question, \"What does it mean for a machine to learn?\"\n\n**Susan:** What do you think it means for a machine to learn, Scott?\n\n**Scott:** I think a basic way to look at it is by asking the question:\n\n\"Is a machine altered after you give it some data. Or, you expose it to an environment. Has it changed and changed for the better?\"\n\nOf course, you could take a car wash and then drive a semi truck through it, and then it just destroys the car wash, but that probably isn't better.\n\n**Susan:** Well, I don't know, it's open up to the sky, the rain can come down. You've got larger volume then, right?\n\n**Scott:** Yeah. But if you wanted to teach a child to read, maybe you show them, \"Hey, this word means this, these sounds mean that, etc.\" And then they try it, and then you say, \"Yeah good job,\" or not. Then the next time they try to read, do they get better? Do they remember some things, do they do things like that?\n\n**Scott:** These are humans learning, or animals learning, doing a similar thing. It's a similar thing with a machine. Doesn't matter what the stimulus is:\n\n*   Do you give it an image?\n*   Do you give it some audio?\n*   Do you give it some text?\n*   Do you give it some clicking on a spot on a website when you're about to buy some shoes?\n\nIt learns from that and it gets better at doing whatever its job is. That's really machine learning.\n\n**Susan:** It seems like the definition's really wrapped around one really important word: better.\n\n**How do we define better?**\n\n**Susan:** Better? That's a really squishy word in some certain problems. That kinda gets into what makes certain problems tractable nowadays. If we can really tightly nail down what is better, we're generally pretty good at it in the computer world and the machine learning world. But, if better is a little bit squishy, are you better at playing basketball today than yesterday? Those get into harder and harder questions, such as: \"Is this a better solution to world peace?\"\n\n**Scott:** Things can get complicated.\n\n## What sorts of questions are a good fit for Machine Learning?\n\n**Susan:** What are the kinds of problems that computers can do, or that machine learning can attack nowadays, that humans can do, and what are the things that humans can't do? Conversely, what are your thoughts on some of the ones that humans can do that machines are having a problem with today?\n\n**Scott:** You have it all four ways:\n\n1.  There are some things humans can do and machines can do.\n2.  There are some things only machines can do, or do well, and humans can't do them very well. Examples of that would be- have a person pick up a house, machines can do that better than a human can.\n3.  Then you can have it the other way around: what can a human do better than a machine?\n4.  What can you have that none of them can do very well?\n\nThe most interesting part is probably when does a human do a better job than a machine or a machine do a better job than a human? What factors go into that? To come back to the standard topics of images, text, audio, time series, where you click on a webpage? What's the next ad I should show you?\n\n\"A human might be able to do a better job than the machine could in normal context, but they would be way too expensive to have them make that decision.\"\n\nFor example, if you had a person sit down and learn all of the dog breeds in the world and really study pictures and give them an infinite amount of time and pump them full of Mountain Dew and Papa John's ... Or Dr. Pepper, in your case, right?\n\n**Susan:** Dr. Pepper and Cheetos, thank you very much.\n\n**Scott:** Okay, sorry. And then really sit down and think about all of this, and you get all the time and all the money in the world to figure these out, and then submit your answer, they still might be able to do better than a machine. But,\n\n**a machine would be able to do that in like 500 milliseconds.** \"I just ran through all of them, and then here it is, and maybe I'm a few percent off, but big deal, it was super cheap for me to do it.\"\n\n\"In the world of machine learning nowadays, it's the big data problems that machines are starting to edge out humans on.\"\n\n**Scott:** When you've got these tremendous data sets, like you're saying, dog breeds or something that would take an expert, a whole lot of years to get really good at, that's where machines are starting to see some good advantages.\n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976756/blog/ai-show-what-does-it-mean-for-a-machine-to-learn/husky_malamute_training_set.jpg)\n\n_What would a husky vs. malamute training set look like?_\n\n**Scott:** Also when it's complicated, but it's not that complicated.\n\n**If there's a lot of things to remember, but it isn't that they all interact with each other and form some complicated mess. It's complicated simply because of how numerous it is.** That's really good for a machine to tackle that. They can handle that cognitive load of juggling a lot of different very simple things, but if they all have to interact then it usually doesn't work out as well. For example, if you wrote an article and said, \"Hey machine, summarize this article for me.\" If a human sat down and read that article and they said, \"Well, the few main things are these and here's a summarization,\" then you would be able to listen to that, feel like you had a good idea of what that was. Not every human would give you the same answer though, right? But a machine probably doesn't even come close nowadays, still.\n\n**Susan:** I think that again you're nailing, from my point of view, one of the big distinguishing pieces here, that says where machines aren't doing well also is tied to where humans give you a lot of different answers. So, if I were to summarize a book for you, I'd get a very different summary than you, as you just pointed out. If I were to give you my impressions on a painting, is a similar example. At that core of the different answers, none of them are necessarily wrong. Some will feel more right or feel more wrong, but that's the core thing, this idea of what is better and what is worse in a squishy world.\n\n**Scott:** Subjectivity is flowing in there a little bit.\n\n**Susan:** That's where human problems are really working out.\n\n\"If there's some very standardized accepted answer, then you build a highly supervised learning technique, and we're starting to learn how to do that fairly well.\"\n\nIt's that unsupervised, semi-supervised, coming up from what the definition of good is, that machines are a little ways away on.\n\n**Scott:** People are trying, there are people in the world trying to do summarization on large bodies of text and make it work.\n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976757/blog/ai-show-what-does-it-mean-for-a-machine-to-learn/4044601053_0e4fea5e34_b.jpg)\n\n_Photo credit: [Steve Jurvetson](https://www.flickr.com/photos/jurvetson/4044601053)._\n\n**Susan:** And they're getting better.\n\n**Scott:** More progress keeps being made, but it's usually in the context of where a human could look at something fairly quickly and give you a quick snap answer on it, is where machines tend to do a fairly good job still. In the case of autonomous cars, \"Hey, you're driving in a street, you see a person, should you stop or not?\" Stop, pretty easy. It didn't take me very long to compute that as a human, didn't take me very long to compute that as a machine either. More in-depth problems, when I have to think about the sequences into the future, or read a long document. Then it's not such a split-second decision for me.\n\n**It takes humans more cognitive load for them to figure this problem out, but machines can't do it at all yet.**\n\n### What's holding machines back?\n\n**Scott:** I think there's a couple areas.\n\n*   Do you have the labeled data for some of it,\n*   Do you have the model architecture that will actually work.\n\nIn both cases there, you can find examples where you don't have enough data or any data at all. If you did have data that problem would be solved. But, then also you could find other cases where all the data in the world still isn't gonna fix a problem until you find a model architecture that can actually support that data and be trained and learn from it. When you as a human learn speech and language, you can work from a few set of small examples, you can talk to people in a normal way, and then when you hear a word that you don't understand but you can start to work out context, and maybe even weeks later you see it used in another context, etc. and you stitch it all back together, and that was a totally [unsupervised way](https://blog.deepgram.com/ai-show-different-types-of-machine-learning/) of learning what that word meant.\n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976758/blog/ai-show-what-does-it-mean-for-a-machine-to-learn/Child-boy-toddler--people..jpg)\n\n_A child learning the meaning of the words \"Share, or else.\"_ \n\nThat's how people learn a large amount fairly quickly in over 10 years of their life, and in the long tails as well, without seeing any real supervised part of it. It's a tricky problem.\n\n**Susan:** It's really hard. I do think that there's an aspect, though, that's pretty important, and that is going back to the fact that humans have an ability to pick their own loss function.\n\n**This is a key to any kind of problem you've got, basically any machine learning problem, you gotta know what's better and you gotta know what's worse.** We humans all have this ability to say, \"I think this is better and I think this is worse,\" and we learn to make these loss functions a little bit better and a little bit worse. You have a different set of loss functions than I do. That is a really tight core of our ability to go into these semi-supervised and unsupervised worlds, is to develop that loss function, start understanding how we can come up with a way of attacking the problem. The first step is just say, \"Did a little better, did it a little worse this time,\" and then changing that.\n\n**Scott:** When machines are learning , that is to say: AI, deep learning, etc., they have a lot freedom in certain areas to do whatever they want to do in order to figure something out. But, they have very strict rules in other areas that are like, \"The only thing that you care about is making this number go up.\" They don't even know what the number means, whatever, all you gotta do is make this number go up. In aggregate, maybe across your whole data set, but you gotta make that number go up. So you're very constrained on that. You can fiddle with all these numbers if you want to try to make that number go up, but that's your job.\n\n\"Humans can use common sense to be like, 'That's a stupid number to be trying to optimize,' where an AI algorithm doesn't do that yet.\"\n\n**Susan:** That's a huge challenge: being able to figure out your own loss function. Once we can get there, I think that the research into that area will lead to good general purpose artificial intelligence, and really crack a lot of problems. As soon as we start allowing that hard, fixed loss function to be modified itself and come up with good ways of dealing with that so it doesn't get out of control and destroy everything, that will really open up a lot of the missing problems, the problems that machines, machine learning can't tackle right now.\n\n## What unexpected things should we expect from ML?\n\n**Scott:** One thing we didn't necessarily talk about a lot is just what unexpected thing, or maybe unexpected, unintuitive thing will a machine be able to do better than a human in the future?\n\n**Susan:** That's such a wide field. We kind of talked about a generic area, things that are big data, but I'll tell you the one that's coming up quick, and everybody sort of gets this, but I think that we'll really see when the numbers come out- self-driving vehicles. This is one of those things that people think is a highly intelligent task, but I honestly think from a machine learning standpoint we're seeing how much easier it is. Let me give you an example of how easy the task is. When you drive 150 miles, how much of that route do you remember? Almost none. Why is it? Because you've kind of automated all of that in your head.\n\n\"That kind of problem, where you can kinda automate it in your head, that means that it basically is a soluble problem so long as we can figure out those little pieces that went into it.\"\n\nComputers are just gonna destroy us. They're gonna be a lot better at doing it, a lot safer, they can look a lot of different ways, they can see in 18 different spectra that we can't. They can avoid that cat and get you to your destination on time.\n\n**Scott:** Same thing with figuring out, \"Is this a cat or a dog?\" You don't think, \"Well, it's got an edge of an ear and it's got ...\" It's a cat.\n\n**Susan:** Definitely a cat. A machine will be like, \"It's a cat.\"\n\n**Scott:** If you don't have to think about it then yeah.\n\n**Susan:** I don't think people understand it, but any kind of problem where you can just go on autopilot, where you've gotten to a level of mastery where you just don't realize it at the end of the task, those are gonna be where machines will eventually get better than us.\n\n**Susan:** The problems where you are always constantly thinking and being creative and coming up with, \"Oh, but we hadn't thought about this,\" and that takes you down a new road, \"We hadn't thought about this,\" and it takes you down a new road, and you're mentally engaged the whole time, those are the long, long, long, hard, far away problems for machine learning.\n\n**Scott:** It's not a closed system that you can just let a machine learning or AI just play in, and say, \"Go figure that out.\" It even takes a while for humans to figure out what loss function, what reward system they would set up for a machine to go learn that.\n\n**Susan:** It's gonna be interesting to see which problems start falling into that fully automated world, versus which don't. But, there is one other aspect of this, also. By automating all those problems, just to get a little off here, people are worried about machines automating everything away.\n\nThe reality is, they're gonna take the 95% that's boring and leave the 5%. That 5% will grow, and that 5% will allow us to focus on the really interesting things and not be bored all the time. We'll probably have more people saying, \"I need some downtime because I'm thinking too much.\"\n\n**Scott:** It already happened. You had mules and oxen turning mills to grind flour back in the day, or you have steam power doing things that humans would have done, or a tractor rather than having the mule go plow the field. You see those types of things already being displaced now. It freed up time for humans, and that's the evolution of things and that's gonna happen again. People that talk about the industrial revolution or the agricultural revolution displacing jobs, totally true. It does in a short period of time, but then people fill in the gap and start doing more creative things.\n\n![alt](https://res.cloudinary.com/deepgram/image/upload/v1661976759/blog/ai-show-what-does-it-mean-for-a-machine-to-learn/Treadmillcrane.jpg)\n\n_Treadwheels or treadmills were used in the middle ages to power cranes, corn mills, and other machinery. Today, we power them electrically so that they help us stay in shape._\n\n**Susan:** We throw around a lot of terminology all the time, it might be important just to point out things like what is a loss function, when we say loss function, cost function, all these different things.\n\n## Reward functions and Loss Functions\n\n**Susan:** [Reward function](https://en.wikipedia.org/wiki/Reinforcement_learning). Generically, what we're talking about here is an observable truth and comparing our prediction to that, and how far away we are between those two. The real answer was 18.2 and we guessed 18.\n\n**Scott:** There you go, you're not too far off there.\n\n**Susan:** There's a certain gap right there.\n\n**Scott:** But, maybe that is really far off.\n\n**Susan:** It could be, depends on your scales here. Maybe you could only predict between 17.9 and 18.3.\n\n**Scott:** But if it's cat or dog, you're building a cat or dog detector, or a hot dog or not, and it says 80% hot dog, 20% not, then okay fine. But, what was the original? The original was not. Then it's like, \"Oh, I'm really far off. I said 0.8 on hot dog but 0.2 on not.\"\n\n**Scott:** I should have been 1 on not and 0 on hot dog.\n\n**Susan:** That's another interesting point. When we talk about how computers or machine learning predicts, generally, not every time, it's not this binary zero/one world. We turned it into that. But, a lot of problems, especially classification problems, come down to a probability of one class versus the probability of another class. When we talk about [loss functions](https://en.wikipedia.org/wiki/Loss_function), the ultimate is to predict 100% on this and 0% on everything else. Even if you predicted 80% on the right class, there's a little bit of gap there, you should have been 100%. So, when we talk about loss, we're talking about compared to the perfect, absolute truth there.\n\n**Scott:** It would be marked as 1, not hot dog, 0, hot dog. But the next example might be a hot dog, and that's a 1, hot dog, and it's a 0, not a hot dog.\n\n**Scott:** But having some room for ambiguity there makes some sense. Maybe the times that it guesses half one, half the other it actually is like a half-eaten hot dog, I don't know.\n\n**Susan:** There's a lot of things that are called hot dogs out there that I'm not sure are actually a meat product.\n\n## What is intelligence?\n\n**Scott:** We said what is learning a little bit, but what is intelligence?\n\n**Susan:** Intelligence is a really hard thing to nail down, especially when it comes to machines. There's a very good chance, a very real chance, the first alien species we'll meet will be a machine, i.e. we create it somehow and suddenly realize it's intelligent. But, there's also a really high chance we'll realize it too late, meaning it was intelligent for a while but it was so alien to us we didn't understand it.\n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976760/blog/ai-show-what-does-it-mean-for-a-machine-to-learn/27640481_cf17dc2133_b.jpg)\n\n_Beluga Whales are have a particularly developed sonar system which they use for \"seeing\" in the pitch-black waters 1000m+ below the sea. They also use this sound system for communication. As humans, we have no appreciation for their sensory experience. Photo: <a href=\"https://www.flickr.com/photos/criminalintent/27640481\">Lars Pougmann</a>_\n\nWhen we think of intelligence, we always think human intelligence.\n\n**You go out into the animal world and you see things that humans couldn't possibly do from a mental standpoint.** Yet, we don't call them intelligent, at least self-aware, like we would think of as ourselves. So there is no one answer, this is what intelligence looks like.\n\n**Scott:** People tend to define it, though, like what can a human do that's hard.\n\n*   Can you speak some sound waves and a human can turn that into words?\n*   Can you show a human an image and they can tell you what the object is?\n\nThese are intelligence tasks.\n\n**Susan:** The big ones are, \"Can you get some new thing that you have very little experience with and come out with some sort of acceptable set of answers on the other end?\" That takes a really high level of function to be able to come up with something like that. That's why we're pretty far away with it in the machine learning world. Everything that we do in the machine learning world all fits within a bin, one single millimeter outside of this bin, and the machine doesn't know what to do.\n\n**Scott:** I think there's an interesting point to be made about this. When there are underlying laws or rules that can't really be broken, there's chaos around those rules and it's kinda hard to figure out what they are. It's not that hard to drop something and say that things always drop. But it is kinda hard to predict the weather or something. Are there underlying rules on how that works? Yes. So essentially, intelligence may be discovering some of these laws as well. Hey, you're given this mess, but there are these underlying rules that you don't know what they are, but you start to discover them a little bit.\n\n**Susan:** Figure out the system.\n\n**Scott:** Take language for example. If you just started spitting out a bunch of different words randomly, it literally would be gibberish. It would take the age of the universe for you to spit out 10 paragraphs that actually make sense if you did it randomly, right? But there are some rules, there's some structure and you start to discover those things and you hang onto them. But, then you move away from that structure just a little bit and that's what creates the fuzz around the world, etc. You can look at how does the earth form and water and eddies flow. There are underlying rules of how fluids move, and how gravity affects things, and how molecules break down. Then you can say, \"Look, that's how it is.\" But if you look from the outside, it kinda looks like chaos.\n\n**Susan:** Our ability to figure out those systems and then use that knowledge to figure out the future is definitely a big portion of how we are intelligent. But again, it's really hard to say, \"What will the first other intelligence we meet look like, and how will it be similar to us? How will it be different to us? Will it be extremely 'better'?\" Again, defining what is better, what is worse here. That would be an interesting thing. It'll be a challenge for us. There's a lot of morals and ethics that go into this which we could probably spend hours on, just that subject alone.\n\n**Scott:** There's been a lot of iteration and time for this to sink in into the data science community, tech companies. They're figuring out how to show ads to people really, really well, probably better than a human can, almost certainly. At least on the scale that we're talking about.\n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976760/blog/ai-show-what-does-it-mean-for-a-machine-to-learn/advertising-ad-1.jpg)\n\n_Google ads for the search term \"advertising.\"_\n\n**Susan:** Yeah, that gets down to the notion that people are actually fairly predictable when you find a niche that they're staying in. If you get into a routine, obviously you're predictable, that's the point of routine. One of the first things I played with when I got in the machine learning world was doing a simple zero-one predictor. You would type in zeros and ones, zeros and ones, zeros and ones, and then after a certain amount of time it would guess and not tell you what it thought your next one would be, a zero or a one, etc. etc. And then at the end it would say, \"I was this percent right.\" And it stunned me. It was like 80-some odd percent right, and I'm pulling a number out of here and suddenly...\n\n**Scott:** You think you're being random.\n\n**Susan:** You think you're being random, but in reality you're incredibly predictable. You're coming up with patterns all the time and following them. This is one of things we were talking about before, what is that computers and machine learning might be able to do that's pretty interesting. It's predicting humans, even though we say humans are so hard to do. Within certain scopes, humans are incredibly predictable.\n\n**Scott:** Yeah, and within a certain accuracy that you're willing to give up. Hey, I don't have to be right 100% of the time.\n\n**Susan:** No. If you can predict 80% of the time. That's really good for a lot of things.\n\n**Scott:** And it's not gonna get tired either, the machine. But, predicting your next Amazon purchase: Hey, you go buy something and it says, \"Look at these eight things,\" and it's like, \"Actually yeah, I kinda wanted that one too.\"\n\n**Susan:** Or more interestingly, predicting what to show you in order to guide you to a more expensive purchase.\n\n**Scott:** I think now, currently, state-of-the-art, really good stuff is happening in ads and recommendation systems.\n\n**Susan:** That's where the money's at.\n\n**Scott:** Where the money's at and where the data's at. People interact with it and train it really well, that's for sure. Images, text is coming around, audio's coming around. Time series type stuff is coming around. Driving cars and whatnot is coming around. You have to be a little bit more forceful with them because it's harder to get the labeled data, to get the scale of data.\n\n**Susan:** Although, as an interesting aside, there's some really great simulated stuff that's going on in the self-driving car world.\n\n**Scott:** Oh definitely, self-driving, Grand Theft Auto V, just drive around. Why not?\n\n**Susan:** Get out there and simulate the data.\n";
						}
						async function compiledContent$47() {
							return load$47().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$47() {
							return (await import('./chunks/index.3dbf031e.mjs'));
						}
						function Content$47(...args) {
							return load$47().then((m) => m.default(...args));
						}
						Content$47.isAstroComponentFactory = true;
						function getHeadings$47() {
							return load$47().then((m) => m.metadata.headings);
						}
						function getHeaders$47() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$47().then((m) => m.metadata.headings);
						}

const __vite_glob_0_13 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$47,
  file: file$47,
  url: url$47,
  rawContent: rawContent$47,
  compiledContent: compiledContent$47,
  default: load$47,
  Content: Content$47,
  getHeadings: getHeadings$47,
  getHeaders: getHeaders$47
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$46 = {"title":"What will the AI Utopia look like? — AI Show","description":"Is there an AI utopia coming? What will it look like? Have a listen to this episode of the AI Show for our ideas.","date":"2019-02-27T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981351/blog/ai-show-what-will-the-ai-utopia-look-like/what-will-ai-utopia-look-like%402x.jpg","authors":["morris-gevirtz"],"category":"ai-and-engineering","tags":["machine-learning","deep-learning"],"seo":{"title":"What will the AI Utopia look like? — AI Show","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981351/blog/ai-show-what-will-the-ai-utopia-look-like/what-will-ai-utopia-look-like%402x.jpg"},"shorturls":{"share":"https://dpgr.am/e00ea0d","twitter":"https://dpgr.am/466176f","linkedin":"https://dpgr.am/069de14","reddit":"https://dpgr.am/b04485e","facebook":"https://dpgr.am/2c79813"}};
						const file$46 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/ai-show-what-will-the-ai-utopia-look-like/index.md";
						const url$46 = undefined;
						function rawContent$46() {
							return "<iframe width=\"100%\" height=\"166\" scrolling=\"no\" frameborder=\"no\" allow=\"autoplay\" src=\"https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/576097014&color=%23ff5500&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&show_teaser=true\"></iframe>\n\n**Scott:** Welcome to the AI show. Today we're asking the question, the very big question. What will the AI Utopia look like?\n\n**Susan:** We were a little negative, Debbie Downer kind of not so happy on the [last podcast](https://blog.deepgram.com/what-does-the-ai-dystopia-look-like-ai-show/).\n\n**Scott:** Why be so negative, man?\n\n**Susan:** Things are going to destroy your life but, man, are there some cool things that it's going to do for us.\n\n**Scott:** Yeah. Like what?\n\n**Susan:** Oh, I'm just looking for all the easy ones, you're going to live longer. Do you want to live longer, Scott?\n\n## How's AI going to help us live longer?\n\n**Susan:** Bioinformatics. NVIDIA had this really cool demo last year where they took an MRI or something and used machine learning to turn it into a 3D view. You know, automatically segmenting out organs and all sorts of stuff. I mean, this is just like the level one stuff.\n\n**Scott:** I'm just going to give you a better view.\n\n**Susan:** Exactly.\n\n**Scott:** Take this thing that used to be slices, turn it into something that you can rotate around and look at it and be like, \"Oh, there it is.\"\n\n![bosluk](https://res.cloudinary.com/deepgram/image/upload/v1661976828/blog/ai-show-what-will-the-ai-utopia-look-like/bos-luk.png)\n\n*An MRI image of a person with an arachnoid cyst in their left, inferior frontal lobe.*\n\n**Susan:** It's amazing. Even without being a doctor you can look at some of the stuff now and say, \"That's clearly not supposed to be there, you may want to get that looked at.\"\n\n**Scott:** Well, speaking of NVIDIA, they use AI now to do [ray tracing](https://en.wikipedia.org/wiki/Ray_tracing_(graphics)) a lot better. Ray tracing allows you to do graphics better. With it you figure out a few things but then fill in the rest using AI and they can speed everything up, make it look awesome\n\n**Susan:** Let's get to brass tacks here. Better video games equals better life.\n\n**Scott:** Everybody likes that one.\n\n**Susan:** The world is just wide open for amazing new things, transformative things. Pretty much everybody has seen cars driving around their city mapping everything around. We all know the autonomous revolution is going to come upon us, but there's so many cool things that it's going to enable.\n\n**Scott:** The cars drive you around, you can take a nap.\n\n**Susan:** I'm not going to lie, there's been a couple of evenings where I wanted a responsible vehicle to take me home and, you know, that will be nice.\n\n**Scott:** The car hasn't had a few...\n\n**Susan:** Exactly. I think you know what I'm talking about, Scott.\n\n**Scott:** Sure, you were just sleepy.\n\n**Susan:** How cool would it be to get into your car after work on a Friday and then, you know, stretch out your arms like this in the nice wide open space because you don't have the driving steering wheel and all there. You know, pull a blanket over yourself and, wake up on the other side of the country, well, not quite but, you know, wake up a thousand miles away.\n\n**Scott:** Hey, we're in San Francisco, why not end up in Denver.\n\n**Susan:** I'm going to go skiing tomorrow.\n\n**Scott:** It's a little far but, yeah.\n\n**Susan:** But just take me there and wake me up every once in a while for a nice little break here and there.\n\n**Scott:** Shake the car a little bit. \"Okay, I'm up, I'm up, what?\"\n\n**Susan:** That's talking about you and me but, what about Grandma and Grandpa? Could you just imagine the amazing freedom that they will have? I don't know about your particular extended family there but, I know that my grandparents basically reach a day when it's probably not a good day for them to drive, right?\n\n**Scott:** Did you have that conversation?\n\n**Susan:** Yeah and the day before they're good; the day after, probably they're not. This independence thing falls off so quickly and yet, if you can have self driving cars then they can be more gradual. You'll still be programming the car to get them where they're going probably. You'll be getting the phone call, \"How do you tell it to go to the store?\"\n\n**Scott:** One of my favorite Uber for X companies out there or, there are two companies, Silver Car and, GoGoGrandparent.\n\n**Susan:** I like it.\n\n**Scott:** Basically Uber for old people, which is awesome. But, their great viral marketing technique was not Facebook ads or videos or anything like that. It was postcards that you would send to your friends. Such a good idea.\n\n**Susan:** I love it.\n\n**Scott:** Why not have the grandparents get in the AI car? Now you don't have to have the human driving you around anymore\n\n**Susan:** They can still make it to Sunday dinners.\n\n![dindin](https://res.cloudinary.com/deepgram/image/upload/v1661976829/blog/ai-show-what-will-the-ai-utopia-look-like/A-Swedish-American-family-in-a-small-Minnesota-tow.jpg)\n\n**Scott:** Exactly. Take a trip to go shopping, do all of that.\n\n**Susan:** So what do you think, Scott, what's a big area you're thinking of?\n\n**Scott:** I'm really big on my life becoming easier, that would be awesome. You get in your car and you're about to drive somewhere and you have to fumble with your Google maps and put in a stupid address for it and be like, \"No, not that address, this other address.\" And, \"Oh, I also want to make a stop along the way to go get some donuts or cookies or, whatever, because, hey I'm going to this meeting.\" Why can't I make all this easier, just talk to it like a human or it just knows that, \"Hey, it's nine o'clock and, Scott's going to work.\"\n\n**Susan:** \"Should we stop at the coffee shop, Scott?\" Politely ask.\n\n**Scott:** \"No, it's okay.\"\n\n**Susan:** \"Maybe not today.\"\n\n**Scott:** But, it suggests a really good thing, bring in donuts or this, \"I already called ahead,\" you know? Oh, great, all we have to do is stop by there.\n\n**Susan:** \"You told me last time your coworkers really appreciated you bringing in donuts.\"\n\n**Scott:** Yeah, exactly. \"I heard,\" you know, \"because I was present.\"\n\n**Susan:** Through the AI grapevine.\n\n**Scott:** I was present because your phone was present and it noticed, you know, that everybody really liked that and so maybe we should do that again.\n\n**Susan:** People are happier with donuts.\n\n**Scott:** We've already kind of noticed that shopping has become easier because of technology. There's a lot of things that've become easier. You still have lots of annoying things that you have to deal with in life and AI won't take care of all of them, but there's a lot that I think it could help with.\n\nFood is another big one for people. Not just agriculture - we should get there - but also, \"What are we going to eat tonight?\" \"Oh, I don't know.\"\n\n**Susan:** If AI can solve that problem, how many more marriages would be saved?\n\n## Solving the discovery problem\n\n**Scott:** It's a discovery problem in a lot of ways, right? If this machine learning program was able to give you something that you guys would both agree on, that's new, that's interesting, that's great.\n\nAnd not only that, it suggests a couple of options for that. You probably would want this, this and, this. And actually, I would want that, let's get that, let's get that, let's just see what happens.\n\n**Scott:** And then we can poo poo it later, we can blame the machine.\n\n**Susan:** Exactly, yeah, offloading blame.\n\n**Scott:** Exactly. The machine did it.\n\n**Susan:** I'm loving the offloading blame right now.\n\n## What about agriculture?\n\n**Susan:** Agriculture is rife with disruption for AI in very amazingly good ways. One of my big hopes is that we will re-enable the small farmer again, and that is because when you look at big agriculture, they're all about taking huge tracts of land and trying to make it look very similar and not flatten it all out, grow the exact same crop and because you get the economies of scale, you get everything the same.\n\n**Scott:** Yeah, farming is hard. There's not a super high yield, food doesn't cost all that much so you have to do a whole bunch of it and you have to do it cheaply.\n\n**Susan:** It takes a lot of intelligence and a whole lot of work to deal with any kind of difference.\n\n**Scott:** Situational awareness as well.\n\n**Susan:** Machine learning can help take that load off. Imagine now you only have 10 or 15 acres and it's got hills and nooks and crannies and all sorts of different stuff. The dirt over here is slightly different than the dirt over there. Machine learning can help learn those differences and tell you, \"Plant these crops there and plant those crops there.\"\n\n![farm](https://images.unsplash.com/photo-1429991889170-afd56b2a1210?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=800&q=60)\n\n**Susan:** Then you take it to the next level - the automated machinery of the future could help automate doing all the farm tasks in a much more varied world.\n\n**Scott:** Well it decides what to plant where, it goes and harvests it.\n\n**Susan:** I think it's a guidance thing. I don't want to fall into the trap of machine learning decides. I think it's an enabling thing more than \"this is what you shall have on your land.\"\n\n**Scott:** Well, I think it's more like, \"Here's the plan, is that okay?\"\n\n**Susan:** \"Yeah, go ahead, go do it, go to town.\" That will allow farmers, like any profession and any professional to focus more on the harder pieces of it. Start taking care of the average, give me a rough plan and maybe I'll tweak it for those little interesting things. I predict prices on this are going to be different, or, maybe farm costs over here are going to be different for things the machine can't take into account and tweak it that way. And those are just a few of the small things. You know, talk about distribution that's going to be revolutionized in farming. It's going to be faster to your table, it's going to be fresher to your table, it's going to be more diverse. If you do enable utilizing more of the land, you're going to enable a more diverse set of crops to come to market.\n\n**Scott:** What about robots cooking?\n\n**Susan:** My wife went to the [robot hamburger joint](http://creator.rest/) here that opened up in San Francisco.\n\n**Scott:** Good?\n\n**Susan:** I think her take was, \"Okay.\"\n\n**Scott:** Well, hey, that's pretty good, right?\n\n**Susan:** But they've just opened up.\n\n**Scott:** I would assume the first one is like, \"No, it's disgusting, you don't want to go there.\"But, hey, \"No, it's okay, it was fine.\" That's a pretty ringing endorsement then.\n\n**Susan:** Yeah, ringing endorsement for a burger from scratch. Well their big thing, I think, was it was literally ground exactly at that point, the moment it was ordered. But, when they grind it they grind it in a way so that it makes it more tender because, now they can literally control as little pieces of meat are put together.\n\n**Scott:** Well, that's cute. Everybody has to have a story so, yeah, that makes sense.\n\n## What about the medical perspective?\n\n**Scott:** We're all going to live longer, we already heard that.\n\n**Susan:** Well, let's talk about you and monitoring you and what you can get out of that. Forget big medicine. Let's not talk about being able to segment the body and find organs and find all sorts of things wrong with you. Let's hook up an AI that monitors you daily, takes your pulse, figures out your heart rate and your respiration and how much you're easting or, what activities you're doing and starts figuring out whether or not you feel good or bad and how to tweak you to get better.\n\n**Scott:** And motivate you to get better.\n\n**Susan:** Wait five more minutes or, right now is the exact perfect time. You know, last time you did exercises these were the ones that seemed to help you more than those, you know? The 16 ounce curls worked best only once a week, not five times a week.\n\n**Susan:** But, monitoring those things that actually make a difference in your life.\n\n**Scott:** I could see how this would be life or death for children.\n\n**Susan:** Oh man.\n\n**Scott:** Start put a monitoring collar on your child. That's the upgrade.\n\nBut, it tells you, are they alive or dead at any point in time? Are they drowning? Are they in distress? Take all the worry off. Now you can have free range kids again.\n\n**Susan:** Just let them go.\n\n**Scott:** Just let them go because, you know, you could do this in the past, you could put all these sensors on them but then, what would you do with all the data? You need some AI to tell you, well, actually, this is fine, they're fine. This is the bad stuff. Oh, whoa, something's going on.\n\n**Susan:** They probably will not be eaten by hyenas right now.\n\n**Scott:** It serves a purpose of a scream, you know, a little scream detector. It could be good.\n\n### Let's talk transportation\n\n**Susan:** So, you know, I have a little bit of an aviation background.\n\n**Scott:** Just a little bit. Navy pilot, no big deal.\n\n**Susan:** You know, people think automation is just taking someone out of the cockpit, they don't think that it gets rid of the cockpit.\n\nOften people don't really realize what that means for the redesign of an aircraft.\n\n**Scott:** Why would they want to do that? Why would you want to take people out of the cockpit?\n\n**Susan:** Well, ignoring the fact that the majority of problems actually happen between the yolk and the seat.\n\n**Scott:** Meaning the person.\n\n**Susan:** Correct, most aviation problems are caused by the human operator. From a design perspective, a lot of the things that you don't realize are designed on aircraft that cause a lot of extra weight and structure have everything to do with just pilots being able to see. For example:\n\n> I've got to be able to see the runway and that means I have to do all sorts of crazy things with flaps and stuff like that just to be able to land and take off the plane, just so I can see the runway. If there's a machine, it doesn't need to see in the same way I do.\n\n**Scott:** It would be lighter, more efficient, less complicated?\n\n**Susan:** You can also design it for a better regime of crews as opposed to the landing and take off areas. Not as challenging design as it was before.\n\n**Scott:** Not just autonomous vehicles but, airplanes. That doesn't have to be remote controlled.\n\n**Susan:** Exactly. The same argument now comes over to the self driving car world. You can see it there too. If you got rid of the center, you know the front steering wheel and all the stuff on the console and all that stuff, you'd get a lot more room.\n\n![car](https://res.cloudinary.com/deepgram/image/upload/v1661976830/blog/ai-show-what-will-the-ai-utopia-look-like/columnless-cart.jpg)\n\n**Scott:** Got room to lounge, man.\n\n**Susan:** And this is not a plug for Tesla but, one of their things is, \"Hey, because we've gotten rid of a lot of the engine components, there's a lot more room inside the cabin.\" Well, now you get even more of those components gone and there's even more you can do.\n\n**Scott:** It's like a hotel room in there.\n\n**Susan:** So, again, going back to the aviation world, you can get small airplanes that go longer, further distances and that just really opens up travel.\n\n**Scott:** Well, then does everyone live in a city now? Like, oh, maybe you go further away, more urban sprawl but, is that a bad thing?\n\n**Susan:** Depends on what you want. There's a lot of stuff I think that could happen. Drone delivery of your milk and eggs out of nowhere.\n\n**Scott:** Amazon Prime for everybody.\n\n**Susan:** Yeah, I've got my own personal lake 300 miles away and here comes the drone.\n\n**Scott:** The land of 10,000 lakes and one of them is mine.\n\n**Susan:** That lakefront property becomes a lot more valuable as Amazon starts delivering there. You're no longer disconnected.\n\n### Think about the energy sector\n\n**Scott:** I think also in the energy sector there's going to be a big deal. For example, better use of the grid. This is already kind of happening but, better systems designed for that and then, systems to switch between, choose what you should do, price it, et cetera, make it more efficient. But also, machine learning techniques to find out new power sources. So, what I mean is, [fusion has been a promise for a long time](https://blog.deepgram.com/how-is-machine-learning-or-deep-learning-affecting-science-ai-show/) and, it actually is really close to happening and it's like, man, what's the real problem here? It might be the people. And saying, \"Hey, here are the laws of physics and here are the constraints that we have, it's probably going to be real complicated to make this work. Can we throw some computing power and a machine learning algorithm to guess at what the best type of fusion device would be?\" And, there's been progress made on this.\n\n**Susan:** Lots of progress. Yeah, I mean, I'm actually hearing the eternal joke of fusion's only 20 years away.\n\n**Scott:** Yeah, 20, 20 years away. Now maybe It might actually be 20 years away.\n\n**Susan:** They're talking about hitting equilibrium points, you know, equal power points or, I forgot that whatever it's called.\n\n**Scott:** Over unity.\n\n**Susan:** Yeah, over unity, there we go and, five, ten years maybe. Now actually capturing back the energy there is a different thing but, you know, it puts out more than you put into it. That's a pretty cool thing.\n\n**Scott:** Absolutely.\n\n**Susan:** I'm super, super stoked.\n\n![tokamak](https://res.cloudinary.com/deepgram/image/upload/v1661976831/blog/ai-show-what-will-the-ai-utopia-look-like/TOKAMAK.jpg)\n\n*A picture of TOKAMAK at the [Princeton Plasma Physics Laboratory (PPPL)](https://www.pppl.gov/)*\n\n**Scott:** I mean if you're turning into a steam thing and you just need to cool it down and whatnot, it can't be too hard to do that but, you know, heat is kind of easy to transport.\n\n**Susan:** But it's all wrapped in those super-conducting magnets. They get heat from in there, outside of there ...\n\n**Scott:** You've got to pipe it out, yeah.\n\n**Susan:** I'm no physics guy.\n\n**Scott:** Let the machine learning algorithm figure out the problems. Tesla's Power wall?\n\n**Susan:** They've done some pretty cool stuff with big batteries in Australia that's really paid off, but their next set of big projects are all about decentralized power. The power wall in the home but also becoming now a municipal power grid because all these things are linked together intelligently to take load and push load and all sorts of different stuff. They're really trying to reshape the grid and, to do that, you're going to have to have some very intelligent controls because it only takes a little bit, everybody deciding at the exact same moment to turn on their toaster, to really throw a system out of whack. You know, talk about civil disobedience, you just put on a big billboard at 6:00PM on Tuesday, every single person go make toast. And then, bam.\n\n**Scott:** I love this idea. I love this idea.\n\n**Susan:** Managing a grid is a lot more complicated than people think and these types of algorithms could potentially really, really cut down on problems with spikes like that.\n\n**Scott:** Like I'm noticing a lot of toaster sales. People talking about toasters going on around here, I'm a little fearful. You may have to crank it up.\n\n**Susan:** Right after we're done here I'm going to open up my browser and I'm going to see an advertisement to sell me a toaster.\n\n**Scott:** I think there's some things in the medical side that we didn't talk about like, identifying cancer, or like, being able to go into this imagery a lot better and tell you what's going on. You touched on like three dimensional reconstruction and being able to look at it better but, you can also have areas pointed out to you like, \"Well, this looks like a problem.\"\n\n**Susan:** This is not your standard kidney. One of the classic things in facial recognition, the first thing you always learn about is using [principal component analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) or something along those lines to say, \"This is what the standard face looks like\" so, recognizing faces and recognizing those things. You can do that with, effectively, a kidney but, with a lot better results as you use much stronger techniques. And, now you walk in for your weekly, your monthly, your yearly exam and, you quickly zip around, do an MRI or whatever technique they're going to use to scan the inside of your body and, poof, areas in you are marked with little highlights. You know, green, yellow, red, black.\n\n**Scott:** Ooh, shouldn't have done that when I was 18, yeah.\n\n**Susan:** It's like you may want to get that removed today? This one, you just may need to exercise a little bit more.\n\n### Are there going to be robot, AI powered doctors?\n\n**Scott:** Or strap you into a chair dentists that do your dental work for you?\n\n**Susan:** Well the whole thing has been a growing thing for a long time and, one of the biggest problems, I understand, in that field is latency. You know, a thousand miles, speed of light and all the equipment in between means, if you're going to move a scalpel and you've got a little bit of a delay there, machine learning could enable that a lot more. Again, getting to the point where the surgeon is maybe doing more about guiding what's going to go on and not exactly doing what's going to go on. It does the perfect stitch for them, they don't actually have to do it themselves.\n\n**Scott:** Do a perfect stitch here and then, do a perfect stitch here.\n\n**Susan:** Exactly. Cauterize over there. I saw a while ag, a really cool thing where they were taking imagery of a beating heart and fixing it with, I think there was like some sort of strobe light that would sync up to it so the doctor would see it synced up and then the scalpel would actually, the tip of the scalpel would move to stay the same amount away from the beating heart so that it looks like it's a still heart and they can actually do surgery in real time on a beating heart.\n\n**Susan:** That is the future of medicine. There's a competition to basically build the tricorder, you know, like every single year they try to incorporate more and more of those things into it and, it only makes sense.\n\n**Susan:** Don't you want your doctor to be able to come in and just go, \"Done.\" You didn't have to set up some sort of appointment and there it is.\n\n**Scott:** Scientific advancement might start coming quicker too as ML and AI just really starts getting applied and used just like electricity did in the 1900's or, people figuring out radioactivity.\n\nIn 10 years try to find a researcher that's not using machine learning to analyze data. It's going to become like water to people.\n\n**Susan:** Well that was your world, teasing a weak signal out using machine learning techniques.\n\n**Scott:** Absolutely. You could try to set up rules and write them but it becomes hard because there are all these exceptions. It might be better if I just tell it: \"That's right, that's wrong, that's right, that's wrong. If this is of that type, that's of this type.\" Then you just do that a few thousand times and then say, \"Figure it out yourself.\" You pick all the parameters in order to satisfy that and it works very well. And, it totally changes how you think about the world. I could spend all my time writing an algorithm or, I could spend all my time getting relevant data and labeling it and then, how is that going to turn out in the end? We're seeing over and over that spending the time on the algorithm is not usually the best option unless you have a very small amount of data.\n\n**Susan:** Well, you know, there's a truism in the machine learning world and, we've kind of hit this before but, data will live forever, your algorithm likely won't.\n\n**Scott:** It always gets better.\n\n**Susan:** The algorithm is [what makes you money right now](https://blog.deepgram.com/ai-show-different-types-of-machine-learning/) but, I can almost guarantee you, 10 years from now, it's not going to be that same model structure, even, fundamental technology. But, you'll probably keep going back to the dataset that you collected.\n\n**Scott:** You see this all over the place. The academic datasets and texts and images and speech that have existed for the past 40 years are still used to train models today.\n\n## What does your AI Utopia look like?\n\n**Scott:** So for my Utopia I want my robot friend. I don't know if it's a phone, if it's a physical thing that can roll around and dance or something stupid but, you know, something that you can talk to and it does stuff for you. Maybe it doesn't have to physically do it for you but, call somebody up or schedule a cake to be ordered or pick up donuts o, suggest food. You're actually like, \"I like that, I'm glad I didn't have to go do that discovery. This is great, you know? Tell me more, friend.\" That will be really dope for me. What about you?\n\n**Susan:** My personal, I like the travel aspect. Personally, that's driven a large chunk of my life - the freedom to go places easily and see places easily.\n\n**Scott:** Teleporter. You're sleeping, you come in and out of consciousness and you wake up in a new place. Hey, that's teleporting man, if you can get moved while you're sleeping.\n\n**Susan:** One of my best travel experiences was we were in Seville in Southern Spain and, we got a night train to Barcelona. We were dead tired because, long story but, we get on this train, we fall asleep in a nice comfortable sleeping car, and we woke up to the Catalina coast with the sunrise. It was like, \"This is what I want for every trip.\"\n\n**Scott:** All traveling should be like this.\n\n**Susan:** It should be the same thing, we're going to Yellowstone or, going to Grand Canyon or just going skiing. It's like,\" I'm going to fall asleep at the end of a normal day. Those dead hours that I'm not conscious of the world, use them productively, get me somewhere cool.\" Get me to wake up at a great place every day and I'll be happy. That to me is a really, really huge thing.\n\n**Scott:** Well we should ask our listeners, what do you think will make the most positive impact? What are the things you're excited about? We'd love to hear about it.";
						}
						async function compiledContent$46() {
							return load$46().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$46() {
							return (await import('./chunks/index.f2f2eac9.mjs'));
						}
						function Content$46(...args) {
							return load$46().then((m) => m.default(...args));
						}
						Content$46.isAstroComponentFactory = true;
						function getHeadings$46() {
							return load$46().then((m) => m.metadata.headings);
						}
						function getHeaders$46() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$46().then((m) => m.metadata.headings);
						}

const __vite_glob_0_14 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$46,
  file: file$46,
  url: url$46,
  rawContent: rawContent$46,
  compiledContent: compiledContent$46,
  default: load$46,
  Content: Content$46,
  getHeadings: getHeadings$46,
  getHeaders: getHeaders$46
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$45 = {"title":"All About Transcription for Real-Time (Live) Audio Streaming","description":"Curious how you can get real-time transcriptions from a live audio or video stream? This post as your answers.","date":"2022-08-22T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981433/blog/all-about-transcription-for-real-time-audio-streaming/all-about-real-time-audio-streaming-thumb-554x220-.png","authors":["keith-lam"],"category":"dg-insider","tags":["contact-center","conversational-ai"],"seo":{"title":"All About Transcription for Real-Time (Live) Audio Streaming","description":"Curious how you can get real-time transcriptions from a live audio or video stream? This post as your answers."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981433/blog/all-about-transcription-for-real-time-audio-streaming/all-about-real-time-audio-streaming-thumb-554x220-.png"},"shorturls":{"share":"https://dpgr.am/49944d6","twitter":"https://dpgr.am/e94382f","linkedin":"https://dpgr.am/7100aba","reddit":"https://dpgr.am/7ce8adc","facebook":"https://dpgr.am/cdb2098"}};
						const file$45 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/all-about-transcription-for-real-time-audio-streaming/index.md";
						const url$45 = undefined;
						function rawContent$45() {
							return "Real-time streaming transcriptions involves taking audio that's being generated live and transcribing it into text. One of the major use cases for real-time streaming is [live captioning](https://blog.deepgram.com/closed-captioning-companies-use-asr/). As speakers talk, text is generated and displayed on the screen. Real-time streaming can also transcribe or caption pre-recorded media that's presented during an event.\n\n## How Does Real-Time Streaming Transcription Work?\n\nReal-time streaming is similar to pre-recorded transcription. The audio goes through the same [speech model](https://offers.deepgram.com/how-ai-speech-models-work-whitepaper) for transcriptions but the audio input and output is configured differently. Input is sent through a live streaming protocol like websocket. The output can be text over websockets also. Adding speech understanding feature to the output stream is mostly done post transcription processing.  Post transcription processing can include [diarization](https://blog.deepgram.com/what-is-speaker-diarization/), [profanity filtering](https://developers.deepgram.com/documentation/features/profanity-filter/), [redaction](https://developers.deepgram.com/documentation/features/redact/), and other features depending on the speech recognition and understanding provider. Here's a code sample of how we do real-time transcription with Deepgram: \n\n![](https://res.cloudinary.com/deepgram/image/upload/v1661976862/blog/all-about-transcription-for-real-time-audio-streaming/code_snippet.png)\n\nYou can get the full instructions and code in our [streaming quickstart guide](https://developers.deepgram.com/documentation/getting-started/streaming/).\n\n## Why is Live Audio Streaming Transcription Used?\n\nReal-time streaming transcription is used to get immediate transcriptions of an audio stream, which is then provided to a human reader or a machine. For a human reader, this is called [live captioning](https://blog.deepgram.com/closed-captioning-companies-use-asr/). The text appears within seconds of the speaker finishing a word. [Captioning](https://blog.deepgram.com/closed-captioning-companies-use-asr/) has many benefits, but one compelling example is to allow hearing-impaired individuals to follow what a speaker is saying.\n\nFor a machine, real-time streaming transcription can be used to transcribe a user's audio responses to an IVR, [voicebot](https://deepgram.com/solutions/voicebots/), or virtual assistant to continue a human-to-machine conversation. Real-time streaming can also be used to search for important information in a conversation as it's happening, and then provide a contact center agent with on-screen tips or hints to help solve a customer's issues or recommend an upsell.\n\n## What Are the Use Cases for Real-Time Streaming Transcription?\n\nAs noted above, live audio streaming has a number of human and machine use cases, including:\n\n* **Agent assistance** - Having an AI read the transcription data can provide support suggestions and upsell recommendations to an agent on the line in real time.\n* **IVR/Voicebots/Virtual assistants** - Quickly transcribe a user's responses so the AI can determine what is said and the intent of it in order to respond quickly and accurately.\n* **Live captioning -** Provide captioning of a live event, [lecture](https://blog.deepgram.com/classroom-captioner/), concert, or webinar for the hearing impaired or others who prefer reading instead of just listening. This can be for in-person or online participants.\n* **Meeting summary and analytics** - Transcribing and analyzing a meeting in real-time allows quicker post-meeting actions, i.e., action items identified, meeting summary shared, and any sales coaching opportunities identified.\n* **Personal captioning** - Provide captioning so that a hearing-impaired patient can understand what's happening.\n* **Real-time analytics** - Stream the audio for transcription and analysis so any issues can immediately be resolved, for example, if an agent did not repeat the compliance statement.\n* **Sales enablement** - Stream the transcription to an AI to gauge the salesperson's sales pitch and recommend better closing tactics immediately after a call or meeting.\n* **Video Gaming** - Stream the conversations between players for easier communication and to monitor inappropriate language.\n\n## What Are the Metrics for Real-Time Streaming?\n\nSimilar to transcribing pre-recorded audio, accuracy is the main metric. **Word Error Rate (WER)** is perhaps the most commonly used quantitative metric for evaluating and comparing the performance of speech-to-text (STT) solutions for accuracy. It is defined as:\n\n> WER = S + D + I / N\n\n* S is the number of word substitutions\n* D is the number of word deletions\n* I is the number of word insertions\n* N is the number of total words in ground truth transcript (Ground truth transcript is a human transcribed passage)\n\nThe lower the WER, the better the transcription is in terms of word accuracy. Accuracy is 1 - WER. We asses that if WER is below 20%, transcripts tend to be more human readable as humans can decipher mistakes and fill in missing words. Anything over 20% would be difficult to read and understand. For machines, our goal is to have a WER of below 10%, which is what we have heard is necessary from our voicebot customers. Another metric used to gauge accuracy is **Word Recall Rate/Word Recognition Rate (WRR)**. WRR measures the percentage of words in the ground truth text that were correctly predicted, or matched (i.e., true positives). This does not include insertions (where there isn't a word in the ground truth, but the transcription has one).\n\n> WRR = # word matches / # of words\n\nThe higher the WRR, the better the transcription is in terms of word accuracy. WRR is again not the perfect metric as it does not include incorrect insertion of words. We assess that if WRR is over 80%, it also tends to be human readable. The third most important metric for real-time streaming is **automatic speech recognition (\\*\\***ASR) latency**\\*\\*,** which is how fast a word appears after the speaker says the word. Unfortunately, latency has various components that you may or may not be able to control. A simple latency formula for real-time streaming in the cloud is:\n\n> Latency (milliseconds) = ASR latency + Internet latency\n\n* ASR latency is how long it takes to transcribe the audio.\n* Internet latency is how long it takes for the audio data to get to the ASR servers and how long it takes the text data to get back to you.\n\nFor example, if you are building a voicebot, you want to minimize the wait time of the user from when a word is spoken to when the AI voicebot responds. Within that wait time, there are [a lot of processes](https://blog.deepgram.com/tips-on-choosing-a-conversational-ai-development-path/) going on. If you want a true human-like voicebot experience, all processing must be done in milliseconds; thus, the ASR latency of that process must be in milliseconds. Internet latency can be minimized with an on-premises deployment of the ASR solution.\n\n## Real-time Streaming Transcription with Deepgram versus Others\n\nNow that you know more about how to gauge real-time streaming transcriptions. Here are the major benefits of Deepgram over other ASR providers:\n\n* **Highest accuracy** - Our various speech models have shown to be highly accurate for various use cases. [Hear and see](https://deepgram.com/asr-comparison/) our accuracy results versus Google and Amazon.\n* **Lowest ASR latency** - Our latency is under 300 milliseconds, which is faster than all other competitors. Amazon, for example, has a latency of 2-3 seconds.\n* **Support for all language and use-case models** - Our real-time streaming supports all our [languages](https://deepgram.com/product/languages/) and [use-case models](https://deepgram.com/product/use-cases/). Some ASR providers limit you to a few languages or use case models.\n* **Choice of deployment** - We offer cloud or on-premise deployments to reduce any Internet latency. All with the same accuracy, features, and tools but in a Docker container.\n\nGet started with our real-time streaming [guide](https://developers.deepgram.com/documentation/getting-started/streaming/), [SDKs](https://developers.deepgram.com/sdks-tools/), or our [API references](https://developers.deepgram.com/api-reference/)—or you can immediately try out our real-time features on our [Console](https://console.deepgram.com/)-sign up and get $150 in credits to give Deepgram a try.\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-how-deepgram-works\"></WhitepaperPromo>";
						}
						async function compiledContent$45() {
							return load$45().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$45() {
							return (await import('./chunks/index.4cdd4bb1.mjs'));
						}
						function Content$45(...args) {
							return load$45().then((m) => m.default(...args));
						}
						Content$45.isAstroComponentFactory = true;
						function getHeadings$45() {
							return load$45().then((m) => m.metadata.headings);
						}
						function getHeaders$45() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$45().then((m) => m.metadata.headings);
						}

const __vite_glob_0_15 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$45,
  file: file$45,
  url: url$45,
  rawContent: rawContent$45,
  compiledContent: compiledContent$45,
  default: load$45,
  Content: Content$45,
  getHeadings: getHeadings$45,
  getHeaders: getHeaders$45
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$44 = {"title":"Collaborative Augmented Reality Note-Taking with AiRNote","description":"The team behind AirNote utilized Deepgram's Speech Recognition API to create an app for generating, editing, and customizing AR sticky notes in a collaborative environment. Read more here.","date":"2022-04-28T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1651110485/blog/2022/04/ar-note-taking-airnote/cover.jpg","authors":["kevin-lewis"],"category":"project-showcase","tags":["hackathon","augmented-reality"],"seo":{"title":"Collaborative Augmented Reality Note-Taking with AiRNote","description":"The team behind AirNote utilized Deepgram's Speech Recognition API to create an app for generating, editing, and customizing AR sticky notes in a collaborative environment. Read more here."},"shorturls":{"share":"https://dpgr.am/9c55e61","twitter":"https://dpgr.am/324762a","linkedin":"https://dpgr.am/79a2e5a","reddit":"https://dpgr.am/0e48208","facebook":"https://dpgr.am/72043a0"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454058/blog/ar-note-taking-airnote/ograph.png"}};
						const file$44 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/ar-note-taking-airnote/index.md";
						const url$44 = undefined;
						function rawContent$44() {
							return "\r\nThe team behind AirNote wanted to bring a fun and novel way to collaborate around note-taking. I sat down with [Peter Zhang](https://pzhang.net), [Shuntian Liu](https://www.linkedin.com/in/shuntian-liu-27b839170/), [Sudarshan Sreeram](https://www.linkedin.com/in/sudarshan-sreeram/), and [Tom Zhao](https://www.linkedin.com/in/zhaoxuan0914/) to ask them about their project.\r\n\r\nThis was a project of many 'firsts' for the team, who made the decision to try out technologies they had not built with before.\r\n\r\nSudarshan tells me that their interest in Augmented Reality goes back to attending the Apple Worldwide Developer Conference in 2018, where they had a [table-based AR game with iPads](https://developer.apple.com/documentation/arkit/swiftshot_creating_a_game_for_augmented_reality). \"AR was an area we wanted to experiment with and often gets overlooked in the wider set of iOS features.\" he says.\r\n\r\nTom continues, \"We explored a few demos from the Apple Developer site. They have an AR game project demo, so we explored building a game first, and that naturally evolved into a shared AR workspace.\"\r\n\r\nHaving seen our [Deepgram 5-minute demo](https://blog.deepgram.com/live-transcription-mic-browser/), they had the idea for AiRNote: an interactive, collaborative note-taking application using Augmented Reality.\r\n\r\n![An iPhone being held with the camera open pointing at a table. On the table is a green post-it note. Overlaid is a text box with the current speech being displayed, and an 'add note' button at the bottom of the screen.](https://res.cloudinary.com/deepgram/image/upload/v1651110488/blog/2022/04/ar-note-taking-airnote/airnote-pic.png)\r\n\r\n## Building AirNote\r\n\r\nShuntian was in charge of building the app's integration with Deepgram's Speech Recognition API and used our [Live Transcriptions with iOS](https://blog.deepgram.com/ios-live-transcription/) blog post as a starting point. Thankfully, many of the snippets from our iOS post could be copied and pasted, meaning less time learning to use Deepgram and more time focusing on complex parts of their project.\r\n\r\nBecause there were so many new things for the AiRNote team to learn, they had to overcome a number of challenges, including learning how to write a native iOS app with ARKit and RealityKit, how to use Blender to create 3D models of sticky notes and pins, and how to collaborating on an XCode project - which, I'm told, was not the easiest thing to do while using git version control.\r\n\r\nI only know the most basic concepts in iOS development, but Sudarshan also spoke about an experience I have encountered first-hand - for anything but the most simple apps, the Apple documentation forces developers to go down many rabbit holes to learn the pre-requisite skills needed to be successful.\r\n\r\nThe team managed to overcome their challenges and build a compelling demo that I had the pleasure to try out. For further development, the team wants to add more ways of interacting with other users, such as supporting drawings, file sharing, or even the ability to share any AR object in the session.\r\n\r\nYou can see the code behind [AiRNote on GitHub](https://github.com/lambda-shuttle/Airnote).\r\n\r\n        ";
						}
						async function compiledContent$44() {
							return load$44().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$44() {
							return (await import('./chunks/index.874fad87.mjs'));
						}
						function Content$44(...args) {
							return load$44().then((m) => m.default(...args));
						}
						Content$44.isAstroComponentFactory = true;
						function getHeadings$44() {
							return load$44().then((m) => m.metadata.headings);
						}
						function getHeaders$44() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$44().then((m) => m.metadata.headings);
						}

const __vite_glob_0_16 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$44,
  file: file$44,
  url: url$44,
  rawContent: rawContent$44,
  compiledContent: compiledContent$44,
  default: load$44,
  Content: Content$44,
  getHeadings: getHeadings$44,
  getHeaders: getHeaders$44
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$43 = {"title":"Why ASR is Important for the Deaf and Hard-of-Hearing Community","description":"This blog post provides some information about the deaf and hard-of-hearing community, including challenges, as well as how ASR can help.","date":"2022-04-08T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981415/blog/asr-important-deaf-hoh-community/why-asr-important-deaf-hoh-community-thumb-554x220.png","authors":["sam-zegas"],"category":"identity-and-language","tags":["heritage","language"],"seo":{"title":"Why ASR is Important for the Deaf and Hard-of-Hearing Community","description":"This blog post provides some information about the deaf and hard-of-hearing community, including challenges, as well as how ASR can help."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981415/blog/asr-important-deaf-hoh-community/why-asr-important-deaf-hoh-community-thumb-554x220.png"},"shorturls":{"share":"https://dpgr.am/cf4c59a","twitter":"https://dpgr.am/3f8b2e9","linkedin":"https://dpgr.am/71af4b0","reddit":"https://dpgr.am/4500fa1","facebook":"https://dpgr.am/14d41c4"}};
						const file$43 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/asr-important-deaf-hoh-community/index.md";
						const url$43 = undefined;
						function rawContent$43() {
							return "Speech recognition and speech understanding technologies are powerful tools for everyone. But for many people with hearing, the implications of speech technology on the deaf community are not immediately obvious. In recognition of [Deaf History Month](https://www.nad.org/2022/01/31/national-deaf-history-month-dates/), we wanted to take stock of the ways that speech technology is being used to serve the deaf and hard-of-hearing (HOH) community, as well as the places where more work is needed.\n\n## Who Are Deaf People?\n\nAccording to the [World Health Organization](https://www.who.int/news-room/fact-sheets/detail/deafness-and-hearing-loss), over 5% of the world's population has significant hearing loss, defined by the WHO as \"hearing loss greater than 35 decibels (dB) in the better hearing ear\". That's roughly 432 million adults and 34 million children. Within the United States alone, the US [National Institutes of Health](https://www.nidcd.nih.gov/health/statistics/quick-statistics-hearing) estimates that there are about 28 million adults between the ages of 20 and 69 living with some degree of hearing loss, or about 14% of the population in that age group. Deaf and HOH people communicate in a variety of different ways, depending on the severity of hearing loss, the age at which hearing loss began, and the educational opportunities available to them. Some people with hearing loss may use spoken language with or without the help of hearing aids. Written communication is a useful tool for connecting across hearing-related language barriers.\n\nAdditionally, some use lip reading to improve their understanding of spoken language. But the means of communication that's the most familiar to people with hearing are signed languages, including American Sign Language, or ASL, in the US. An important linguistic note is in order before we get any further. Signed languages are full-fledged languages, and not just gestures or pantomime. They have complete grammars, rules for use, and communicate all of the same ideas, and at the same level of depth and clarity, that spoken languages do. Something that many hearing people don't realize is that there is quite a bit of diversity across sign languages around the world. There is no universal sign language. ASL is prevalent in the United States and Canada as well as about 20 other countries around the world.\n\nASL itself is a descendant of a French Sign Language (FSL) that developed in the mid 1700s and which laid the groundwork for many modern sign languages used throughout Europe and the Americas. These sign languages in the French family tree may not be mutually intelligible, but they resemble each other much more closely than they resemble other sign languages. For example, British Sign Language (BSL) developed independently of French Sign Language and is markedly different from it. BSL and its relatives are used in many former British colonies aside from the US and Canada.\n\nAs a result, a deaf person from the US and a deaf person from the UK could communicate in written English but would not not be able to communicate in their respective sign languages, whereas a ASL speaker might be able to use some signed communication with a speaker of French Sign Language, despite not being able to use the same written language. If you want to dive in and learn more about signed languages around the world, you can check out a map of global sign language families [here](https://en.wikipedia.org/wiki/Sign_language#/media/File:Sign_language_families.svg).\n\n## Pandemic Challenges for the Deaf and Hard-of-Hearing Communities\n\nThe pandemic created additional challenges for people who are deaf or hard of hearing. The masking requirements that began in early 2020 created a sudden disruptive problem for people who relied on lip reading to communicate. The need to control the spread of an airborne disease while maintaining a line of sight to the speaker's lips led to the creation of [see-through masks](https://www.hearinglikeme.com/what-to-know-about-clear-medical-masks-for-lip-reading/), but their adoption was limited. Here, automatic speech recognition can be part of the solution. We were excited to see a recent project from [Zack Freedman](https://www.youtube.com/c/ZackFreedman). He created a hoodie with an embedded screen that displayed everything he said, transcribed by Deepgram.\n\n<iframe title=\"YouTube video player\" src=\"https://www.youtube.com/embed/mTK8dIBJIqg\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\nKevin Lewis, a Senior Developer Advocate at Deepgram, has developed a similar piece of [wearable tech that uses Deepgram's speech recognition API to create real-time transcripts of spoken language](https://twitter.com/_phzn/status/1478504867584958464?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1478821408486699009%7Ctwgr%5E%7Ctwcon%5Es2_&ref_url=https%3A%2F%2Fwww.tomshardware.com%2Fnews%2Fraspberry-pi-speech-to-text-ai-badge) in any of Deepgram's languages. These projects are not only useful for people with hearing loss, but also for anyone who has ever struggled to understand speech through a mask, or simply struggled to understand a conversation partner in a loud space.\n\nDevices for in-person transcription are not the only way that the deaf and HOH communities can benefit from speech recognition. As the world has shifted more toward virtual business, these communities benefit from automating captioning on meeting platforms. Real-time automatic transcription is a major focus for Deepgram. For example, we offer [highly accurate real-time transcription at low cost per audio hour](https://developers.deepgram.com/use-cases/realtime-meeting-transcription/) and our product is [easy to integrate with meetings platforms, such as Zoom](https://developers.deepgram.com/sdks-tools/tools/integrate-zoom/).\n\n## The Future of Voice Technology for the Deaf\n\nAt Deepgram, we're always interested to see [projects that attempt to convert sign language into written text](https://www.vice.com/en/article/zmgnd9/app-to-translate-sign-language). These projects have strong parallels to what we do, which is use [deep neural networks to convert audio inputs of spoken language into written text](https://blog.deepgram.com/deep-learning-speech-recognition/)-but in the case of signed languages, the inputs are visual patterns (as with an augmented reality app) or movement-based inputs (as with a wearable sensor system). Many such projects in the past have failed to account for the full range of expression that goes into sign languages, and American Sign Language in particular. Hearing people tend to over-focus on the more obvious movements of the hands and arms while missing crucial grammatical information conveyed by facial expressions-such as highlighting the intended topic of a statement by raising the eyebrows while the hand sign is made. ASL also has a notably different sentence structure than spoken English.\n\nIn fact, linguists do not consider ASL \"a way of speaking English\" at all. ASL is its own independent language with its own unique grammar and vocabulary, which means that converting ASL to written English actually requires *translation* rather than transcription. This is made even more challenging by the fact that there is no agreed-upon way of writing ASL. Although various systems have been created in the past, none have caught on with the deaf community, and aren't widely used outside of small academic circles.\n\n## Current Challenges for the Deaf Community\n\nDespite efforts to develop technology to assist the deaf and HOH community, challenges remain. Many of these challenges stem from language barriers between deaf people and hearing people. These language barriers make it more difficult for deaf and HOH people to access education and employment, and as a result, can also impact access to health insurance and thus medical care-all of which lead in turn to higher rates of poverty. It's estimated that [about 20% of working-age deaf and HOH adults in the US live in poverty](https://www.verywellhealth.com/what-challenges-still-exist-for-the-deaf-community-4153447), almost double the rate of adults with hearing.\n\nAnd social challenges also exist for the deaf and HOH community. Without a shared means of communication with their peers-and, in some cases, even within their family-people who are deaf or hard of hearing can find themselves socially isolated. For anyone interested in learning more about issues that impact the deaf and HOH people, there are many organizations working to support and empower the community. A useful list of organizations can be found [here](https://clerccenter.gallaudet.edu/national-resources/info/info-to-go/national-resources-and-directories/organizations.html).\n\n## Wrapping Up\n\nIf you're working on a project to transcribe speech for the deaf and HOH community and would like to learn more about how Deepgram can help, [feel free to reach out](https://deepgram.com/contact-us/), or [sign up for a free API key](https://console.deepgram.com/signup) to give the system a try on your own.";
						}
						async function compiledContent$43() {
							return load$43().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$43() {
							return (await import('./chunks/index.02b91561.mjs'));
						}
						function Content$43(...args) {
							return load$43().then((m) => m.default(...args));
						}
						Content$43.isAstroComponentFactory = true;
						function getHeadings$43() {
							return load$43().then((m) => m.metadata.headings);
						}
						function getHeaders$43() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$43().then((m) => m.metadata.headings);
						}

const __vite_glob_0_17 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$43,
  file: file$43,
  url: url$43,
  rawContent: rawContent$43,
  compiledContent: compiledContent$43,
  default: load$43,
  Content: Content$43,
  getHeadings: getHeadings$43,
  getHeaders: getHeaders$43
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$42 = {"title":"Asynchronous Logic to Write a Vue 3 and Deepgram Captions Component","description":"In this segment, learn how to use Vue 3 composables to power a text-captions component that integrates with Deepgram's speech-to-text API. Read more here!","date":"2022-04-01T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1648829347/blog/2022/03/asynchronous-logic-to-write-a-vue-3-and-deepgram-captions-component/Building-Livestreaming-w-AmazonIVS.jpg","authors":["sandra-rodgers"],"category":"tutorial","tags":["aws","javascript","serverless"],"seo":{"title":"Asynchronous Logic to Write a Vue 3 and Deepgram Captions Component","description":"In this segment, learn how to use Vue 3 composables to power a text-captions component that integrates with Deepgram's speech-to-text API. Read more here!"},"shorturls":{"share":"https://dpgr.am/5f3d18c","twitter":"https://dpgr.am/6dfa73e","linkedin":"https://dpgr.am/27c8482","reddit":"https://dpgr.am/ec771f4","facebook":"https://dpgr.am/cb3aca0"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454007/blog/asynchronous-logic-to-write-a-vue-3-and-deepgram-captions-component/ograph.png"}};
						const file$42 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/asynchronous-logic-to-write-a-vue-3-and-deepgram-captions-component/index.md";
						const url$42 = undefined;
						function rawContent$42() {
							return "\nThis is the final post of my series, \"How to Build a Live Streaming Web Application with Amazon IVS and Deepgram.\"\n\n<Panel type=\"info\" title=\"Build a Live Streaming Web Application with Amazon IVS and Deepgram (SERIES)\">\n<ol> \n<li><a href=\"https://blog.deepgram.com/build-a-livestream-web-application-with-amazon-ivs-and-deepgram/\">How to Build a Live Streaming Web Application with Amazon IVS and Deepgram</a></li>\n<li><a href=\"https://blog.deepgram.com/build-a-livestream-web-application-vue-and-express-setup/\"> Build a Live Streaming Web Application: Vue and Express Setup</a></li>\n<li><a href=\"https://blog.deepgram.com/how-to-write-vue-3-composables-for-a-third-party-API-integration/\"> How to Write Vue 3 Composables for a Third-Party API Integration</a></li>\n\n<li><a href=\"https://blog.deepgram.com/asynchronous-logic-to-write-a-vue-3-and-deepgram-captions-component/\"> Asynchronous Logic to Write a Vue 3 and Deepgram Captions Component</a></li>\n</ol>\n</Panel>\n\nFor today's post, I recommend getting a Deepgram API key to use in this project, which anyone can get by heading to the [Deepgram console](https://console.deepgram.com/signup?jump=keys).\n\n## Introduction - Async and the Composition API\n\nToday's post will cover how to use Vue 3 composables to power a text-captions component that integrates with Deepgram's speech-to-text API. Some of the things I'll cover today are:\n\n*   Using `async` and `await` to write a composable that fetches a temporary API key from Deepgram.\n*   Using Vue 3's `watch` method to react to data that is updating in real-time as Deepgram sends a text transcription back through a browser WebSocket.\n*   Writing logic that is sensitive to the order things occur - i.e., asynchronous logic that flows between the component and the composable.\n\nThis post assumes some knowledge of Vue 3, in particular Vue composables. For a refresher on Vue 3, check out my series [Diving Into Vue 3](https://blog.deepgram.com/diving-into-vue-3-getting-started/).\n\nToday I will build the `AudioCaptions.vue` component. (For the `VideoPlayer` component, see my [previous post](https://blog.deepgram.com/how-to-write-vue-3-composables-for-a-third-party-API-integration/#composable-to-bring-in-an-external-script) in the series.) Here is the page with minimal styling. I've put a red box around the `AudioCaptions.vue` component:\n\n![StreamChannel page emphasizing AudioCaptions.vue component](https://res.cloudinary.com/deepgram/image/upload/v1648829348/blog/2022/03/asynchronous-logic-to-write-a-vue-3-and-deepgram-captions-component/StreamChannel_captions.png)\n\nWhere it says \"Deepgram Not Connected,\" there will be text captions that display in real-time along with the video stream.\n\nHere is a diagram of what I will build today:\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1648829348/blog/2022/03/asynchronous-logic-to-write-a-vue-3-and-deepgram-captions-component/AudioTranscription_Final.png\" alt=\"Audio Transcription Feature Diagram\" style=\"width: 75%; margin:auto;\">\n\nThis feature will rely on Vue 3's Composition API, especially Vue Composables, to put Deepgram captions on the screen.\n\n## Composables and Asynchronous Logic\n\nComposables are a feature of the Vue 3 Composition API; **custom composables** are the ones I build myself with the intention of encapsulating reusable, stateful logic.\n\nI feel like it is somewhat of an art learning how to write composables. The key to writing them well is making them as generic as possible so that they can be reused in many contexts.\n\nFor example, I could write a function that does everything I need it to do to create text captions on the screen using the Deepgram API - the function would include logic to get an API key, turn on the browser microphone, get the audio stream from the microphone, and then send the stream through a WebSocket. I could call the composable `useDeepgram`.\n\nHowever, there are several logical concerns in that one large `useDeepgram` function that could be broken out into other composable functions. While it's easier just to write it all in one file, it means I could only use it in situations that are exactly like this project.\n\nThe challenge of breaking it apart is that the logic to get the transcription from Deepgram depends on certain things happening first, such as the API key arriving and the microphone being turned on. When I break that logic apart into separate functions, I have to be conscious of the order that those functions run, the state that gets updated in multiple functions (and making sure the functions stay in sync), and the conventions for writing asynchronous logic. Not to mention the challenge of updating the component in real-time with the data that comes through the WebSocket.\n\nThe point is that writing composables in the real world can be challenging, so learning some strategies for dealing with more complicated situations, particularly asynchronous logic, is worth it. Because the beauty of composables is that if you write them well, you have a clean, reusable function that you'll return to again and again.\n\n## Composable Using Async and Await\n\nHere is the `AudioCaptions.vue` component right now, before I add the feature logic:\n\n```js\n<template>\n  <div>\n    <p>Status Will Go Here</p>\n  </div>\n</template>\n\n<script>\nexport default {\n  setup() {\n    return {};\n  },\n};\n</script>\n```\n\nIn the template where it says \"Status Will Go Here,\" I plan to add a reactive variable. That value will update to show the audio captions after everything is working. For now, I've just hard-coded that text.\n\n### useDeepgramKey Composable\n\nThe first composable I'm going to write will be called `useDeepgramKey.js`, and its purpose will be to fetch a temporary API key. If I fetch a temporary API key from Deepgram, I can use the key in the browser and not worry about exposing the key since the key will expire almost immediately. Read more about this feature in a blog post that Kevin wrote about [protecting your Deepgram API key](https://blog.deepgram.com/protecting-api-key/).\n\nOn the backend, I have set up an endpoint to receive the fetch request from the composable. That endpoint can be seen in the `server.js` file in my repo [here](https://github.com/deepgram-devs/livestream-amazonIVS-and-deepgram/blob/deepgram-composables/server.js).\n\nNow I'll create the `useDeepgramKey.js` composable.\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1648829348/blog/2022/03/asynchronous-logic-to-write-a-vue-3-and-deepgram-captions-component/useDeepgramKey.png\" alt=\"Create useDeepgramKey.js file in Composables folder\" style=\"width: 50%; margin:auto;\">\n\n### Tip #1 - Use async and await to write a composable that returns a promise.\n\nI will do three things to make this composable run asynchronously:\n\n1.  Write the composable as an async function using `export default async` to make the composable itself know to wait for the fetch request to finish.\n\n2.  Encapsulate the fetch request in its own async function called `async function getKey()`, which is called inside the composable using the `await` keyword.\n\n3.  In the component `AudioCaptions`, use a `.then()` when I call the composable so that I get access to the returned state after the Promise completes.\n\nHere is the composable to start. The `key` will update to be the API key when that arrives from the backend, and `DGStatus` will update with a message if there is an error.\n\n```js\nimport { ref } from 'vue'\nlet key = ref('')\nlet DGStatus = ref('Deepgram Not Connected')\n\nexport default async function useDeepgramKey() {\n  return { key, DGStatus }\n}\n```\n\nNow I'll write an async function that will perform all the logic of getting the temporary key. I'll name it `getKey()` and I will use a try-catch block to make the fetch request and handle any errors:\n\n```js\nasync function getKey() {\n  try {\n    const res = await fetch('http://localhost:8080/deepgram-token', {\n      headers: { 'Content-type': 'application/json' },\n    })\n    if (res) {\n      const response = await res.json()\n      // update with temporary api key:\n      key.value = response.key\n      return key\n    }\n  } catch (error) {\n    if (error) {\n      // update to show error message on screen:\n      DGStatus.value = 'Error. Please try again.'\n    }\n  }\n}\n```\n\nTo make sure this runs, I need to call the function in the composable. I will add `await getKey()` to the async function that will be exported. Using `await` is to go along with `async` that I used on the composable function itself. These two keywords together tell the composable that it must wait until the `getKey` function resolves.\n\nHere is the composable in its entirety:\n\n```js\nimport { ref } from 'vue'\nlet key = ref('')\nlet DGStatus = ref('Deepgram Not Connected')\n\nasync function getKey() {\n  try {\n    const res = await fetch('http://localhost:8080/deepgram-token', {\n      headers: { 'Content-type': 'application/json' },\n    })\n    if (res) {\n      const response = await res.json()\n      // update with temporary api key:\n      key.value = response.key\n      return key\n    }\n  } catch (error) {\n    if (error) {\n      // update to show error message on screen:\n      DGStatus.value = 'Error. Please try again.'\n    }\n  }\n}\n\nexport default async function useDeepgramKey() {\n  // call function:\n  await getKey()\n  return { key, DGStatus }\n}\n```\n\nI can `console.log(key.value)` to make sure the key is arriving successfully.\n\nThen I'll go back to `AudioCaptions.vue` to wire up a reactive reference that will update to show the error status message if the key does not arrive. I'll create a `ref` called `deepgramStatus` and replace the hardcoded \"Status Will Go Here\" with that variable.\n\n```js\n<template>\n  <div>\n    <p>{{ deepgramStatus }}</p>\n  </div>\n</template>\n\n<script>\nimport { ref } from \"vue\";\nexport default {\n  setup() {\n    let deepgramStatus = ref(\"Deepgram Not Connected\");\n    return { deepgramStatus };\n  },\n};\n</script>\n```\n\n**I also need to call the composable function in the component.** If I don't call it, the logic won't run. Since it is a promise, I will use a `.then()` method on it to get the result, which will be the `key` and `DGStatus` values. I only need the `DGStatus`, so I'll set that to the `deepgramStatus` ref.\n\nHere's the `AudioCaptions.vue` script now:\n\n```js\n<script>\nimport { ref } from \"vue\";\nimport useDeepgramKey from \"@/composables/useDeepgramKey\";\nexport default {\n  setup() {\n    let deepgramStatus = ref(\"Deepgram Not Connected\");\n\n    // use .then() to wait for promise resolution\n    useDeepgramKey().then((res) => {\n      deepgramStatus.value = res.DGStatus.value;\n    });\n\n    return { deepgramStatus };\n  },\n};\n</script>\n```\n\nIf I want to see the error message, I can delete a character in the fetch request URL, making it `http://localhost:8080/deepgram-toke`, which is incorrect. That will cause the fetch request to fail, and I'll see the error message.\n\n![Error. Please Try Again.](https://res.cloudinary.com/deepgram/image/upload/v1648829348/blog/2022/03/asynchronous-logic-to-write-a-vue-3-and-deepgram-captions-component/error-message.png)\n\n## Composable That Relies on Asynchronous Events in Other Composables\n\nNow I will begin to tackle the `useDeepgramSocket` composable. This composable will take an audio stream from the browser microphone and send it to Deepgram by way of a browser WebSocket. It relies on two other composables to do this:\n\n1.  `useDeepgramKey` - I need to get the temporary API key from the composable I just made, `useDeepgramKey`, to send it in the request to Deepgram; otherwise, Deepgram won't be able to fulfill the request.\n\n2.  `useMicrophone` - I need to get an audio stream from the browser microphone. That audio data will be sent to Deepgram to be transcribed into text that will be put onto the screen as captions.\n\nI haven't created the `useMicrophone` composable yet, so I'll make a quick detour right now to write that composable.\n\n### useMicrophone Composable\n\nThe `useMicrophone` composable will rely on the browser Media Stream API and the `getUserMedia` method to request permission to use the browser microphone of the user and pull the audio from it. Since there are several other blog posts in [Deepgram Docs](https://developers.deepgram.com/) about this nifty API, I won't go into detail about how it works. Check out [Brian's post](https://blog.deepgram.com/getting-started-with-mediastream-api/) for a general introduction to it.\n\nThis composable is also going to use an `async` function since the `getUserMedia` method requires waiting for the user to give permission to use the microphone. The time involved means that this method returns a promise. I already know how to write this type of composable since I just did it in the last section.\n\nI'll make the composable an `async` function and I'll also write the logic to get the audio stream as an `async` function. Here is the composable in its entirety:\n\n```js\nasync function getAudio() {\n  try {\n    const mediaStream = await navigator.mediaDevices.getUserMedia({\n      audio: true,\n    })\n    const mediaRecorder = new MediaRecorder(mediaStream, {\n      audio: true,\n    })\n    return mediaRecorder\n  } catch (e) {\n    console.error(e)\n  }\n}\n\nexport default async function useMicrophone() {\n  const microphone = await getAudio()\n  return { microphone }\n}\n```\n\nNow it's ready for me to use in the next composable I will write.\n\n### useDeepgramSocket Composable\n\nFirst, I'll import this composable into `AudioCaptions.vue` and call it. That way, everything I write in `useDeepgramSocket` will run and I can check my progress as I build this composable.\n\n```js\n<script>\nimport { ref } from \"vue\";\nimport useDeepgramKey from \"@/composables/useDeepgramKey\";\nimport useDeepgramSocket from \"@/composables/useDeepgramSocket\";\nexport default {\n  setup() {\n    let deepgramStatus = ref(\"Deepgram Not Connected\");\n\n    useDeepgramKey().then((res) => {\n      deepgramStatus.value = res.DGStatus.value;\n    });\n\n    // call this so the composable runs as I work on it\n    useDeepgramSocket();\n\n    return { deepgramStatus };\n  },\n};\n</script>\n```\n\nI know I need to have access to the temporary API key from `useDeepgramToken` and to the microphone from `useMicrophone`. **I will start by setting up my composable to show that I have access to them within the same scope.**\n\nBoth composables return a promise. That means I will need to use syntax that will make the functions run but wait for the promise to resolve before moving on to the next thing.\n\n### Tip #2 - Use `.then()` to chain each composable that returns a promise to run asynchronously if returned values need to be in the same scope\n\nHere's what I mean:\n\n```js\nimport useDeepgramKey from './useDeepgramKey'\nimport useMicrophone from './useMicrophone'\n\nexport default function useDeepgramSocket() {\n  // chain .then() methods for each composable:\n  useDeepgramKey().then((keyRes) => {\n    useMicrophone().then((microphoneRes) => {\n      let apiKey = keyRes.key.value\n      let microphone = microphoneRes.microphone\n\n      console.log(apiKey)\n      console.log(microphone)\n\n      // WEBSOCKET FUNCTION WILL GO HERE\n    })\n  })\n  return {}\n}\n```\n\nI have named the result argument in each `.then()` a name that shows which composable they came from - `keyRes` and `microphoneRes`, which makes it easy for me to see what each of them represents. The `keyRes` is a `ref`, so I must drill all the way down to the `.value` property. The `microphoneRes` is a Vue 3 `readonly` property, which is why I don't have to drill down as far.\n\nNow that I have the values, I can write a function that encapsulates the logic to open the WebSocket.\n\n### openDeepgramSocket Function\n\nI will write a function called `openDeepgramSocket` that will do the following:\n\n*   Create the socket with `new WebSocket(URL, deepgram protocols)`.\n*   Open the socket with `socket.onopen`. When it opens, I'll add an event listener to the microphone to take in the audio stream and send it through the socket.\n*   Have `socket.onclose` listen for when the channel closes.\n\nI will also create a reactive reference called `DGStatus_socket` to update the status of the transcription along the way. That value will be returned to the `AudioCaptions.vue` component as the text captions.\n\nHere is the function:\n\n```js\nfunction openDeepgramSocket(apiKey, microphone) {\n  const socket = new WebSocket(\n    'wss://api.deepgram.com/v1/listen?punctuate=true',\n    ['token', apiKey]\n  )\n\n  socket.onopen = () => {\n    if (microphone.state != 'recording') {\n      DGStatus_socket.value = 'Connected to Deepgram'\n      console.log('Connection opened.')\n\n      microphone.addEventListener('dataavailable', async (event) => {\n        if (event.data.size > 0 && socket.readyState == 1) {\n          socket.send(event.data)\n        }\n      })\n\n      microphone.start(200)\n    }\n  }\n\n  socket.onmessage = (message) => {\n    const received = JSON.parse(message.data)\n    const transcript = received.channel.alternatives[0].transcript\n    if (transcript && received.is_final) {\n      DGStatus_socket.value = transcript + ''\n      // shows the transcript in the console:\n      console.log(DGStatus_socket.value)\n    }\n  }\n\n  socket.onclose = () => {\n    console.log('Connection closed.')\n  }\n}\n```\n\nI have to make sure to call the function in the composable:\n\n```js\nexport default function useDeepgramSocket() {\n  useDeepgramKey().then((keyRes) => {\n    useMicrophone().then((microphoneRes) => {\n      let apiKey = keyRes.key.value\n      let microphone = microphoneRes.microphone\n\n      // Call function:\n      openDeepgramSocket(apiKey, microphone)\n    })\n  })\n  return {}\n}\n```\n\nNow I see the transcript coming back to me because I have added a console.log to show it:\n\n![transcript returned in console with messages](https://res.cloudinary.com/deepgram/image/upload/v1648829348/blog/2022/03/asynchronous-logic-to-write-a-vue-3-and-deepgram-captions-component/transcript_console.png)\n\nI'm ready to put that transcript onto the screen as the captions!\n\n### Vue watch to Update Transcript Status\n\nI will use the reactive reference `DGStatus_socket` in the composable `useDeepgramSocket` to update the captions in `AudioCaptions.vue`. To do that, I need to return it from the composable and then destructure it in the component `AudioCaptions.vue`.\n\nHere is the `useDeepgramSocket` composable where I return the `DGStatus_socket` value (excluding the large `openDeepgramSocket` function):\n\n```js\nimport { ref } from \"vue\";\nimport useDeepgramKey from \"./useDeepgramKey\";\nimport useMicrophone from \"./useMicrophone\";\n\n// create status ref\nlet DGStatus_socket = ref(\"\");\n\nfunction openDeepgramSocket(apiKey, microphone) {\n...\n}\n\nexport default function useDeepgramSocket() {\n  useDeepgramKey().then((keyRes) => {\n    useMicrophone().then((microphoneRes) => {\n      let apiKey = keyRes.key.value;\n      let microphone = microphoneRes.microphone;\n\n      openDeepgramSocket(apiKey, microphone);\n    });\n  });\n\n  // return status ref to component\n  return { DGStatus_socket };\n}\n```\n\nIn `AudioCaptions.vue`, I destructure the `DGStatus_socket` so I have access to it:\n\n```js\nconst { DGStatus_socket } = useDeepgramSocket()\n```\n\nIs it working? Not yet. I have to update the `deepgramStatus` ref that is connected to the template if I want to see those captions on the screen.\n\n### Tip #3: Use watch to update a value in the component and trigger a side effect in-sync with that change\n\nAccording to the Vue documentation, `watch` is used in \"cases where we need to perform 'side effects' in reaction to state changes - for example, mutating the DOM or changing another piece of state based on the result of an async operation.\"\n\nThis example of putting the captions on the screen fits that description exactly. I want the `deepgramStatus` value to update if `DGStatus_socket` from the composable `useDeepgramSocket` changes, and I want that state change to trigger the effect of the text updating in the DOM.\n\nI will add a watcher to the `AudioCaptions` component:\n\n```js\nwatch(DGStatus_socket, () => {\n  deepgramStatus.value = DGStatus_socket.value\n})\n```\n\nAnd this is what the component in its entirety looks like now:\n\n```js\n<template>\n  <div>\n    <p>{{ deepgramStatus }}</p>\n  </div>\n</template>\n\n<script>\nimport { ref, watch } from \"vue\";\nimport useDeepgramKey from \"@/composables/useDeepgramKey\";\nimport useDeepgramSocket from \"@/composables/useDeepgramSocket\";\nexport default {\n  setup() {\n    let deepgramStatus = ref(\"Deepgram Not Connected\");\n\n    useDeepgramKey().then((res) => {\n      deepgramStatus.value = res.DGStatus.value;\n    });\n\n    const { DGStatus_socket } = useDeepgramSocket();\n\n    watch(DGStatus_socket, () => {\n      deepgramStatus.value = DGStatus_socket.value;\n    });\n\n    return { deepgramStatus };\n  },\n};\n</script>\n\n```\n\nAnd with that, I have my captions powered by Deepgram! Check out the code for this post on my repo branch [deepgram-composables](https://github.com/deepgram-devs/livestream-amazonIVS-and-deepgram/tree/deepgram-composables).\n\n![Captions working](https://res.cloudinary.com/deepgram/image/upload/v1648829348/blog/2022/03/asynchronous-logic-to-write-a-vue-3-and-deepgram-captions-component/captions.gif)\n\n## Conclusion\n\nToday I built the final component of my project, a full-stack video streaming application with text captions.\n\nThis post contained the barebones logic for the captions feature, but in my actual project, I have added styling to improve the user experience, and I've added buttons to turn the captions on or off. Check out the repo [here](https://github.com/deepgram-devs/livestream-amazonIVS-and-deepgram).\n\nHere is the final project:\n\n![Final project demo](https://res.cloudinary.com/deepgram/image/upload/v1648829348/blog/2022/03/asynchronous-logic-to-write-a-vue-3-and-deepgram-captions-component/VideoExample.gif)\n\nIt's been a great experience learning about Amazon IVS and Deepgram, and I've gotten the chance to get a better taste of how to take advantage Vue 3's composition API.\n\nIf you enjoyed this series, please follow me on [Twitter](https://twitter.com/sandra_rodgers_) to receive updates on future series I have in the works!\n\n        ";
						}
						async function compiledContent$42() {
							return load$42().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$42() {
							return (await import('./chunks/index.aaf5f3fb.mjs'));
						}
						function Content$42(...args) {
							return load$42().then((m) => m.default(...args));
						}
						Content$42.isAstroComponentFactory = true;
						function getHeadings$42() {
							return load$42().then((m) => m.metadata.headings);
						}
						function getHeaders$42() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$42().then((m) => m.metadata.headings);
						}

const __vite_glob_0_18 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$42,
  file: file$42,
  url: url$42,
  rawContent: rawContent$42,
  compiledContent: compiledContent$42,
  default: load$42,
  Content: Content$42,
  getHeadings: getHeadings$42,
  getHeaders: getHeaders$42
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$41 = {"title":"Add Live Speech Bubbles To YouTube Videos with Autobubble","description":"Using facial recognition and speech recognition to create live speech bubbles.","date":"2022-03-04T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1646039788/blog/2022/03/autobubble-youtube-speech-bubbles/AutoBubble.jpg","authors":["kevin-lewis"],"category":"project-showcase","tags":["facial-recognition","nodejs"],"seo":{"title":"Add Live Speech Bubbles To YouTube Videos with Autobubble","description":"Using facial recognition and speech recognition to create live speech bubbles."},"shorturls":{"share":"https://dpgr.am/54a290e","twitter":"https://dpgr.am/ae46666","linkedin":"https://dpgr.am/75df551","reddit":"https://dpgr.am/91c9f29","facebook":"https://dpgr.am/dbcb307"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454009/blog/autobubble-youtube-speech-bubbles/ograph.png"}};
						const file$41 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/autobubble-youtube-speech-bubbles/index.md";
						const url$41 = undefined;
						function rawContent$41() {
							return "\r\nBack in January, we supported Hack Cambridge - a 24-hour student hackathon. The team behind AutoBubble wanted to see if they could improve the display of captions for online videos. I sat down with [Andy Zhou](https://github.com/Flesh12138), [Conall Moss](https://www.linkedin.com/in/conall-moss-408037221/), [Dan Wendon-Blixrud](https://dwb.no), and [Lochlann-B Baker](https://github.com/Lochlann-B/) to ask them about their project.\r\n\r\n## The Project\r\n\r\n\"There were a lot of challenges and prompts at Hack Cambridge, but the Deepgram challenge was both the most flexible and the coolest\" explains Conall. \"We knew we were going to use it but then had to think of an idea.\"\r\n\r\nDan continues: \"A lot of speaker communication comes through facial expressions, and while closed captions are super useful, they are generally in a fixed position. We wanted to build a project which allows for captioning while allowing the depth of expression.\"\r\n\r\nWith that, AutoBubble was born. It is a Chrome extenion that uses facial recognition and Deepgram's Speech Recognition API to place captions next to a speaker's face in a YouTube video.\r\n\r\n![A screenshot of a video with a man speaking - head in the middle of the frame. To the right of his head is a sentence, with all-but-the-last word slightly bolded.](https://res.cloudinary.com/deepgram/image/upload/v1646039808/blog/2022/03/autobubble-youtube-speech-bubbles/screenshot.jpg)\r\n\r\n## First-Time Hackers\r\n\r\nThe team behind AutoBubble are all first-year Computer Science students at the University of Cambridge and, amazingly, were taking part in their very first hackathon. All of the team had the same sentiment. In the words of Lochlann:\r\n\r\n> I expected a super intense competitive event, but it was chill, social, and friendly. All of the activities aside from hacking led to an excellent experience and environment to build a project.\r\n\r\n## Building AutoBubble\r\n\r\nAs soon as the team landed on an idea, they broke it down into pieces and assigned work to each member. They created a shared document to detail what each of their modules would do and the expected inputs/outputs, making it much easier to glue the project together at the end.\r\n\r\nConall got to work on integrating Deepgram, and thanks to the [documentation](https://developers.deepgram.com/documentation/), [tutorials](https://blog.deepgram.com/categories/tutorial/), and [example projects](https://developers.deepgram.com/use-cases/), he could treat them as building blocks to build what they needed.\r\n\r\nMeanwhile, Lochlann started to work on facial recognition with [face-api.js](https://github.com/justadudewhohacks/face-api.js), which was a challenge. Still, the moment he overcame the hurdle and got it working, the team knew it provided many cool opportunities for this project.\r\n\r\nAndy built the simple but effective UI for the project, and described that \"care was put in to how the captions were styled, including a subtle indication of when a word in a phrase was said. A lot of balancing took place in the finer details of the captions themselves - making sure they weren't too long to be distracting, while also not too short that they disappear too quickly.\"\r\n\r\nDan built the Chrome extension to act as the glue for the project, and the team's shared document made this a lot easier. As a note, I have been involved with hundreds of hackathons, and I have never once seen a team be so intentional with documentation from the outset. It seems to have really paid off!\r\n\r\n## The Winner Is...\r\n\r\nThere were nearly 30 projects that incorporated Deepgram at Hack Cambridge, but this simple idea with a rock-solid execution was super impressive. Once the extension is installed, any YouTube video could start receiving these new captions, and they looked great.\r\n\r\nIf you're interested in seeing how AutoBubble was built, you can find the code across two repositories - one for the [server](https://github.com/dantechguy/autobubbleserver) and one for the [client](https://github.com/dantechguy/autobubbleclient).\r\n\r\n        ";
						}
						async function compiledContent$41() {
							return load$41().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$41() {
							return (await import('./chunks/index.b4737962.mjs'));
						}
						function Content$41(...args) {
							return load$41().then((m) => m.default(...args));
						}
						Content$41.isAstroComponentFactory = true;
						function getHeadings$41() {
							return load$41().then((m) => m.metadata.headings);
						}
						function getHeaders$41() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$41().then((m) => m.metadata.headings);
						}

const __vite_glob_0_19 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$41,
  file: file$41,
  url: url$41,
  rawContent: rawContent$41,
  compiledContent: compiledContent$41,
  default: load$41,
  Content: Content$41,
  getHeadings: getHeadings$41,
  getHeaders: getHeaders$41
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$40 = {"title":"How Automatic Speech Recognition Can Help Educators","description":"ASR in the classroom can support students with a wide range of learning needs. Learn how current tools are lacking and how ASR can help.","date":"2022-03-11T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981410/blog/automatic-speech-recognition-education/How-ASR-can-help-educators-thumb-554x220%402x.png","authors":["chris-doty"],"category":"speech-trends","tags":["education"],"seo":{"title":"How Automatic Speech Recognition Can Help Educators","description":"ASR in the classroom can support students with a wide range of learning needs. Learn how current tools are lacking and how ASR can help."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981410/blog/automatic-speech-recognition-education/How-ASR-can-help-educators-thumb-554x220%402x.png"},"shorturls":{"share":"https://dpgr.am/14164ee","twitter":"https://dpgr.am/b5fa773","linkedin":"https://dpgr.am/f826d8b","reddit":"https://dpgr.am/95e5d3b","facebook":"https://dpgr.am/f9161d2"}};
						const file$40 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/automatic-speech-recognition-education/index.md";
						const url$40 = undefined;
						function rawContent$40() {
							return "If you've ever been in a large classroom for a lecture or talk, you've probably experienced struggling to hear the speaker. Even with modern A/V equipment (which isn't present in every classroom), it can still sometimes be hard to hear what a speaker is saying. Now imagine you're not a native speaker of English. Or you have issues with your hearing. Or you have a learning disability that makes it hard to keep up with spoken speech. Paired with poor classroom acoustics, any one of these issues can make it difficult for students to succeed. As a university lecturer, I've encountered all of these problems in the classroom. And it's hard to figure out how best to help students who are struggling with these issues. In this blogpost, I want to review some of the ways that these issues are currently dealt with, and then look at a new solution, powered by [automatic speech recognition](https://blog.deepgram.com/what-is-asr/), that's the best option for creating a classroom that's accessible to everyone.\n\n## Current Solutions and Their Problems\n\nI'm not the first person to realize that classroom lectures can present challenges to people from different backgrounds. In this section, I want to review some of the more traditional approaches to classroom accessibility and why they fall short.\n\n### Posted Slides and Lecture Notes\n\nOne option that's become especially common in recent years is for professors to post their PowerPoint slides for students to review either before or after class. Although this can help, it doesn't address the fact that classes don't always follow the slides directly, and instructors often say other things in class (like answering questions, or reminding students about upcoming assignments).\n\n### Class Recordings\n\nAnother option is to record lectures, and post them online for students to watch after class. This option has been especially popular during the pandemic, as students may be quarantining and not able to attend in person. But most students don't have time to effectively attend the same class twice. And that's especially true with students who might be having trouble understanding the content or keeping up with coursework, regardless of the reason; asking them to attend every class session twice, while also keeping up on their assignments, simply doesn't work.\n\n### In-class Notetaker\n\nOne option for students with a documented disability is to have someone else in the class take notes for them-this is usually organized officially through the institution's disability services office, and pays another student in the class, who would be taking notes anyway, to share them with the student in question. This is better than some of the other options discussed. Since the content is coming directly from the lectures, students can use the notes afterwards to help clarify and understand course content. But this isn't available to all students (for example, non-native speakers), and it relies on the student taking notes to get it right. If they misunderstand something or get it wrong, that gets passed on to other students.\n\n## An Alternative: Live Class Transcripts\n\nWhat students need is the ability to revisit the material in a way that makes what happens in the classroom accessible to as many people as possible, and is something that can be used on the fly so that students have help understanding what is happening when it's happening. \n\nOur Senior Developer Advocate Kevin Lewis recently [built a tool to transcribe classroom lectures](https://blog.deepgram.com/classroom-captioner/), providing a live, automatically generated transcript to make classroom content more accessible. This method can also help people who have difficulty hearing the speaker, for whatever reason, or keeping track of what's been said and how the lecture is organized. Live transcripts are also useful for non-native speakers, who might hear a word they don't know, but not know how to spell it to look it up (a common problem with English)-and it works either during class from the live transcript, or by reviewing it later.\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-how-deepgram-works\"></WhitepaperPromo>\n\n## Conclusion\n\nAlthough there are lots of options for supporting students in the classroom, providing live transcriptions has definite advantages to support all students, regardless of their specific needs. If you'd like to see how in-class transcription can help you, [check out Kevin's blogpost with all the details](https://blog.deepgram.com/classroom-captioner/)—and a one-click deploy. If you've got other ideas for using speech recognition in the classroom, you can contact us or fill out our [Speech-to-text Self Assessment](https://deepgram.typeform.com/to/d3zTk2eI) to see if Deepgram might be able to help.";
						}
						async function compiledContent$40() {
							return load$40().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$40() {
							return (await import('./chunks/index.8453b92a.mjs'));
						}
						function Content$40(...args) {
							return load$40().then((m) => m.default(...args));
						}
						Content$40.isAstroComponentFactory = true;
						function getHeadings$40() {
							return load$40().then((m) => m.metadata.headings);
						}
						function getHeaders$40() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$40().then((m) => m.metadata.headings);
						}

const __vite_glob_0_20 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$40,
  file: file$40,
  url: url$40,
  rawContent: rawContent$40,
  compiledContent: compiledContent$40,
  default: load$40,
  Content: Content$40,
  getHeadings: getHeadings$40,
  getHeaders: getHeaders$40
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3$ = {"title":"Automatically Transcribe, Summarize, and Send Phone Call Summaries","description":"Learn how to use Twilio Functions and Deepgram's summarization feature to send all phone call participants a bite-sized version of what happened.","date":"2022-10-06T19:43:14.093Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1664560715/blog/automatically-transcribe-summarize-and-send-phone-call-summaries/2209-Automatically-Transcribe-Summarize-and-Send-Phone-Call-Summaries-blog_2x_wvgy7d.jpg","authors":["kevin-lewis"],"category":"tutorial","tags":["javascript","twilio"],"shorturls":{"share":"https://dpgr.am/bd1239a","twitter":"https://dpgr.am/0daa6ee","linkedin":"https://dpgr.am/e0856ca","reddit":"https://dpgr.am/1e08cde","facebook":"https://dpgr.am/969a1cd"}};
						const file$3$ = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/automatically-transcribe-summarize-and-send-phone-call-summaries/index.md";
						const url$3$ = undefined;
						function rawContent$3$() {
							return "\nIn this tutorial, we'll use Twilio Functions and Deepgram's [summarize](https://developers.deepgram.com/documentation/features/summarize/) feature to send call summaries via SMS once a conversation has ended.\n\nUsers can call a Twilio phone number, which will be forwarded to your agent while recording. Once the call is completed, we will get a transcript and summary of the call using Deepgram. Finally, the summary will be sent to both the caller and the agent via SMS.\n\nWe've previously written a post on [transcribing phone calls with Twilio Functions](https://blog.deepgram.com/transcribe-phone-calls-with-twilio-functions-and-deepgram/). We'll run through all the steps here, but do take a look for more detail.\n\n## Before You Start\n\nYou will need a Deepgram API Key - [get one here](https://console.deepgram.com/signup?jump=keys). You will also need a [Twilio account](https://console.twilio.com/) and a phone number in your account with SMS and Voice capabilities. Finally, you'll need two phones to test your project - one to make the call and one to receive.\n\n## Set Up Twilio Functions\n\nInside the Twilio Console, head to **Developer Tools -> Functions & Assets** and create a new service. A service can contain multiple Twilio Functions and assets related to a single project. It’s important that you create a new service here and not a standalone function.\n\nIn the Dependencies section, add `@deepgram/sdk` (you can omit the version for the latest).\n\nIn the Environment Variables section, add a key called `DEEPGRAM_KEY` with the value of your API Key generated in your Deepgram console. Create a second key called `FORWARDING_NUMBER` with the value of your agent phone number with [E.164 formatting](https://support.twilio.com/hc/en-us/articles/223183008-Formatting-International-Phone-Numbers) like this: +14155552671.\n\n## Record and Forward Inbound Calls\n\nRename the `/welcome` function to `/inbound`. Replace the whole file with the following:\n\n```js\nexports.handler = function(context, event, callback) {\n  let twiml = new Twilio.twiml.VoiceResponse()\n  const dial = twiml.dial({\n    record: 'record-from-answer-dual',\n    recordingStatusCallback: '/recordings'\n  })\n  dial.number(process.env.FORWARDING_NUMBER)\n  return callback(null, twiml)\n}\n```\n\nWhen this function receives incoming call data, it will forward it to your agent while recording it. Once completed, call data will be sent to `/recordings` (which will be created in the next section).\n\nSave the function, and click *Deploy All*. Once deployed, this function is ready to be used. Go to your Twilio number settings, and when a call comes in, select Function. Select your service and the `/inbound` function path.\n\n![When a call comes in, use a Function. Default service with the /inbound function path.](https://res.cloudinary.com/deepgram/image/upload/v1663789484/blog/2022/09/automatically-transcribe-summarize-and-send-phone-call-summaries/set-inbound-endpoint_rw5v5z.png)\n\n## Transcribe and Summarize Call\n\nCreate a new function - `/transcribe`. Delete the boilerplate and set up the function with the following code:\n\n```js\nconst { Deepgram } = require('@deepgram/sdk')\nconst deepgram = new Deepgram(process.env.DEEPGRAM_KEY)\n\nexports.handler = async function(context, event, callback) {\n  const { RecordingUrl, CallSid } = event\n  const twilioClient = context.getTwilioClient()\n  const { from: caller, to: twilioNumber } = await twilioClient.calls(CallSid).fetch()\n\n  // Further code here\n\n  return callback(null, true)\n}\n```\n\nThis code takes the `CallSid` and looks up the call for further call information. The caller’s phone number is now available in a variable called `caller`, and the number they called as `twilioNumber`. Now generate a transcription with Deepgram’s Node.js SDK:\n\n```js\nconst transcriptionFeatures = { punctuate: true, tier: 'enhanced', summarize: true }\nconst { results } = await deepgram.transcription.preRecorded({ url: RecordingUrl }, transcriptionFeatures)\nconst { summaries } = results.channels[0].alternatives[0]\n```\n\n`summaries` is an array of objects containing summary text and what time period it is summarizing. Add the following to turn it into one string that can be sent via SMS:\n\n```js\nconst summary = summaries.map(s => s.summary).join('\\n\\n')\n```\n\n## Send Summary Messages\n\nNow that you have a summary of the call provided by Deepgram's [summarization](https://developers.deepgram.com/documentation/features/summarize/) feature, it's time to send it to both the caller and the agent. Just below `summary` add the following:\n\n```js\nfor(let number of [process.env.FORWARD_NUMBER, caller]) {\n  await twilioClient.messages.create({\n    body: summary,\n    to: number,\n    from: twilioNumber\n  })\n}\n```\n\nSave both files again and deploy all functions in your service. Call your Twilio number, pick it up on your 'agent device', speak, and hang up. You should receive a summary message a few seconds later.\n\n## In Summary\n\nGetting high-quality summaries from your audio is as simple as adding one parameter in your Deepgram request. They're super useful in understanding what was said and what needs to happen next.\n\nIf you have questions about anything in this post, we’d love to hear from you. Head over to [our forum](https://github.com/orgs/deepgram/discussions/categories/q-a) and create a new discussion with your questions, or send us a tweet [@DeepgramAI](https://twitter.com/DeepgramAI)\n\nThe final code is as follows:\n\n```js\n// /inbound\nexports.handler = function(context, event, callback) {\n  let twiml = new Twilio.twiml.VoiceResponse()\n  const dial = twiml.dial({\n    record: 'record-from-answer-dual',\n    recordingStatusCallback: '/transcribe'\n  })\n  dial.number(process.env.FORWARDING_NUMBER)\n  return callback(null, twiml)\n}\n\n// /transcribe\nconst { Deepgram } = require('@deepgram/sdk')\nconst deepgram = new Deepgram(process.env.DEEPGRAM_KEY, 'api.beta.deepgram.com')\n\nexports.handler = async function(context, event, callback) {\n  const { RecordingUrl, CallSid } = event\n  const twilioClient = context.getTwilioClient()\n  const { from: caller, to: twilioNumber } = await twilioClient.calls(CallSid).fetch()\n\n  const transcriptionFeatures = { punctuate: true, tier: 'enhanced', summarize: true }\n  const { results } = await deepgram.transcription.preRecorded({ url: RecordingUrl }, transcriptionFeatures)\n  const { summaries } = results.channels[0].alternatives[0]\n  const summary = summaries.map(s => s.summary).join('\\n\\n')\n\n  for(let number of [process.env.FORWARDING_NUMBER, caller]) {\n    await twilioClient.messages.create({\n      body: summary,\n      to: number,\n      from: twilioNumber\n    })\n  }\n\n  return callback(null, true)\n}\n```\n\n";
						}
						async function compiledContent$3$() {
							return load$3$().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3$() {
							return (await import('./chunks/index.bb2804e6.mjs'));
						}
						function Content$3$(...args) {
							return load$3$().then((m) => m.default(...args));
						}
						Content$3$.isAstroComponentFactory = true;
						function getHeadings$3$() {
							return load$3$().then((m) => m.metadata.headings);
						}
						function getHeaders$3$() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3$().then((m) => m.metadata.headings);
						}

const __vite_glob_0_21 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3$,
  file: file$3$,
  url: url$3$,
  rawContent: rawContent$3$,
  compiledContent: compiledContent$3$,
  default: load$3$,
  Content: Content$3$,
  getHeadings: getHeadings$3$,
  getHeaders: getHeaders$3$
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3_ = {"title":"Automatically Transcribing Podcast Episodes with Pipedream and Python","description":"Learn how to automatically transcribe and email new podcast episodes as a newsletter with Deepgram's Speech Recognition API and Pipedream.","date":"2022-09-09T18:59:15.762Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981800/blog/2022/09/automatically-transcribe-new-podcasts-pipedream-python/cover.jpg","authors":["kevin-lewis"],"category":"tutorial","tags":["python"],"shorturls":{"share":"https://dpgr.am/870760a","twitter":"https://dpgr.am/aad9a28","linkedin":"https://dpgr.am/6e3bb68","reddit":"https://dpgr.am/51f5012","facebook":"https://dpgr.am/4a3ecae"}};
						const file$3_ = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/automatically-transcribing-podcast-episodes-with-pipedream-and-python/index.md";
						const url$3_ = undefined;
						function rawContent$3_() {
							return "\nI love podcasts, but I rarely find the time to listen to all the new episodes of the ones I love. However, I often find time to read an article or newsletter so I thought it would be fun to automatically transcribe new episodes and send myself the output via email.\n\nWe'll be using Pipedream - an online workflow builder that integrates with many services while allowing us to write code in Python, JavaScript, Go, and more. Each workflow has one trigger that starts it and any number of actions that can happen as a result.\n\nBefore you start, you will need a [Pipedream account](https://pipedream.com), a Google account, and a [free Deepgram API Key](https://console.deepgram.com/signup?jump=keys).\n\nCreate a new empty Pipedream workflow for this project.\n\n## Trigger Your Workflow When New Episodes Are Released\n\nAll Podcasts publish via a public RSS feed, which is updated whenever there is a new episode. Pipedream has a built-in trigger which will check every 15 minutes if a new item has been added.\n\nUse the 'New Item in RSS Feed' trigger with any podcast RSS feed - you can use `https://feeds.buzzsprout.com/1976304.rss`, which is for the new Deepgram podcast [Voice of the Future](https://deepgram.com/voiceofthefuturepodcast/).\n\n![Trigger configuration showing a feed URL, timer, and an empty 'name' textbox](https://res.cloudinary.com/deepgram/image/upload/v1661981806/blog/2022/09/automatically-transcribe-new-podcasts-pipedream-python/trigger-create.png)\n\nClick **Create Source** and select the first event. This is representative of one podcast episode.\n\n![Select event shows a dropdown of items. The first reads \"Can AI get a read on you\"](https://res.cloudinary.com/deepgram/image/upload/v1661981806/blog/2022/09/automatically-transcribe-new-podcasts-pipedream-python/trigger-select.png)\n\nOnce selected, you can see all of the data contained in that RSS feed entry - including the episode's metadata and direct media link. All of this data can be used in future steps within the workflow.\n\n![A large amount of JSON-formatted data abotu the episode. Lightly-highlighted is the direct media URL.](https://res.cloudinary.com/deepgram/image/upload/v1661981806/blog/2022/09/automatically-transcribe-new-podcasts-pipedream-python/trigger-data.png)\n\n## Transcribe Podcast With Python\n\nCreate a new step (now an 'action') that will be run whenever the workflow is triggered. Pick **Python -> Run Python Code**. In the **Configure** section, click **Add an App** and select **Deepgram**. Insert your API Key and save it before continuing.\n\n![Pipedream step showing a Deepgram account has been configured, with boilerplate code in the editor.](https://res.cloudinary.com/deepgram/image/upload/v1661981806/blog/2022/09/automatically-transcribe-new-podcasts-pipedream-python/python-config.png)\n\nDelete all of the code except `def handler(pd: \"pipedream\"):`, which is the required function signature that will be executed when this step is reached in the workflow. Make sure you have indented your code underneath this line. Then, get the URL from the trigger and your Deepgram API Key from the configured app:\n\n```py\nurl = pd.steps[\"trigger\"][\"event\"][\"enclosures\"][0][\"url\"]\ntoken = pd.inputs[\"deepgram\"][\"$auth\"]['api_key']\n```\n\nAs mentioned above, Pipedream requires the `def handler(pd: \"pipedream\"):` signature for the main function in a Python step. Because of this, the asynchronous Deepgram Python SDK won't be usable in this context. Instead, we'll directly make an API request with the `requests` library.\n\nAt the very top of your code, add the following line:\n\n```py\nimport requests\n```\n\nThen, at the bottom of your `handler` function, prepare your Deepgram API request:\n\n```py\nlisten = \"https://api.deepgram.com/v1/listen?tier=enhanced&punctuate=true&diarize=true&paragraphs=true\"\nheaders = { \"Authorization\": f\"Token {token}\" }\njson = { \"url\": url }\n```\n\nThis request will use Deepgram's [enhanced tier](https://developers.deepgram.com/documentation/features/tier/), [diarization (speaker detection)](https://developers.deepgram.com/documentation/features/diarize/), and format the output using [punctuation](https://developers.deepgram.com/documentation/features/punctuate/) and [paragraphs](https://developers.deepgram.com/documentation/features/paragraphs/).\n\nNow that you are set up, make the request, extract the formatted response, and return the value:\n\n```py\nr = requests.post(listen, json=json, headers=headers)\nresponse = r.json()\ntranscript = response['results']['channels'][0]['alternatives'][0]['paragraphs']['transcript']\nreturn transcript\n```\n\nFinal code:\n\n```py\nimport requests\ndef handler(pd: \"pipedream\"):\n  url = pd.steps[\"trigger\"][\"event\"][\"enclosures\"][0][\"url\"]\n  token = pd.inputs[\"deepgram\"][\"$auth\"]['api_key']\n  listen = \"https://api.deepgram.com/v1/listen?tier=enhanced&punctuate=true&diarize=true&paragraphs=true\"\n  headers = { \"Authorization\": f\"Token {token}\" }\n  json = { \"url\": url }\n  r = requests.post(listen, json=json, headers=headers)\n  response = r.json()\n  transcript = response['results']['channels'][0]['alternatives'][0]['paragraphs']['transcript']\n  return transcript\n```\n\nClick **Test** and the URL from the trigger will be sent to Deepgram, and the returned value will be shown in Pipedream:\n\n![A formatted transcript is shown in Pipdream's 'Exports' section for the Python step. In it is a set of paragraphs each starting with ''](https://res.cloudinary.com/deepgram/image/upload/v1661981806/blog/2022/09/automatically-transcribe-new-podcasts-pipedream-python/python-data.png)\n\n## Send Email With Transcript\n\nNow a transcript has been automatically generated, you can do anything with it - either through Pipedream's integrations or by adding another Python step. As mentioned at the start of this post, the outcome of this project is to send yourself an email with the content of the podcast. You can also include variables in the subject line.\n\nAdd a **Send Email To Self** step, and set the subject line to:\n\n```\nNew episode of {{steps.trigger.event.meta.title}}: {{steps.trigger.event.title}}\n```\n\nSet the text to:\n\n```\nEpisode description: {{steps.trigger.event.description}}\\n\\n{{steps.python.$return_value}}\n```\n\nIt should look like the following:\n\n![A send email to self step with a subject line and text containing variables](https://res.cloudinary.com/deepgram/image/upload/v1661981806/blog/2022/09/automatically-transcribe-new-podcasts-pipedream-python/email-config.png)\n\nTest the step, and you should receive an email in just a few seconds. Deploy the workflow, and enjoy reading new podcast episodes. If you have questions about anything in this post, we’d love to hear from you. Head over to [our forum](https://github.com/orgs/deepgram/discussions/categories/q-a) and create a new discussion with your questions, or send us a tweet [@DeepgramAI](https://twitter.com/DeepgramAI)";
						}
						async function compiledContent$3_() {
							return load$3_().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3_() {
							return (await import('./chunks/index.2018308e.mjs'));
						}
						function Content$3_(...args) {
							return load$3_().then((m) => m.default(...args));
						}
						Content$3_.isAstroComponentFactory = true;
						function getHeadings$3_() {
							return load$3_().then((m) => m.metadata.headings);
						}
						function getHeaders$3_() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3_().then((m) => m.metadata.headings);
						}

const __vite_glob_0_22 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3_,
  file: file$3_,
  url: url$3_,
  rawContent: rawContent$3_,
  compiledContent: compiledContent$3_,
  default: load$3_,
  Content: Content$3_,
  getHeadings: getHeadings$3_,
  getHeaders: getHeaders$3_
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3Z = {"title":"Bekah Hawrot Weigel Joins the Developer Relations Team","description":"Bekah Hawrot Weigel joins the Developer Relations team at Deepgram!","date":"2022-02-08T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1643986477/blog/2022/02/bekah-joins-deepgram/kettlebells%402x.jpg","authors":["bekah-hawrot-weigel"],"category":"devlife","tags":["team"],"seo":{"title":"Bekah Hawrot Weigel joins the Developer Relations team at Deepgram as the Technical Community Builder","description":"Bekah Hawrot Weigel joins the Developer Relations team at Deepgram! "},"shorturls":{"share":"https://dpgr.am/1038070","twitter":"https://dpgr.am/37b7b7f","linkedin":"https://dpgr.am/cdd73bd","reddit":"https://dpgr.am/7b02210","facebook":"https://dpgr.am/5304e5c"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453856/blog/bekah-joins-deepgram/ograph.png"}};
						const file$3Z = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/bekah-joins-deepgram/index.md";
						const url$3Z = undefined;
						function rawContent$3Z() {
							return "\r\nHey friends! I'm excited to be a new addition to the Deepgram Developer Relations team as the Technical Community Builder. I'm lucky enough to be bringing my non-traditional background as an educator, my technical training as a developer, and my community organizing experience into this position.\r\n\r\nI never really dreamt of a career change. In fact, I loathed the days my family would drive an hour to CompUSA and browse computer-related things after we got our first computer. And even after nearly ten years of teaching college English, a career pivot wasn't on my mind. But after experiencing birth trauma with my fourth child, my whole world was turned upside down. As I tried to work my way back to some semblance of normal after surgery, my husband--a second career developer himself--suggested that I learn to code. What I thought would be a couple of lessons from freecodecamp to get him to drop the subject turned into the thing that I needed in my life.\r\n\r\nNot only did the trauma stop cycling through my head when I was coding, but I also found community in the tech world that I didn’t expect. For the first time in a long time, I felt like I was a *part* of something with the Moms in Tech group I belonged to. It changed the way that I saw myself and the world around me. Because of the experience that led me into tech, I felt a drive to make sure that no one felt as alone as I did in the early days of my trauma.\r\n\r\nAfter finishing bootcamp, I was fortunate to find work as an independent contractor where I had support, encouragement, and felt safe. But about a year into it, the pandemic hit. I lost my work for a while, and found myself doing school at home with my four kids, interviewing, and crying every night. Then one day it hit me: I knew these feelings and I knew they grew when we’re isolated. So I opened up Twitter and asked who wanted to meet up for virtual coffee. That single message turned into the developer organization [Virtual Coffee](https://virtualcoffee.io/), where we continue to meetup, support each other, and grow together.\r\n\r\nFor me, that's the most important part of tech--creating space for people to feel comfortable sharing, learning, and mentoring together. When we feel safe, we grow and we support each other. As part of Deepgram's Developer Relations team as the Technical Community Builder, I get to keep doing that. I get to find new ways to support the tech community, to learn from others, and to share what I've learned. I can't think of anything better than that.\r\n\r\nOutside of tech and momming my four kids, you can find me swinging heavy kettlebells, eating spicy foods, writing, or playing outside. If you have any questions or just want to say “Hey!” you can pretty much always [find me on Twitter](https://twitter.com/BekahHW).\r\n\r\n        ";
						}
						async function compiledContent$3Z() {
							return load$3Z().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3Z() {
							return (await import('./chunks/index.826cc687.mjs'));
						}
						function Content$3Z(...args) {
							return load$3Z().then((m) => m.default(...args));
						}
						Content$3Z.isAstroComponentFactory = true;
						function getHeadings$3Z() {
							return load$3Z().then((m) => m.metadata.headings);
						}
						function getHeaders$3Z() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3Z().then((m) => m.metadata.headings);
						}

const __vite_glob_0_23 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3Z,
  file: file$3Z,
  url: url$3Z,
  rawContent: rawContent$3Z,
  compiledContent: compiledContent$3Z,
  default: load$3Z,
  Content: Content$3Z,
  getHeadings: getHeadings$3Z,
  getHeaders: getHeaders$3Z
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3Y = {"title":"The Best 8 Deepgram Projects from Hack Cambridge","description":"Deepgram recently participated in Hack Cambridge—here are the top 8 projects that used our API to create something amazing.","date":"2022-05-11T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981420/blog/best-8-deepgram-projects-hack-cambridge/best-dg-projects-hack-cambridge-thumb-554x220%402x.png","authors":["chris-doty"],"category":"dg-insider","tags":["education"],"seo":{"title":"The Best 8 Deepgram Projects from Hack Cambridge","description":"Deepgram recently participated in Hack Cambridge—here are the top 8 projects that used our API to create something amazing."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981420/blog/best-8-deepgram-projects-hack-cambridge/best-dg-projects-hack-cambridge-thumb-554x220%402x.png"},"shorturls":{"share":"https://dpgr.am/8a4831c","twitter":"https://dpgr.am/4f84b00","linkedin":"https://dpgr.am/ccd93c1","reddit":"https://dpgr.am/3811f57","facebook":"https://dpgr.am/1442a9c"}};
						const file$3Y = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/best-8-deepgram-projects-hack-cambridge/index.md";
						const url$3Y = undefined;
						function rawContent$3Y() {
							return "Back in January, we had the opportunity to support [Hack Cambridge](https://hackcambridge.com/)—a student hackathon sponsored by the University of Cambridge. We ran a challenge for participants to use Deepgram in some way to create a fun and unique hackathon project. [Developer outreach and support](https://blog.deepgram.com/) is a big focus for the Deepgram team, and supporting these students in building cutting-edge projects using our API is a passion for the whole company—we're always pumped to see folks attending events and running hackathons. I don't think it's any exaggeration to say that we were absolutely blown away by the quality of the projects that incorporated Deepgram and used speech-to-text to create something new. Over the last few months, [Kevin Lewis](https://blog.deepgram.com/authors/kevin-lewis/), a Senior Developer Advocate here at Deepgram, has been publishing descriptions of some of the projects that used Deepgram. Read on to learn more about the incredibly creative projects that came out of Hack Cambridge 2022.\n\n## Deepgram Hack Cambridge Projects\n\nHere's a list of eight of the best projects from the hackathon. Each one is linked to Kevin's full post about the project on our [developer blog](https://blog.deepgram.com/) if you'd like to read more or take a look at the code for the projects.\n\n1. [Add Live Speech Bubbles to Youtube Videos with AutoBubble](https://blog.deepgram.com/autobubble-youtube-speech-bubbles/) - The winning project of our challenge was by AutoBubble, which uses Deepgram not just to add captions to video, but to add those captions as speech bubbles into the video-super cool!\n2. [Voice Control Your Browser with Stëmm](https://blog.deepgram.com/voice-control-browser-stemm/) - We're a big fan of saying that voice is the next interface-and as speech-to-text tools get better and better, voice interfaces will continue to become easier to use and more powerful. The Stëmm team took advantage of Deepgram to build a system that would allow for voice control of Google Chrome, doing everything from opening new tabs to searching Google.\n3. [Create Comic Books from Videos with yack!](https://blog.deepgram.com/comic-books-videos-yack/) - Using a combination of computer vision and Deepgram, the yack! team built a tool that takes a video as an input, chooses single frames of the video to create a comic book, and then uses Deepgram to transcribe the audio and create speech bubbles to go along with the images. Try it yourself at [yack.ml](https://yack.ml/)!\n4. [Creating Contextual Video Overlays with TomScottPlus](https://blog.deepgram.com/contextual-video-overlay-tomscottplus/) - Ever wanted to know more about a topic mentioned in a YouTube video? TomScottPlus has your back. Using a transcript generated from a YouTube video, the tool overlays information from Wikipedia that's relevant to what's happening on screen-a sort of modern [Pop-Up Video](https://www.youtube.com/watch?v=km728FNBInA).\n5. [Sharpen Your Foreign Language Skills With Triolingo's Chatbot](https://blog.deepgram.com/foreign-language-practice-triolingo/) - There are lots of language-learning apps available, but few provide a real chance to practice speaking skills, never mind actual conversations. But by using Deepgram's Python SDK and a chatbot powered by GPT-3, Triolingo gives users a chance to practice their oral language skills, even if they don't have a conversation partner.\n6. [Use Your Voice to Draw with ARTiculate](https://blog.deepgram.com/draw-with-your-voice-articulate/) - If you've never thought about how you might draw verbally, you're certainly not alone. But for people who can't use traditional input devices, the ability to draw with their voice has the potential to unlock their creative potential. This project was created to provide a way for people to draw who can't use traditional input devices, using Deepgram to transcribe their commands.\n7. [Collaborative Augmented Reality Note-Taking with Airnote](https://blog.deepgram.com/ar-note-taking-airnote/) - The Airnote project combines augmented reality, note-taking, and Deepgram's speech-to-text API to create a collaborative experience to transcribe notes.\n8. [Practice Spelling Bees with Spelling Hero](https://blog.deepgram.com/practice-spelling-bees-hero/) - If you get pumped about spelling bees and dream of competing someday, you've now got a tool to help you practice and get ready for your next bee. Spelling Hero lets you set up drills with varying levels of difficulty, and uses Deepgram to transcribe your oral answer for the full bee experience.\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-how-deepgram-works\"></WhitepaperPromo>\n\n## Wrapping up\n\nIf you'd like to see what you can build with Deepgram's API, you can [sign up for a key](https://console.deepgram.com/signup), get some free credits, and start building in less than five minutes. Have questions about how we can help? [Contact us](https://deepgram.com/contact-us/) and we'll be happy to talk through your needs and use cases. And if you want to stay in the loop about other ways people can use Deepgram to support the creation of amazing voice technology, you can sign up for our newsletter below.";
						}
						async function compiledContent$3Y() {
							return load$3Y().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3Y() {
							return (await import('./chunks/index.c7485c26.mjs'));
						}
						function Content$3Y(...args) {
							return load$3Y().then((m) => m.default(...args));
						}
						Content$3Y.isAstroComponentFactory = true;
						function getHeadings$3Y() {
							return load$3Y().then((m) => m.metadata.headings);
						}
						function getHeaders$3Y() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3Y().then((m) => m.metadata.headings);
						}

const __vite_glob_0_24 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3Y,
  file: file$3Y,
  url: url$3Y,
  rawContent: rawContent$3Y,
  compiledContent: compiledContent$3Y,
  default: load$3Y,
  Content: Content$3Y,
  getHeadings: getHeadings$3Y,
  getHeaders: getHeaders$3Y
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3X = {"title":"Best Python Tools for Manipulating Audio Data","description":"A comprehensive guide on using Python to work with audio files.","date":"2022-06-13T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1654884969/blog/2022/06/best-python-audio-manipulation-tools/cover.jpg","authors":["yujian-tang"],"category":"tutorial","tags":["python"],"seo":{"title":"Best Python Tools for Manipulating Audio Data","description":"A comprehensive guide on using Python to work with audio files."},"shorturls":{"share":"https://dpgr.am/737f53a","twitter":"https://dpgr.am/b27d19f","linkedin":"https://dpgr.am/de1fc03","reddit":"https://dpgr.am/42f1085","facebook":"https://dpgr.am/29312b6"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454091/blog/best-python-audio-manipulation-tools/ograph.png"}};
						const file$3X = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/best-python-audio-manipulation-tools/index.md";
						const url$3X = undefined;
						function rawContent$3X() {
							return "\nPython provides us with many packages to manipulate audio data, but they don’t all work. As Python developers, we’re all familiar with the usual challenges - solving environments, compatibility between versions, and finding the right packages. This post covers how to do all of that for working with audio data. You can find a [GitHub repo here](https://github.com/deepgram-devs/basic_audio_data_manip) with all the samples discussed below.\n\nNote that this post is written using Python 3.9 as many audio packages have not yet been upgraded to work with 3.10 or 3.11. In this post we’ll cover:\n\n*   [An Introduction to Audio Data](#an-introduction-to-audio-data)\n*   [Ways To Use Audio Data](#ways-to-use-audio-data)\n*   [Recording Audio Data With Python](#recording-audio-data-with-python)\n*   [Playing Audio Data With Python](#playing-audio-data-with-python)\n*   [Clipping Audio Data With Python](#clipping-audio-data-with-python)\n*   [Manipulating Audio Data Sampling Rates With Python](#manipulating-audio-data-sampling-rates-with-python)\n*   [Changing Volume of Audio Data With Python](#changing-volume-of-audio-data-with-python)\n*   [Combining Two Audio Files With Python](#combining-two-audio-files-with-python)\n*   [Overlay Two Audio Files With Python](#overlay-two-audio-files-with-python)\n*   [Changing Audio Data File Formats With Python](#changing-audio-data-file-formats-with-python)\n*   [Transcribe Audio Data With a Web API](#transcribe-audio-data-with-a-web-api)\n*   [Summary of the Best Python Tools for Manipulating Audio Data](#summary-of-the-best-python-tools-for-manipulating-audio-data)\n\n## An Introduction to Audio Data\n\nWhat is audio data? Simply put, audio data is any data format that comes in the form of audio. At a fundamental level, there are not many ways to represent sound. The large number of audio data file types is due to the number of approaches to compress and package the data. Let’s take a look at two fundamental concepts before we jump deeper - sampling rates and types of audio data formats.\n\n### What Is a Sampling Rate?\n\nA sampling rate, sometimes also referred to as a frame rate, is the number of times per second that we measure the amplitude of the signal. It is measured in frames per second or Hertz (Hz). An ideal rate can be determined using [Nyquist’s Sampling Theorem](https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem). Some common sampling rates are 16 kHz (common for VoiP), 44.1 kHz (common in CDs), and 96 kHz (common in DVDs and Blu-Ray).\n\n### Types of Audio Data\n\n![](https://res.cloudinary.com/deepgram/image/upload/v1654876366/blog/2022/06/best-python-audio-manipulation-tools/types-of-audio-data.png)\n\nThere’s one well known way to represent sound - using waves. However, computers can represent that data in many ways. The most common audio data file types are `.wav` and `.mp3`. The main difference between `.wav` and `.mp3` files is that `.wav` files are not compressed and `.mp3` files are. This makes `.wav` files great for when you need the highest quality audio and `.mp3` files best when you need fast streaming.\n\nOther file types include Audio Interchange File Format (AIFF), raw Pulse Code Modulation (PCM) data, and Advanced Audio Coding (AAC). AIFF is a file format developed by Apple to be used on Mac OS much like WAVE files were initially developed by Microsoft. PCM is the raw audio data format. AAC’s were meant to be the successor to MP3 files, but did not catch on as a replay format. However, it did catch on as a popular streaming audio format and is used by YouTube, iPhones, and Nintendo.\n\n## Ways To Use Audio Data\n\nAudio data is becoming more and more ubiquitous. It’s created on phone calls, remote video meeting recordings, for music and videos, and so much more. How can we use all this audio data being created? We can layer sound bites for music, change the volume of different sound streams to make it easier to hear everyone from a VoiP call, or transcribe a call to read later. Of course, this only covers some of the things we can do with sound data, there are many other valuable ways to use audio that we haven’t even discovered.\n\n## Recording Audio Data With Python\n\nTwo of the most basic things you can do with audio data in Python are playing and recording audio. In the next two sections we’ll cover how to use two popular Python sound libraries to play and record audio data. First, we’ll take a look at how to record audio data with `sounddevice` and `pyaudio`.\n\nBefore we get started with the code, we’ll have to install the prerequisite libraries. PyAudio is a wrapper around PortAudio. The installation will be different for Windows and Mac. On Mac OS, you can use `brew` to install `portaudio` after installing your X-Code tools. For Windows, see [this answer on StackOverflow](https://stackoverflow.com/questions/52283840/i-cant-install-pyaudio-on-windows-how-to-solve-error-microsoft-visual-c-14/52284344#52284344).\n\nHow to Install PyAudio (on Mac):\n\n    xcode-select --install\n    brew remove portaudio\n    brew install portaudio\n    pip install pyaudio\n\nIf you are having trouble, use this command instead to specify your build locations for portaudio:\n\n    pip install --global-option='build_ext' --global-option=\"-I$(brew --prefix)/include\" --global-option=\"-L$(brew --prefix)/lib\" pyaudio\n\nTo install `python-sounddevice`, run the line `pip install sounddevice scipy` in the command line. We will need `scipy` for downloading the streamed data and for later use.\n\n### Use PyAudio To Record Sound\n\nIn this example, we’re going to use PyAudio and the Python native `wave` library to record some sound and save it into a file. We’ll start by importing the two libraries and declaring constants. The constants we need to declare up front are the chunk size (the number of frames saved at a time), the format (16 bit), the number of channels (1), the sampling rate (44100), the length of our recording (3 seconds), and the filename we want to save our recording to.\n\nFrom there, we create a PyAudio object. We’ll use the PyAudio object to create a stream with the constants we set above. Then, we’ll initialize an empty list of frames to hold the frames. Next, we’ll use the stream to read data while we still have time left in our 3 second timeframe and save it in the chunk size of 1024 bits.\n\nWe need to close and terminate our stream after 3 seconds. Finally, we’ll use the `wave` library to save the streamed audio data into a `.wav` file with the preset constants we declared above.\n\n```py\nimport pyaudio\nimport wave\n\nchunk = 1024\nsample_format = pyaudio.paInt16\nchannels = 1\nfs = 44100 # frames per channel\nseconds = 3\nfilename = \"output_pyaudio.wav\"\n\np=pyaudio.PyAudio()\n\nprint(\"Recording ...\")\n\nstream = p.open(format = sample_format,\n                channels = channels,\n                rate = fs,\n                frames_per_buffer =  chunk,\n                input = True)\n\nframes = []\nfor i in range(0, int(fs/chunk * seconds)):\n    data = stream.read(chunk)\n    frames.append(data)\n\nstream.stop_stream()\nstream.close()\np.terminate()\n\nprint(\"... Ending Recording\")\nwith wave.open(filename, 'wb') as wf:\n    wf.setnchannels(channels)\n    wf.setsampwidth(p.get_sample_size(sample_format))\n    wf.setframerate(fs)\n    wf.writeframes(b''.join(frames))\n    wf.close()\n```\n\n### Record With Python-Sounddevice\n\nPython-Sounddevice, or just `sounddevice` when you import it or install it through `pip`, is a simple sound recording and playing library. We can see below that it takes much less code to simply make a recording than `pyaudio`.\n\nFirst, we import the libraries we need, `sounddevice` and the `write` function from `scipy.io.wavfile`. Next, we declare a couple constants for the sampling rate and the length of recording. Once we have those, we just record, ask `sounddevice` to wait, and then use `scipy` to write the recorded audio.\n\n```py\nimport sounddevice as sd\nfrom scipy.io.wavfile import write\n\nfs = 44100\nseconds = 3\n\nrecording = sd.rec(int(seconds*fs), samplerate=fs, channels=1)\nsd.wait()\nwrite('output_sounddevice.wav', fs, recording)\n```\n\n## Playing Audio Data With Python\n\nPlaying audio data is the sister function to recording audio data. Only being able to record data would be almost useless. For this section, we’re going to use the same libraries we did above, `pyaudio`, and `sounddevice`. However, we will also need one more library for using `sounddevice` to play audio data, `soundfile`. We need to run this command in the terminal: `pip install soundfile` to install it.\n\n### Use Pyaudio To Play Audio\n\nOnce again, we’ll use the built-in `wave` library along with `pyaudio` to play some sound. We’ll use the recording we made above and the same chunk size. We will start our functionality by opening the file and creating a `pyaudio` object.\n\nWe will then use the `pyaudio` object to open a stream with specifications extracted from the wave file. Next, we’ll create a data object that reads the frames of the wave file in the specified chunk size. To actually play the sound, we’ll loop through the data file and write it to the stream while it is not an empty bit. Finally, we’ll close the stream and terminate the `pyaudio` object.\n\n```py\nimport pyaudio\nimport wave\n\n# declare constants and initialize portaudio/pyaudio object\nfilename = 'output_pyaudio.wav'\nchunk = 1024\nwf = wave.open(filename, 'rb')\npa = pyaudio.PyAudio()\n\n# create stream using info from the file\nstream = pa.open(format = pa.get_format_from_width(wf.getsampwidth()),\n                channels = wf.getnchannels(),\n                rate = wf.getframerate(),\n                output = True)\n\n# read in the frames as data\ndata = wf.readframes(chunk)\n\n# while the data isn't empty\nwhile data != b'':\n    stream.write(data)\n    data = wf.readframes(chunk)\n\n# cleanup\nstream.close()\npa.terminate()\n```\n\n### Play Audio With Python-sounddevice\n\nOnce again, we see the simplification of playing audio that `sounddevice` offers over `pyaudio`. Remember that both the playing and recording simplifications also come with the need to install and use two extra libraries though.\n\nFor this example, we will import the `sounddevice` and `soundfile` libraries. Then, we will feed the filename to `soundfile` to `read` us the data and the sampling rate. Finally, we use `sounddevice` to `play` the resulting sound and make the process wait while the sound finishes playing.\n\n```py\nimport sounddevice\nimport soundfile\n\nfilename = \"output_sounddevice.wav\"\ndata, fs = soundfile.read(filename, dtype='float32')\nsounddevice.play(data, fs)\nstatus = sounddevice.wait()\n```\n\n## Clipping Audio Data With Python\n\nNow that we’ve covered the simplest acts of playing and recording audio, let’s move onto how to change the audio data. The first thing we’ll cover is clipping or trimming audio data. For this example, we’ll need to install two more libraries, `pydub` and `ffmpeg-python`. We can install these with pip in the command line as usual using `pip install pydub ffmpeg-python`.\n\n### Clip Audio With Pydub\n\nAs we will see here and further down the post, the `pydub` library is a swiss army knife of audio manipulation tools. To trim audio data with `pydub`, we only need the `AudioSegment` object. To start, we’ll define the first and last milliseconds we want to clip out. Then, we’ll load the audio file with `AudioSegment`.\n\nTo clip our audio file down, we’ll create a list that only contains the data from the start to the end millisecond in our audio file. Finally, we’ll use the `export` function of the `AudioSegment` object we extracted to save the file in `.wav` format.\n\n```py\nfrom pydub import AudioSegment\n\n# start at 0 milliseconds\n# end at 1500 milliseconds\nstart = 0\nend = 1500\n\nsound = AudioSegment.from_wav(\"output_pyaudio.wav\")\nextract = sound[start:end]\n\nextract.export(\"trimmed_output_pydub.wav\", format=\"wav\")\n```\n\n### Trim Audio Clips With FFMPEG\n\nFFMPEG is a well known audio manipulation library, usually used in the command line. You can use the `sys` and `subprocess` libraries that are native to Python to use FFMPEG, but I find that using the SDK is easier and more satisfying.\n\nEven though we had to install this SDK with `pip install ffmpeg-python`, we actually import it as just `ffmpeg`. The first thing we’ll do is get an input object. Then, we’ll use the `ffmpeg.input` object to call the `atrim` function and trim the recording down to 1 second. Next, we’ll create an output using the newly cut data. Finally, we’ll need to call `ffmpeg` to actually run the `output` call and save our file.\n\n```py\nimport ffmpeg\n\naudio_input = ffmpeg.input(\"output_sounddevice.wav\")\naudio_cut = audio_input.audio.filter('atrim', duration=1)\naudio_output = ffmpeg.output(audio_cut, 'trimmed_output_ffmpeg.wav', format='wav')\nffmpeg.run(audio_output)\n```\n\n## Manipulating Audio Data Sampling Rates With Python\n\nHere’s where things get a bit trickier. Sampling rates play a huge part in how audio data sounds. When you’re changing the number of frames per second that represents a sound, a lot of funky things can happen. If not done right, it can affect the speed, the pitch, and the quality of your sound. For our examples, we’ll be using `pydub` and `scipy`. Both libraries we already downloaded earlier.\n\n![Sample Rate Change in Audio File](https://res.cloudinary.com/deepgram/image/upload/v1654876364/blog/2022/06/best-python-audio-manipulation-tools/sample-rate.png)\n\n### Pydub\n\nAs mentioned above, `pydub` has a variety of tools for manipulating audio data, and changing the sample rate safely is one of them. The `AudioSegment` object has a wonderful `set_frame_rate` command that can be used to set the frame rate of the audio segment without changing the pitch, speed, or applying any other distortions. To save it, we will use the `export` function again.\n\n```py\nfrom pydub import AudioSegment\n\nsound = AudioSegment.from_wav('output_pyaudio.wav')\nsound_w_new_fs = sound.set_frame_rate(16000)\nsound_w_new_fs.export(\"new_fs_output_pydub.wav\", format=\"wav\")\n```\n\n### Scipy\n\nIf you are so inclined and also mathematically skilled, you can apply your own sampling rate change with `scipy`. The most popular reasons to use `scipy` to customize your sampling is for advanced use cases like research, music, or special effects.\n\nRead in the sample rate and audio data using `wavfile`. Using the read-in sample rate and a desired new sample rate, create a new number of samples. To do the resampling, we call on the `resample` function from `scipy.signal`. This resample function applies a fourier transform and may cause distortion. This is a much more advanced technique and should probably only be used if necessary.\n\n```py\nfrom scipy.io import wavfile\nimport scipy.signal\n\nnew_fs = 88200\n# open data\nsample_rate, data = wavfile.read('output_pyaudio.wav')\n\n# resample data\nnew_num_samples = round(len(data)*float(new_fs)/sample_rate)\ndata = scipy.signal.resample(data, new_num_samples)\nwavfile.write(filename=\"new_fs_output_scipy.wav\", rate=88200, data=data)\n```\n\n## Changing Volume of Audio Data with Python\n\nThe next four things we’re going to cover all use `pydub`. Partially because it is the easiest to use, partially because it is the only one that is updated enough to work with the latest versions of Python.\n\nChanging volume with the `AudioSegment` object from `pydub` is extremely easy. After importing the libraries and functions we need and opening up the `.wav` file, the only thing we need to do is add or subtract from the object representing the open `.wav` file.\n\nThe `play` function plays both sounds, one after the other, so that we can hear that one is louder and the other is softer. We can save the files using the `export` function as we have been doing so far.\n\n```py\nfrom pydub import AudioSegment\nfrom pydub.playback import play\n\nsound = AudioSegment.from_wav(\"new_fs_output_pydub.wav\")\n\n# 3 dB up\nlouder = sound + 3\n# 3 dB down\nquieter = sound - 3\n\nplay(louder)\nplay(quieter)\n\nlouder.export(\"louder_output.wav\", format=\"wav\")\nquieter.export(\"quieter_output.wav\", format=\"wav\")\n```\n\n## Combining Two Audio Files With Python\n\nWe can also use `pydub` to combine two audio data files in Python. Once we open up the `.wav` files with `AudioSegment`, all we have to do to append them is add them. We can play the sound to make sure we got it and then export it into a file.\n\n```py\nfrom pydub import AudioSegment\nfrom pydub.playback import play\n\nsound1 = AudioSegment.from_wav(\"louder_output.wav\")\nsound2 = AudioSegment.from_wav(\"quieter_output.wav\")\n\ncombined = sound1 + sound2\n\nplay(combined)\ncombined.export(\"louder_and_quieter.wav\", format=\"wav\")\n```\n\n## Overlay Two Audio Files With Python\n\nYou’d think that overlaying audio data would be harder than combining them, but the `AudioSegment` object from the `pydub` library makes it quite easy. All we do is call the `overlay` function and pass it the sound we want to overlay and the position (in milliseconds) we want to overlay it at.\n\n```py\nfrom pydub import AudioSegment\nfrom pydub.playback import play\n\nsound1 = AudioSegment.from_wav(\"louder_output.wav\")\nsound2 = AudioSegment.from_wav(\"quieter_output.wav\")\n\noverlay = sound1.overlay(sound2, position=1000)\n\nplay(overlay)\noverlay.export(\"overlaid_1sec_offset.wav\", format=\"wav\")\n```\n\n## Changing Audio Data File Formats With Python\n\nWe’ve covered a lot of ways to manipulate audio data in Python from the basic playing and recording to overlaying different sounds. What if we just need our audio file in a different format? We can also use `pydub` to convert audio formats.\n\nThe `AudioSegment` object’s `export` function that we’ve been using in the sections above has the capability to define the output object’s format. All we have to do to save it as a different format is pass that format to the `export` function. In the example below, I’ve opened up a `.wav` file and saved it as a `.mp3`.\n\n```py\nfrom pydub import AudioSegment\n\nwav_audio = AudioSegment.from_wav(\"louder_output.wav\")\nmp3_audio = wav_audio.export(\"louder.mp3\", format=\"mp3\")\n```\n\n## Transcribe Audio Data With a Web API\n\nFinally, we come to the last bit of audio data manipulation to be covered in this primer to audio data in Python. Having the transcription of an audio file can be useful for many reasons. You can use it to [visualize your data](https://blog.deepgram.com/python-graphing-transcripts/), [search for keywords from a library of audio files](https://blog.deepgram.com/python-script-compliance/), or get inputs for Natural Language Understanding (NLU) models.\n\nTo transcribe your audio with a Web API, you’ll need to sign up for a [free Deepgram API key](https://console.deepgram.com/signup?jump=keys) and run `pip install deepgram-sdk` in your terminal.\n\nWe’ll be using `asyncio` to asynchronously make the API request. Before we create our function, we need to make sure that we have the API key and the file name. The function will initialize the Deepgram SDK, open the local file, and send it to the transcription endpoint. Then it will wait for the response and dump it into a JSON file formatted with a tab of 4 spaces. We will then both print the JSON result to the terminal and save it as a `.txt` file. Finally, we’ll use `asyncio` to run the function.\n\n```py\nfrom deepgram import Deepgram\nfrom config import dg_secret_key\nimport asyncio, json\n\nDEEPGRAM_API_KEY = dg_secret_key\nPATH_TO_FILE = 'louder_output.wav'\n\nasync def main():\n    # Initializes the Deepgram SDK\n    deepgram = Deepgram(DEEPGRAM_API_KEY)\n    # Open the audio file\n    with open(PATH_TO_FILE, 'rb') as audio:\n        # ...or replace mimetype as appropriate\n        source = {'buffer': audio, 'mimetype': 'audio/wav'}\n        response = await deepgram.transcription.prerecorded(source, {'punctuate': True})\n        json_obj = json.dumps(response, indent=4)\n        print(json_obj)\n        with open(\"transcribed.txt\", \"w\") as f:\n            f.write(json_obj)\n\nasyncio.run(main())\n```\n\n## Summary of the Best Python Tools for Manipulating Audio Data\n\nAudio data covers a large swath of data that isn’t commonly thought about when it comes to data analysis and manipulation. In this post, we covered how to use some of the most up-to-date Python libraries for manipulating audio data.\n\nWe learned how to play and record audio data using `pyaudio` and `python-sounddevice`. How to trim data with `pydub` and `ffmpeg`, and how to resample data with `pydub` and `scipy`. Then we saw how we can use `pydub` as a swiss army knife of audio to change the volume, combine or overlay files, and change the file format. Finally, we saw how we can easily use the Deepgram SDK to transcribe audio data.\n\n## Further Reading\n\n*   [Lecture Notes on Audio Data from Penn](https://www.ling.upenn.edu/courses/Spring_2003/ling538/Lecnotes/AudioData.html)\n*   [Audio File Formats](https://en.wikipedia.org/wiki/Audio_file_format)\n\n        ";
						}
						async function compiledContent$3X() {
							return load$3X().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3X() {
							return (await import('./chunks/index.10c57d2a.mjs'));
						}
						function Content$3X(...args) {
							return load$3X().then((m) => m.default(...args));
						}
						Content$3X.isAstroComponentFactory = true;
						function getHeadings$3X() {
							return load$3X().then((m) => m.metadata.headings);
						}
						function getHeaders$3X() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3X().then((m) => m.metadata.headings);
						}

const __vite_glob_0_25 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3X,
  file: file$3X,
  url: url$3X,
  rawContent: rawContent$3X,
  compiledContent: compiledContent$3X,
  default: load$3X,
  Content: Content$3X,
  getHeadings: getHeadings$3X,
  getHeaders: getHeaders$3X
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3W = {"title":"Which Speech Recognition Model is Best for My Business?","description":"If youre shopping for automatic speech recognition (ASR), you have options. Learn some of the features that make Deepgram the best choice.","date":"2022-05-23T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981408/blog/best-speech-recognition-model-business/which-speech-model-best-for-biz-thumb-554x220%402x.png","authors":["keith-lam"],"category":"product-news","tags":["deep-learning"],"seo":{"title":"Which Speech Recognition Model is Best for My Business?","description":"If youre shopping for automatic speech recognition (ASR), you have options. Learn some of the features that make Deepgram the best choice."},"shorturls":{"share":"https://dpgr.am/4c2774a","twitter":"https://dpgr.am/ccd2306","linkedin":"https://dpgr.am/a945e65","reddit":"https://dpgr.am/83efd1c","facebook":"https://dpgr.am/ecd2b58"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981408/blog/best-speech-recognition-model-business/which-speech-model-best-for-biz-thumb-554x220%402x.png"}};
						const file$3W = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/best-speech-recognition-model-business/index.md";
						const url$3W = undefined;
						function rawContent$3W() {
							return "\r\nA funny thing happened when Deepgram first decided to use [end-to-end deep learning](https://blog.deepgram.com/deep-learning-speech-recognition/) (E2EDL) to design our next-generation speech-to-text (STT) solution. We found that this approach was hugely flexible and easier to optimize than traditional STT. We didn't have to reconnect and optimize multiple models (acoustic, pronunciation, and language) every time we wanted to make a change. And we could retrain and enhance our speech models without starting from scratch. With transfer learning, we could build new speech models faster. This trait of our technology has allowed us to build different base speech models for different use cases and needs. It also allows us to tailor models in cases where a customer needs something specific that we don't currently offer. Let's take a look at the two types of models that we offer here at Deepgram and what each is good for.\r\n\r\n## 1\\. Language-by-Use Case Models\r\n\r\nAll of our use case-specific models are available in various English dialects. We are expanding into different language-by-use case combinations as we continue to train and optimize our speech models for specific circumstances, such as call centers or meeting transcription, as well as expanding [the spoken languages we offer](https://deepgram.com/product/languages/). Our customers have found that combining a spoken language and use case to create a speech model that works specifically for their needs is more accurate than Big Tech's out-of-the-box, one-size-fits-none models. These targeted models have the fastest speed and are optimized for the best scalability. Our models can transcribe one hour of pre-recorded audio in 30 seconds. These models are great for all applications, especially ones that need very high speeds or cost savings for on-prem use. You also don't need to trade off speed or scalability for high accuracy and because we have multiple models for different use cases-unlike Big Tech-our models tend to be more accurate as well.\r\n\r\n<WhitepaperPromo whitepaper=\"latest\"></WhitepaperPromo>\r\n\r\n\r\n\r\n## 2\\. Higher Accuracy Enhanced Models\r\n\r\nWe also built our next-generation architecture with the highest English language accuracy on long-tail words or words that are not as common in regular conversations. This new architecture was rebuilt from our current architecture to optimize accuracy on more words.  This new enhanced speech model architecture is best suited where you have keywords and terms that you must get correct but are not in normal conversations; like fiduciary, biodiversity, formulae. Some use cases can be Conversational AI for B2B, technical support contact centers, or technical meetings or seminars.\r\n\r\n## 3\\. Models Tailored for Your Business\r\n\r\nBut what if we don't have a use case model specifically for your needs? Maybe your audio has a lot of background noise, accents, jargon, or product and company names; all of this can sometimes create problems for off-the-shelf models. If that's the case for you, here at Deepgram we can customize a model for your specific use case. These tailored models can be trained and deployed within weeks and are specifically targeted to address the characteristics of your use case that might make it hard for an off-the-shelf model. To make sure that the tailored model really does address your specific issues, the data for training these models requires audio from your specific business. The more \"real world\" audio from your business, the better the accuracy. Having an employee read off a script or list of terms creates poor data vs. recording your employee and customer having a conversation. Although we like to say that the more real-world audio you can provide, the better, we've seen good accuracy improvement with less than 10 hours of audio.\r\n\r\n## Deciding Which ASR Platform is Best for You\r\n\r\nThere are obviously a lot of factors that go into deciding which ASR system will work best for you, beyond the ability to tailor models. If you'd like to read more the factors that you should consider when shopping for an ASR platform, check out [How to Evaluate an ASR Platform](https://offers.deepgram.com/how-to-evaluate-deep-learning-asr-platform-solution-brief), or fill out our free [Speech-to-Text Self Assessment](https://deepgram.typeform.com/to/d3zTk2eI). Still have questions? [Contact us](https://deepgram.com/contact-us) to talk through your use case and see which of our models is best for you.\r\n";
						}
						async function compiledContent$3W() {
							return load$3W().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3W() {
							return (await import('./chunks/index.6347321a.mjs'));
						}
						function Content$3W(...args) {
							return load$3W().then((m) => m.default(...args));
						}
						Content$3W.isAstroComponentFactory = true;
						function getHeadings$3W() {
							return load$3W().then((m) => m.metadata.headings);
						}
						function getHeaders$3W() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3W().then((m) => m.metadata.headings);
						}

const __vite_glob_0_26 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3W,
  file: file$3W,
  url: url$3W,
  rawContent: rawContent$3W,
  compiledContent: compiledContent$3W,
  default: load$3W,
  Content: Content$3W,
  getHeadings: getHeadings$3W,
  getHeaders: getHeaders$3W
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3V = {"title":"Best Speech-to-Text APIs in 2022","description":"There’re a lot of options out there for speech-to-text APIs. Learn about the pros and cons of the various options here.","date":"2022-04-19T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981417/blog/best-speech-to-text-apis/best-stt-apis-in-2022-thumb-554x220%402x.png","authors":["aimie-ye"],"category":"speech-trends","tags":["amazon","deep-learning","voice-strategy"],"seo":{"title":"Best Speech-to-Text APIs in 2022","description":"There’re a lot of options out there for speech-to-text APIs. Learn about the pros and cons of the various options here."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981417/blog/best-speech-to-text-apis/best-stt-apis-in-2022-thumb-554x220%402x.png"},"shorturls":{"share":"https://dpgr.am/d8522a7","twitter":"https://dpgr.am/f045856","linkedin":"https://dpgr.am/6bbbb48","reddit":"https://dpgr.am/0fb8492","facebook":"https://dpgr.am/df40d77"}};
						const file$3V = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/best-speech-to-text-apis/index.md";
						const url$3V = undefined;
						function rawContent$3V() {
							return "If you've been shopping for a speech-to-text (STT) solution for your business, you're not alone. In our recent [State of Voice Technology 2022](https://deepgram.com/state-of-voice-technology-2022/) report, 99% of respondents said they viewed voice-enabled experiences as a critical part of their company's future enterprise strategy. But the sheer number of options for speech transcription might be overwhelming if you aren't familiar with the space-from Big Tech to open source options, there's a ton of choices, with different price points and different feature sets to choose from. Although this diversity is great, it can also make it confusing when you're trying to compare different options and pick the right solution for you.\n\nIn this blog post, we're going to break down the various STT APIs available today, telling you their various pros and cons, and providing a ranking that we think accurately represents the current STT landscape.  Before we get to the ranking, we're going to break down exactly what a speech-to-text API is, the core features you'd expect a STT API to have, and some key use cases for speech-to-text APIs.\n\n## What is a Speech-to-Text API?\n\nAt its core, a speech-to-text application programming interface (API) is simply the ability to call a service to transcribe audio into speech. The STT service will take the provided audio file, process it using either machine learning or a set of tools that combines machine learning with rule-based approaches, and then provide a transcript of what it thinks was said.\n\n## Key Features of Speech-to-Text APIs\n\nIn this section, we'll survey some of the most common features that STT APIs offer. The key features that are offered by each API differ, and your use cases will dictate your priorities and needs in terms of which features to focus on.\n\n* **Accurate transcription** - The most important thing, regardless of what you're using STT for, is accurate transcription. If you're getting back transcripts that look like MadLibs, it's unlikely you're going to get much business value from them. The absolute baseline accuracy for readable transcriptions is 80%.\n* **Batch or pre-recorded transcription capabilities** - Batch transcription won't be needed by everyone, but for many use cases, you'll want a service that you can send batches of files to to be transcribed, rather than having to do it one-by-one on your end.\n* **Real-time streaming** - Again, not everyone will need real-time streaming. However, if you want to use STT to create, for example, truly [conversational AI](https://deepgram.com/solutions/voicebots/) that can respond to customer inquiries in real time, you'll need to use a STT API that returns its results as quickly as possible.\n* **Multi-language support** - If you're planning to handle multiple languages or dialects, this should be a key concern. And even if you aren't planning on multilingual support now, if there's any chance that you would in the future, you're best off starting with a service that offers many languages and is always expanding to more.\n* **Automatic punctuation & capitalization** - Depending on what you're planning to do with your transcripts, you might not care if they're formatted nicely. But if you're planning on surfacing them publicly, having this included in what the STT API provides can save you time.\n* **Profanity filtering or redaction** - If you're using STT as part of an effort for community moderation, you're going to want a tool that can automatically detect profanity in its output and censor it or flag it for review.\n* **Topic detection** - If you're looking to process large volumes of audio in order to better understand what's being discussed, a STT API that offers topic detection could be something you want to focus on.\n* **Custom vocabulary** - Being able to define custom vocabulary is helpful if your audio has lots of custom terms, abbreviations, and acronyms that an off-the-shelf model wouldn't have been exposed to.\n* **Keyword boosting** - Similar to defining custom vocabulary, keyword boosting lets you make it more likely that the STT API will predict words that are particularly important or common in your audio.\n* **Tailored models** - If keyword boosting and custom vocabulary aren't enough for your needs and you're still seeing poor accuracy, you might want to look for a provider that will let you tailor a model for your specific needs, based on your own data. This typically improves accuracy beyond what any out-of-the-box solution can.\n* **Accepts multiple audio formats** - Another concern that won't be present for everyone is whether or not the STT API can process audio in different formats. If you have audio coming from multiple sources that aren't encoded in the same format, having a STT API that removes the need for converting to different types of audio can save you time and money.\n\n## Speech-to-Text Use Cases\n\nAs noted at the outset, voice technology that's built on the back of STT APIs is a critical part of the future of business. So what are some of the most common use cases for speech-to-text APIs? Let's take a look.\n\n* **Smart assistants** - Smart assistants like Siri and Alexa are perhaps the most frequently encountered use case for speech-to-text, taking spoken commands, converting them to text, and then acting on them.\n* **Conversational AI** - Voicebots let humans speak and, in real time, get answers from an AI. Converting speech to text is the first step in this process, and it has to happen quickly for the interaction to truly feel like a conversation.\n* **Sales and support enablement** - Sales and support digital assistants that provide tips, hints, and solutions to agents by transcribing, analyzing and pulling up information in real time. It can also be used to gauge sales pitches or sales calls with a customer.\n* **Contact centers** - Contact centers can use STT to create transcripts of their calls, providing more ways to evaluate their agents, understand what customers are asking about, and provide insight into different aspects of their business that are typically hard to assess.\n* **Speech analytics** - Broadly speaking, speech analytics is any attempt to process spoken audio to extract insights. This might be done in a call center, as above, but it could also be done in other environments, like meetings or even speeches and talks.\n* **Accessibility** - Providing transcriptions of spoken speech can be a huge win for accessibility, whether it's [providing captions for classroom lectures](https://blog.deepgram.com/classroom-captioner/) or creating badges that transcribe speech on the fly.\n\n## Top 8 Speech Recognition APIs\n\nWith that background out of the way, let's dive into our ranking, and what we think are the top 8 speech-to-text APIs available today.\n\n### 1. Deepgram Speech-to-Text API\n\n* **Summary:** We might be biased, but we think Deepgram is the best STT API on the market. We're a [developer-focused ASR provider with SDKs](https://developers.deepgram.com/), providing an API that data scientists and developers can use to convert messy, unstructured audio data into accurate and structured transcriptions in batch or real-time-both on premises and in the cloud. Deepgram out of the box accuracies are in the 90%+ range with an option to customize speech models and reach even higher accuracies. Deepgram also has the fastest ASR in the market, with a 1200x real-time speed for batch processing and has less than a 300 millisecond lag on real-time streaming. If you'd like to give Deepgram a try, you can sign up for a [free API key](https://console.deepgram.com/signup) or [contact us](https://deepgram.com/contact-us/) if you have questions.\n* **Architecture:** Built on the latest [end-to-end deep learning neural networks](https://blog.deepgram.com/deep-learning-speech-recognition/)\n* **Pros:**\n\n  * Highest out-of-the-box and tailored model accuracy\n  * Fastest speed\n  * High customization within days\n  * Easy to start with [Console](https://console.deepgram.com/) \n* **Cons:**\n\n  * Fewer languages than big tech ASR, but [we're regularly releasing new languages](https://blog.deepgram.com/deepgram-language-speech-models/)\n* **Price:** $0.75/audio hour\n\n<WhitepaperPromo whitepaper=\"latest\"></WhitepaperPromo>\n\n### 2. Amazon Transcribe\n\n* **Summary:** Amazon Transcribe is a consumer oriented product coming out of the development of the Alexa voice assistant. Transcribe's command-and-response transcription for short audio is very good. Their accuracy is on the higher end of ASR providers for consumer audio data but not as good with business audio. Their speed is slow with only a 4X speed up on batch transcriptions. Limited customization can be done with their Custom Language Model or keyword boosting. Besides their general speech model, they also have a more expensive medical offering. Limited support as it is not one of their main products. Can be deployed in the cloud only.\n* **Architecture:** Traditional four-step STT model with added AI components\n* **Pros:**\n\n  * Brand name\n  * Easy to integrate if you are already in the AWS ecosystem\n  * Good choice for short audio for command and response\n  * Fairly good accuracy with consumer audio\n  * Good scalability, except for costs\n* **Cons:**\n\n  * Poor accuracy with business audio or audio with lots of terminology\n  * Slow speed\n  * Limited support\n  * Cloud deployment only\n  * High cost\n* **Price:** $1.44/audio hour general, $4.59/audio hour medical\n\n### 3. Google Speech-to-Text\n\n* **Summary:** Google STT is a tiny part of their overall business. The product was initially built for their Google Home voice assistant, and their focus is more on short command-and-respond applications, similar to Amazon Transcribe. Their accuracy is middle of the road, and not one of the higher accuracy ASR systems. Their speeds is slow with only a 2.5X real time speed up on batch transcriptions. Plus, there's little option for customization with just keyword boosting allowed. Support is also very poor. Can be deployed in the cloud or on premise.\n* **Architecture:** Traditional four-step STT model with added AI components\n* **Pros:**\n\n  * Brand name\n  * Easy to integrate if you are already in the Google ecosystem\n  * Good choice for short audio for command and response\n  * Good scalability, except for costs\n* **Cons:**\n\n  * Poor accuracy with business audio with lots of terminology\n  * Slow speed\n  * No support\n  * High costs\n* **Price:** $1.44/audio hour for standard models, $2.16/audio hour for enhanced models (assumes data logging opt-out; rounded up to 15-second increments in utterances)\n\n### 4. Speechmatics\n\n* **Summary:** Speechmatics is a UK company focused more on the UK market. Their accuracy is in the mid range but they're very slow, with a batch processing speed that is 2.5X real time. They're also one of the higher-priced ASR solutions on the market. They have limited customization with a custom library where you need to also provide the phonetic \"sounds like\" words for training. Can be deployed in the cloud or on premise.\n* **Architecture:** Traditional four-step STT model with added AI components\n* **Pros:**\n\n  * Fair accuracy\n* **Cons:**\n\n  * High cost\n  * Slow speed\n  * Limited customization\n* **Price:** $2.75/audio hour\n\n### 5. AssemblyAI\n\n* **Summary:** AssemblyAI's main advantage is their high accuracy in certain use cases that don't involve lots of terminology, jargon, or accent. Their speed is slow-4X real-time batch transcription-and adding an additional channel or other features reduces the speed even more. They have very limited customization in the form of keyword boosting, and hence it doesn't work well for terminology it's never heard or novel accents and dialects. Their language support is also very limited.\n* **Architecture:** End-to-end deep learning neural network\n* **Pros:**\n\n  * High accuracy for non-technical US English\n  * Low cost\n* **Cons:**\n\n  * Difficulty with lots of terminology, jargon, and accents\n  * Slow speed\n  * Limited customization\n* **Price:** $0.90/audio hour\n\n### 6. IBM Watson\n\n* **Summary:** IBM Watson's STT was a good early ASR provider but has been outpaced by other providers. They have very poor accuracy, are slow, and any customization will take months to years and cost thousands of dollars. Their batch transcription speeds are 4.3X and don't offer self training. Can be deployed in the cloud or on premise.\n* **Architecture:** Traditional four-step STT model with added AI components\n* **Pros:**\n\n  * Brand name\n* **Cons:**\n\n  * Poor accuracy\n  * Slow speed\n  * No self-training\n  * Slow customization\n* **Price:** $1.20/audio hour\n\n### 7. Microsoft Azure\n\n* **Summary:** Similar to Amazon and Google, Microsoft's main effort in STT has been developing a consumer product Cortana. Microsoft is trained on consumer data and is good at command-and-response, but not as good as Google or Amazon. Their accuracy is poor and on par with IBM Watson. Their speed is also slow at 4.3X real-time speed on batch. Limited customization with only custom vocabulary boosting. Can be deployed in the cloud or on premise.\n* **Architecture:** Traditional four-step STT model with added AI components\n* **Pros:**\n\n  * Brand name\n  * No real-time streaming\n  * Good choice for short audio for command and response\n  * Good scalability, except for costs\n* **Cons:**\n\n  * Poor accuracy with business audio or audio with lots of terminology\n  * Slow speed\n  * Limited customization\n  * High cost\n* **Price:** $1.40/audio hour\n\n### 8. Kaldi\n\n* **Summary:** Kaldi isn't technically a STT API, but it is one of the best-known open-source tools, so it's worth discussing here. Because Kaldi is not a ready built ASR solution, the ASR solution needs to be built from Kaldi and trained with various audio corpora with Kaldi to have an actual ASR solution. The biggest issue with Kaldi is the training data that is available to use. If the training data matches your real-world audio well, the accuracy is fair, if not, then it will be very poor. There is no support but documentation and an open source community.\n* **Architecture:** Traditional four-step STT model with the acoustic model using AI\n* **Pro:**\n\n  * Inexpensive\n* **Cons:**\n\n  * Very poor real world accuracy (20-40% with public training data)\n  * Requires a lot of self training to be usable\n  * Speed will be very slow due to architecture\n  * Lots of developer work needed to integrate well with your systems.\n* **Price:** $0.00/audio hour\n\n![](https://res.cloudinary.com/deepgram/image/upload/v1661976859/blog/best-speech-to-text-apis/Screen-Shot-2022-04-19-at-10.15.45-AM.png)\n\n## Conclusion\n\nThere you have it-our top eight speech-to-text APIs in 2022. We hope that this helps you demystify some of the confusion around the proliferation of options that exist in this space, and gives you a better sense of which provider might be the best for your particular use case. If you'd like to give Deepgram a try for yourself, you can sign up for a [free API key](https://console.deepgram.com/signup) or [contact us](https://deepgram.com/contact-us/) if you have questions about how you might use Deepgram for your STT needs.";
						}
						async function compiledContent$3V() {
							return load$3V().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3V() {
							return (await import('./chunks/index.b6635dd0.mjs'));
						}
						function Content$3V(...args) {
							return load$3V().then((m) => m.default(...args));
						}
						Content$3V.isAstroComponentFactory = true;
						function getHeadings$3V() {
							return load$3V().then((m) => m.metadata.headings);
						}
						function getHeaders$3V() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3V().then((m) => m.metadata.headings);
						}

const __vite_glob_0_27 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3V,
  file: file$3V,
  url: url$3V,
  rawContent: rawContent$3V,
  compiledContent: compiledContent$3V,
  default: load$3V,
  Content: Content$3V,
  getHeadings: getHeadings$3V,
  getHeaders: getHeaders$3V
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3U = {"title":"Brian Barrow Joins the Developer Relations Team","description":"Brian Barrow Joins the Developer Relations Team","date":"2021-11-09T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1641852668/blog/2021/11/brian-barrow-hello/disc-golf-cover.png","authors":["brian-barrow"],"category":"devlife","tags":["team"],"seo":{"title":"Brian Barrow Joins the Developer Relations Team","description":"Brian Barrow Joins the Developer Relations Team"},"shorturls":{"share":"https://dpgr.am/bd6aedb","twitter":"https://dpgr.am/aca9c1a","linkedin":"https://dpgr.am/a2122f6","reddit":"https://dpgr.am/9b57b72","facebook":"https://dpgr.am/b4ca151"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453795/blog/brian-barrow-hello/ograph.png"}};
						const file$3U = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/brian-barrow-hello/index.md";
						const url$3U = undefined;
						function rawContent$3U() {
							return "\r\nHi, I'm Brian. I am a Developer Experience Engineer here at Deepgram where I'll be helping build our developer education platform. My goal is to help developers in any way I can.\r\n\r\nI started my career in Supply Chain Management building spreadsheets to analyze purchasing decisions. I loved creating macros to build tools for my team to use in order to make a better informed decision. It was in building those spreadsheets that I learned the art of the Google search. After talking with some Frontend Developers at work one day I decided to look into learning to code and have loved it ever since.\r\n\r\nEarly on in my learning process I helped establish a meetup for the local FreeCodeCamp group in Salt Lake City. Meeting with other people in the learning process and presenting on what I had been learning was instrumental in my personal growth. I eventually decided to jumpstart my learning by attending a coding bootcamp. After completing that and getting my first job as a professional developer I started meeting weekly with the local FreeCodeCamp study group to do what I could to help others in their learning process. I also have also helped organize the UtahJS conference the past few years.\r\n\r\nPeople often talk about being \"passionate\" about what you do for a living. I can honestly say that I am not passionate about programming. However, I love the opportunites that programming gives me. I absolutely love teaching people and helping them learn new things. I love that being a developer has given me the opportunity to solve problems in my day to day work and work with really smart and interesting people. So while I'm not passionate about development itself, I am passionate about the things it allows me to do.\r\n\r\nOutside of programming I have a lot of things keeping me busy. I love spending time with my wife and two kids. When I'm not spending time with them at any number of activities, I'm probably playing disc golf or watching it on YouTube. I was fortunate to be able to witness [the greatest shot in the sport's history this summer](https://www.youtube.com/watch?v=0lpcB5PJBVo).\r\n\r\n        ";
						}
						async function compiledContent$3U() {
							return load$3U().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3U() {
							return (await import('./chunks/index.c12426e0.mjs'));
						}
						function Content$3U(...args) {
							return load$3U().then((m) => m.default(...args));
						}
						Content$3U.isAstroComponentFactory = true;
						function getHeadings$3U() {
							return load$3U().then((m) => m.metadata.headings);
						}
						function getHeaders$3U() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3U().then((m) => m.metadata.headings);
						}

const __vite_glob_0_28 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3U,
  file: file$3U,
  url: url$3U,
  rawContent: rawContent$3U,
  compiledContent: compiledContent$3U,
  default: load$3U,
  Content: Content$3U,
  getHeadings: getHeadings$3U,
  getHeaders: getHeaders$3U
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3T = {"title":"Build a Live Streaming Web Application: Vue and Express Setup","description":"In this series, learn how to build a live streaming web application using Deepgram's speech-to-text API and Amazon Interactive Video Service.","date":"2022-03-18T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1647979609/blog/2022/03/build-a-livestream-web-application-vue-and-express-setup/Building-Livestreaming-w-AmazonIVS.jpg","authors":["sandra-rodgers"],"category":"tutorial","tags":["aws","javascript","serverless"],"seo":{"title":"Build a Live Streaming Web Application: Vue and Express Setup","description":"In this series, learn how to build a live streaming web application using Deepgram's speech-to-text API and Amazon Interactive Video Service."},"shorturls":{"share":"https://dpgr.am/c1a13f1","twitter":"https://dpgr.am/e7a043f","linkedin":"https://dpgr.am/ec098b9","reddit":"https://dpgr.am/952b793","facebook":"https://dpgr.am/3d3cf68"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454011/blog/build-a-livestream-web-application-vue-and-express-setup/ograph.png"}};
						const file$3T = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/build-a-livestream-web-application-vue-and-express-setup/index.md";
						const url$3T = undefined;
						function rawContent$3T() {
							return "\n## Introduction\n\n<Panel type=\"info\" title=\"Build a Live Streaming Web Application with Amazon IVS and Deepgram (SERIES)\">\n<ol> \n<li><a href=\"https://blog.deepgram.com/build-a-livestream-web-application-with-amazon-ivs-and-deepgram/\">How to Build a Live Streaming Web Application with Amazon IVS and Deepgram</a></li>\n<li><a href=\"https://blog.deepgram.com/build-a-livestream-web-application-vue-and-express-setup/\"> Build a Live Streaming Web Application: Vue and Express Setup</a></li>\n<li><a href=\"https://blog.deepgram.com/how-to-write-vue-3-composables-for-a-third-party-API-integration/\"> How to Write Vue 3 Composables for a Third-Party API Integration</a></li>\n\n<li><a href=\"https://blog.deepgram.com/asynchronous-logic-to-write-a-vue-3-and-deepgram-captions-component/\"> Asynchronous Logic to Write a Vue 3 and Deepgram Captions Component</a></li>\n</ol>\n</Panel>\n\nIn my last post, I introduced how to build a vanilla Javascript and HTML live streaming web application with text captions. It included a barebones frontend that was just one page - the video player and captions.\n\nToday I'm going to start building a full-stack application using the same technology, except instead of vanilla Javascript, I will use Vue 3.\n\nThe full-stack application will be expanded from the barebones version to include features that are often necessary for real-world applications, such as:\n\n*   a landing page that asks for a code to enter the site\n*   a router (**Vue Router**) to implement routes and a navigation guard so users can't access beyond the entry page unless they enter the correct code\n*   a state management library (**Vuex**) which will update state to allow access to the stream page if the correct code is entered\n*   a backend server (**node.js** and **express**) with two endpoints, one for getting a **Deepgram** temporary key so I can keep my Deepgram API key secure, and one for verifying the secret code the user must input to enter the site\n\nI recommend reading [the first post](https://blog.deepgram.com/build-a-livestream-web-application-with-amazon-ivs-and-deepgram/#setting-up-amazon-ivs) to go through how to set up the video streaming technologies, **Open Broadcaster Software**, and **Amazon IVS**. And for an introduction to Vue 3, it might be worth checking out my series [Diving Into Vue 3](https://blog.deepgram.com/diving-into-vue-3-getting-started/).\n\n## What I'll Build Today\n\nIn this post, I'll set up the router (Vue Router) and Vuex so that I can put a navigation guard on the entry page. I will also set up the backend server so I can keep my entry code secure.\n\nHere is the diagram to show the data flow and structure for what I will build today. The focus will be on building the protected entry with a navigation guard. If it seems complicated in the diagram, it won't by the end of the post because I plan to walk-through building this feature step-by-step.\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1647979619/blog/2022/03/build-a-livestream-web-application-vue-and-express-setup/ProtectedEntrytoSite.png\" alt=\"Protected entry diagram\" style=\"width: 75%; margin:auto;\">\n\n## Setting up the Vue Project\n\nAfter [installing the Vue CLI](https://cli.vuejs.org/guide/installation.html), I can create a new Vue project with this command:\n\n```bash\nvue create NAME-OF-PROJECT\n```\n\nI'll be sure to choose 'Manually select features' so I can add some dependencies during the creation of the project:\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1647979619/blog/2022/03/build-a-livestream-web-application-vue-and-express-setup/Presets.png\" alt=\"Presents including router and vuex\" style=\"width: 75%; margin:auto;\">\n\nThese are the presets I selected. I definitely recommend selecting Router and Vuex since those are required for this project, but the others are just my personal preference :\n\n![Presets for linting and router](https://res.cloudinary.com/deepgram/image/upload/v1647979619/blog/2022/03/build-a-livestream-web-application-vue-and-express-setup/Presets2.png)\n\nI'll select Vue 3, and then I'll `cd` into the folder.\n\nI can install all the rest of the dependencies now, or I can install them one by one as I need them. Here is a list of the dependencies I'll be using in this project:\n\n*   vue-router@4 (already installed when I manually selected presets)\n*   vuex@next (already installed when I manually selected presets)\n*   express\n*   dotenv\n*   cors\n*   body-parser\n*   @deepgram/sdk\n\nI can install them all at once with this command:\n\n```bash\nnpm install vue-router@4 vuex@next express dotenv cors body-parser @deepgram/sdk\n```\n\n## The Folders Structure\n\nA Vue application that is created using the Vue CLI starts out with this structure:\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1647979619/blog/2022/03/build-a-livestream-web-application-vue-and-express-setup/Vue-Folder-Structure.png\" alt=\"Vue folder structure after creating new project\" style=\"width: 50%; margin:auto;\">\n\nThe `src` contains the folders that will make up the frontend, and I will add two files to the root of the project which will be required for the backend. Those two files will be a `server.js` file and a `.env` file. The final structure can be seen [here](https://github.com/deepgram-devs/livestream-amazonIVS-and-deepgram) at the root level of the project in its GitHub repo.\n\n### SRC Folder\n\nI'll go over the SRC folder in more detail because the way I organize this folder reflects my thinking about how I'll make the application work when considering how the pieces should fit together.\n\nHere is the file structure of the SRC for the final project (these folders can be set up now or as I progress through building the project):\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1647979619/blog/2022/03/build-a-livestream-web-application-vue-and-express-setup/SRC-Folder-Structure.png\" alt=\"Folder structure of SRC folder\" style=\"width: 50%; margin:auto;\">\n\nI'll go over the pieces starting from the bottom.\n\n### main.js\n\nThis is the file where Vue will be initialized, and also where Vue Router and Vuex (the store) will be brought into the project as plugins by way of `app.use()`.\n\n### App.vue\n\nThis is the top-most parent component, i.e., the **root component**. It holds all the Vue code (coming in as code in this file or code in child components) that will be injected into the `index.html` file, inside the `div` that has `id=\"app\"`.\n\n### views\n\nThese are the pages that will be the main routes of the project. There will be a page that the user first lands on where they must enter a code. I will name it `EnterCode.vue`. And there will be a page that shows the live stream, with the video player and audio captions. I'll name that page `StreamChannel.vue`.\n\nHere is a screenshot of what the views will look like by the end of this post. A demo of the finished project can be found in the first post, which shows the styled version with the video live stream page.\n\n![Image of two page views](https://res.cloudinary.com/deepgram/image/upload/v1647979619/blog/2022/03/build-a-livestream-web-application-vue-and-express-setup/Two-Page-Views.png)\n\n### store\n\nThis is the folder that contains the Vuex store, a state management file. Some state properties need to be widely available throughout the application, not just in one component or one parent-child component. Vuex makes those state properties that I have put in the store available in any Vue files throughout the project.\n\n### router\n\nThis is the folder that contains the routes and creates the router, so that each view (page) can be navigated to when a user clicks on the link to that page. Using Vue Router means that a user can switch between page views without the page being reloaded every time the route changes.\n\n### composables\n\nThis folder contains composition functions that run the logic to make stuff happen, such as getting the user's microphone or bringing in the Amazon IVS player. Composition functions, or 'composables', are a Vue 3 way of encapsulating logic that can then be run inside the setup function inside a component. Read more about composables [here](https://blog.deepgram.com/diving-into-vue-3-reusability-with-composables/#reusability-in-the-composition-api).\n\n### components\n\nThe components folder contains two components that will make up the StreamChannel page - the **VideoPlayer** component and the **AudioCaptions** component. I'll build these components in the next post in this series.\n\n## Set up Vue Router and the Views\n\nThe first thing I will do is create the two main pages I intend to make as part of this application - the landing page and the live stream page.\n\nI will create a views folder and put those two main page files in the folder:\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1647979619/blog/2022/03/build-a-livestream-web-application-vue-and-express-setup/Views.png\" alt=\"Views folder with files\" style=\"width: 50%; margin:auto;\">\n\nFor each file, I'll be sure to put in some HTML that will display on the page, so I can see that my router is working:\n\n```html\n<template>\n  <div>\n    <h1>Enter Code</h1>\n  </div>\n</template>\n```\n\nWhen I created my project with the Vue CLI, I opted to add the Vue Router dependency, which means I have a router already created for me - the `index.js` file in the `router` folder. The index.js already has two routes set up - home and about.\n\nIf I start up the development server, I see that the links at the top of the page will take me between the two routes.\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1647979619/blog/2022/03/build-a-livestream-web-application-vue-and-express-setup/Home-About-Links.png\" alt=\"Home and About links\" style=\"width: 30%; margin:auto;\">\n\nSince my project is only going to have two pages, I am just going to change the **home** and **about** views of the bootstrapped project to be the **EnterCode** view and the **StreamChannel** View.\n\nIn the router file, the three things each route object needs to have are:\n\n*   the path - the URL path for the route\n*   the name - the name I want to give this route (optional, but in my case, I do need it)\n*   the component - the component (view file) that will be loaded for this route\n\nHere is the code to set that up in the router:\n\n```js\nimport { createRouter, createWebHistory } from 'vue-router'\nimport EnterCode from '../views/EnterCode.vue'\n\nconst routes = [\n  {\n    path: '/',\n    name: 'EnterCode',\n    component: EnterCode,\n  },\n  {\n    path: '/stream-channel',\n    name: 'StreamChannel',\n    component: () =>\n      import(/* webpackChunkName: \"about\" */ '../views/StreamChannel.vue'),\n  },\n]\n\nconst router = createRouter({\n  history: createWebHistory(process.env.BASE_URL),\n  routes,\n})\n\nexport default router\n```\n\nI need to go back to App.vue and change the router-links to match my new routes. Even though I've named the landing page `EnterCode.vue`, I'll refer to it has **Home** for the user. Notice that the `to=\"\"` attribute contains the path:\n\n```html\n<template>\n  <nav>\n    <router-link to=\"/\">Home</router-link> |\n    <router-link to=\"/stream-channel\">Channel</router-link>\n  </nav>\n  <router-view />\n</template>\n```\n\nNow I see that my two main pages can be accessed (without reloading the page!) when I click the link to switch to each route:\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1647979619/blog/2022/03/build-a-livestream-web-application-vue-and-express-setup/Home-Channel-Links.png\" alt=\"Home (enter-code) and Channel Links\" style=\"width: 30%; margin:auto;\">\n\n### Vue Router Navigation Guards\n\nI want the landing page to have a form input where the user has to enter a code, and if the code is correct, they can move on to the channel page. See the code for this input [here](https://github.com/deepgram-devs/livestream-amazonIVS-and-deepgram/blob/vuex-no-server-yet/src/views/EnterCode.vue#L4).\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1647979619/blog/2022/03/build-a-livestream-web-application-vue-and-express-setup/Input.png\" alt=\"Input on enter code page\" style=\"width: 50%; margin:auto;\">\n\nBut if the code they enter is incorrect, I want to keep them from being able to navigate to that page. Right now, the channel page is completely open, and I can access it just by clicking the **Channel** link.\n\nI can set up a **navigation guard** to perform some logic when the router-link is clicked ([Read the docs](https://router.vuejs.org/guide/advanced/navigation-guards.html) for more information about navigation guards).\n\nThe logic will check to see if the correct code has been entered. If it has, the router will navigate to the channel page. If not, it will send them back to the home (`EnterCode.vue`) page.\n\nTo add a navigation guard, I use the `beforeEnter` guard:\n\n```js\n{\n path: \"/stream-channel\",\n name: \"StreamChannel\",\n component: () => import(\"../views/StreamChannel.vue\"),\n beforeEnter(to, from, next) {\n   // ADD LOGIC HERE to check state of allowed access\n   next();\n },\n},\n```\n\nThe `next()` function will move the navigation forward to the route. I can use an **if statement** to keep that from happening unless certain circumstances are met.\n\nIf I include a name of a route in the next() function, the navigation will move to the route I have identified by name. In this example, it would cause navigation to stay on the **EnterCode** page since I have identified that `next` should move me to that route.\n\n```js\nbeforeEnter(to, from, next) {\n  // ADD LOGIC HERE to check state of allowed access\n  next({ name: \"EnterCode\" });\n},\n```\n\nI need to have some way of keeping track of the state of whether the correct code was entered or not. I will use the state management system, Vuex, which means I can now put this code in my router since the very next thing I will do is set up the store so there is a state property for `allowAccess`:\n\n```js\n{\n path: \"/channel\",\n name: \"StreamChannel\",\n component: () => import(\"../views/StreamChannel.vue\"),\n beforeEnter(to, from, next) {\n   if (store.state.allowAccess === true) {\n     next();\n   } else {\n     next({ name: \"EnterCode\" });\n     alert(\"Please enter the secret code\");\n   }\n },\n}\n```\n\nI'll probably see this error in the browser now since I've referenced `store.state.allowAccess`, but the router file doesn't know what the `store` instance is.\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1647979619/blog/2022/03/build-a-livestream-web-application-vue-and-express-setup/error-store.png\" alt=\"Error: 'store' is not defined\" style=\"width: 50%; margin:auto;\">\n\nTo make it go away, I need to import the store into the router by adding the following import statement to the router folder's `index.js` file.\n\n```js\nimport store from '../store'\n```\n\nThe [GitHub repo](https://github.com/deepgram-devs/livestream-amazonIVS-and-deepgram/blob/vuex-no-server-yet/src/router/index.js) has the code in its entirety for this stage of the project.\n\n## Set up Vuex to Manage State\n\nEven though this is a small application, I am choosing to include a Vuex store because it is common to use Vuex for the purpose of keeping track of whether a user is authenticated/logged-in or not.\n\nIn this project, I will use the Vuex store to keep track of whether a correct entry code has been entered or not by the user. The state property will be called `allowAccess`, and its default state will be false. It will change to true when a user enters the correct code.\n\nHere is the store with the state property I have created:\n\n```js\nimport { createStore } from 'vuex'\n\nconst store = createStore({\n  state() {\n    return {\n      allowAccess: false,\n    }\n  },\n})\nexport default store\n```\n\nI need to connect the input on the landing (EntryCode) page with this property in state. When the user types a correct code into that input, the submit button triggers a function that checks if the code is correct, and then if it is, dispatches an action to the store, which will cause `allowAccess` to update to `true`.\n\nThe way Vuex causes state to change is through this pattern:\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1647979626/blog/2022/03/build-a-livestream-web-application-vue-and-express-setup/vuex.png\" alt=\"Vuex pattern\" style=\"width: 80%; margin:auto;\">\n\nVue Component dispatch action -> Store action commit mutation -> Store mutation change state\n\nHere is that flow in the actual code in my project.\n\n1.  The Vue component *EnterCode* form submit button triggers `submitCode()`:\n\n```html\n<form @submit.prevent=\"submitCode\">\n  <label for=\"code\"> Code: </label>\n  <input v-model=\"code\" type=\"password\" name=\"code\" value />\n  <button type=\"submit\" name=\"button\" class=\"dg-btn\">Submit</button>\n</form>\n```\n\n2.  The `submitCode()` method dispatches the `verifyCode` action (which is [passed as a string](https://vuex.vuejs.org/guide/actions.html#dispatching-actions) `'verifyCode'`) with a *payload* of `true` or `false`:\n\n```js\nfunction submitCode() {\n  if (code.value === 'pb') {\n    store.dispatch('verifyCode', true)\n  } else {\n    store.dispatch('verifyCode', false)\n  }\n  code.value = ''\n}\n```\n\n3.  The `verifyCode` action in the store commits the `verifyCode` mutation, sending the `true` or `false` payload, referred to hear as `status`:\n\n```js\nactions: {\n verifyCode({ commit }, status) {\n   commit(\"verifyCode\", status);\n },\n},\n```\n\n4.  The `verifyCode` mutation changes state so `allowAccess` equals the `status` payload of `true` or `false`\n\n```js\nmutations: {\n verifyCode(state, status) {\n   state.allowAccess = status;\n },\n},\n```\n\nThe last thing to do is navigate with the router to the *StreamChannel* page if a correct code has been entered (i.e., state for `allowAccess` in the store has changed to true) or alert the user if they have entered the wrong code.\n\n```js\n//StreamChannel.vue\n\nfunction submitCode() {\n  if (code.value === 'PB') {\n    store.dispatch('verifyCode', true)\n  } else {\n    store.dispatch('verifyCode', false)\n  }\n  // Navigate if correct code, alert if not correct\n  if (store.state.allowAccess) {\n    router.push({ name: 'StreamChannel' })\n  } else {\n    alert('Incorrect code')\n  }\n  code.value = ''\n}\n```\n\nThis version of the project, with the setup I just did for Vue Router and Vuex, can be seen in its entirety in the repo branch titled [vuex-no-server-yet](https://github.com/deepgram-devs/livestream-amazonIVS-and-deepgram/tree/vuex-no-server-yet).\n\n### Security\n\nThe example so far is not very secure because I check for the correct code on the frontend, and that entry code is right there in plain Vue (no pun intended) in the client:\n\n```js\nif (code.value === \"pb\")\n```\n\nThe more secure way to handle this would be to create a backend server file with an endpoint to check if the code is correct and store that code securely in the server, or even better (so it does not get exposed in GitHub) in a `.env` file.\n\nIn the next section, I'll introduce that more secure method of checking the code. I'll still use everything I just set up for Vuex, but instead of checking the code in the `EnterCode.vue` file, I'll use a fetch request to send that code to the backend for verification.\n\n## Create the Server with Express\n\nNow I'll set up my server file, which I'll use today for making the entry code more secure, and which I'll use in the next post for setting up the Deepgram token endpoint.\n\nAt the root of the project, I'll create a `server.js` file and a `.env` file.\n\nI'll add this code to create my basic server.\n\n```js\n// bring in node modules\nrequire('dotenv').config()\nconst express = require('express')\nconst app = express()\nvar bodyParser = require('body-parser')\nconst cors = require('cors')\n\n// identify port number\nconst port = 8080\n\n// express use() function to add third-party middleware\napp.use(cors())\napp.use(bodyParser.json())\n\n// ENDPOINT WILL GO HERE\n\n// Connect host to port\napp.listen(port, () => {\n  console.log(`Example app listening at http://localhost:${port}`)\n})\n```\n\n### Dependencies\n\nHere is a brief explanation of each dependency I'm using in this server file.\n\n#### express\n\n[Express](https://expressjs.com/) is a *node.js* framework that gives me utility methods and middleware to help with setting up routing endpoints (the endpoints deal with requests that come in from the frontend).\n\n#### body-parser\n\nBody-parser is middleware that can take an incoming request body and parse the data. I'll be using the JSON parser because the secret entry code will be sent from the frontend as JSON.\n\n#### cors\n\nThis is another middleware package that will help to handle requests from the frontend, specifically cross-origin requests.\n\nSince the client and server have a different origin from each other (such as localhost:8080 vs. localhost:8081), I would need to add a CORS response header `access-control-allow-origins` with information about permitted origins. The CORS middleware will add that header automatically for every request that is sent to the server.\n\n#### dotenv\n\nThis is a very important node module. It allows me to use environment variables in my server file, pulling the values for those variables from the `.env` file. The `.env` file will never be tracked by git, so I can put anything super-secret in the `.env` file and not worry that it will end up on Github for the world to see.\n\n### Run the server\n\nNow I can start up the server to check that everything is working so far.\n\nI will add a script command to the `package.json` file to make it easier to start up the server. In my `package.json`, in the \"scripts\" object, I'll add one for \"start\":\n\n```js\n\"scripts\": {\n  \"serve\": \"vue-cli-service serve\",\n  ...\n  \"start\": \"node server.js\"\n},\n```\n\nNow, in the terminal, when I'm in the project folder, I can type `npm run start` to start the backend server, and if I open another terminal, I can type `npm run serve` to start the frontend development server.\n\n### Add the secret code value to `.env`\n\nBefore I set up the endpoint, I want to put the expected code value in the `.env` file so that I can access it in the server using `process.env.SECRET_CODE`.\n\nIn the `.env` file, I will add the secret code that I want users to type into the input on the frontend to be able to enter the site. I can add any value I want. In this case, I'll just make the value 'code' (in the earlier section, I used 'pb' as the code, but I'm using 'code' here to make it more obvious that this is the code):\n\n```bash\nSECRET_CODE=\"code\"\n```\n\n### Create the secret-code endpoint\n\nNow I'll start on the backend creating the endpoint. The endpoint will expect a value to come in from the frontend (the code entered by the user). Since a value is being sent back, this will be a POST request endpoint.\n\n```js\napp.post('/secret-code', async (req, res) => {})\n```\n\nThe app.post() method is available because of express, and it requires the route path as a slash and whatever I want to name the path, in this case `\"/secret-code\"`.\n\nWhen the code is sent from the frontend to the backend, it comes in as part of the request body (which is an object), so I will get the code value from `req.body`. Because I plan to send it back in a JSON object as `{ code: \"code\" }`, the value will come back as `req.body.code`.\n\n```js\nif (req.body.code._value === process.env.SECRET_CODE)\n```\n\nHowever, the value that is sent back is put into another object, which I see when I examine the shape of the req.body.code by using `console.log`. I see this:\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1647979619/blog/2022/03/build-a-livestream-web-application-vue-and-express-setup/reqbodycode.png\" alt=\"req.body.code object shape\" style=\"width: 50%; margin:auto;\">\n\nThe `_value` property actually contains the value that I want to match to my code value I have put in the `.env` file. So I need to access it with `req.body.code._value`. This is how I can check that there is a match:\n\n```js\napp.post('/secret-code', async (req, res) => {\n  if (req.body.code._value === process.env.SECRET_CODE) {\n    res.status(200).json('Correct code')\n  } else {\n    res.status(200).json('Incorrect code')\n  }\n})\n```\n\nThe `res.status()` method will send back the result status code, and the .json() method will make sure it returns to the frontend as JSON data.\n\nNow I can go back to the frontend and write a fetch request that will send the secret code that I need for this verification.\n\n### Fetch POST request\n\nTo write a fetch POST request, I will use [the fetch API](https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API/Using_Fetch).\n\nTo write a fetch request, I use the `fetch()` method with a first argument of the endpoint path (including the port number) and the second argument an options object:\n\n```js\nfetch('http://localhost:8080/secret-code', requestOptions)\n```\n\nThe options object will give information about the type of request (POST), the headers (to tell that the content will be of the type JSON) and the request body (I will send a JSON object that contains the code):\n\n```js\nconst requestOptions = {\n  method: 'POST',\n  headers: { 'Content-Type': 'application/json' },\n  body: JSON.stringify({ code: code }),\n}\n```\n\nThe fetch request will send the code to the backend server, and since that will take a little bit of time, I will use a promise to wait for the response, doing the following action once that response has been returned. To use a promise, I will attach a `.then()` to the request, which means that anything inside the `.then()` will happen after the response is returned from the fetch request.\n\nOnce the response returns, I will turn it back into JSON and then take that response data to verify if it is correct or not. Depending on if it is correct, I update the state in the store.\n\nRemember, for a verified response of true, I am expecting the backend server to return a response with a JSON string of `\"Correct code\"`:\n\n```js\nfetch('http://localhost:8080/secret-code', requestOptions)\n  .then((response) => response.json())\n  .then((data) => {\n    if (data === 'Correct code') {\n      //dispatch payload to store\n    }\n  })\n  .then(() => {\n    if (store.state.allowAccess) {\n      // navigate to channel or alert that code is wrong\n    }\n  })\n```\n\nI've created a branch in the repo called [vuex-with-server ](https://github.com/deepgram-devs/livestream-amazonIVS-and-deepgram/blob/vuex-with-server/src/views/EnterCode.vue)where the code from this step can be viewed in its entirety.\n\n## Conclusion\n\nIn this post, I set up a Vue project and added some of the common features used in a full-stack application: routes, a state management library, and a backend server with endpoints.\n\nNow I am ready to build the **Channel** page, which will contain the video player for my live stream and the audio captions that display the transcript.\n\nPreviously, I went over how to build a live stream web application with **Amazon IVS**, using only vanilla javascript and HTML. But now, I want to use the Vue framework to help me build this in a way that is more reusable, and that incorporates common practices for using Vue.js.\n\nIn my next post, I will introduce how to build the **Amazon IVS** player using **Vue 3 composables**, which will give me a chance to discuss more specifically some of the new features of Vue 3.\n\nI will also introduce how to use Vue 3 composables to create the **Deepgram captions**, and I'll include a more secure way of connecting to Deepgram by requesting a Deepgram token from the backend.\n\nI hope you'll join me for my next post. As always, feel free to reach out on [Twitter](https://twitter.com/sandra_rodgers_).\n\n        ";
						}
						async function compiledContent$3T() {
							return load$3T().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3T() {
							return (await import('./chunks/index.f9ebd95f.mjs'));
						}
						function Content$3T(...args) {
							return load$3T().then((m) => m.default(...args));
						}
						Content$3T.isAstroComponentFactory = true;
						function getHeadings$3T() {
							return load$3T().then((m) => m.metadata.headings);
						}
						function getHeaders$3T() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3T().then((m) => m.metadata.headings);
						}

const __vite_glob_0_29 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3T,
  file: file$3T,
  url: url$3T,
  rawContent: rawContent$3T,
  compiledContent: compiledContent$3T,
  default: load$3T,
  Content: Content$3T,
  getHeadings: getHeadings$3T,
  getHeaders: getHeaders$3T
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3S = {"title":"How to Build a Live Streaming Web Application with Amazon IVS and Deepgram","description":"In this series, learn how to build a live streaming web application using Deepgram's speech-to-text API and Amazon Interactive Video Service.","date":"2022-03-11T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1646946075/blog/2022/03/build-a-livestream-web-application-with-amazon-ivs-and-deepgram/Building-Livestreaming-w-AmazonIVS.jpg","authors":["sandra-rodgers"],"category":"tutorial","tags":["aws","javascript","serverless"],"seo":{"title":"How to Build a Live Streaming Web Application with Amazon IVS and Deepgram","description":"In this series, learn how to build a live streaming web application using Deepgram's speech-to-text API and Amazon Interactive Video Service."},"shorturls":{"share":"https://dpgr.am/3f16a52","twitter":"https://dpgr.am/bca64b1","linkedin":"https://dpgr.am/d95369c","reddit":"https://dpgr.am/4ad64e5","facebook":"https://dpgr.am/347b3d4"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454013/blog/build-a-livestream-web-application-with-amazon-ivs-and-deepgram/ograph.png"}};
						const file$3S = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/build-a-livestream-web-application-with-amazon-ivs-and-deepgram/index.md";
						const url$3S = undefined;
						function rawContent$3S() {
							return "\nIn this series, I will build a live streaming web application with text captions. Companies such as Twitch, StreamYard, Facebook Live, and many others provide live streaming on the web as a product, so I got curious about how I might go about building my own version.\n\n<Panel type=\"info\" title=\"Build a Live Streaming Web Application with Amazon IVS and Deepgram (SERIES)\">\n<ol>\n<li><a href=\"https://blog.deepgram.com/build-a-livestream-web-application-with-amazon-ivs-and-deepgram/\">How to Build a Live Streaming Web Application with Amazon IVS and Deepgram</a></li>\n<li><a href=\"https://blog.deepgram.com/build-a-livestream-web-application-vue-and-express-setup/\"> Build a Live Streaming Web Application: Vue and Express Setup</a></li>\n<li><a href=\"https://blog.deepgram.com/how-to-write-vue-3-composables-for-a-third-party-API-integration/\"> How to Write Vue 3 Composables for a Third-Party API Integration</a></li>\n\n<li><a href=\"https://blog.deepgram.com/asynchronous-logic-to-write-a-vue-3-and-deepgram-captions-component/\"> Asynchronous Logic to Write a Vue 3 and Deepgram Captions Component</a></li>\n</ol>\n</Panel>\n\nThe main technologies I will use to build the live stream and text captions functionality are:\n\n*   **Open Broadcast Software** - an open-source software used to capture video streams\n*   **Amazon IVS** - an AWS service that can receive a video stream from OBS and put that stream into the browser, optimizing the entire process\n*   **Deepgram** - a speech-to-text API that can receive an audio stream and return a text-transcript\n\nI will build two versions of this project. I'll use the following to build each project:\n\n1.  **HTML/Javascript** - The first version I build (the **vanilla version**) will be focused on creating a **front-end** with a very limited back-end, emphasizing the most barebones approach to getting the application working.\n\n2.  **Vue.js/Node.js** - For the second version (the **framework version**), I will use Vue.js, a Javascript framework that gives me the tools I need to more easily include important features for security like routing and navigation guards. I will build a **full-stack** video streaming application with a **node.js** server to help me add a layer of security for some of the data I need to protect.\n\nHere is a gif to demonstrate the final project:\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1646946089/blog/2022/03/build-a-livestream-web-application-with-amazon-ivs-and-deepgram/VideoExample.gif\" alt=\"demo of final project\" style=\"width: 75%; margin:auto;\">\n\n## Project One: Vanilla Video Stream Player\n\nNow I'll start by building the first version of the project. I'll build a 'vanilla' video streaming player in the sense that I will only use HTML and Javascript on the front-end, and the only back-end will be the work I do to get Amazon IVS set up to receive the OBS stream.\n\nI want to keep it as simple as possible, focusing on how to build a **video streaming player in the browser that includes text captions**. This means I will not take into account real-world requirements such as hiding API keys or creating an entry page to restrict access to the video stream. Later, in the Vue.js version I build, I'll add those pieces, but to start, I just want to get the video player working - I want it to play my live stream and display text captions for what I'm saying as I stream to viewers.\n\nHere is a diagram to demonstrate the core technology for the **video streaming part** of the project:\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1646946508/blog/2022/03/build-a-livestream-web-application-with-amazon-ivs-and-deepgram/VideoStream.png\" alt=\"Video stream diagram\" style=\"width: 75%; margin:auto;\">\n\nThe diagram presents this flow: the webcam takes in the video stream --> OBS captures that video stream so it can be sent along to Amazon IVS --> Amazon IVS provides a service to take in the stream, optimize it, and send it in a format to the browser so that it can be used in an HTML video player --> the HTML video element plays the optimized video stream.\n\nHere is a diagram to demonstrate the core technology for the **text captions part** of the project:\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1646946508/blog/2022/03/build-a-livestream-web-application-with-amazon-ivs-and-deepgram/AudioTranscription.png\" alt=\"Audio transcription diagram\" style=\"width: 75%; margin:auto;\">\n\nThe general flow for the text captions technology will be something like this:\n\nThe browser Media Streams API gets permission to use the browser microphone --> the microphone takes in an audio stream of the audio that plays from the live stream --> The Deepgram API opens a web socket channel in the browser to send the audio stream to Deepgram --> the browser receives the JSON object in return that contains the text-transcript --> Javascript puts the text onto the page as captions as the video is playing.\n\nNow that I have a high-level picture of how this project will be built, I am ready to build the barebones front-end video stream application. (In the next post in this series, I will build the Vue.js/Node.js full-stack application with added functionality.)\n\n## Setting up Open Broadcast Software\n\nThe first thing I need is software to capture my video stream on my computer. I'll use the [Open Broadcast Software](https://obsproject.com/).\n\n### What is OBS?\n\nFor anyone serious about streaming, OBS is a powerful tool. It is a free open source software that gives many configuration options for capturing and editing a stream. I can edit every aspect of my stream and create scenes made up of multiple sources such as images, text, etc. I can mix audio, switch between scenes with transitions, adjust the layouts, and so much more.\n\nThe stream I capture in OBS can be connected to a streaming platform such as Twitch, YouTube, or others, and it will deliver my stream to that platform; however, for this project, my goal is to stream to a web application that I make myself.\n\nOBS takes some effort to learn, but I only need to familiarize myself with a few parts of it if I'm going to set it up to capture a stream and connect to Amazon IVS (Amazon IVS is not a streaming platform - it is more like an SDK that helps make the stream easier to handle when I build my front-end).\n\n### Set up Live Streaming with OBS\n\nTo set up OBS for my project, I will:\n\n1.  Go to [obsproject.com](https://obsproject.com/) and choose the operating system I use. I'll download the software.\n\n2.  Run the OBS software. In the **Sources** panel, I'll click the **plus** sign to add a new source. I'll select **Video Capture Device**, and in the window that pops up, I'll select the camera I want to use to capture my stream (my computer camera or webcam).\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1646946718/blog/2022/03/build-a-livestream-web-application-with-amazon-ivs-and-deepgram/VideoCaptureDevice.png\" alt=\"Select Video Capture Device to add source from computer camera\" style=\"width: 80%; margin:auto;\">\n\n3.  Make sure the source is selected in the sources panel (I may have other sources that I have set up), then I'll click on **Settings** in the far-right **Controls** panel.\n\n4.  Select **Stream** in the left column of the window that opens up. The **Service** will remain **Custom**, but I notice that I could select a streaming platform such as Twitch or YouTube if I weren't planning to build my own streaming application.\n\n5.  There is nothing more to do until I create the Amazon IVS channel. But I know that later I will take the **Server** and the **Stream Key** information from Amazon IVS for the specific channel I create in the AWS console.\n\n![Stream settings](https://res.cloudinary.com/deepgram/image/upload/v1646946718/blog/2022/03/build-a-livestream-web-application-with-amazon-ivs-and-deepgram/StreamSettings.png)\n\n## Setting up Amazon IVS\n\nIn this step, I will create an Amazon IVS channel, which is where my video stream from OBS will feed into once I connect them.\n\n### What is Amazon IVS?\n\nThe 'IVS' in Amazon IVS stands for Interactive Video Service. The website for [Amazon IVS](https://aws.amazon.com/ivs/) describes it as a \"managed live streaming solution\" that I can use to send \"live streams to Amazon IVS using streaming software\" and \"make low-latency live video available to any viewer around the world.\" In addition, I \"can easily customize and enhance the audience experience through the Amazon IVS player SDK.\"\n\nSo what does this mean?\n\nThe fact is, building a video player browser can be very complicated. Amazon IVS takes away much of the challenge, allowing me to focus on the design of my front-end rather than the nitty-gritty of the video player. If I did it all without Amazon IVS, I could use the HTML native [video tag](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/video#usage_notes), but then there would be much to do to optimize the video stream that comes through (there's an interesting article about this [here](https://medium.com/canal-tech/how-video-streaming-works-on-the-web-an-introduction-7919739f7e1)). A developer could dedicate their entire career to getting good at building stuff that manages audio and video streams in the browser.\n\nAmazon IVS will optimize the stream to make it work for viewers watching it in my web application anywhere in the world. It also provides an SDK for the video player, which I can bring into the browser by adding a script. That script will take control of the video element in my HTML and add all the magic that Amazon IVS does under the hood. The Amazon IVS video player is built for the purpose of streaming live video, so I don't have to build my own complicated video player.\n\nOne important thing to consider is cost. AWS is not free, and while it is very cost-effective for a bigger streaming platform like Twitch (the Twitch streaming technology is powered by Amazon IVS), an individual developer like myself building a small project for fun might not find it to be the best option.\n\nThe good news is a new user of Amazon IVS can enjoy the free tier, which gives the following:\n\n*   Five hours of live video input for a basic channel per month\n*   100 hours of SD live video output per month\n\nThis is enough to build this project and not be charged, as long as I am careful about turning off my stream in OBS when I'm not using it. (Yes, I did forget to do this one time and clocked several hours in Amazon IVS.) Be sure to read through the [pricing details](https://aws.amazon.com/ivs/pricing/) and be vigilant about turning off the stream when you don't need it to be on.\n\n### Set up Amazon IVS\n\nNow I'll set up a channel in Amazon IVS. The channel will take my video stream from the OBS software on my computer and make that stream available in a video player that I will bring into the browser with the Amazon IVS SDK (so many acronyms!).\n\nIn order to do this, I'll need to [create an AWS account](https://portal.aws.amazon.com/billing/signup). This will require billing information.\n\nIn addition, I'll need to set up AWS Identity and Access Management (IAM), which adds a 'policy' to my account that allows me to create an AWS IVS channel. This is standard for doing anything in AWS - the first step is to configure IAM so that users of the AWS console have specific permissions. I am the only user of my console, so I'm not worried about restricting any permissions in my account.\n\n[This guide](https://docs.aws.amazon.com/ivs/latest/userguide/getting-started-iam-permissions.html) walks through how to set up the IAM permissions so that a user can create an AWS IVS channel.\n\nNow I can navigate to IVS to create a channel. In the top search bar, I can type 'IVS' to find Amazon Interactive Video Service.\n\n![AWS search bar](https://res.cloudinary.com/deepgram/image/upload/v1646946745/blog/2022/03/build-a-livestream-web-application-with-amazon-ivs-and-deepgram/SearchBar.png)\n\nThis takes me to the Amazon IVS console. I will click the **Create channel** button to create my channel.\n\n![Click button to create IVS channel](https://res.cloudinary.com/deepgram/image/upload/v1646946808/blog/2022/03/build-a-livestream-web-application-with-amazon-ivs-and-deepgram/CreateChannel.png)\n\nI can name my stream and stick with the **Default** configuration. Then I'll scroll down and click **Create channel**.\n\n![Set up IVS with default configuration](https://res.cloudinary.com/deepgram/image/upload/v1646946808/blog/2022/03/build-a-livestream-web-application-with-amazon-ivs-and-deepgram/IVSSetup.png)\n\nThis will create the channel and then put me on that channel's page in the console. This is where I can configure the channel and get the information I need to connect my video stream in OBS to this channel. I need to find this section of the page:\n\n![Info about channel for OBS and video player](https://res.cloudinary.com/deepgram/image/upload/v1646946838/blog/2022/03/build-a-livestream-web-application-with-amazon-ivs-and-deepgram/ChannelInfo.png)\n\nThere are three pieces of information I am going to need for my project. Two are to connect OBS to Amazon IVS, and one is to bring the stream from Amazon IVS (with all its optimizations) into the browser video player:\n\n*   **Ingest server** - put this in OBS settings for my stream\n*   **Stream key** - put this in OBS settings for my stream\n*   **Playback URL** - use this as the src for my script that I put in the video player\n\nI have already set up OBS, so I can just go back to the settings for my stream and add the **Ingest server** and **Stream key**. The **Playback URL** will be used later.\n\n![Stream settings](https://res.cloudinary.com/deepgram/image/upload/v1646946718/blog/2022/03/build-a-livestream-web-application-with-amazon-ivs-and-deepgram/StreamSettings.png)\n\nNow, if I go back to the OBS controls and click on **Start Streaming**, my stream should be fed to Amazon IVS, and I am able to see it in the Amazon IVS channel page where it says **Live stream**:\n\n![Live stream in channel page](https://res.cloudinary.com/deepgram/image/upload/v1646946953/blog/2022/03/build-a-livestream-web-application-with-amazon-ivs-and-deepgram/LiveStream.png)\n\n### Connect Front-end Video Player to Amazon IVS\n\nThe back-end is done (AWS takes care of most of the work). Now I can build the front-end, which I will do using vanilla Javascript and HTML.\n\nIn the `<head>` tag of my HTML document, I will include the script for the Amazon IVS player. Amazon IVS explains how to do this setup [here](https://docs.aws.amazon.com/ivs/latest/userguide/player-web.html), for those who want to go straight to the source.\n\n```html\n<head>\n  <meta charset=\"UTF-8\" />\n  <title>Video Stream Demo</title>\n  <script src=\"https://player.live-video.net/1.6.1/amazon-ivs-player.min.js\"></script>\n</head>\n```\n\nThis will load the IVS Player, and I will have access to the `IVSPlayer` variable in the global context. I can type that variable into the console to take a look at the module that has been loaded. There are quite a few properties that could be of use to me, depending on my project's needs.\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1646946972/blog/2022/03/build-a-livestream-web-application-with-amazon-ivs-and-deepgram/ConsoleIVSPlayer.png\" alt=\"IVSPlayer in console\" style=\"width: 50%; margin:auto;\">\n\nIn the `<body>` tag, I will include a `<video>` player that has an `id` of `video-player` (this id can be renamed, as long as the javascript I write to find this element looks for that specific id).\n\n```html\n<body>\n  <video\n    width=\"520\"\n    height=\"440\"\n    id=\"video-player\"\n    controls\n    playsinline\n  ></video>\n</body>\n```\n\nIn the browser, I see the video player, but there is no stream coming through. That is because I have only brought in the Amazon IVS player; I have not yet connected the player to my stream channel.\n\nI will use javascript to put my stream channel into the player.\n\n```js\n<script>\nif (IVSPlayer.isPlayerSupported) {\n  const player = IVSPlayer.create();\n  player.attachHTMLVideoElement(document.getElementById(\"video-player\"));\n  player.load(\"PLAYBACK_URL\");\n  player.play();\n}\n</script>\n```\n\nWhere it says `PLAYBACK_URL` in the code example, I need to put the string for my playback URL, which I can find in the Amazon IVS console for my channel.\n\n![Playback URL](https://res.cloudinary.com/deepgram/image/upload/v1646946995/blog/2022/03/build-a-livestream-web-application-with-amazon-ivs-and-deepgram/PlaybackConfiguration.png)\n\nThen I can turn on my stream in OBS, and I should see my stream in the browser!\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1646947012/blog/2022/03/build-a-livestream-web-application-with-amazon-ivs-and-deepgram/StartStream.png\" alt=\"OBS Start stream\" style=\"width: 50%; margin:auto;\">\n\n### Use Deepgram to Create Text Captions\n\nThe second part of this project, after getting the live stream video player working, is creating text captions. The captions will display what is being said in the live stream as I am streaming.\n\nI will need to do two things: use my computer's microphone to listen to the audio that is being outputted from the live stream, and then send that audio stream to Deepgram to turn it into a text transcription.\n\n### What is the Media Streams API?\n\nThe browser contains several APIs for working with audio and video. I need to use one that lets me **gain access to the user's microphone**. If I can gain that access, I can record the audio from the live stream and send it on to Deepgram to get the text transcript.\n\nThe **Media Streams API** contains many interfaces and methods for working with **audio and video data**. There is already a really great guide for how it works [here](https://blog.deepgram.com/getting-started-with-mediastream-api/), so I won't go over all the details. I just need to understand that the Media Streams API has so much that I can use when I'm working with audio or video data in the browser. In fact, I'm pretty sure the Amazon IVS SDK uses it under the hood as part of their video player.\n\n### Get Audio with Media Streams API\n\nI will use the `getUserMedia` method from this API. To get access to the user's microphone, I can write this javascript:\n\n```js\n<script>\n//Get access to user's microphone\nnavigator.mediaDevices.getUserMedia({ audio: true }).then((res) => {\n  mediaRecorder = new MediaRecorder(res, {\n    audio: true,\n  });\n});\n</script>\n```\n\nThis will cause the browser to ask for permission to use the microphone.\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1646947043/blog/2022/03/build-a-livestream-web-application-with-amazon-ivs-and-deepgram/GetMicrophone.png\" alt=\"Request permission to use microphone\" style=\"width: 50%; margin:auto;\">\n\nIf the user gives permission, then I'll have access to the live stream audio to send to Deepgram.\n\n## Create Text Captions with Deepgram API\n\nIn the next step, I will use the Deepgram API to take the audio data and turn it into text.\n\n### What is Deepgram?\n\nDeepgram is an ASR technology (ASR stands for Automatic Speech Recognition). It uses pretty advanced AI and deep learning technology to take speech from audio files or streams and turn it into text. There are probably a million ways to use this technology in a project. It's a fun API to get comfortable with for this reason.\n\nIf I'm going to use Deepgram in my project, I need to create an account [here](https://console.deepgram.com/signup?jump=keys). This will give me an API key and $150 in free credit, so I won't need to enter billing information just to get started (unlike AWS).\n\nNow I can connect to the Deepgram socket with my API key.\n\n### Connect to Deepgram to Get Transcription\n\nI want to get the transcription and display it under the video player, so I will create an HTML element for that transcript. I'll give it the **id** of `captions`.\n\n```html\n<p id=\"captions\"></p>\n```\n\nI'm going to follow the tutorial my colleague Kevin Lewis wrote about [getting live speech transcriptions in the browser](https://blog.deepgram.com/live-transcription-mic-browser/). He explains that I need to connect to Deepgram with a WebSocket. I have to make sure I have access to the microphone before I open the WebSocket, so I will put the logic to connect to Deepgram inside the `.then()` that is attached to the `getUserMedia` function call.\n\n```js\nnavigator.mediaDevices.getUserMedia({ audio: true }).then((stream) => {\n  ...\n  const socket = new WebSocket(\"wss://api.deepgram.com/v1/listen\", [\n    \"token\",\n    \"YOUR_KEY_HERE\",\n  ]);\n});\n\n```\n\nI will put my API key where it says \"YOUR\\_KEY\\_HERE\".\n\nOnce the socket is open, I can add an event listener that listens for when there is audio data that has come through the microphone. When that happens, I can take that audio data and send it through the Deepgram socket to Deepgram.\n\n```js\nsocket.onopen = () => {\n  mediaRecorder.addEventListener('dataavailable', async (event) => {\n    if (event.data.size > 0 && socket.readyState == 1) {\n      socket.send(event.data)\n    }\n  })\n  mediaRecorder.start(1000)\n}\n```\n\nDeepgram will send the transcribed audio back to me as text. It will come in the form of a JSON object, so I need to drill down to the `transcript` property using dot notation. I will use `document.querySelector(#captions)` to put the transcript onto the screen under the video element.\n\n```js\nsocket.onmessage = (message) => {\n  const received = JSON.parse(message.data)\n  const transcript = received.channel.alternatives[0].transcript\n  if (transcript && received.is_final) {\n    document.querySelector('#captions').textContent += transcript + ' '\n  }\n}\n```\n\nHere is all the Javascript code for building the text captions feature:\n\n```js\n    <script>\n    // Get access to user's microphone\n    navigator.mediaDevices.getUserMedia({ audio: true }).then((stream) => {\n      const mediaRecorder = new MediaRecorder(stream);\n\n      // Open connection to Deepgram\n      const socket = new WebSocket(\"wss://api.deepgram.com/v1/listen\", [\n        \"token\",\n        \"YOUR_KEY_HERE\",\n      ]);\n\n      // Listen for audio data coming from microphone and send it to Deepgram\n      socket.onopen = () => {\n        mediaRecorder.addEventListener(\"dataavailable\", async (event) => {\n          if (event.data.size > 0 && socket.readyState == 1) {\n            socket.send(event.data);\n          }\n        });\n        mediaRecorder.start(1000);\n      };\n\n      // Put the transcript onto the screen in the #captions element\n      socket.onmessage = (message) => {\n        const received = JSON.parse(message.data);\n        const transcript = received.channel.alternatives[0].transcript;\n        if (transcript && received.is_final) {\n          document.querySelector(\"#captions\").textContent += transcript + \" \";\n        }\n      };\n\n      socket.onclose = () => {\n        console.log({ event: \"onclose\" });\n      };\n\n      socket.onerror = (error) => {\n        console.log({ event: \"onerror\", error });\n      };\n    });\n    </script>\n```\n\nAnd here is the HTML:\n\n```html\n<html lang=\"en\">\n  <head>\n    <meta charset=\"UTF-8\" />\n    <title>Video Stream Demo</title>\n    <!-- Amazon IVS SDK video player -->\n    <script src=\"https://player.live-video.net/1.6.1/amazon-ivs-player.min.js\"></script>\n  </head>\n  <body>\n    <video\n      width=\"520\"\n      height=\"440\"\n      id=\"video-player\"\n      controls\n      playsinline\n    ></video>\n    <p id=\"captions\"></p>\n    <!-- scripts -->\n  </body>\n</html>\n```\n\nNow I can start my live stream, and the text captions will be displayed under the video player!\n\n## Conclusion\n\nIn this tutorial, I built a 'vanilla' live stream player with text captions. I demonstrated how to use the technologies Amazon IVS and Deepgram using fundamentals of web development - HTML and Javascript. You can find the repo for this vanilla Javascript project [here](https://github.com/deepgram-devs/deepgram-livestream-javascript).\n\nBut most front-end developers rely on frameworks to build projects like these. And there are other considerations I need to make in regards to keeping my Deepgram API key secure and limiting who has access to this website.\n\nIn the next part of the series, **I will improve this project by building it using Vue.js (specifically Vue 3) for the front-end, and node.js for the back-end**. I will include some of the real-world practices for building a full-stack application. I'll need a server file so I can incorporate more security, which I'll build with node.js, and I'll build an entry page with VueRouter navigation guards so that users must enter a code to see my live stream.\n\nVue.js is my favorite Javascript framework, and I have written a series on [Diving Into Vue 3](https://blog.deepgram.com/diving-into-vue-3-setup-function/), which is worth checking out if you want to come along with me for the rest of this series to build a full-stack live stream application in Vue.js.\n\nPlease follow me on [Twitter](https://twitter.com/sandra_rodgers_) if you find my tutorials useful!\n\n        ";
						}
						async function compiledContent$3S() {
							return load$3S().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3S() {
							return (await import('./chunks/index.6e32bfe0.mjs'));
						}
						function Content$3S(...args) {
							return load$3S().then((m) => m.default(...args));
						}
						Content$3S.isAstroComponentFactory = true;
						function getHeadings$3S() {
							return load$3S().then((m) => m.metadata.headings);
						}
						function getHeaders$3S() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3S().then((m) => m.metadata.headings);
						}

const __vite_glob_0_30 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3S,
  file: file$3S,
  url: url$3S,
  rawContent: rawContent$3S,
  compiledContent: compiledContent$3S,
  default: load$3S,
  Content: Content$3S,
  getHeadings: getHeadings$3S,
  getHeaders: getHeaders$3S
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3R = {"title":"Build a Presentation Coaching Application with Recall","description":"Learn how to use Deepgram customer Recall.ai to get data from live video calls in just a few lines of code.","date":"2022-10-19T19:50:23.850Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1664560621/blog/build-a-presentation-coaching-application-with-recall/2209-Build-a-Presentation-Coaching-App-with-Recall-blog_2x_cnyagk.jpg","authors":["kevin-lewis"],"category":"tutorial","tags":["partner","javascript"],"shorturls":{"share":"https://dpgr.am/45503ab","twitter":"https://dpgr.am/c900012","linkedin":"https://dpgr.am/f9bf8ce","reddit":"https://dpgr.am/d96ca5d","facebook":"https://dpgr.am/b528b52"}};
						const file$3R = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/build-a-presentation-coaching-application-with-recall/index.md";
						const url$3R = undefined;
						function rawContent$3R() {
							return "\n[Recall](https://www.recall.ai) provides a developer API to get real-time meeting data from a number of different platforms. It does this by sending Recall bots into meetings to observe what is happening and then provide data on-demand or in real-time.  As well as transcripts, they provide metadata including a participant list and linked calendar invite.\n\nIn this tutorial, you will build a virtual presentation coaching application. The application will allow you to send a Recall bot into a Zoom call, remove them, and get insights once the call is over. One of the great things about Recall is their support of other platforms like Google Meet, Microsoft Teams, and WebEx with no additional code.\n\nFor this project, we'll complete the following steps:\n\n1.  A﻿dd a bot to a Zoom call\n2.  G﻿et data about speakers in the call\n3.  C﻿alculate speaker turn counts (to see if you took up more turns than others)\n4.  C﻿reate a speaker-separated transcript\n5.  C﻿alculate talk-time per speaker\n\n## Before You Start\n\nMake sure you have [Node.js](https://nodejs.org/en/) installed. You will need a [Deepgram API Key](https://console.deepgram.com/signup?jump=keys) and a [Recall API Key](https://www.recall.ai).\n\nCreate a new directory for this project and open it in a code editor. Create a `.env` file and populate it with your keys:\n\n    RECALL_API_KEY=your-key-here\n    DEEPGRAM_API_KEY=your-key-here\n\nCreate a `package.json` file with `npm init -y` and then install our dependencies:\n\n```shell\nnpm install dotenv express hbs axios\n```\n\nCreate an `index.js` file and open it in your code editor.\n\n## Set Up Application\n\nImport your dependencies:\n\n```javascript\nimport 'dotenv/config'\nimport axios from 'axios'\nimport express from 'express'\n```\n\nSet up your express application:\n\n```javascript\nconst app = express()\napp.set('view engine', 'hbs')\napp.use(express.urlencoded({ extended: false }))\n\n// Further code goes here\n\nconst PORT = process.env.PORT || 3000\napp.listen(PORT, () => console.log(`Listening on port ${PORT}`))\n```\n\nCreate a route handler to load the initial page. Firstly, create a `views` directory and an `index.hbs` file inside of it. `.hbs` files use [Handlebars](https://handlebarsjs.com) to add conditional and looping logic to HTML files. In the new view file, add:\n\n```html\n<h1>Call Coacher</h1>\n```\n\nInside of `index.js`, render the view:\n\n```javascript\napp.get('/', (req, res) => res.render('index'))\n```\n\nStart your server with `node index.js`, visit [localhost:3000](http://localhost:3000), and you should see **Call Coacher**.\n\n## Create a Recall.ai Helper Function\n\n[Recall's API Reference](https://recallai.readme.io/) shows all of the available endpoints to manage bots - your application will use four of them. To make your code more readable, create a reusable `recall()` helper method at the very bottom of your `index.js` file:\n\n```javascript\nasync function recall(method, path, data) {\n  try {\n    const payload = {\n      method,\n      url: `https://api.recall.ai/api/v1${path}`,\n      headers: {\n          Authorization: `Token ${process.env.RECALL_API_KEY}`\n      }\n    }\n    if(data) payload.data = data\n    const response = await axios(payload)\n    return response.data\n  } catch(error) {\n    throw error\n  }\n}\n```\n\nNow, for example, endpoints can be accessed like so:\n\n```javascript\nconst bots = await recall('get', '/bot')\nconst newBot = await recall('post', '/bot', { meeting_url: '...' })\n```\n\n## Use Recall.ai To Add a Bot to a Zoom Call\n\nAdd a new form to `views/index.hbs`:\n\n```html\n<h2>Add a bot to a call</h2>\n<form action=\"/join\" method=\"post\">\n    <label for=\"meeting_url\">Meeting URL</label>\n    <input type=\"text\" id=\"meeting_url\" name=\"meeting_url\"><br>\n\n    <label for=\"bot_name\">Bot Name</label>\n    <input type=\"text\" id=\"bot_name\" name=\"bot_name\">\n\n    <input type=\"submit\" value=\"join\">\n</form>\n```\n\nProviding a bot name is optional, but your application will allow users to specify it. When submitted, this form will send a POST request to `/join`. Its payload will contain `meeting_url` and `bot_name`.\n\nAdd the following to `index.js` underneath the existing route handler for the homepage:\n\n```javascript\nlet bots = []\napp.post('/join', async (req, res) => {\n    try {\n        const { meeting_url, bot_name } = req.body\n        // Adds bot to call, returned data does not include meeting_url\n        const bot = await recall('post', '/bot', { meeting_url, bot_name })\n        // Add new bot to bots array\n        bots.push({ ...bot, meeting_url })\n        // Re-render the homepage, making a message available to the template\n        res.render('index', { message: 'The bot has joined your call' })\n    } catch(error) {\n        console.log(error)\n        res.render('index', { message: 'There has been a problem adding the bot' })\n    }\n})\n```\n\nBeing able to send dynamic data into templates is a feature available by including handlebars in our application. At the bottom of `index.hbs` show the message:\n\n```html\n<p>{{ message }}</p>\n```\n\nThe message is empty (leaving an empty paragraph) when initially loading the page and will show the message after submitting the form.\n\n*Try it out! Restart your server, create a new Zoom call, get the meeting invite URL and submit it in the form. You should have a bot immediately join you with the bot name you specified.*\n\n## Make a Recall.ai Bot Leave a Zoom Call\n\nCurrently, the only way to make the bot leave the call is to end it for everyone (or manually remove it in the Zoom interface). Recall also provide an endpoint to remove a bot. Add a new form below the previous one in `index.hbs`:\n\n```html\n<h2>Leave call</h2>\n<form action=\"/leave\" method=\"post\">\n    <label for=\"meeting_url\">Meeting URL</label>\n    <input type=\"text\" id=\"meeting_url\" name=\"meeting_url\">\n    <input type=\"submit\" value=\"leave\">\n</form>\n```\n\nIn `index.js` create a new route handler:\n\n```javascript\napp.post('/leave', async (req, res) => {\n  try {\n    const { meeting_url } = req.body\n    // Get the bot from the bots array with matching meeting_url\n    const { id } = bots.find(bot => bot.meeting_url == meeting_url)\n    // Remove bot form call\n    await recall('post', `/bot/${id}/leave_call`)\n    // Redirect to /:botId\n    res.redirect(`/${id}`)\n  } catch(error) {\n    console.log(error)\n    res.render('index', { message: 'There has been a problem removing the bot' })\n  }\n})\n```\n\n*Restart your server and try to add and remove a bot. The bot should leave the call when the new form is submitted, and you should be redirected to a new page (causing an error because it does not yet exist.)*\n\n## Show Data From Call\n\nCreate a new `data.hbs` file in the `views` directory:\n\n```html\n<h1>Data for {{ id }}</h1>\n{{#if video_url}}\n  <a href=\"{{video_url}}\">Watch video until {{ media_retention_end }}</a>\n{{/if}}\n```\n\nIn `index.js` add a new route handler:\n\n```javascript\napp.get('/:botId', async (req, res) => {\n  try {\n    // Get bot data\n    const bot = await recall('get', `/bot/${req.params.botId}`)\n    // Get transcript (each object is one speaker turn)\n    const turns = await recall('get', `/bot/${req.params.botId}/transcript`)\n\n    // Further code here\n\n    // Return all properties in bot object\n    res.render('data', bot)\n  } catch(error) {\n    res.send('There has been a problem loading this bot data')\n  }\n})\n```\n\n*Restart your server, start a new Zoom call (preferably with someone else), speak for a couple of minutes, remove the bot with the form, and you should be redirected to a page.*\n\n![Webpage showing the bot ID and a single link with the video recording link](https://res.cloudinary.com/deepgram/image/upload/v1663790129/blog/2022/10/build-a-presentation-coaching-application-with-recall/video_url_y6mxzu.png)\n\n### Get All Speaker Usernames\n\nA full timeline for the call including who spoke and when is made available as part of the `bot` object. Extract just usernames and de-duplicate the list by adding the following:\n\n```javascript\nconst { timeline } = bot.speaker_timeline\nlet usernames = [...new Set(timeline.map(turn => { username: turn.users[0].username }))]\n```\n\nUpdate the `res.render()` method to the following:\n\n```javascript\nres.render('data', { ...bot, usernames })\n```\n\nFinally, add a list of who spoke to the bottom of `data.hbs`:\n\n```html\n<h2>Who spoke:</h2>\n<ul>\n  {{#each usernames}}\n    <li>\n      <span>{{ this.username }}</span>\n    </li>\n  {{/each}}\n</ul>\n```\n\n![At the bottom of the page is a two-item bullet list, each showing one username.](https://res.cloudinary.com/deepgram/image/upload/v1663790129/blog/2022/10/build-a-presentation-coaching-application-with-recall/who-spoke_ydnkfh.png)\n\n### Show Each Speaker's Turn Count\n\nBelow where `usernames` is defined, add the following:\n\n```javascript\nfor(let i=0; i<usernames.length; i++) {\n  let userTurns = timeline.filter(turn => turn.users[0].username == usernames[i])\n  usernames[i] = {\n    username: usernames[i],\n    turns: userTurns.length\n  }\n}\n```\n\nNow each `username` in the `usernames` array also has a `turns` property, which is equal to the number of times they spoke in the call. Update the loop to show the new data:\n\n```html\n{{#each usernames}}\n  <li>\n    <span>{{ this.username }}</span>\n    <span>{{ this.turns }} turns speaking</span>\n  </li>\n{{/each}}\n```\n\n### Display Call Transcript with Usernames\n\nRecall is a Deepgram customer and provides our accurate AI-powered transcription within their product. The transcript is already available in our application in the `turns` variable. Add the following below the for loop in `index.js`:\n\n```javascript\nlet transcript = []\nfor(let i=0; i<turns.length; i++) {\n  // Get all words for this turn\n  const turnWords = turns[i].words\n  // Form a single stream of words\n  const words = turnWords.map(w => w.text).join(' ')\n  // Add to transcript array along with speaker username\n  transcript.push({ speaker: turns[i].speaker, words })\n}\n```\n\nAdd the transcript to the rendered data:\n\n```javascript\nres.render('data', { ...bot, usernames, transcript })\n```\n\nFinally, in `data.hbs`, add the following to the bottom:\n\n```html\n<h2>Transcript</h2>\n{{#each transcript}}\n  <p><b>{{ this.speaker }}: </b>{{ this.words }}</p>\n{{/each}}\n```\n\n![Webpage now shows everything that was said, split by turns. Each turn starts with the speaker's username, and then what they said.](https://res.cloudinary.com/deepgram/image/upload/v1663790129/blog/2022/10/build-a-presentation-coaching-application-with-recall/transcript_qnaka4.png)\n\n### Calculate Each Speaker's Speaking Time\n\nEach word in the transcript is accompanied by a word's start and end time. Using this data, each speaker's 'talking time' can be calculated. Firstly, `turns` is added to `usernames[i]`, add a new `speakTime` value:\n\n```javascript\nusernames[i] = {\n  username: usernames[i],\n  turns: userTurns.length,\n  speakTime: 0\n}\n```\n\nCalculate the `speakTime` just after you add transcripts with `transcripts.push()`, and add it to the speaker's entry in the `username` array:\n\n```javascript\nconst speakTime = +(turnWords[turnWords.length-1].end_timestamp - turnWords[0].start_timestamp).toFixed(2)\nconst user = usernames.findIndex(u => u.username == turns[i].speaker)\nusernames[user].speakTime += speakTime\n```\n\nFinally, update `data.hbs` to contain this new data just below where each speaker's turns are shown:\n\n```html\n<span>{{ this.speakTime }}s total talking time</span>\n```\n\n![Each username now displays the number of seconds they spoke for](https://res.cloudinary.com/deepgram/image/upload/v1663790129/blog/2022/10/build-a-presentation-coaching-application-with-recall/talk_time_wqtcod.png)\n\n## The World Is Your Oyster\n\nThis application only scratches the surface of the analysis you can perform with data returned by Recall and Deepgram. You may choose to [detect non-inclusive language](https://developers.deepgram.com/blog/2022/09/uninclusive-language-retext/), summarize what has been said, and more. Recall provides a developer-friendly way to avoid writing 'glue' into various conferencing platforms, so if you want to use Google Meet, Microsoft Teams, WebEx, or others, there is no more code to write. Fab!\n\nIf you have any questions, please don't hesitate to get in touch. We love to help!\n\n";
						}
						async function compiledContent$3R() {
							return load$3R().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3R() {
							return (await import('./chunks/index.59973521.mjs'));
						}
						function Content$3R(...args) {
							return load$3R().then((m) => m.default(...args));
						}
						Content$3R.isAstroComponentFactory = true;
						function getHeadings$3R() {
							return load$3R().then((m) => m.metadata.headings);
						}
						function getHeaders$3R() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3R().then((m) => m.metadata.headings);
						}

const __vite_glob_0_31 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3R,
  file: file$3R,
  url: url$3R,
  rawContent: rawContent$3R,
  compiledContent: compiledContent$3R,
  default: load$3R,
  Content: Content$3R,
  getHeadings: getHeadings$3R,
  getHeaders: getHeaders$3R
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3Q = {"title":"Build a To-do List App with Pinia and Vue 3","description":"Learn about Vue 3's new official state management system Pinia while building a to-do list app. Bonus - add typescript!","date":"2022-04-15T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1649693948/blog/2022/04/build-a-todo-list-with-pinia-and-vue-3/Build-Todo-list-w-Vue3-Pinia%402x.jpg","authors":["sandra-rodgers"],"category":"tutorial","tags":["vuejs","javascript","typescript"],"seo":{"title":"Build a To-do List App with Pinia and Vue 3","description":"Learn about Vue 3's new official state management system Pinia while building a to-do list app. Bonus - add typescript!"},"shorturls":{"share":"https://dpgr.am/9d6a0bc","twitter":"https://dpgr.am/9218baf","linkedin":"https://dpgr.am/b0575b7","reddit":"https://dpgr.am/8761a94","facebook":"https://dpgr.am/f7cf134"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453154/blog/build-a-todo-list-with-pinia-and-vue-3/ograph.png"}};
						const file$3Q = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/build-a-todo-list-with-pinia-and-vue-3/index.md";
						const url$3Q = undefined;
						function rawContent$3Q() {
							return "\nI was building a Vue 3 project for my recent blog series on [how to build a full-stack live streaming web app](https://blog.deepgram.com/build-a-livestream-web-application-vue-and-express-setup/). I wanted to use Vuex to manage some global state properties. It was my first time using Vuex with Vue 3 since I began my journey to learn the Composition API.\n\nWhen I arrived at the [Vuex documentation page](https://vuex.vuejs.org/), I saw this:\n\n![Announcement: The official state management library for Vue has changed to Pinia ](https://res.cloudinary.com/deepgram/image/upload/v1649699509/blog/2022/04/build-a-todo-list-with-pinia-and-vue-3/announcement-pinia-official_.png)\n\nWell, that was a surprise! I had been hearing the word \"Pinia\" in relation to Vue but didn't know exactly what it was. **Pinia is now the official state management library for Vue**!\n\nI pushed onwards with using Vuex in that project but made a mental note to come back soon to Pinia to find out what it is all about.\n\nSoon is now! Today I will learn a little about Pinia by building a to-do list. I'll show how I build it and provide some of my thoughts about the experience. Let's dive in!\n\n## The Project\n\nHere is a screenshot of the final project. It's a to-do list that lets me **add**, **delete**, and **check off** an item on the list.\n\nThe project repo can be found [here](https://github.com/SandraRodgers/todo-pinia).\n\n![Example of the to-do list app I'll build](https://res.cloudinary.com/deepgram/image/upload/v1649860305/blog/2022/04/build-a-todo-list-with-pinia-and-vue-3/todo-list-example1.png)\n\n## Getting Started with Pinia\n\nI'll create my Vue project (making sure to select Vue 3 since I want to use the Composition API). Pinia also works with Vue 2, but I've personally gone totally in on Vue 3 (and haven't looked back - check out [my series on Vue 3](https://blog.deepgram.com/diving-into-vue-3-getting-started/) to read about my journey).\n\n```bash\nvue create todo-pinia\n```\n\nAfter I `cd` into the project folder, I'll install pinia:\n\n```bash\nnpm install pinia\n```\n\nThen I'll go into the `main.js` file and import `createPinia`. This creates a Pinia instance to be used by my application. The `.use()` tells the Vue app to install Pinia as a plugin.\n\n```js\nimport { createApp } from 'vue'\nimport { createPinia } from 'pinia'\nimport App from './App.vue'\n\ncreateApp(App).use(createPinia()).mount('#app')\n```\n\nIn the `src` folder, in `components`, I'll create the three components that will make up my todo list app - `TodoApp.vue` (the parent component), `TodoForm.vue` (a child component), and `TodoList.vue` (another child component).\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1649694505/blog/2022/04/build-a-todo-list-with-pinia-and-vue-3/components-folder-with-files.png\" alt=\"Components folder with files\" style=\"width:50%\" />\n\nHere is the plan for how these components will be organized in the browser:\n\n![Example of the to-do list app with component outline](https://res.cloudinary.com/deepgram/image/upload/v1649861642/blog/2022/04/build-a-todo-list-with-pinia-and-vue-3/todo-list-example_components.png)\n\nIn each component, I can quickly scaffold out the basic code structure for my template and script. I do that with an extension in VS Code called [Vue VSCode Snippets](https://marketplace.visualstudio.com/items?itemName=sdras.vue-vscode-snippets). Since I have that, I just type the letters **vbase-3**, and the code writes itself for me:\n\n![vbase-3 snippet to scaffold out my code](https://res.cloudinary.com/deepgram/image/upload/v1649694505/blog/2022/04/build-a-todo-list-with-pinia-and-vue-3/vbase-snippet.png)\n\nNow I'll import each component to where it needs to be -`TodoForm.vue` and `TodoList.vue` into the `TodoApp.vue` - and I'll import the `TodoApp.vue` component into `App.vue`. I like to write the name of the component in each to start so I can see them on the screen.\n\nHere's my screen now. The layout is there, but no logic or styles yet:\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1649694505/blog/2022/04/build-a-todo-list-with-pinia-and-vue-3/component-layout-browser.png\" alt=\"Component layout in browser\" style=\"width:50%\" />\n\n## Pinia - What is it?\n\nNext, I'll create a store and set up my global state with [Pinia](https://pinia.vuejs.org/).\n\nThe concept of a Pinia store is the same as it is for Vuex or Redux - **it is a place to hold global state**, and it makes it easy for any component in the project to track changes to that global state.\n\nNot all state needs to go in the store - just state properties that I want to make available throughout the app. This is especially useful when I want to share state between two sibling components like the `TodoForm.vue` and `TodoList.vue` because I can avoid sending props down ('prop drilling') and emitting events up through the parent.\n\n## Define a Pinia Store\n\nI will create a `store` folder in `src`, and in the folder, I'll make a file called `useTodoListStore.js`. I'm naming it starting with the word 'use' because a common convention of Vue 3, both for Pinia store files and for Vue composables, is to start the file name with 'use'.\n\nI can have as many stores as I want; in fact, **I should have separate stores for separate logical concerns**, similar to how Vue 3 composables are built around distinct logical concerns. Each store should be in a different file.\n\nHowever, since this is such a small project, I only need one store - **one store for the to-do list logic**.\n\nI will first import the `defineStore` function from Pinia. Under the hood, this is going to create the `useStore` function that I will need in my components to retrieve the store I made.\n\n```js\nimport { defineStore } from 'pinia'\n```\n\nI set it to a `const` and use the keyword `export` since I'll need to be able to import it into my components.\n\nThis `defineStore` function will take two arguments: a string (the unique name of the store) and an object (options such as state, getters, and actions).\n\n```js\nimport { defineStore } from 'pinia'\n\nexport const useTodoListStore = defineStore('todoList', {\n  // state\n  // getters\n  // actions\n})\n```\n\n## State, Getters, and Actions\n\nThe options that I pass to the `defineStore` function are my store's `state`, `getters`, and `actions`. Unlike Vuex, there is no longer the need for `mutations`. This makes me happy!\n\nI always found `mutations` confusing because it felt like I was repeating myself when I had to write an action to commit a mutation, which would then make the state change. Pinia has gotten rid of that middleman, and instead, the flow is just **action -> change state**.\n\nI already have a mental model around the way `methods`, `data`, and `computed` work in Vue 2. The `methods` make stuff happen, the `data` contains my state properties, and the `computed` returns an automatically updated property that has had a calculation performed on it.\n\nPinia's options follow the same mental model - I can think of the `state` as being like `data` in the Vue Options API, the `actions` like `methods`, and the `getters` like `computed` properties.\n\nI really like this change, and it's one of the first things that made me think, \"Wow, I think I'm really going to like Pinia!\"\n\n### Create Initial State\n\nNow I'll start creating a global state object in my `useTodoListStore`.\n\nThe state is actually a function, and it's recommended that I use an arrow function (this is because Pinia has excellent Typescript integration, and using an arrow function will allow Typescript inference to work on the state properties).\n\nI'll add a `todoList` property, which will be an array meant to contain each to-do item (each item is going to be an object, but there's nothing in the `todoList` array at the moment).\n\n```js\nimport { defineStore } from 'pinia'\n\nexport const useTodoListStore = defineStore('todoList', {\n  state: () => ({\n    todoList: [],\n  }),\n})\n```\n\n### Actions - Add and Delete an Item\n\nI can also set up my first action. I know the main logic to start will be adding an item to the to-do list. I'll write a function `addTodo` that will perform the logic of pushing an item object into the `todoList` array.\n\nIndividual `actions` are methods within the `actions` object in the store.\n\nI will also add an `id` property to state since I will want each item to have an id that increments each time a new item is pushed into the `toDoList` array:\n\n```js\nimport { defineStore } from 'pinia'\n\nexport const useTodoListStore = defineStore('todoList', {\n  state: () => ({\n    todoList: [],\n    id: 0,\n  }),\n  actions: {\n    addTodo(item) {\n      this.todoList.push({ item, id: this.id++, completed: false })\n    },\n  },\n})\n```\n\nMaybe while I'm here, I should go ahead and write an action to delete an item from the to-do list since I know I'll want to have a delete feature. Under the last line of code in the `addToDo` action, I'll add a `deleteTodo`:\n\n```js\ndeleteTodo(itemID) {\n  this.todoList = this.todoList.filter((object) => {\n    return object.id !== itemID;\n  });\n},\n```\n\n### Input Form to Add an Item\n\nI'll jump back into the `TodoForm.vue` component now. I want to write a form to enter a to-do item. I'll use the dev-tools to check that the item is getting into the `state` I set up in the Pinia store.\n\nIn the `template`, I'll create the basic form:\n\n```html\n<!-- TodoForm.vue -->\n\n<template>\n  <form @submit.prevent=\"\">\n    <input v-model=\"todo\" type=\"text\" /><button>Add</button>\n  </form>\n</template>\n```\n\nThe input has a `v-model=\"todo\"` which I'll connect to a `ref` in the `script` to make this property reactive so it updates as the user types the item into the input:\n\n```js\n// TodoForm.vue\n\n<script>\nimport { ref } from \"vue\";\nexport default {\n  setup() {\n    const todo = ref(\"\");\n    return { todo };\n  },\n};\n</script>\n```\n\nI haven't added a method yet for the `@submit` event listener because I need to set up the logic in the `script` first. The submit button is going to trigger a function to add an item to the todo list, so I'll need to somehow invoke the `addTodo` action in the store.\n\n## Access Pinia Store from a Component\n\nTo use a Pinia store in a component, I need to import the store and then set a `const store` to the invoked store function:\n\n```js\n// TodoForm.vue\n\nimport { useTodoListStore } from '@/store/useTodoListStore'\nexport default {\n  setup() {\n    const todo = ref('')\n    // use Pinia store:\n    const store = useTodoListStore()\n\n    return { todo }\n  },\n}\n```\n\nNow I will have access to state, actions, and getters in the store through that `const store`.\n\nI'll write a method in the `TodoForm.vue` component that will be triggered when the submit button is clicked. I want that method to do two things: add an item to the `todoList` array in the store, and clear the `todo` `ref` so it returns to being an empty string after the item is added to the list:\n\n```js\n// in setup function in script in TodoForm.vue:\n\nfunction addItemAndClear(item) {\n  if (item.length === 0) {\n    return\n  }\n  // invokes function in the store:\n  store.addTodo(item)\n  todo.value = ''\n}\n```\n\nAnd I'll make sure that function is added to the form's `@submit` event listener in the template:\n\n```js\n<form @submit.prevent=\"addItemAndClear(todo)\">\n```\n\nI'll type `npm run serve` in the terminal to start up the Vue development server.\n\nNow I can open the Vue dev-tools and see that the item is being added to the `todoList` array in the store.\n\n![Gif showing an item added to the to-do list and data in the store](https://res.cloudinary.com/deepgram/image/upload/v1649704298/blog/2022/04/build-a-todo-list-with-pinia-and-vue-3/add-todo.gif)\n\n## Reactive Properties in Pinia\n\nIn the previous section, I used an action from the Pinia store - `addTodo` - in my `todoForm.vue` component. In this section, I'll use a state property in the `todoList.vue` component, and I need it to be reactive to changes that might happen. I'll be using it in the component `template`, and it has to be reactive so it updates in sync with the state change.\n\nThere's an important function I'll want to use that comes with the Pinia library - `storeToRefs`. Each to-do list item displayed in the `todoList` component will actually come from the store, and since the store's state is an object, I will use this helper method to destructure the returned object without losing reactivity. It is similar to Vue 3's [utility function `toRefs`](https://vuejs.org/api/reactivity-utilities.html#torefs). I'll demonstrate its usage as I build the next feature.\n\n### Todo List - Show Item\n\nI want access to the `todoList` that's in the store (which now has data to represent the items I've added to the list), so in the `todoList.vue` component I'll need to bring in the store, just like I did in `todoForm.vue`. I'll also set `const store` to the invoked store function.\n\nThen I need to wrap the `todoList` property that I want to pull from the store in the function `storeToRefs`:\n\n```js\n<script>\nimport { useTodoListStore } from \"../store/useTodoListStore\";\nimport { storeToRefs } from \"pinia\";\nexport default {\n  setup() {\n    const store = useTodoListStore();\n    // storeToRefs lets todoList keep reactivity:\n    const { todoList } = storeToRefs(store);\n\n    return { todoList };\n  },\n};\n</script>\n```\n\nNow I can use `todoList` in my `template`, and it will stay in sync with the store. I'll write a `v-for` loop to create the list:\n\n```html\n<template>\n  <div v-for=\"todo in todoList\" :key=\"todo.id\">\n    <div>{{ todo.item }}</div>\n  </div>\n</template>\n```\n\nAnd the list is displaying now:\n\n![Gif showing the list as items are added](https://res.cloudinary.com/deepgram/image/upload/v1649863854/blog/2022/04/build-a-todo-list-with-pinia-and-vue-3/show-list-example_.gif)\n\n### To-do List - Mark as Completed\n\nI want to add some styles to each item to show if the to-do item has been completed.\n\nFirst, I need the logic to toggle an item to be complete or not complete. Right now, in the store, each item that is added to the list also has a `completed` property set to `false`:\n\n```js\n// useTodoListStore.js\n\nthis.todoList.push({ item, id: this.id++, completed: false })\n```\n\nI can write an action in the store to toggle that to true:\n\n```js\ntoggleCompleted(idToFind) {\n      const todo = this.todoList.find((obj) => obj.id === idToFind);\n      if (todo) {\n        todo.completed = !todo.completed;\n      }\n    },\n```\n\nIn the `todoList.vue` component, I'll add a checkmark emoji as a `span` to the `template` with an event listener to listen for a click on the checkmark. The Unicode is `&#10004;` for a checkmark.\n\n```js\n<div v-for=\"todo in todoList\" :key=\"todo.id\">\n    <div>\n      <span>{{ todo.item }}</span>\n      <span @click.stop=\"toggleCompleted(todo.id)\">&#10004;</span>\n    </div>\n  </div>\n```\n\nHowever, I need to make sure that I have brought `toggleCompleted` into the component. Since it's an **action** method and not a reactive state property, I won't use `storeToRefs` for `toggleCompleted`:\n\n```js\n<script>\nimport { useTodoListStore } from \"../store/useTodoListStore\";\nimport { storeToRefs } from \"pinia\";\nexport default {\n  setup() {\n    const store = useTodoListStore();\n    const { todoList } = storeToRefs(store);\n    // destructuring action method doesn't require using storeToRefs:\n    const { toggleCompleted } = store;\n\n    return { todoList, toggleCompleted };\n  },\n};\n</script>\n```\n\nTo add the styles, I first will add a dynamic class to the to-do item `span` in the template:\n\n```js\n<span :class=\"{ completed: todo.completed }\">{{ todo.item }}</span>\n```\n\nAnd CSS to change the look of the item as it is toggled true and false:\n\n```css\n/* CSS Styles */\n\n.completed {\n  text-decoration: line-through;\n}\n```\n\n![Gif showing an item marked complete with checkmark](https://res.cloudinary.com/deepgram/image/upload/v1649864263/blog/2022/04/build-a-todo-list-with-pinia-and-vue-3/check-complete.gif)\n\n### To-Do List - Delete Item\n\nI had already added the `deleteTodo` function to the store, so I can jump into writing the delete feature in the `todoList.vue` component.\n\nI'll do the same thing I did in the previous section, bringing in the store's action `deleteTodo` and using a cross mark emoji for the delete button. I won't explain every step since I just need to repeat what I did in the previous section for marking an item complete, but this time hooking it up to the delete action. But I'll show the code.\n\nHere's the `todoList.vue` component after I added the delete feature:\n\n```js\n// todoList.vue\n\n<template>\n  <div v-for=\"todo in todoList\" :key=\"todo.id\">\n    <div>\n      <span :class=\"{ completed: todo.completed }\">{{ todo.item }}</span>\n      <span @click.stop=\"toggleCompleted(todo.id)\">&#10004;</span>\n      <span @click=\"deleteTodo(todo.id)\">&#10060;</span>\n    </div>\n  </div>\n</template>\n\n<script>\nimport { useTodoListStore } from \"../store/useTodoListStore\";\nimport { storeToRefs } from \"pinia\";\nexport default {\n  setup() {\n    const store = useTodoListStore();\n    const { todoList } = storeToRefs(store);\n    const { toggleCompleted, deleteTodo } = store;\n\n    return { todoList, toggleCompleted, deleteTodo };\n  },\n};\n</script>\n\n<style>\n.completed {\n  text-decoration: line-through;\n}\n</style>\n\n```\n\nAnd here is the **store** now that I have all the logic working:\n\n```js\n// useTodoListStore\n\nimport { defineStore } from 'pinia'\n\nexport const useTodoListStore = defineStore('todoList', {\n  state: () => ({\n    todoList: [],\n    id: 0,\n  }),\n  actions: {\n    addTodo(item) {\n      this.todoList.push({ item, id: this.id++, completed: false })\n    },\n    deleteTodo(itemID) {\n      this.todoList = this.todoList.filter((object) => {\n        return object.id !== itemID\n      })\n    },\n    toggleCompleted(idToFind) {\n      const todo = this.todoList.find((obj) => obj.id === idToFind)\n      if (todo) {\n        todo.completed = !todo.completed\n      }\n    },\n  },\n})\n```\n\nI've finished a barebones to-do list app with Pinia, minus styling. This code is available on the 'just-pinia' branch of [my project repo](https://github.com/SandraRodgers/todo-pinia/tree/just-pinia) for anyone who would like to see it in its entirety.\n\n## Bonus Section: Add Typescript\n\nOne of the best features of Pinia is that **it works very well with Typescript**. I first chose to build the to-do list without Typescript so I could just focus on how to use Pinia, but I also want to demonstrate how it works with Typescript since that is a huge advantage of Pinia.\n\nSetting up Vuex with Typescript was always challenging for me because of the need to create custom complex wrappers. It wasn't easy to just dive in.\n\nBut with Pinia, I don't have to do that. I can just add Typescript to my project and start using it.\n\nI'll add Typescript to my existing project with this command:\n\n```js\nvue add Typescript\n```\n\nWhen it prompts me to make some choices, I'll be sure to say yes to \"Convert all .js files to .ts\". That way it will turn the store file into a `.ts` file.\n\n![Prompts when adding Typescript to Vue project](https://res.cloudinary.com/deepgram/image/upload/v1649694505/blog/2022/04/build-a-todo-list-with-pinia-and-vue-3/add-typescript-to-vue-prompts.png)\n\nThen I'll delete the `HelloWorld` file because I don't need that. I might need to delete one of the `extends` properties from the `.eslintrc.js` file.\n\nI'll go to the store file and see that Typescript is pointing out all the missing types I need to add.\n\n![Store with Typescript errors](https://res.cloudinary.com/deepgram/image/upload/v1649694505/blog/2022/04/build-a-todo-list-with-pinia-and-vue-3/store-with-typescript-errors.png)\n\nI'm not going to go through how to use Typescript since this blog post isn't meant to teach how to write Typescript. But I'll add the types and show how my store looks after I revise it to include Typescript:\n\n```js\nimport { defineStore } from \"pinia\";\n\ninterface ToDoItem {\n  item: string;\n  id: number;\n  completed: boolean;\n}\n\nexport const useTodoListStore = defineStore(\"todoList\", {\n  state: () => ({\n    todoList: [] as ToDoItem[],\n    id: 0,\n  }),\n  actions: {\n    addTodo(item: string) {\n      this.todoList.push({ item, id: this.id++, completed: false });\n    },\n    deleteTodo(itemID: number) {\n      this.todoList = this.todoList.filter((object) => {\n        return object.id !== itemID;\n      });\n    },\n    toggleCompleted(idToFind: number) {\n      const todo = this.todoList.find((obj) => obj.id === idToFind);\n      if (todo) {\n        todo.completed = !todo.completed;\n      }\n    },\n  },\n});\n```\n\nIn the components, I'll need to add `lang=\"ts\"` to the script and import `defineComponent`. The export will need to be wrapped in the `defineComponent` function.\n\n```js\n<script lang=\"ts\">\nimport { defineComponent } from \"vue\";\nexport default defineComponent({\n...\n});\n</script>\n```\n\nAnd that's how I would add Typescript to my project after-the fact; although I highly recommend starting the project from the beginning with Typescript, since it will help with the developer experience of catching errors and thinking about types.\n\nThe Typescript version of the to-do list can be found in my repo on the branch called [pinia-typescript](https://github.com/SandraRodgers/todo-pinia/tree/pinia-typescript).\n\n## Conclusion\n\nI went through creating a to-do list using just Pinia and then I also showed how to build one with Typescript. I've since added styles and an alert feature to the application, and the most updated code can be found on the main branch of the [project repo](https://github.com/SandraRodgers/todo-pinia/tree/main).\n\nI hope this blog post has been helpful. I'm very excited about Pinia because of how straightforward it was to jump in and start using, especially with Typescript.\n\nIf you have any questions, feel free to reach out on [Twitter](https://twitter.com/sandra_rodgers_)!\n\n        ";
						}
						async function compiledContent$3Q() {
							return load$3Q().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3Q() {
							return (await import('./chunks/index.8e05243f.mjs'));
						}
						function Content$3Q(...args) {
							return load$3Q().then((m) => m.default(...args));
						}
						Content$3Q.isAstroComponentFactory = true;
						function getHeadings$3Q() {
							return load$3Q().then((m) => m.metadata.headings);
						}
						function getHeaders$3Q() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3Q().then((m) => m.metadata.headings);
						}

const __vite_glob_0_32 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3Q,
  file: file$3Q,
  url: url$3Q,
  rawContent: rawContent$3Q,
  compiledContent: compiledContent$3Q,
  default: load$3Q,
  Content: Content$3Q,
  getHeadings: getHeadings$3Q,
  getHeaders: getHeaders$3Q
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3P = {"title":"Project Upgrade: Voice Controlled To-Do List App with Deepgram and Vue 3","description":"Using Vue 3 & Deepgram's Speech-to-Text API, update the classic to-do list project by adding voice controls!","date":"2022-05-02T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1651517750/blog/2022/05/build-a-voice-controlled-to-do-list-app-with-deepgram-and-vue-3/Build-Todo-list-w-Vue3-Pinia%402x.jpg","authors":["sandra-rodgers"],"category":"tutorial","tags":["vuejs","pinia"],"seo":{"title":"Project Upgrade: Voice Controlled To-Do List App with Deepgram and Vue 3","description":"Using Vue 3 & Deepgram's Speech-to-Text API, update the classic to-do list project by adding voice controls!"},"shorturls":{"share":"https://dpgr.am/0b45872","twitter":"https://dpgr.am/0456c9d","linkedin":"https://dpgr.am/e8b79dc","reddit":"https://dpgr.am/8689f6d","facebook":"https://dpgr.am/a85e2b8"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454078/blog/build-a-voice-controlled-to-do-list-app-with-deepgram-and-vue-3/ograph.png"}};
						const file$3P = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/build-a-voice-controlled-to-do-list-app-with-deepgram-and-vue-3/index.md";
						const url$3P = undefined;
						function rawContent$3P() {
							return "\nRecently I wrote about a project I did to help me learn Pinia, Vue 3's new official state management system. I built a basic to-do list app:\n\n![Example to-do list app](https://res.cloudinary.com/deepgram/image/upload/v1651517752/blog/2022/05/build-a-voice-controlled-to-do-list-app-with-deepgram-and-vue-3/todo-list-example1.jpg)\n\nIt dawned on me that a fun way to jazz up this project would be to use Deepgram to make the app **voice-powered** so that a user can **speak** commands to add, delete, or check-off items on the list.\n\nI'm inspired by my colleague Bekah's series about [updating portfolio projects](https://blog.deepgram.com/freecodecamp-quote-generator-upgrade/). A voice-based to-do list app would be a lot more interesting than a regular to-do list app!\n\n## Project Overview\n\n<Panel type=\"info\" title=\"Build a To-Do List App With Vue 3, Pinia, and Deepgram (SERIES)\">\n<ol>\n<li><a href=\"https://blog.deepgram.com/build-a-todo-list-with-pinia-and-vue-3/\" target=\"_blank\">Build a To-do List App with Pinia and Vue 3</a></li>\n<li><a href=\"https://blog.deepgram.com/build-a-voice-controlled-to-do-list-app-with-deepgram-and-vue-3/\" target=\"_blank\"> Build a Voice Controlled To-Do List App with Deepgram and Vue 3</a></li>\n</ol>\n</Panel>\n\nThe project I originally did can be found in this [repo](https://github.com/deepgram-devs/todo-pinia), and the accompanying blog post is [here](https://blog.deepgram.com/build-a-todo-list-with-pinia-and-vue-3/). Check it out to build the standard to-do list project with Vue 3 and Pinia.\n\nIn this iteration of the project, I'll continue to use [Pinia](https://pinia.vuejs.org/) to manage global state, but I'll add Deepgram so I can use Deepgram's speech-to-text API to help me power the voice-control feature. If you want to build this voice-control feature along with me, I've created a starting branch [here](https://github.com/deepgram-devs/todo-pinia-deepgram/tree/starter-branch).\n\n### Deepgram Live Streaming Logic\n\nThere are several articles on Deepgram Docs about how to use Deepgram in Javascript to convert an audio stream of speech into text, including one I wrote about how to use it with Vue 3. I won't go over in detail here how I built Vue composables to integrate Deepgram's speech-to-text API. Use these resources or take a look at my repo (in the [composables folder](https://github.com/deepgram-devs/todo-pinia-deepgram/tree/main/src/composables)) to learn how I did this using Vue 3 and the Deepgram Node SDK:\n\n<Panel type=\"info\" title=\"Additional Resources\">\n<ul>\n<li><a href=\"https://blog.deepgram.com/asynchronous-logic-to-write-a-vue-3-and-deepgram-captions-component/\">Asynchronous Logic to Write a Vue 3 and Deepgram Captions Component</a> by <a href=\"https://blog.deepgram.com/authors/sandra-rodgers/\">Sandra Rodgers</a></li>\n<li><a href=\"https://blog.deepgram.com/live-transcription-mic-browser/\">Get Live Speech Transcriptions In Your Browser</a> by <a href=\"https://blog.deepgram.com/authors/kevin-lewis/\">Kevin Lewis</a></li>\n<li><a href=\"https://blog.deepgram.com/protecting-api-key/\">Browser Live Transcription - Protecting Your API Key</a> by <a href=\"https://blog.deepgram.com/authors/kevin-lewis/\">Kevin Lewis</a></li>\n</ul>\n\n</Panel>\n\n### Focus: Voice-Control Feature\n\nFor today's project, I will focus on dealing with the transcript of text that I get back from Deepgram, analyzing it for speech commands to add, delete, and check off items in the to-do list. This logic will all be part of a voice-control feature.\n\n## Create the Component and Connect Deepgram\n\nThe voice-control feature that I build today will be one component with two main elements:\n\n1.  a `button` that, when clicked, **turns on or off voice-control mode** (i.e., toggles the connection to Deepgram)\n2.  a `div` that contains text which identifies the status of whether Deepgram is **connected and listening**, if it **misheard what was said**, or if it is **not connected**.\n\nHere's a minimally styled version of this project that highlights the component I'll be building today:\n\n![To-do list app highlighting component](https://res.cloudinary.com/deepgram/image/upload/v1651517754/blog/2022/05/build-a-voice-controlled-to-do-list-app-with-deepgram-and-vue-3/TodoSpeech.png)\n\nIn the components folder with the other Todo components, I'll create a component called `TodoSpeech.vue`. It will be a child component of `TodoApp.vue`.\n\nIn `TodoSpeech.vue`, I'll start by adding the button element and status div to the template. I'll use a speech bubble emoji on the button, and I'll create an event listener that I plan to program to toggle the Deepgram connection when the button is clicked.\n\n```html\n<!-- in template: -->\n<button @click=\"toggleListen()\">💬</button>\n<div>{{ deepgramStatus }}</div>\n```\n\nI'll also create a `deepgramStatus` ref, which I'll program to update when Deepgram is connected and ready to receive audio data. The initial value will report that Deepgram is not connected:\n\n```js\n// in script:\nlet deepgramStatus = ref('Deepgram Not Connected')\n```\n\nI'll connect to Deepgram using the composable `useDeepgramSocket` (already built in a previous post), which I import in the script. I also run the composable in the setup function so that it runs at the beginning of the component lifecycle. I destructure four properties off of the composable: `DG_socket`, `DG_transcript`, `openStream`, `closeStream`.\n\n```js\n<script>\nimport { ref } from \"vue\";\nimport useDeepgramSocket from \"@/composables/useDeepgramSocket\";\n\nexport default {\n  setup() {\n    const { DG_socket, DG_transcript, openStream, closeStream } = useDeepgramSocket();\n    let deepgramStatus = ref(\"Deepgram Not Connected\");\n\n    return { deepgramStatus };\n  },\n};\n</script>\n```\n\nThe `openStream` and `closeStream` methods that I destructured off of the `useDeepgramSocket` composable will toggle on and off when the button is clicked and `toggleListen` runs. Notice that I created the `isListening` ref to update the toggle status:\n\n```js\nlet isListening = ref(false)\n\nfunction toggleListen() {\n  if (!isListening.value) {\n    openStream()\n    isListening.value = true\n  } else {\n    closeStream()\n    isListening.value = false\n  }\n}\n```\n\nI'll use a watcher to watch the Deepgram socket's status in the composable. That way, the `div` in the template will update when I click the button and Deepgram is connecting, connected, or not connected:\n\n```js\nwatch(DG_socket, () => {\n  if (DG_socket.value === 'Connecting') {\n    deepgramStatus.value = 'Connecting'\n  } else if (DG_socket.value === 'Not Connected') {\n    deepgramStatus.value = 'Voice Controls Off'\n  } else if (DG_socket.value === 'Closing connection...') {\n    deepgramStatus.value = 'Closing connection...'\n  } else {\n    deepgramStatus.value = 'Listening'\n  }\n})\n```\n\nHere is the entire component now. I should be able to click the button and see the status change to show the status of the Deepgram connection:\n\n```js\n<template>\n  <div>\n    <button @click=\"toggleListen()\">💬</button>\n    <div>{{ deepgramStatus }}</div>\n  </div>\n</template>\n\n<script>\nimport { ref, watch } from \"vue\";\nimport useDeepgramSocket from \"@/composables/useDeepgramSocket\";\n\nexport default {\n  setup() {\n    const { DG_socket, DG_transcript, openStream, closeStream } =\n      useDeepgramSocket();\n\n    let deepgramStatus = ref(\"Deepgram Not Connected\");\n    let isListening = ref(false);\n\n    function toggleListen() {\n      if (!isListening.value) {\n        openStream();\n        isListening.value = true;\n      } else {\n        closeStream();\n        isListening.value = false;\n      }\n    }\n\n    watch(DG_socket, () => {\n      if (DG_socket.value === \"Connecting\") {\n        deepgramStatus.value = \"Connecting\";\n      } else if (DG_socket.value === \"Not Connected\") {\n        deepgramStatus.value = \"Voice Controls Off\";\n      } else if (DG_socket.value === \"Closing connection...\") {\n        deepgramStatus.value = \"Closing connection...\";\n      } else {\n        deepgramStatus.value = \"Listening\";\n      }\n    });\n\n    return { deepgramStatus, toggleListen };\n  },\n};\n</script>\n```\n\n## Analyze Deepgram Response for Voice Commands\n\nThe logic to add, delete, or check-off an item from the list will be based on what the speaker says. I need to program this application to listen for commands such as \"add!\" or \"delete!\".\n\nTo do that, I will need to understand the form of the text transcript I'll be getting back from Deepgram.\n\n### Deepgram Audio Stream to Text\n\nWhen I started using Deepgram to transcribe audio streams, I was surprised that Deepgram was able to send back the text almost immediately, after each phrase or sentence rather than waiting until the socket closes and sending back the entire transcription.\n\nHow is it able to send a response back continuously, as the audio is streaming, and how is it able to send meaningful chunks, such as at the end of a sentence or a pause in speech? This is thanks to two of its features: **Endpointing** and **Interim Results**.\n\nTo put it very simply, **Endpointing** is how Deepgram uses pauses in speech to process that speech into meaningful text chunks. It can hear longer silences in speech and use them to identify finished thoughts, i.e., phrases or sentences.\n\n**Interim results** is Deepgram analyzing speech as accurately and as quickly as possible with the little bits of info it gets on the fly; then, after it gets more data to work with, it corrects the transcription. That is why as one or two words come back, one might be wrong, but then when the sentence comes back, the whole thing is more accurate.\n\nIn my case, I'm using the **Interim Results** property `is_final` to identify a complete voice command. When `is_final` is `true`, it means I'm dealing with a full statement or a complete command such as \"Add walk the dog to the list!\" The `is_final` property breaks the transcript up at the end of a fully-processed statement:\n\n```js\n// in useDeepgramSocket.js:\nconst transcript = received.channel.alternatives[0].transcript\nif (transcript && received.is_final) {\n  DG_transcript.value = transcript + ''\n}\n```\n\nThis is a pretty cool feature because it means that a user can speak a command, such as \"Add walk the dog to the list\", and Deepgram can identify that it is the end of the command based on the flow of the speech. It will send me that sentence back to deal with in text form, and then I can use logic to dig in and find the specific command word, such as *add* or *delete*. Nice!\n\n<Panel type=\"info\" title=\"Additional Resources\">\n<ul>\n<li><a href=\"https://developers.deepgram.com/documentation/guides/understand-endpointing-interim-results/\">Understanding Endpointing and Interim Results When Transcribing Live Streaming Audio</a></li>\n<li><a href=\"https://developers.deepgram.com/documentation/features/interim-results/\">Interim Results</a></li>\n<li><a href=\"https://developers.deepgram.com/documentation/features/endpointing/\"> Endpointing</a></li>\n</ul>\n</Panel>\n\n## Add To-Do Item With Voice-Control\n\nNow I'll write logic to analyze a voice command for the words \"add to do\", and if the command has those words (such as in the command \"ADD TO DO walk the dog!\"), the item will be added to the `todoList` array in the Pinia store.\n\nI also want to keep track of the number of commands given, so if Deepgram doesn't transcribe the command correctly due to problems interpreting the speech, I can check that count value against the number of items in the store `todoList` array, and report back to the user that the command was misunderstood and didn't make it in.\n\nHere is pseudo-code for what I need to write:\n\n```js\nfunction addTodo(command) {\n  // Create array of Regular Expression words to identify in the text string, such as [/^add to do/, /^ad to do/]\n  // Loop through regex values using .find()\n  // Turn the command into a standardized string - lower case, no final punctuation, trim whitespace\n  // Check if the command string contains the regex value using .test\n  // If so, remove the command phrase ADD TO DO to create new string that is just the todo item\n  // add todo to the Pinia store todo-list\n  // reset the count of uttered commands to match the length of the todo list array in the store\n}\nwatch(utterance, () => {\n  // watch for a command and if there is one, add it to the list\n})\n```\n\nPHEW! That is a lot of logic to get through. Best to take it one step at a time.\n\n### Create a Set of Regular Expressions\n\nI want to analyze the command string such as \"Add to do walk the dog\" for the three words \"add to do\". I will:\n\n*   Create a regular expression by enclosing it within slashes `/-/`\n*   Use the character `^` before the command phrase to identify that the phrase should be at the beginning of the string\n\nSo the regular expression for \"add to do\" that I will use is `/^add to do/`.\n\nHowever, since there is the possibility that Deepgram could transcribe this with 'ad' instead of 'add' (seems unlikely, but I want to be prepared), I'll also use `/^ad to do/`. In fact, I'll match to anything that could be a homophone.\n\nI need an array to create a set of all the options. (The reason I don't include something like `/^add two do/` is because it's not grammatically correct, and Deepgram is smart enough to know not to transcribe things as ungrammatical.)\n\n```js\nconst addRegEx = [/^add to do/, /^ad to do/, /^add to dew/]\n```\n\n### Use the Array Method .find()\n\nI'll use the array method `.find` to loop through the array of regular expressions. It will search for the first item in the array that matches. If it finds one, there's no need to continue looping through since all we need is one match:\n\n```js\nfunction addTodo(command) {\n  const addRegEx = [/^add to do/, /^ad to do/, /^add to dew/]\n  // loop through array to find first match:\n  addRegEx.find((reg) => {})\n}\n```\n\n### Write a Method to Standardize the Command String\n\nI want the command to be lowercase, to not include periods, commas, question marks, or quotation marks, and to not have extra whitespace at the end. (I could entirely turn off punctuation in Deepgram, but I like the transcript to include apostrophes in words such as \"she's.\")\n\nI'll create a function to take the command string and standardize it:\n\n```js\nfunction standardizeUtterance(command) {\n  const punctuation = /[.,?\"]+/g\n  const change = command.toLowerCase().replace(punctuation, '').trim()\n  return change\n}\n```\n\nNotice that I used another regular expression, `/[.,?\"]+/g`. This paired with the `replace` method will search for any of those punctuation marks throughout the entire string and replace them with`\"\"` (which is nothing).\n\nThen I use this `standardizeUtterance` function inside the `addTodo` function. I'll add it before the loop so that it doesn't run for every loop:\n\n```js\nfunction addTodo(command) {\n  const addRegEx = [/^add to do/, /^ad to do/, /^add to dew/]\n  // clean up utterance\n  const item = standardizeUtterance(command)\n  addRegEx.find((reg) => {\n    // use item for more logic\n  })\n}\n```\n\n### Test that the Item Matches the Regular Expression\n\nI'll write an `if` statement to say that if the string starts with the regular expression command such as `/^add to do/`, then do something else (the something else will involve adding it to the to-do list).\n\nI use the method `.test`, which is a javascript method used to match a regular expression with a string, returning `true` or `false` depending on if there is a match or not.\n\n```js\n// inside addTodo method:\naddRegEx.find((reg) => {\n  if (reg.test(item)) {\n  }\n})\n```\n\n### Remove the Command Phrase from the String\n\nIf there is a match, I will want to add the string to the to-do list. But right now the whole string also contains the command phrase \"add to do\" as in the sentence \"Add to do walk the dog.\" I do not want \"add to do\" to be part of the string that goes into the to-do list array in the store.\n\nI'll write a function that takes the string and removes the phrase. I'll need to give it both the full command, and the regular expression (which is the command phrase). The method `replace` will search for the phrase and replace it with `\"\"`, i.e., nothing.\n\n```js\nfunction removeCommandPhrase(command, reg) {\n  const change = command.replace(reg, '').trim('')\n  return change\n}\n```\n\nThen I add it to `addTodo`:\n\n```js\n// inside addTodo method:\n\naddRegEx.find((reg) => {\n  if (reg.test(item)) {\n    // remove command phrase ADD TO DO\n    const todo = removeCommandPhrase(item, reg);\n    }\n  });\n}\n```\n\n### Add the To-Do Item to the To-Do List Array\n\nI'm almost done! This is the most important step. I can add the to-do item to the list in the store.\n\nI have to go back up to the start of the `script` and import the store. I also need to run the store function and set it to a variable that I can use (I'll set it to `store`)\n\n```js\n<script>\nimport { ref, watch } from \"vue\";\nimport useDeepgramSocket from \"@/composables/useDeepgramSocket\";\n// import store:\nimport { useTodoListStore } from \"../store/useTodoListStore\";\n\nexport default {\n  setup() {\n    const { DG_socket, DG_transcript, openStream, closeStream } =\n      useDeepgramSocket();\n     // run function and set to variable:\n    const store = useTodoListStore();\n    ...\n    function addTodo(command) {\n    ...\n    });\n}\n```\n\nThen inside `addTodo` I will use the `store.addTodo` function that is already in the store as an action (I created it in the first post in this series.)\n\n```js\n// inside addTodo method:\n\naddRegEx.find((reg) => {\n  if (reg.test(item)) {\n    // remove command phrase ADD TO DO\n    const todo = removeCommandPhrase(item, reg)\n\n    // add to store\n    store.addTodo(todo)\n  }\n})\n```\n\nNow the logic is there to add the item to the to-do list. I just need to do one more thing to make it show up on the screen.\n\n### Watch For a Command, Add it To The List\n\nRight now, the `addTodo` function with all the logic to add an item to the list never runs. I need to make it run somehow.\n\nOriginally, I set up Deepgram to turn on when the button is clicked. When the logic runs to create the WebSocket connection to Deepgram, I also get a value from it that I defined as `DG_transcript`. This value holds the transcript string that comes back after a user says something such as \"Add to do walk the dog\".\n\nI want to trigger `addTodo` to run **every time a new command is said**. And I know I'll be using that transcript for logic to keep track of how many times a command has been said (I'll go over that in the next section). So I'm going to create a ref in this `TodoSpeech.vue` component called `utterance`, which will stay in sync with the `DG_transcript` from the `useDeepgramSocket.js` composable.\n\n```js\n// inside setup() function in TodoSpeech.vue\nconst { DG_socket, DG_transcript, openStream, closeStream } =\n  useDeepgramSocket()\n// create ref\nlet utterance = ref(DG_transcript)\n```\n\nNow the `utterance` ref is in sync with the transcript that comes from Deepgram. I can watch that `utterance` ref for changes, and if there is a change, the `addTodo` function will run.\n\n```js\nwatch(utterance, () => {\n  if (utterance.value !== '') {\n    addTodo(utterance.value)\n  }\n})\n```\n\nNow when I use the voice-control feature to add an item to the list, I see it show up on the screen. Woo-hoo!\n\n## Dealing with Misunderstood Commands\n\nAutomatic Speech Recognition technology has gotten really good, but there can still be mistakes. People might mumble or slur some of the words, or the garbage truck could be making a lot of noise in the background. Deepgram's ASR technology can handle a lot, but I have to expect that sometimes it will mishear something.\n\nWhat if I say \"Add to do walk the dog\", but Deepgram mishears it as \"And to do walk the dog\"? I want to be ready for that and for any other situation. What if the user forgets to add the command and just says \"Walk the dog!\"\n\nI'm going to program my app to show a message to the user when this happens. It will display \"I didn't catch that\" when it doesn't hear a match to the command phrase.\n\nThe logic for this will depend on **counting how many times a command has been spoken**. If the speaker says one thing, and it gets added to the list, then the count should be one. But if the speaker says another thing after that, and it doesn't get added to the list, then the count is at two, but the list is at one. So there was a misunderstanding.\n\nHere is the logic that will run:\n\n```js\nwatch(utterance, () => {\n  if (utterance.value !== '') {\n    count.value++\n    addTodo(utterance.value)\n    alertMisunderstood()\n  }\n})\n```\n\nIf a command is given, the count is increased by one. Then the`addTodo` function runs.\n\nInside `addTodo`, I check for a match. If there is a match, it means that an item is added to the list. After it is added to the list, I will reset the count to match the number of items in the list:\n\n```js\nfunction addTodo(command) {\n  const addRegEx = [/^add to do/, /^ad to do/, /^add to dew/]\n  // clean up utterance\n  const item = standardizeUtterance(command)\n  addRegEx.find((reg) => {\n    if (reg.test(item)) {\n      // remove command phrase ADD TO DO\n      const todo = removeCommandPhrase(item, reg)\n      // add to store\n      store.addTodo(todo)\n      // reset count\n      count.value = store.todoList.length\n    }\n  })\n}\n```\n\nHowever, in the `if` statement above, I test for a match. If it does NOT find a match, none of that logic inside of it runs. So that would result in the count having increased, but the number of items in the to-do list not having increased.\n\nIn that case, I need to write logic to notice that discrepancy and alert the user:\n\n```js\nfunction alertMisunderstood() {\n  // if count doesn't equal todo list length, the command was misunderstood\n  if (count.value !== store.todoList.length) {\n    deepgramStatus.value = \"I didn't catch that\"\n  }\n}\n```\n\nI don't want the phrase \"I didn't catch that\" to remain on the screen forever. I'll have it disappear after a second and return to \"Listening\":\n\n```js\nfunction alertMisunderstood() {\n  // if count doesn't equal todo list length, the command was misunderstood\n  if (count.value !== store.todoList.length) {\n    deepgramStatus.value = \"I didn't catch that\"\n    setTimeout(() => {\n      deepgramStatus.value = 'Listening'\n    }, 1000)\n  }\n}\n```\n\nNow, everything in the watcher is set up to make sure that when a user gives a command, it is either added to the to-do list, or reported back with a message to the user that the command was misunderstood.\n\n```js\nwatch(utterance, () => {\n  if (utterance.value !== '') {\n    count.value++\n    addTodo(utterance.value)\n    alertMisunderstood()\n  }\n})\n```\n\n## Delete To-Do Item With Voice-Control\n\nThe hard part of this is done. I walked through the step-by-step logic to analyze voice-control command strings. Now that I want to delete an item, I can use the same logic.\n\nI'll write out the pseudo-code and supply my logic. I won't go through it step-by-step again since the only thing that will be different is writing the step **to remove an item from the store to-do list** instead of add an item.\n\nAnyone following along with this post and building the voice-control feature could now take some time to write a `deleteTodo` function. I recommend copying the pseudo-code below (the commented-out steps) and then writing each step of the logic.\n\n### Pseudo-Code:\n\n```js\nfunction deleteTodo(command) {\n  // Create an array of Regular Expression words to identify in the text, such as [/^delete/];\n  // Loop through regex values using .find\n  // Turn the command into a standardized string - lower case, no final punctuation, trim whitespace\n  // Check if the command string contains the regex value using .test()\n  // If so, remove the command phrase DELETE to create new string that is just the todo item\n  // Loop through store to-do list and for each item, check if that item matches the new string that is just the todo item\n  // if item in store todo list matches, delete from store to-do list\n  // reset the count of uttered commands to match the length of the todo list array in the store\n}\n\nwatch(utterance, () => {\n  // watch for a command to run delete logic\n})\n```\n\nHere is the code I wrote for this voice-controlled delete logic:\n\n```js\nfunction deleteTodo(command) {\n  const deleteRegEx = [/^delete/]\n  // clean up utterance\n  const item = standardizeUtterance(command)\n  deleteRegEx.find((reg) => {\n    if (reg.test(item)) {\n      // remove command phrase DELETE\n      const todo = removeCommandPhrase(item, reg)\n      store.todoList.forEach((storeTodo) => {\n        // if item in store todo list matches this utterance,\n        if (storeTodo.item === todo) {\n          // delete from store\n          store.deleteTodo(storeTodo.id)\n          // reset count\n          count.value = store.todoList.length\n        }\n      })\n    }\n  })\n}\n```\n\nAnd in the watcher:\n\n```js\nwatch(utterance, () => {\n  if (utterance.value !== '') {\n    count.value++\n    addTodo(utterance.value)\n    deleteTodo(utterance.value)\n    alertMisunderstood()\n  }\n})\n```\n\n## Check-Off To-Do Item With Voice-Control\n\nThe logic to check off an item on the list is exactly the same as the logic to delete an item, except instead of calling `store.deleteTodo(storeTodo.id)`, I will call `store.toggleCompleted(storeTodo.id)`.\n\nHere is the logic to check-off an item with voice-control:\n\n```js\nfunction checkOffTodo(command) {\n  const checkOffRegEx = [/^check off/]\n  const item = standardizeUtterance(command)\n  checkOffRegEx.find((reg) => {\n    if (reg.test(item)) {\n      const todo = removeCommandPhrase(item, reg)\n      store.todoList.forEach((storeTodo) => {\n        if (storeTodo.item === todo) {\n          // toggle completed in store:\n          store.toggleCompleted(storeTodo.id)\n          count.value = store.todoList.length\n        }\n      })\n    }\n  })\n}\n```\n\nAnd the watcher:\n\n```js\nwatch(utterance, () => {\n  if (utterance.value !== '') {\n    count.value++\n    addTodo(utterance.value)\n    deleteTodo(utterance.value)\n    checkOffTodo(utterance.value)\n    alertMisunderstood()\n  }\n})\n```\n\n## Conclusion\n\nThat concludes this post on how to add voice-control to a Vue 3 to-do list app. It was well worth the trouble to turn a classic to-do list project into a more exciting voice-based app that uses Deepgram's speech-to-text API.\n\nFeel free to reach out with questions on [Twitter](https://twitter.com/sandra_rodgers_). Happy coding!\n\n        ";
						}
						async function compiledContent$3P() {
							return load$3P().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3P() {
							return (await import('./chunks/index.5e33c272.mjs'));
						}
						function Content$3P(...args) {
							return load$3P().then((m) => m.default(...args));
						}
						Content$3P.isAstroComponentFactory = true;
						function getHeadings$3P() {
							return load$3P().then((m) => m.metadata.headings);
						}
						function getHeaders$3P() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3P().then((m) => m.metadata.headings);
						}

const __vite_glob_0_33 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3P,
  file: file$3P,
  url: url$3P,
  rawContent: rawContent$3P,
  compiledContent: compiledContent$3P,
  default: load$3P,
  Content: Content$3P,
  getHeadings: getHeadings$3P,
  getHeaders: getHeaders$3P
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3O = {"title":"Building an npm Package","description":"Create, publish, and use your first npm package returning values, functions, and classes.","date":"2021-12-06T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1638810126/blog/2021/12/build-npm-packages/building-an-npm-package%402x.jpg","authors":["kevin-lewis"],"category":"tutorial","tags":["nodejs"],"seo":{"title":"Building an npm Package","description":"Create, publish, and use your first npm package returning values, functions, and classes."},"shorturls":{"share":"https://dpgr.am/87d0a0c","twitter":"https://dpgr.am/7b35feb","linkedin":"https://dpgr.am/72dcd11","reddit":"https://dpgr.am/2731585","facebook":"https://dpgr.am/db18251"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453825/blog/build-npm-packages/ograph.png"}};
						const file$3O = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/build-npm-packages/index.md";
						const url$3O = undefined;
						function rawContent$3O() {
							return "\r\nIf you're a JavaScript developer, you've almost certainly used npm before. The Node Package Manager is a registry that allows developers to package up code and share it using a common set of commands - most commonly `npm install package-name`. Our own Node.js SDK is available on npm under [@deepgram/sdk](https://www.npmjs.com/package/@deepgram/sdk).\r\n\r\nIn this post, we'll create, publish, install, and use our first npm package. We'll then extend the functionality and end up with a more complex package that will let users query [The Open Movie Database](http://www.omdbapi.com).\r\n\r\nOur final class-based package code can be found at <a href=\"https://github.com/deepgram-devs/npm-package\">https://github.com/deepgram-devs/npm-package</a>.\r\n\r\n## Before We Start\r\n\r\nYou will need:\r\n\r\n*   Node.js installed on your machine - [download it here](https://nodejs.org/en/).\r\n*   An npm account - [get one here](https://www.npmjs.com/signup).\r\n*   An Open Movie Database API Key - [get one here](http://www.omdbapi.com/apikey.aspx) and be sure to use the verification link in the email with the key.\r\n\r\nCreate a new directory and open it in your code editor of choice.\r\n\r\n## Creating an npm Package\r\n\r\nCreate a `package.json` file and populate it with minimal information required for an npm package:\r\n\r\n```json\r\n{\r\n  \"name\": \"@username/first-package\",\r\n  \"version\": \"0.0.1\"\r\n}\r\n```\r\n\r\nThe `name` must be unique across all of npm. To aid this, and help list packages with the same author, we can 'scope' packages to a user or organization. Replace `username` with your npm username to scope it. Some other notes about choosing your package name:\r\n\r\n*   You cannot use uppercase letters.\r\n*   You can only use URL-safe characters.\r\n*   The maximum character length is 214.\r\n\r\nThe `version` should follow [semantic versioning](https://docs.npmjs.com/about-semantic-versioning), which is `NUMBER.NUMBER.NUMBER`. Every time we publish an update to our package, the version must be different from previously-published versions.\r\n\r\nIf not specified, the default file for your project will be `index.js`. Create a file and open it in your code editor:\r\n\r\n```js\r\nconst value = 42\r\nmodule.exports = value\r\n```\r\n\r\nThis is a viable, though not terribly useful, npm package - it will always return a fixed value of `42`. The `module.exports` value can be anything - a fixed value, an object with multiple values, a function, a class, or any other data.\r\n\r\nWhile fixed values may have limited use, they are useful in some contexts - the `profane-words` package I used in my [automatic profanity censoring](https://blog.deepgram.com/censor-profanity-nodejs/) post used a fixed array value to include a list of almost 3000 profanities instead of me needing to include them a more manual way.\r\n\r\n## Publishing an npm Package\r\n\r\nOpen your terminal and navigate to your project directory and run the following commands:\r\n\r\n```bash\r\nnpm login\r\nnpm publish --access=public\r\n```\r\n\r\nYou have now published your first ever npm package - congratulations! If you go to <a href=\"https://www.npmjs.com/package/@username/first-package\">https://www.npmjs.com/package/@username/first-package</a> you should see it. Reminder: if ever you are publishing again, you must increase the version in `package.json`,or you will get an error.\r\n\r\n## Testing Your npm Package\r\n\r\nWant to use your package locally to test it before publishing? Create a new file in your repository called `scratchpad.js` (you can call it anything - this is what I use) and open it on your code editor:\r\n\r\n```js\r\nconst firstPackage = require('./index.js')\r\nconsole.log(firstPackage) // 42\r\n```\r\n\r\nRun this file with `node scratchpad.js`.\r\n\r\nIf you want to exclude this file from being downloaded by users when they install your package, add it to a `.gitignore` file. Create one now and enter the filenames you want to be excluded (one per line):\r\n\r\n    scratchpad.js\r\n\r\n## Using Your npm Package\r\n\r\nCreate a brand new directory outside of this project. Navigate to it in a terminal, and type:\r\n\r\n    npm install @username/first-package\r\n\r\nCreate an `index.js` file to require and use the package:\r\n\r\n```js\r\nconst firstPackage = require('@username/first-package')\r\nconsole.log(firstPackage) // 42\r\n```\r\n\r\n## Exporting Functions\r\n\r\nAs mentioned above, you can export any JavaScript value or datatype in your package. Replace the content of your `index.js` with the following:\r\n\r\n```js\r\nconst value = 42\r\n\r\nfunction sum(a, b) {\r\n  return a + b\r\n}\r\n\r\nmodule.exports = {\r\n  value,\r\n  sum,\r\n}\r\n```\r\n\r\nThis is exporting an object with both the fixed value and the function. Update `scratchpad.js` and then rerun it:\r\n\r\n```js\r\nconst firstPackage = require('./index.js')\r\nconsole.log(firstPackage) // { value: 42, sum: [Function: sum] }\r\nconsole.log(firstPackage.sum(1, 3)) // 4\r\n```\r\n\r\nYou may have seen object destructing when requiring packages. Here's how it looks:\r\n\r\n```js\r\nconst { sum } = require('./index.js')\r\nconsole.log(sum(1, 3)) // 4\r\n```\r\n\r\nThis takes the `sum` property in the object returned by our package and makes it available as a top-level variable called `sum`. This is what we do with our [Deepgram Node.js SDK](https://developers.deepgram.com/sdks-tools/sdks/node-sdk/):\r\n\r\n```js\r\nconst { Deepgram } = require('@deepgram/sdk')\r\n```\r\n\r\n## Exporting Classes\r\n\r\nExporting one or more functions is quite a common behavior of npm packages, as is exporting a class. Here's what interacting with a class-based package looks like courtesy of the Deepgram Node.js SDK:\r\n\r\n```js\r\nconst { Deepgram } = require('@deepgram/sdk')\r\nconst deepgram = new Deepgram('DEEPGRAM_API_KEY')\r\ndeepgram.transcription\r\n  .preRecorded({\r\n    url: 'https://static.deepgram.com/examples/nasa-spacewalk-interview.wav',\r\n  })\r\n  .then((transcript) => {\r\n    console.log(transcript)\r\n  })\r\n```\r\n\r\nLet's create our own exported class for the Open Movie Database. First, install the `axios` package that will help us make API calls. In your terminal:\r\n\r\n    npm install axios\r\n\r\nOnce you do this take a look at `package.json` - the `dependencies` section will be created for you. When users install your package, it will also install axios for them, along with axios' dependencies, and so on.\r\n\r\nReplace the whole content of `index.js` with the following:\r\n\r\n```js\r\nconst axios = require('axios')\r\n\r\nclass OpenMovieDatabase {\r\n  constructor(apiKey) {\r\n    this.apiKey = apiKey\r\n  }\r\n\r\n  async get(parameters) {\r\n    try {\r\n      const { data } = await axios({\r\n        method: 'GET',\r\n        url: 'http://www.omdbapi.com',\r\n        params: { apikey: this.apiKey, ...parameters },\r\n      })\r\n      return data\r\n    } catch (error) {\r\n      console.log(error.response)\r\n      throw error.response.data\r\n    }\r\n  }\r\n}\r\n\r\nmodule.exports = OpenMovieDatabase\r\n```\r\n\r\nReplace `scratchpad.js` with the following:\r\n\r\n```js\r\nconst OpenMovieDatabase = require('./index')\r\nconst omdb = new OpenMovieDatabase('YOUR_KEY_HERE')\r\n\r\nomdb\r\n  .get({ t: 'Inside Out' })\r\n  .then((results) => {\r\n    console.log({ results })\r\n  })\r\n  .catch((error) => {\r\n    console.log({ error })\r\n  })\r\n```\r\n\r\nOnce the package is required, an instance needs to be created. The `constructor` we define expects an `apiKey` which is then stored in that instance. When using the package, we only need to provide the key once and then use `omdb.get()` as many times as we want without needing to provide our key as it is automatically included in the API request `params`. Nifty, right?\r\n\r\nBefore publishing this again, be sure to add `node_modules` to a new line in your `.gitignore`.\r\n\r\n## Wrapping Up\r\n\r\nThis package lacks lots of features I would expect as a user:\r\n\r\n*   More robust error handling.\r\n*   Checking required parameters are provided and providing useful errors if not.\r\n*   Splitting the 'get' and 'search' requests.\r\n\r\nOur final class-based package code can be found at <a href=\"https://github.com/deepgram-devs/npm-package\">https://github.com/deepgram-devs/npm-package</a>.\r\n\r\nWhen putting together my first npm packages, I found most tutorials stopped at the most basic examples or assumed lots of additional knowledge. I hope this has provided more helpful context and helped you get your first package published. If it did - please let us know what you've published so we can celebrate together - we are [@DeepgramDevs](https://twitter.com/DeepgramDevs) on Twitter or <a href=\"mailto:devrel@deepgram.com\">devrel@deepgram.com</a> via email. \r\n\r\n        ";
						}
						async function compiledContent$3O() {
							return load$3O().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3O() {
							return (await import('./chunks/index.2f1130f2.mjs'));
						}
						function Content$3O(...args) {
							return load$3O().then((m) => m.default(...args));
						}
						Content$3O.isAstroComponentFactory = true;
						function getHeadings$3O() {
							return load$3O().then((m) => m.metadata.headings);
						}
						function getHeaders$3O() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3O().then((m) => m.metadata.headings);
						}

const __vite_glob_0_34 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3O,
  file: file$3O,
  url: url$3O,
  rawContent: rawContent$3O,
  compiledContent: compiledContent$3O,
  default: load$3O,
  Content: Content$3O,
  getHeadings: getHeadings$3O,
  getHeaders: getHeaders$3O
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3N = {"title":"Build With the Official Deepgram SDKs","description":"Add speech-to-text to your application even faster with the new Node.js and Python SDKs for the Deepgram API.","date":"2021-07-22T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981374/blog/build-with-the-official-deepgram-sdks/build-w-official-dg-sdks%402x.jpg","authors":["michael-jolley"],"category":"product-news","tags":["nodejs","python","sdk"],"seo":{"title":"Build With the Official Deepgram SDKs","description":"Add speech-to-text to your application even faster with the new Node.js and Python SDKs for the Deepgram API."},"shorturls":{"share":"https://dpgr.am/9b93576","twitter":"https://dpgr.am/5dd8351","linkedin":"https://dpgr.am/40298d0","reddit":"https://dpgr.am/c8ec55a","facebook":"https://dpgr.am/e68d84d"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981374/blog/build-with-the-official-deepgram-sdks/build-w-official-dg-sdks%402x.jpg"}};
						const file$3N = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/build-with-the-official-deepgram-sdks/index.md";
						const url$3N = undefined;
						function rawContent$3N() {
							return "\r\nWe love empowering our developer communities to take full advantage of voice in their applications. So, we're announcing two new official SDKs for Node.js and Python. Both of these SDKs make it easier than ever to add automated speech-to-text recognition to your applications.\r\n\r\n## How Easy Is It?\r\n\r\nTo get started, make sure you have a Deepgram account by signing up at https://console.deepgram.com/signup. After signing up, log in and get an API key. Then, in your terminal, run the appropriate command below to install the SDK.\r\n\r\n#### **Node.js**\r\n\r\n```\r\nnpm install @deepgram/sdk\r\n```\r\n\r\n#### **Python**\r\n\r\n```\r\npip install deepgram-sdk\r\n```\r\n\r\nOnce the SDK has been installed, the following snippets will allow you to transcribe a prerecorded audio file. Be sure to replace `YOUR_DEEPGRAM_API_KEY` with the API key, you created earlier.\r\n\r\n#### **Node.js**\r\n\r\n```js\r\nconst { Deepgram } = require(\"@deepgram/sdk\");\r\n\r\nconst deepgramApiKey = \"YOUR_DEEPGRAM_API_KEY\";\r\n\r\nfunction main() {\r\n  return new Promise((resolve, reject) => {\r\n    (async () => {\r\n      try {\r\n        const deepgram = new Deepgram(deepgramApiKey);\r\n        const transcription = await newDeepgram.transcription.preRecorded(\r\n          {\r\n            url: \"https://static.deepgram.com/examples/Bueller-Life-moves-pretty-fast.wav\",\r\n          },\r\n          {\r\n            punctuate: true,\r\n          }\r\n        );\r\n        console.dir(transcription, { depth: null });\r\n        resolve();\r\n      } catch (err) {\r\n        console.log(`Err: ${err}`);\r\n        reject(err);\r\n      }\r\n    })();\r\n  });\r\n}\r\nmain();\r\n```\r\n\r\n#### **Python**\r\n\r\n```python\r\nfrom deepgram import Deepgramimport asyncio, json\r\n\r\nDEEPGRAM_API_KEY = 'YOUR_API_KEY'\r\n\r\nasync def main():\r\n    # Initializes the Deepgram SDK\r\n  dg_client = Deepgram(DEEPGRAM_API_KEY)\r\n  source = {'url': 'https://static.deepgram.com/examples/Bueller-Life-moves-pretty-fast.wav'}\r\n  response = await dg_client.transcription.prerecorded(source)\r\n  print(json.dumps(response, indent=4))\r\n\r\nasyncio.run(main())\r\n```\r\n\r\n## Contributions Welcome\r\n\r\nOur SDKs are still in their infancy, but we're building them in public. We welcome all issues and pull requests to our [Node.js](https://github.com/deepgram/node-sdk) and [Python](https://github.com/deepgram/python-sdk) repositories. Of course, we'd also love to hear what you're building, so tweet at [@DeepgramDevs](https://twitter.com/DeepgramDevs) and let us know!\r\n";
						}
						async function compiledContent$3N() {
							return load$3N().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3N() {
							return (await import('./chunks/index.bc21e34d.mjs'));
						}
						function Content$3N(...args) {
							return load$3N().then((m) => m.default(...args));
						}
						Content$3N.isAstroComponentFactory = true;
						function getHeadings$3N() {
							return load$3N().then((m) => m.metadata.headings);
						}
						function getHeaders$3N() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3N().then((m) => m.metadata.headings);
						}

const __vite_glob_0_35 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3N,
  file: file$3N,
  url: url$3N,
  rawContent: rawContent$3N,
  compiledContent: compiledContent$3N,
  default: load$3N,
  Content: Content$3N,
  getHeadings: getHeadings$3N,
  getHeaders: getHeaders$3N
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3M = {"title":"Building 404 Pages That Bring Joy","description":"How we transformed the bad experience of landing on a 404 page into an enjoyable experience with a game.","date":"2022-03-10T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1646855876/blog/2022/03/building-404-pages-that-bring-joy/Turning-Our-404-Page-Into-a-Game%402x.jpg","authors":["michael-jolley"],"category":"tutorial","tags":["game-dev","nuxtjs"],"seo":{"title":"Building 404 Pages That Bring Joy","description":"How we transformed the bad experience of landing on a 404 page into an enjoyable experience with a game."},"shorturls":{"share":"https://dpgr.am/6d0c342","twitter":"https://dpgr.am/462bfa3","linkedin":"https://dpgr.am/fb46e5a","reddit":"https://dpgr.am/4bfe39e","facebook":"https://dpgr.am/24df052"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454014/blog/building-404-pages-that-bring-joy/ograph.png"}};
						const file$3M = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/building-404-pages-that-bring-joy/index.md";
						const url$3M = undefined;
						function rawContent$3M() {
							return "\r\nYou clicked on that link with high expectations. You knew you were about to find\r\nthe answers you'd been searching for. Then it happened. You saw the dreaded 404\r\nerror letting you know that the content you were looking for wasn't there. Maybe\r\nthe content lives at a new location or perhaps it's been permanently removed.\r\nRegardless of the reason, your high hopes have been dashed and you're left to\r\nbegin your search again.\r\n\r\nIt's a terrible experience and one we wanted to make better. But before we get\r\ninto the details of what we've done, let's talk about where the idea started.\r\n\r\n## To Those About to Hack, We Salute You\r\n\r\nAt Deepgram, we have a goal that every interaction with us should be pleasant\r\nand (hopefully) enjoyable. We spend a lot of time thinking of how to make that\r\nhappen. One of the methods we use for brainstorming ideas and gathering feedback\r\nis hosting internal hack-a-thons (known internally as GRAMJAMs.) Last year, 8\r\nteams competed to be the GRAMJAM champion and one of the entries was a super\r\nfun game called MadGab.\r\n\r\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1645937086/blog/2022/03/building-404-pages-that-bring-joy/gj2021.png\" alt=\"GRAMJAM 2021 logo\" style=\"width: 50%;margin:auto;\"/>\r\n\r\nThe premise was simple: present the user with a nonsensical phrase that is\r\nphonetically similar to a real-life phrase. Players would read the nonsensical\r\nphrase aloud and attempt to then say the associated real-life phrase. An example\r\nwould be \"mass turk hard\" in the place of \"Mastercard.\" It was a great\r\nexperience. The game's format was simple to understand and perform and it was\r\nhighly addictive. We knew immediately that we had to make this available for\r\nothers. This brings us back to the 404 page.\r\n\r\n## Making it Helpful\r\n\r\nWe knew we couldn't just have a game on our 404 page. While it's a fun\r\ndistraction, we realized that the visitor came with a purpose. That purpose\r\nneeded to be our number one priority. How could we help them reach their\r\nobjective?\r\n\r\n### Can We Find What You're Looking For?\r\n\r\nWouldn't it be great if we could \"guess\" what you were looking for and provide\r\nyou with a link to it? Fortunately, as we've built our site, we've tried to do\r\na good job of creating routes that consist of keywords associated with the\r\ncontent of the page. This means we can assume that parsing a route (even one\r\nthat resulted in a 404) should provide keywords that are relevant to what the\r\nuser was trying to reach.\r\n\r\nWe then send those parsed words to Algolia to search our site and display the\r\nthree most relevant results to the user. With a little luck, the results shown\r\nwill provide the information the visitor was looking for and they're only a\r\nclick away from continuing their journey.\r\n\r\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1646884394/blog/2022/03/building-404-pages-that-bring-joy/results.png\" alt=\"Search results on the 404 page\"\r\nstyle=\"width:75%;margin:auto;\"/>\r\n\r\n### Quick Access to Search\r\n\r\nWhat if there were no results or the results didn't meet the user's need. Without\r\nadditional information, we can't move the user forward. So we added a search\r\ninput to the page to give quick access to finding what they're looking for. Yes,\r\nwe have a search bar on the top of every page in the navigation, but we don't\r\nwant users to expend unnecessary brain power trying to find it. Putting the\r\nsearch input front-and-center allows them to be on their way as efficiently as\r\npossible.\r\n\r\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1646883861/blog/2022/03/building-404-pages-that-bring-joy/search-bar.png\" alt=\"Search bar on the 404 page\"\r\nstyle=\"width:75%;margin:auto;\"/>\r\n\r\nNow that we've done all we can to provide relevant information and paths\r\nforward, let's try to brighten their day by providing a chance to have fun\r\nbefore they move on.\r\n\r\n## Making it Enjoyable\r\n\r\nAfter some brainstorming, we had a list of requirements to recreate MadGab on\r\nour platform:\r\n\r\n*   Any API keys should remain on the server, or be short-lived (i.e. less than 5 minutes)\r\n*   Store the collection of gibberish phrases/answers on the server to protect them from cheaters 😁\r\n\r\nWe hope to open-source MadGab in the future, but, for now, I'll share how we\r\nachieved the goals above.\r\n\r\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1646945899/blog/2022/03/building-404-pages-that-bring-joy/madgram.png\" alt=\"Screenshot of the MadGram game on our 404 page\"\r\nstyle=\"width:75%;margin:auto;\"/>\r\n\r\n### Protecting API Keys\r\n\r\nMadGab connects to the Deepgram API via a WebSocket and sends audio from the\r\nusers' microphone to be transcribed. This requires us to send an API key in the\r\nheader of that connection. That means an API key will be exposed to the client.\r\nTo minimize any risk, we wanted to use short-lived API keys. Because the\r\ndeveloper platform is hosted on Netlify, functions seemed like a good option for\r\nproviding a way to create a temporary API key to use for the game.\r\n\r\nLuckily for us, the Deepgram API allows creating API keys with a specified\r\ntime-to-live. So we imported the Deepgram Node SDK and use it to create a key\r\nthat lives for 5 seconds. That's just long enough for our front-end to connect\r\nto the Deepgram API before it expires. Below is the code for the Netlify\r\nfunction that generates and returns the API key.\r\n\r\n```js\r\nconst { Deepgram } = require('@deepgram/sdk')\r\nrequire('dotenv').config()\r\n\r\nconst deepgram = new Deepgram(process.env.DEEPGRAM_API_KEY)\r\nconst deepgramProjectId = process.env.DEEPGRAM_PROJECT_ID\r\n\r\nexports.handler = async function (event, context) {\r\n  try {\r\n    const key = await deepgram.keys.create(\r\n      deepgramProjectId,\r\n      'Temp 404 key',\r\n      ['member'],\r\n      {\r\n        timeToLive: 5\r\n      })\r\n\r\n    return {\r\n      statusCode: 200,\r\n      body: JSON.stringify(key),\r\n    }\r\n  }\r\n  catch (err) {\r\n    console.log(err)\r\n\r\n    return {\r\n      statusCode: 500,\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n### No Peeking at the Answers\r\n\r\nOur teams at Deepgram had already collected a list of phonetic phrases matched\r\nwith the actual word phrases, so we were able to use that list in the game.\r\nWhile we could have created an array of those phrases in our component, we\r\ndidn't want users to be able to find them in the code and cheat. To solve this,\r\nwe created another Netlify function that could be called to return phrases\r\non-demand.\r\n\r\nEach phrase is defined as an object with three properties:\r\n\r\n*   `id`: a unique identifying number for the phrase\r\n*   `suggestion`: the phonetic phrase\r\n*   `result`: the phrase to be spoken\r\n\r\nTo prevent the need to call the function after every round of the game, the\r\nfunction returns up to three phrase objects at a time. However, we don't want to\r\nsend the same phrase to the same user until they've played every phrase\r\navailable. This requires us to track which phrases the user has played on the\r\nclient side. Then, each time we request new phrases, we'll send an array of the\r\nID's of each phrase the user has played in the body of the request. So the first\r\nthing the function should do is ensure the request is sent via HTTP `POST`.\r\n\r\n```js\r\n// Only allow POST\r\nif (event.httpMethod !== 'POST') {\r\n  return {\r\n    statusCode: 405,\r\n    body: 'Method Not Allowed',\r\n    headers: {\r\n      Allow: 'Get',\r\n    },\r\n  }\r\n}\r\n```\r\n\r\nNext, it will parse the request body to get the id's of the phrases the user has\r\nalready attempted.\r\n\r\n```js\r\nconst userChoices = []\r\nif (event.body !== undefined) {\r\n  const req = JSON.parse(event.body)\r\n  userChoices.push(...req.choices)\r\n}\r\n```\r\n\r\nCurrently, our phrases are stored in an array inside the function and called\r\n`choices`. So the next step is to filter the `choices` array to remove any\r\npreviously used phrases. If we've reached the end of choices, then we restart the\r\ngame and begin sending previously used phrases again. We'll also set the\r\n`restart` variable to true and return that as well. This notifies the\r\nclient-side that we've restarted and it should clear its cache of previously\r\nused phrases.\r\n\r\n```js\r\nlet restart = false\r\nconst availableChoices = choices.filter(f => !userChoices.includes(f.id))\r\nif (availableChoices.length === 0) {\r\n  availableChoices.push(...choices)\r\n  restart = true\r\n}\r\n```\r\n\r\nNow we want to select three random choices from `availableChoices`. To do that,\r\nwe created a `getRandomChoice` function that can return a random phrase from\r\n`availableChoices`.\r\n\r\n```js\r\nfunction getRandomChoice(availableChoices) {\r\n  const randomNumber = Math.floor(Math.random() * (availableChoices.length))\r\n  return availableChoices.splice(randomNumber, 1)[0]\r\n}\r\n```\r\n\r\nThen we can call that function three times to gather the three phrases to return\r\nto the client-side. If less than three phrases remain, we just return the\r\nremaining phrases.\r\n\r\n```js\r\nif (availableChoices.length > 3) {\r\n  selectedChoices.push(getRandomChoice(availableChoices))\r\n  selectedChoices.push(getRandomChoice(availableChoices))\r\n  selectedChoices.push(getRandomChoice(availableChoices))\r\n}\r\nelse {\r\n  selectedChoices.push(...availableChoices)\r\n}\r\n```\r\n\r\nFinally, we return the `selectedChoices` array and the `restart` boolean to the\r\nclient-side.\r\n\r\n```js\r\nreturn {\r\n  statusCode: 200,\r\n  body: JSON.stringify({\r\n    restart,\r\n    choices: selectedChoices\r\n  }),\r\n}\r\n```\r\n\r\n## Better Experiences are a Core Value\r\n\r\nAt the heart of this project is a desire to provide a better experience for\r\ndevelopers. Our team at Deepgram spends a LOT of time focused on how to make\r\nthat happen. From the experience of signing up, working in our console, using\r\nour SDKs, and yes, even our 404 page. We want every encounter with Deepgram to\r\nbe informative, helpful, and pleasant. So while 404 pages interrupt your flow of\r\nwork, hopefully, these changes empower you to find what you need faster, while\r\nalso providing an enjoyable experience.\r\n\r\nWould you be interested in a MadGab component to use on your projects? Follow us\r\non Twitter at [@DeepgramDevs](https://twitter.com/DeepgramDevs) and let us know.\r\n\r\n        ";
						}
						async function compiledContent$3M() {
							return load$3M().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3M() {
							return (await import('./chunks/index.45d03faf.mjs'));
						}
						function Content$3M(...args) {
							return load$3M().then((m) => m.default(...args));
						}
						Content$3M.isAstroComponentFactory = true;
						function getHeadings$3M() {
							return load$3M().then((m) => m.metadata.headings);
						}
						function getHeaders$3M() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3M().then((m) => m.metadata.headings);
						}

const __vite_glob_0_36 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3M,
  file: file$3M,
  url: url$3M,
  rawContent: rawContent$3M,
  compiledContent: compiledContent$3M,
  default: load$3M,
  Content: Content$3M,
  getHeadings: getHeadings$3M,
  getHeaders: getHeaders$3M
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3L = {"title":"Building a Conversational AI Flow with Deepgram","description":"Learn how to use endpointing and interim results to build a conversational AI flow.","date":"2022-09-23T17:38:18.493Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1664336619/blog/Building-Conversational-AI-Flow-with-Deepgram/2209-Building-Conversational-AI-Flow-with-Deepgram-thumb-554x220_vkzldi.png","authors":["shir-goldberg"],"category":"tutorial","tags":["conversational-ai","endpointing","interim-results"],"seo":{"title":"Building a Conversational AI Flow with Deepgram","description":"Learn how to use endpointing and interim results to build a conversational AI flow."},"shorturls":{"share":"https://dpgr.am/daf3c23","twitter":"https://dpgr.am/b6afcdf","linkedin":"https://dpgr.am/3be026a","reddit":"https://dpgr.am/96c7583","facebook":"https://dpgr.am/a099e21"}};
						const file$3L = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/building-a-conversational-ai-flow-with-deepgram/index.md";
						const url$3L = undefined;
						function rawContent$3L() {
							return "\nHow do you know when someone is finished talking? Before I started working at Deepgram, I hadn’t thought about this question much. When having conversations in person, us humans can use all sorts of contextual cues, body language, and societal norms to figure out when someone has finished their thought and we can jump in with our own opinion. But as we’ve all seen over Zoom during the last few years, figuring out when someone is done talking is a lot harder to do virtually. It’s even harder when the listener isn’t human at all—and is a machine learning model transcribing speech!\n\nBusiness problems that need speech-to-text often also need an understanding of when a speaker has completed their thought. One common use case for this is building conversational AI bots that need to respond to a user’s queries. The bot needs to be careful both to not to cut the user off, and to respond in a timely enough manner that the conversation feels “real-time”.\n\nDeepgram’s real-time speech-to-text service provides two main mechanisms that can help build a conversational flow. One is interim results, and the other is endpointing. Together, the two can give you information about when a speaker has finished talking, and when your system should respond.\n\n[Interim results](https://developers.deepgram.com/documentation/features/interim-results/), which are disabled by default, are sent back every few seconds. These messages, marked with `is_final=false`,  indicate that Deepgram is still gathering more audio and the transcription results may change as additional context is given. Once Deepgram has collected enough audio to make the best possible prediction, it will finalize the prediction and send back a transcript marked with `is_final=true`.\n\n[Endpointing](https://developers.deepgram.com/documentation/features/endpointing/), which is enabled by default, is an algorithm that detects the end of speech. When endpointing triggers, Deepgram immediately sends back a message. These messages will be marked with `speech_final=true` to indicate an endpoint was detected and `is_final=true `to indicate the transcription is finalized.\n\nThe simplest way to determine when someone is done talking is based on silence. Endpointing can give you almost immediate feedback when silence is detected, which may be useful for applications that prioritize quick processing of results. Here’s a code example that uses your microphone and the Python package beepy to play a notification sound when Deepgram detects an endpoint.\n\nTo run the code, install beepy using `pip install beepy`. Then save the following code as `endpointing.py`. Turn your volume up so you’ll be able to hear the beep sound, and run the code:\n\n`python endpointing.py  -k 'YOUR_DG_API_KEY'`\n\n```python\nimport pyaudio\nimport asyncio\nimport sys\nimport websockets\nimport time\nimport json\nimport argparse\nimport beepy\n\nFORMAT = pyaudio.paInt16\nCHANNELS = 1\nRATE = 16000\nCHUNK = 8000\n\naudio_queue = asyncio.Queue()\n\ndef callback(input_data, frame_count, time_info, status_flag):\n    audio_queue.put_nowait(input_data)\n    return (input_data, pyaudio.paContinue)\n\nasync def run(key):\n    extra_headers={\n        'Authorization': 'Token {}'.format(key)\n    }\n    async with websockets.connect('wss://api.deepgram.com/v1/listen?endpointing=true&encoding=linear16&sample_rate=16000&channels=1&interim_results=false', extra_headers = extra_headers) as ws:\n        async def microphone():\n            audio = pyaudio.PyAudio()\n            stream = audio.open(\n                format = FORMAT,\n                channels = CHANNELS,\n                rate = RATE,\n                input = True,\n                frames_per_buffer = CHUNK,\n                stream_callback = callback\n            )\n\n            stream.start_stream()\n\n            while stream.is_active():\n                await asyncio.sleep(0.1)\n\n            stream.stop_stream()\n            stream.close()\n\n        async def sender(ws):\n            try:\n                while True:\n                    data = await audio_queue.get()\n                    await ws.send(data)\n            except Exception as e:\n                print('Error while sending: '.format(str(e)))\n                raise\n\n        async def receiver(ws):\n            transcript = ''\n            async for msg in ws:\n                msg = json.loads(msg)\n\n                if len(msg['channel']['alternatives'][0]['transcript']) > 0:\n                    if len(transcript):\n                        transcript += ' '\n                    transcript += msg['channel']['alternatives'][0]['transcript']\n                    print(transcript, end = '\\r')\n\n                    if msg['speech_final']:\n                        print(transcript)\n                        beepy.beep(sound=1)\n                        transcript = ''\n\n        await asyncio.wait([\n            asyncio.ensure_future(microphone()),\n            asyncio.ensure_future(sender(ws)),\n            asyncio.ensure_future(receiver(ws))\n        ])\n\ndef parse_args():\n    \"\"\" Parses the command-line arguments.\n    \"\"\"\n    parser = argparse.ArgumentParser(description='Submits data to the real-time streaming endpoint.')\n    parser.add_argument('-k', '--key', required=True, help='YOUR_DEEPGRAM_API_KEY (authorization)')\n    return parser.parse_args()\n\ndef main():\n    args = parse_args()\n\n    asyncio.get_event_loop().run_until_complete(run(args.key))\n\nif __name__ == '__main__':\n    sys.exit(main() or 0)\n```\n\nAs you may notice when using `endpointing.py`, Deepgram will detect that you have finished speaking as soon as possible, meaning that in a conversational flow, this logic can easily cut you off mid-sentence every time you make even a minor pause. Rather than responding immediately, many applications will want to wait for a few seconds after a speaker finishes talking. This can be especially effective in conversational AI, where users may be speaking for long durations and occasionally pause mid-thought—waiting a few seconds to respond may result in a more natural conversational flow. A combination of endpointing and interim results can be used to determine when a desired duration of silence has passed.\n\nHere’s a code example that uses your microphone and the Python package beepy to play a notification sound after the number of seconds defined in a configurable `SILENCE_INTERVAL` has passed. (The default is 2.0, but this can be specified when running the script.)\n\nTo run the code, install beepy using `pip install beepy`. Then save the following code as `silence_interval.py`. Turn your volume up so you’ll be able to hear the beep sound, and run the code:\n\n`python silence_interval.py  -k 'YOUR_DG_API_KEY' [-s SILENCE_INTERVAL_SECONDS_FLOAT]`\n\n```python\nimport pyaudio\nimport asyncio\nimport sys\nimport websockets\nimport json\nimport beepy\nimport shutil\nimport argparse\n\nFORMAT = pyaudio.paInt16\nCHANNELS = 1\nRATE = 16000\nCHUNK = 8000\n\nterminal_size = shutil.get_terminal_size()\n\naudio_queue = asyncio.Queue()\n\ndef callback(input_data, frame_count, time_info, status_flag):\n    audio_queue.put_nowait(input_data)\n    return (input_data, pyaudio.paContinue)\n\nasync def run(key, silence_interval):\n    async with websockets.connect(\n        'wss://api.deepgram.com/v1/listen?endpointing=true&interim_results=true&encoding=linear16&sample_rate=16000&channels=1',\n        extra_headers={\n            'Authorization': 'Token {}'.format(key)\n        }\n    ) as ws:\n        async def microphone():\n            audio = pyaudio.PyAudio()\n            stream = audio.open(\n                format = FORMAT,\n                channels = CHANNELS,\n                rate = RATE,\n                input = True,\n                frames_per_buffer = CHUNK,\n                stream_callback = callback\n            )\n\n            stream.start_stream()\n\n            while stream.is_active():\n                await asyncio.sleep(0.1)\n\n            stream.stop_stream()\n            stream.close()\n\n        async def sender(ws):\n            try:\n                while True:\n                    data = await audio_queue.get()\n                    await ws.send(data)\n            except Exception as e:\n                print('Error while sending: '.format(str(e)))\n                raise\n\n        async def receiver(ws):\n            transcript = ''\n            last_word_end = 0.0\n            should_beep = False\n\n            async for message in ws:\n                message = json.loads(message)\n\n                transcript_cursor = message['start'] + message['duration']\n\n                # if there are any words in the message\n                if len(message['channel']['alternatives'][0]['words']) > 0:\n                    # handle transcript printing for final messages\n                    if message['is_final']:\n                        if len(transcript):\n                            transcript += ' '\n                        transcript += message['channel']['alternatives'][0]['transcript']\n                        print(transcript)\n                        # overwrite the line regardless of length\n                        # https://stackoverflow.com/a/47170056\n                        print('\\033[{}A'.format(len(transcript) // int(terminal_size.columns) + 1), end='')\n\n                    # if the last word in a previous message is silence_interval seconds\n                    # older than the first word in this message (and if that last word hasn't already triggered a beep)\n                    current_word_begin = message['channel']['alternatives'][0]['words'][0]['start']\n                    if current_word_begin - last_word_end >= silence_interval and last_word_end != 0.0:\n                        should_beep = True\n\n                    last_word_end = message['channel']['alternatives'][0]['words'][-1]['end']\n                else:\n                    # if there were no words in this message, check if the the last word\n                    # in a previous message is silence_interval or more seconds older\n                    # than the timestamp at the end of this message (if that last word hasn't already triggered a beep)\n                    if transcript_cursor - last_word_end >= silence_interval and last_word_end != 0.0:\n                        last_word_end = 0.0\n                        should_beep = True\n\n                if should_beep:\n                    beepy.beep(sound=1)\n                    should_beep = False\n                    # we set/mark last_word_end to 0.0 to indicate that this last word has already triggered a beep\n                    last_word_end = 0.0\n                    transcript = ''\n                    print('')\n\n        await asyncio.wait([\n            asyncio.ensure_future(microphone()),\n            asyncio.ensure_future(sender(ws)),\n            asyncio.ensure_future(receiver(ws))\n        ])\n\ndef parse_args():\n    \"\"\" Parses the command-line arguments.\n    \"\"\"\n    parser = argparse.ArgumentParser(description='Submits data to the real-time streaming endpoint.')\n    parser.add_argument('-k', '--key', required=True, help='YOUR_DEEPGRAM_API_KEY (authorization)')\n    parser.add_argument('-s', '--silence', required=False, help='A float representing the number of seconds of silence to wait before playing a beep. Defaults to 2.0.', default=2.0)\n    return parser.parse_args()\n\ndef main():\n    args = parse_args()\n\n    loop = asyncio.get_event_loop()\n    asyncio.get_event_loop().run_until_complete(run(args.key, float(args.silence)))\n\nif __name__ == '__main__':\n    sys.exit(main() or 0)\n```\n\nThese two examples should give you an idea of how different conversational flow mechanisms feel, and how they can be incorporated into different types of real-time speech-to-text applications. Both can be found in this [GitHub repo](https://github.com/deepgram/conversational-ai-flow).\n\nWe hope these examples help as you decide how to best utilize Deepgram's functionality. Happy building!\n\n";
						}
						async function compiledContent$3L() {
							return load$3L().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3L() {
							return (await import('./chunks/index.b479022c.mjs'));
						}
						function Content$3L(...args) {
							return load$3L().then((m) => m.default(...args));
						}
						Content$3L.isAstroComponentFactory = true;
						function getHeadings$3L() {
							return load$3L().then((m) => m.metadata.headings);
						}
						function getHeaders$3L() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3L().then((m) => m.metadata.headings);
						}

const __vite_glob_0_37 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3L,
  file: file$3L,
  url: url$3L,
  rawContent: rawContent$3L,
  compiledContent: compiledContent$3L,
  default: load$3L,
  Content: Content$3L,
  getHeadings: getHeadings$3L,
  getHeaders: getHeaders$3L
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3K = {"title":"Building the Future of Voice - Scott Stephenson, CEO, Deepgram - Project Voice X","description":"Building the future of Voice presented by Scott Stephenson, CEO of Deepgram, presented on day one of Project Voice X. ","date":"2021-12-09T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981393/blog/building-the-future-of-voice-scott-stephenson-ceo-deepgram-project-voice-x/proj-voice-x-session-scott-stephenson-blog-thumb-5.png","authors":["claudia-ring"],"category":"speech-trends","tags":["project-voice-x"],"seo":{"title":"Building the Future of Voice - Scott Stephenson, CEO, Deepgram - Project Voice X","description":"Building the future of Voice presented by Scott Stephenson, CEO of Deepgram, presented on day one of Project Voice X. "},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981393/blog/building-the-future-of-voice-scott-stephenson-ceo-deepgram-project-voice-x/proj-voice-x-session-scott-stephenson-blog-thumb-5.png"},"shorturls":{"share":"https://dpgr.am/124b7f6","twitter":"https://dpgr.am/90b1aa4","linkedin":"https://dpgr.am/350b8f6","reddit":"https://dpgr.am/b61b7c2","facebook":"https://dpgr.am/fcbf36f"}};
						const file$3K = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/building-the-future-of-voice-scott-stephenson-ceo-deepgram-project-voice-x/index.md";
						const url$3K = undefined;
						function rawContent$3K() {
							return "*This is the transcript for the opening keynote presented by Scott Stephenson, CEO of Deepgram, presented on day one of Project Voice X.* \n\n*The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was **94% accurate.**  Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram’s API.  If you want to see if Deepgram is right for your use case, [contact us](https://deepgram.com/contact-us/).*\n\n\\[Scott Stephenson:] Hey, everybody. Thank you for for listening and coming to the event. I know it’s kind of interesting. Yeah. This… for us, at least, at Deepgram, this is a first in-person event back. Maybe for most people, that that might be the case. But, yeah, I’m happy to be here, happy to meet all the new people. And I just wanna thank all the other speakers before me as well, a lot of great discussion today. And I’m also going to talk about the future of voice, not the distant, distant future. I’ll talk about the next couple years. But I’ll give you a little bit of background on myself, though, before we do that.\n\nSo I’m Scott Stephenson. I’m a CEO and cofounder of Deepgram. I’m also… I’m a technical CEO. So if people out there wanna talk, you know, shop with me, that’s totally fine. I I was a particle physicist before, so I I built deep underground dark matter detectors and and that kind of thing. So if you’re interested in that, happy to talk about that as well. But but, yeah, Deepgram is a company that builds APIs. You just you just saw some of the performance of it, but we build speech APIs in order to enable the next generation of voice products to be built. And that can be in real time. It can be in batch mode, but we want to enable developers to build that piece. So think of Deepgram, like, you… in the same category that you would Stripe or Twilio or something like that in order to enable developers to build voice products.\n\nI’m gonna I’m gonna talk about something in particular, and then we’ll maybe go a little bit broader after that. But I’ll I’ll talk about something that’s happening right now, which is we’re starting to see the adoption of automation and AI in the call center space and in other areas as well. But, in particular, you see it, like, really ramping up in meetings and call center. But one thing I see is this this this this trend of the old way, which was if you wanted to figure out if you had happy customers or how well things are going or that type of thing, what you would do is ask them. You you would send out a survey. You’d try to corner them. You know, you try to do these very unscalable things or things that maybe had questionable statistics behind them on on how how well they worked. But you basically didn’t have an alternative otherwise.\n\nAnd so I’m here to talk about that a little bit about what’s actually happening now is, yes, some of that old stuff is still happening, but you already hear me saying old stuff. Because what’s actually happening now is people are saying, hey. We already have piles of data. There’s phone calls happening. There’s voice bot interactions. There’s emails. There’s other things that are coming in here.\n\nLooks like we lost our connection. I’m not sure why. K. Sorry about that. We’ll see if I can… I don’t think I hit anything, but let’s see.\n\nOk. So so it can be text interactions. It can be voice interactions. There’s there’s a a number of ways that customers will interact with your product. Could just be clicks on your website, that type of thing. But if if you have to ask for the survey, if you have to, you know, send a text afterward, that type of thing, then you’re gonna get a very biased response. People that hate you or love you. What about the in between? What about what they were thinking in the moment? You know, that type of thing. And so I’m I’m\n\n> I’m saying, hey. That was the old way. The new way is if if you want the act… if you want to figure out what’s going on, doing product development with your customers, see how all that goes, stop asking them and just start listening to them.\n\nSo what do I mean by that? I mean, just take the data as it sits and be able to analyze it at scale to figure out what’s going on. And so this could be a retail space, where you have microphones and people are talking about certain clothes that they want to try on. Hey. I hate this. That looks great, etcetera, that type of thing. Just listen to them in situ. If it’s a call center, somebody calling in saying, I hate this. I like this, etcetera, like, right in right in situ in in in the data, looking inside.\n\nAnd there’s… essentially, for data scientists, like myself and lots of other people, they look at this huge data lake, dark dataset that previously just was, like, untouched. And maybe if you’re in the call center space, you know that there would be, like, a QA sampling kind of thing that would happen. Maybe one percent of calls would be listened to by a team of, like, ten people, where each person would listen to, like, ten calls a day or something like that. And they would try to randomly sample and and see what they could get out of it, but it’s a really, really small percentage. And, you know, it had marginal success, had some success but marginal.\n\nAnd so the next step in obvious evolution there is if you can automate this process and make it actually good and and and, again, see previous demo, then you can learn a lot about it, about what they’re actually saying for real. And so now the question would be, though. Alright. That’s that’s great. You told us there was something previously that was really hard to do, and now it’s really easy to do or it seems like it’s easy to do. How does that all work? It seems hard. Well, it is hard. Audio is a really hard problem, and I’m just going to describe that a little bit to you.\n\nSo take five seconds and look at this image, and I won’t say anything. Ok. Does anybody remember what happened in that image? There’s there’s a dog, has a blue Frisbee. There’s a girl probably fifty feet in the background with a… with, like, a bodyboard. There’s another bodyboard. And I don’t know why I keep going out here. But from the from the image perspective, you can glean a lot of information from a very small amount of time. And this makes labeling images really easy and inexpensive in a lot of ways. It… trust me. It’s still expensive. But compared to audio and and many other sources, it’s it’s, like, a lot easier. But you can get this whole story just from that quick five seconds. But now what if what if you were asked to look at this hour-long lecture for five seconds and then tell me, like, what happened. Right? You know, you get five seconds. Like, I’m not gonna play, you know, biochemistry lecture for you. But but but, nevertheless, five seconds with that, you’ll get five words, you know, ten, fifteen words, like, whatever. Right? You’re… there isn’t much you can do. Right? And so, hopefully, that illustrates a little bit about audio that’s happening. It’s real time. It’s very hard for a person to parallel process.\n\nYou can you can recognize that when you’re in a cocktail-party problem, which we’ll all have later today when we’re all sitting around trying to talk to each other, many conversations going on, etcetera. You can only really track one, maybe one and a half. You know? That kinda thing. But but part part of the reason… or… you know, part of the reason for that is humans are actually pretty good in real time in conversation. They can understand what what other humans are doing, but we have specialized hardware for it. You know? We have our brain that we… and ears and everything that is tuned just for listening to other humans talk and computers don’t. They they see the image like what is on the right here. And it’s… that’s that’s not the whole, like, sixty hour lecture. That’s, like, a couple seconds of it. You know? So it’s a whole mess. Right? And if you were to if you were to try to represent that entire lecture that way, you, as a human, would have no idea what’s going on. You might be able to say, a person was talking in this area or not. That that might be a question you can answer, but but, otherwise, you have no idea what they’re saying.\n\nBut if you run it through our ears and through our brain in real time, then we can understand it pretty well. But now how do you get a machine to actually do that? And that’s the real trick. Right? And if you have a machine that can do it, then it’s probably very valuable because now you can understand humans at scale. And that’s the type of thing that we build. I’ll just point out a few of these problems again, like, in a in a call center space. Maybe you have specific product names or company names or industry terms and jargon or certain acronyms that you’re using if you’re a bank or whatever it is. There’s also how it said, so maybe you’re speaking in a specific language. English, for instance. You could have a certain accent, dialect, etcetera. You could be upset or not. There could be background noise. You could be using a phone or some other device to actually talk to people, like calling in on a meeting or something. And, generally, when you’re having these conversations, they’re in a conversational style.\n\nIt’s a fast type of speaking, like I’m doing right now. Sorry about that. But but, nevertheless, it’s very conversational. You’re not trying to purposefully slow down and enunciate so that the machine can understand you. You know? And so that’s a challenge you have to deal with. And then audio… actually, a lot of people might think of audio as not that big of, like, a file, but it’s about the third the size of video. So there’s a lot of information packed in audio in order to make it sound good enough for you to understand. And there’s different encoding and different compression techniques, and, you know, there could be several channels, like one person talking on one… like like an agent and then, like, the customer that’s calling in, so multiple channels.\n\nAnd so, anyway, audio is very varied and very difficult. But the the way that you have to handle this, and I I… I’ll talk technically just for a second here. But as you need a lot of data that is labeled, but it’s not just a lot of it. Like, the hour count matters, but it doesn’t matter as much as you might think. But it definitely matters. You need a lot in different areas, so young, old, male, female, low noise, high noise, in the car, not, upset, not upset, you know, very happy, etcetera. And you need this in multiple languages, multiple accents, etcetera, and you all… you need that all labeled as, like, a a a data plus truth pair, essentially. So here’s the audio and here’s the transcript that goes with that audio, and then the machine can learn by example. And you can kind of see that if you can read it here.\n\nAudio comes in labeled, and that’s converted into training data. And then that makes a trained model, which you use, like, convolutional neural networks, dense dense layers or recurrent neural networks or attention layers or that type of thing is just kind of… I think of it like a like a periodic table of chemical elements, but, instead, it’s just for deep learning. You have these different things that care about spatial or temporal or that type of thing. But, nevertheless, mix those up, put them in the right order, expose them to a bunch of training data. And then you have a general model and… which is if many, if people in here have used APIs before or, you know, you’ve used your, like, keyboard on your phone, then, generally, what you’re using is is a general model, which is a a model that’s a one size fits all or jack-of-all-trades. Not necessarily a master of one, but it… it’s it’s designed to hopefully do pretty ok with the general population.\n\nBut if you have a specific task, like you’re our customer, like Citibank, and you’re like, hey. I have bankers, and they’re talking about bank stuff. And we wanna make sure that they’re compliant in the things that they’re doing, then you can go through this bottom loop here that says taking new data and label that to do transfer learning on it. So you have a… your original model that is already really good at English or other languages, and then now you expose it to a specific domain and then serve that in order to accomplish to your task.\n\nSo, anyway, if you wanna talk about the technical side of it later, I’m happy to happy to discuss, but that’s how it all works under the hood. But now you can get wet… you can get rid of the old way, or keep doing it if if you find value in it, but add the new way, which is doing a hundred percent analysis across all your data. Do it in a reliable way. Do it in a scalable way and with high accuracy. And so I think you you guys… Cyrano, one of our customers, you just saw the demo. We’re we’re pumped to have them, and we can discuss a little bit later. Again, we have a booth back there. But… about the use case there, so I I won’t take too much time talking about them, but another company that maybe some have heard heard of in the audience is Valyant.\n\nAnd Valyant does fast-food ordering. So… well, it doesn’t have to be fast food. It could be many other things, but, typically, it’s fast food. So think of driving through and you’re in your car or you’re on your mobile phone or you’re at a kiosk, and you’re trying to order food inside a mall or something like that. And you have a very specific menu items, like Baconator or something like that. And you’d like your model to be very good at those things in this very noisy environment. And how do you do that? Well, you just saw magic. You have a general model, and then you transfer learn that model into their domain, and then it works really well. And another place that this works really well is in space.\n\nSo NASA is one of our customers. They they do space-to-ground communication. So we worked with the CTO and their team at the Johnson Space Center in Houston in Houston. And what they’re trying to do is under… essentially, there’s always a link between the international space station and ground. And there’s people listening all the time, like actual humans. Three of them sitting down and transcribing everything that’s happening. And, hey, can we, like, reduce that number to, like, two people or one person or zero people or something like that. And this is the problem that they’re working on, and it’s it’s a… it’s a big challenge, and their their CTO says, hey. The problem’s right in our faces. We have we have all of seventy five hundred pieces of jargon and acronyms, and it’s a scratchy, you know, radio signal, and it’s it’s it’s not a pleasant thing to try to transcribe. But, hey, if there were a machine that could listen to this and do a good job of it, then that would be very valuable to them. And so, anyway, they they tried a lot of vendors.\n\nAnd as you can maybe imagine using a general model approach, which is what pretty much everybody else does, then it doesn’t work that well. But if you use this transfer-learning approach, then it works really well handling background noise, jargon, multi speaker, etcetera. And, yeah, happy to talk about these use cases later if you want to. But… so, yeah, moving on to act three here a little bit, which is just a comment on what’s happening in the world. We’re in the middle of the intelligence revolution, like it or not. There was the agricultural revolution, industrial revolution, information revolution. We’re in the intelligence revolution right now. Automation is going to happen. Thirty years from now we’re gonna look back and be like, oh, yeah. Those were the beginning days. You know? Just like if anybody in here was around when the Internet was first starting to form. You remember, you know, when you got your first email account, and you’re like, holy shit. This is crazy. You know, you can talk to anybody anywhere.\n\nAnd, anyway, same… we’re we’re in that we’re in that point in time right now. And another comment is that real time is finally real. A couple of years ago, it wasn’t, and now it actually is. That’s partially because the models are getting good enough to do things in real time, but it’s also partially because the computational techniques have become sophisticated enough, but not as heavy lifting plus other… like, NVIDIA has gotten good essentially at processing larger model sizes and that type of thing. So, essentially, confluence of hardware plus software plus new techniques being implemented in software can make real time… high-accuracy real time actually real. And then these techniques, if you’re if you’re in a general domain, you can get, like, ninety percent plus accuracy with just a general model.\n\nOr if you if you need that kind of accuracy, but it’s kind of a niche domain, then you can go after a custom-trained model. And this stuff all works now, basically. So a lot a lot of times come… people come to me and say, like, hey. Is it possible too? And the the… in audio, the answer is almost like it… it’s almost always yes, unless it’s something that a human really can’t do. Like, can you build a lie detector? Like, well, I mean, kind of. Yes. But as as well as maybe you could as a human. You know? May… I mean, not quite that well, but close. Right? But think about all the other things that a human can do. Like, they can jump into a conversation, tell you how many people are speaking when they’re speaking, what words they’re talking about, what topic they’re talking about, that type of thing.\n\nThese things are all possible, but it’s still kind of, like, the railroad was built across the United States, but there’s a whole bunch of other work that has to be done and so all that stuff is happening now. So, yeah, the the people who win the west, you know, over the next couple decades will be the ones that empower the developers to build this architecture to… you know, the the railroad builders and the towns that sprout up along those railroad lines. But it’s going to be in compliance spaces, voice bots, call centers, the meetings, podcast. It’s gonna be all over the place. I mean, for voice, it’s just like a… it’s the natural human commit.\n\nI’m up here talking right now. Right? Like, this is the natural communication mechanism, and it just previously was enabled by connectivity. So you could call somebody. You could do that you could do that type of thing, but there was never any automation or intelligence behind it, and now that’s all changing. And, you know, our lives are gonna change massively because of it. And I think the productivity of the world is gonna go up significantly, which previous, you know, talks that we heard today we’re all hinting at that. You know, our lives are gonna change a lot. So I’d… I I don’t know if there’s… if I if… maybe people are tired of hands. I don’t know. But are there start-up founders in the room here? Like, any… ok. Cool. So if you’re if if you’re into voice and you’re a start-up founder, you might wanna know about this.\n\nSo Deepgram has a start-up program that we gave away ten million dollars in speech-recognition credit. And what you can do is apply. Get up to a hundred thousand dollars for your start-up. And the reason we do this is to help push forward the the new the new products being built. I I don’t I don’t know if people are quite aware of what happened with AWS, GCP, Azure, etcetera over the last ten, fifteen years. But they gave away a lot of credit, and a lot of tech companies benefited as they grew their company on that initial credit. And, of course, you know, for Deepgram, the goal here is if you grow a whole bunch and become awesome then, you know, great. You’re you’re a customer of Deepgram, and we’re happy to help you. And another one.\n\nSo tomorrow evening, we have… how far away is it? A mile, but we have buses. We have party buses. Ok. So tomorrow evening, we’re going to… we… we’re gonna have a private screening of Dune. So if if you haven’t seen Dune yet, now’s an opportunity to do it at a place called Suds n n Cinema Suds n Cinema, which is a pretty dope theater, and we could all watch Dune together if you wanna come hang out and do that. We also have a booth. And we have a special… a a new a new member of our Deepgram team. We call him our VP of Intrigue. If you want to come meet him, come talk to us at the booth. I won’t say anymore. But thank you, everybody, for listening to the talk, and I I appreciate everybody coming out to hear it.\n\n\n\n ![](https://res.cloudinary.com/deepgram/image/upload/v1661976850/blog/building-the-future-of-voice-scott-stephenson-ceo-deepgram-project-voice-x/scott-stephenson-script-analysis-chart%402x-1-300x16.png)\n\n\n\n*To learn more about real-time voice analysis, check out [Cyrano.ai](https://www.cyrano.ai/).*";
						}
						async function compiledContent$3K() {
							return load$3K().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3K() {
							return (await import('./chunks/index.719ef3f0.mjs'));
						}
						function Content$3K(...args) {
							return load$3K().then((m) => m.default(...args));
						}
						Content$3K.isAstroComponentFactory = true;
						function getHeadings$3K() {
							return load$3K().then((m) => m.metadata.headings);
						}
						function getHeaders$3K() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3K().then((m) => m.metadata.headings);
						}

const __vite_glob_0_38 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3K,
  file: file$3K,
  url: url$3K,
  rawContent: rawContent$3K,
  compiledContent: compiledContent$3K,
  default: load$3K,
  Content: Content$3K,
  getHeadings: getHeadings$3K,
  getHeaders: getHeaders$3K
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3J = {"title":"Celebrating Black History Month with a Vision of More Inclusive Speech Recognition","description":"To mark Black History Month, we’re shining a spotlight on a major dialect of American English, African American Vernacular English (AAVE), and its status in speech recognition.","date":"2021-02-26T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981362/blog/celebrating-black-history-month-with-a-vision-of-more-inclusive-speech-recognition/celebrating-black-history-2021%402x.jpg","authors":["sam-zegas"],"category":"identity-and-language","tags":["education","inclusion","language"],"seo":{"title":"Celebrating Black History Month with a Vision of More Inclusive Speech Recognition","description":"To mark Black History Month, we’re shining a spotlight on a major dialect of American English, African American Vernacular English (AAVE), and its status in speech recognition."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981362/blog/celebrating-black-history-month-with-a-vision-of-more-inclusive-speech-recognition/celebrating-black-history-2021%402x.jpg"},"shorturls":{"share":"https://dpgr.am/c8fb788","twitter":"https://dpgr.am/fb73689","linkedin":"https://dpgr.am/7d8c869","reddit":"https://dpgr.am/e96216d","facebook":"https://dpgr.am/5d70ef9"}};
						const file$3J = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/celebrating-black-history-month-with-a-vision-of-more-inclusive-speech-recognition/index.md";
						const url$3J = undefined;
						function rawContent$3J() {
							return "February is Black History Month! To mark this time of celebration and reflection, we'd like to shine a spotlight on one of the major dialects of American English, African American Vernacular English (AAVE), and share our perspective on its status in [speech recognition](https://blog.deepgram.com/what-is-asr/).\n\n## What Exactly is AAVE?\n\nHistorically also known as Ebonics, [AAVE](https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199795390.001.0001/oxfordhb-9780199795390-e-5?rskey=Y8e3Sn&result=3#:~:text=It%20is%20now%20widely%20accepted,the%2017th%20and%2018th%20centuries.) is one of several dozen prominent varieties of American English, along with regional varieties like Midland American English and Southern American English, and ethno-cultural varieties like the dialects of Latino American English. AAVE's origins reach back to the earliest communities of Black people in North America, enslaved and free, who drew influences from African languages and mixed languages called *creoles* while building language communities oriented by necessity around English. AAVE developed in Black communities through the centuries - through slavery and emancipation, reconstruction and Jim Crow, the Great Migration and Harlem Renaissance, the Civil Rights Era, and into the present day.\n\nYet throughout the course of its development, AAVE has never existed in isolation. It has been mutually influenced by so-called [Standard American English (SAE)](https://www.thoughtco.com/standard-american-english-1692134), a term which describes a \"neutral\" or \"newscaster\" dialect of American English, as well as other regional and community-based dialects. Today's AAVE is far from homogenous. In fact, quite the opposite. AAVE is the term that linguists and language researchers use to talk about a series of accents and dialects spoken predominantly by African Americans in the United States. The grouping is highly diverse, with many regional and community-based variations.\n\nThat said, not every Black person in the US speaks in a way that linguists would describe as AAVE. Rather, AAVE refers to a specific combination of semantic, syntactic, and phonological features that are widely used in many but not all Black communities. Language scientists recognize AAVE as a fully-formed linguistic system that is distinct from SAE in its grammar and vocabulary. Speakers of AAVE are consistently able to identify whether a phrase is correct or incorrect AAVE. This ability of the speech community to identify \"well formedness\" is a crucial linguistic litmus test that speaks to the stability and consistency of AAVE as a dialect. Among other things, AAVE grammar is linguistically noteworthy for its [distinctive uses](https://ygdp.yale.edu/phenomena/invariant-be) of [verbal aspect](https://ygdp.yale.edu/phenomena/perfective-done) and its tendency for [copula deletion](https://ygdp.yale.edu/phenomena/null-copula). We must also confront a sad history of AAVE being dismissed as \"slang,\" \"uneducated,\" or otherwise deficient. Such characterizations were frequently used to promote racist perspectives and are inconsistent with our scientific understanding of AAVE as simply one dialect among many varieties of American English. AAVE in all its complexity is the subject of ongoing research by academics in linguistics, cultural studies, and other fields.\n\n## Code-Switching Poses Challenges for ASR\n\nMore importantly though, to its speakers, AAVE is a medium for daily life. It's a way of talking with friends and family as well as a [medium for music](https://www.xxlmag.com/great-albums-from-rappers-with-poetic-flows/), poetry, and other art forms. AAVE's use by Black music artists in particular has brought it to a truly global stage and vaulted it into the American cultural mainstream. Many AAVE speakers are equally comfortable expressing themselves in AAVE and SAE, and may switch between the two from conversation to conversation or phrase to phrase, depending on the social context in which they are speaking.\n\nThe phenomenon of speakers switching between dialects is called *code-switching* and is an interesting challenge for speech recognition because it requires the system to properly identify and process each dialect as seamlessly as speakers talk. Code-switching takes place not only between dialects but also between languages. For example, Deepgram has done extensive work to improve ASR in situations where speakers code-switch between Spanish and English in the course of a conversation, as is common for many bilingual speakers.\n\n## ASR Needs to Evolve Beyond SAE\n\nTo date, speech recognition technology has focused on Standard American English. SAE occupies a privileged status that linguists refer to as a \"prestige dialect\"-that is, a dialect that members of a language community perceive, on the whole, as the most prestigious. SAE functions as the primary dialect of business, government, media, and formal education in the United States. These circumstances have led speech recognition companies to focus on serving the needs of SAE speech communities first. As a result, today's speech recognition systems return a higher error rate when transcribing AAVE and other dialects as compared to SAE, and are poorly equipped to handle situations where speakers code-switch between AAVE and SAE.\n\nThe consequence is that speakers of AAVE and other dialects may need to code-switch into SAE or \"sound white\" to be better understood by speech recognition systems. This must change. First and foremost, we see it as ethically important to make speech recognition work for an ever-expanding circle of dialect speakers, including AAVE speakers. Furthermore, we believe that no company in the speech recognition space will be successful with an exclusive focus on prestige dialects like SAE. Makers of speech technologies must set their sights on learning to process the full, rich spectrum of human speech. Deepgram is committed to improving accuracy for AAVE. We hope other speech companies will join us in that effort. For more information on AAVE, check out some of our favorite voices on the subject:\n\n* [African American Vernacular English ' Morgan Gill ' TEDxYouth@RMSST](https://www.youtube.com/watch?v=rNjhB1DW_-s)\n* [Linguists on African American Language: John Baugh](https://www.youtube.com/watch?v=6EQdJy24JrQ)\n* [\"African American English through the Years\" - A Presentation by Dr. Lisa Green of UMass Amherst](https://www.youtube.com/watch?v=x6UpGwH6YBs)";
						}
						async function compiledContent$3J() {
							return load$3J().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3J() {
							return (await import('./chunks/index.cfaacbeb.mjs'));
						}
						function Content$3J(...args) {
							return load$3J().then((m) => m.default(...args));
						}
						Content$3J.isAstroComponentFactory = true;
						function getHeadings$3J() {
							return load$3J().then((m) => m.metadata.headings);
						}
						function getHeaders$3J() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3J().then((m) => m.metadata.headings);
						}

const __vite_glob_0_39 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3J,
  file: file$3J,
  url: url$3J,
  rawContent: rawContent$3J,
  compiledContent: compiledContent$3J,
  default: load$3J,
  Content: Content$3J,
  getHeadings: getHeadings$3J,
  getHeaders: getHeaders$3J
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3I = {"title":"Celebrating Jewish American Heritage Month","description":"Learn about the history of the Jewish people & how their culture and languages have influenced the US—and maybe pick up some Yiddish, too.","date":"2022-05-19T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981422/blog/celebrating-jewish-american-history-month/celebrating-jewish-american-history-month-thumb-55.png","authors":["sam-zegas"],"category":"identity-and-language","tags":["heritage","language"],"seo":{"title":"Celebrating Jewish American Heritage Month","description":"Learn about the history of the Jewish people & how their culture and languages have influenced the US—and maybe pick up some Yiddish, too."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981422/blog/celebrating-jewish-american-history-month/celebrating-jewish-american-history-month-thumb-55.png"},"shorturls":{"share":"https://dpgr.am/0cc7f48","twitter":"https://dpgr.am/122f347","linkedin":"https://dpgr.am/644576f","reddit":"https://dpgr.am/6b6dc63","facebook":"https://dpgr.am/84ac991"}};
						const file$3I = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/celebrating-jewish-american-history-month/index.md";
						const url$3I = undefined;
						function rawContent$3I() {
							return "May is Jewish American History Month, and *oy vey*, there is so much to write about! At Deepgram, we always have speech and language on our minds-and this month, we're excited to write about Jewish American language. But to start, who *are* the Jewish people anyway, and how did their communities develop into the form we know today?\n\n## Who are the Jewish People?\n\nThe Jews have a very long sense of their own identity; in fact, it is currently the year 5782 in the [Jewish calendar](https://en.wikipedia.org/wiki/Hebrew_calendar). The historical homeland of the Jewish people was in the region of today's Israel and Palestine (broadly referred to as the Levant in many historical sources), where the city of Jerusalem was the holy center of the Jewish world. Even in the most ancient history, though, Jewish communities also existed in neighboring areas, including in the Ancient Greek world, Egypt and North Africa, and Babylon. Many stories from these early minority Jewish communities that existed outside of the Levant appear in the Old Testament.\n\nThe origins of most of the Jewish groups that we see around the world today date back to political events that occurred two thousand years ago in the Levant. Major upheavals came between the year 0 and the year 70 CE, when armed fighting between the Roman Empire and the Jewish population of Jerusalem led to the siege and sacking of Jerusalem and the [destruction of the Second Temple](https://rpl.hds.harvard.edu/faq/destruction-second-temple-70-ce#:~:text=The%20Jews%20led%20a%20revolt,the%20wall%20to%20later%20periods)). This began a large-scale migration of the Jewish people in every direction that continued for many centuries, through to the [Muslim conquest of the Levant](https://en.wikipedia.org/wiki/Muslim_conquest_of_the_Levant) in 638 CE. After that time, although a sizable population of Jewish people remained in the Levant, the population in other regions began to overshadow that of the original Jewish homeland.\n\nThe result of this migration is that people who identify as Jews come from many far-flung parts of the world, where their communities have existed for thousands of years. This scattering of Jewish communities is known as the *Jewish diaspora*. Most if not all diaspora communities lived in places where they were surrounded by much larger populations of other ethnic and linguistic groups. This pattern led to cultural dialog and intermixing, and Jewish communities often adopted languages and cultural traditions that resembled those of the surrounding society while still preserving their unique Jewish identity.\n\n## The Jewish Diaspora\n\nOver the centuries, several broad ethnic groups emerged. The Jewish people who migrated to Spain and Portugal in ancient times became the [Sephardic Jews](https://en.wikipedia.org/wiki/Sephardi_Jews) (from Hebrew *sepharad*, meaning *Spain*). The Sephardic community developed a Romance language called Ladino, which is a close relative of Spanish that preserves many words from Hebrew. During the [Spanish Inquisition](https://en.wikipedia.org/wiki/Spanish_Inquisition) in the late 1400s, Sephardic Jews were forced to convert to Christianity or leave Spain. Many migrants who fled the inquisition moved to the eastern Mediterranean, which led to large communities of Ladino speakers settling in the Ottoman Empire, and especially in the cities of Thessaloniki and Izmir in modern Greece and Turkey.\n\nOther Jews migrated in ancient times to Northern Europe, particularly to France and Germany. Over the centuries, these people developed into the [Ashkenazi Jews](https://en.wikipedia.org/wiki/Ashkenazi_Jews) (from Hebrew *ashkenaz*, meaning *Germany*). These communities developed a Germanic language called Yiddish, which is related to German and also has strong influences from Slavic languages and Hebrew. Over time, the Ashkenazi population shifted eastward to Central and Eastern Europe, where the [policies of the Polish-Lithuanian Commonwealth](https://en.wikipedia.org/wiki/History_of_the_Jews_in_Poland_before_the_18th_century) were friendly toward Jewish settlers. As a result, many modern Ashkenazi Jews trace their ancestry back to Poland, Lithuania, Belarus, Ukraine, and Russia.\n\nOther Jews stayed in the Middle East and became [Mizrahi Jews](https://en.wikipedia.org/wiki/Mizrahi_Jews) (from Hebrew *mizrach*, meaning *east).* Mizrahi communities developed several languages depending on their location, including Aramaic, Judeo-Arabic, and Judeo-Persian. Some groups that started in the Mizrahi region migrated farther and settled in more isolated areas: the [Bukhara community](https://en.wikipedia.org/wiki/Bukharan_Jews) in Uzbekistan, the [Kaifeng community](https://en.wikipedia.org/wiki/Kaifeng_Jews) in China, the [Beta Israel community](https://en.wikipedia.org/wiki/Beta_Israel) in Ethiopia, and various [communities in India](https://en.wikipedia.org/wiki/History_of_the_Jews_in_India), to name just a few. These groups developed Jewish languages that were related to those spoken in the societies where they settled.\n\n## Jewish Migration to the US\n\nJewish migration to the US [accelerated in the late 1800s](https://saveellisisland.org/about-us/blog/item/88-jewish-immigration-to-america.html) as Ashkenazi people in particular left Europe in search of a better life in America. Ashkenazis established large communities throughout the Northeast, and in New York City in particular. The next major upheaval came during the Second World War. The spread of Nazism through Europe targeted Jewish people, first for oppression and then for elimination. 6 million Jewish people were murdered at the hands of the Nazis-about [35% of the world's Jewish population](https://www.washingtonpost.com/news/worldviews/wp/2015/07/02/has-the-global-jewish-population-finally-rebounded-from-the-holocaust-not-exactly/) at the time-alongside millions of people from [other minority groups](https://hmh.org/library/research/minority-victims-guide/) that the Nazis marked for extermination. These groups included disabled people, the Roma, LGBT people, political dissenters, and others.\n\nMillions of Jews who escaped the Holocaust became refugees. The majority of these migrants went to the United States and to Palestine, which was divided into [Israel and Palestine in 1948](https://history.state.gov/milestones/1945-1952/creation-israel#:~:text=On%20May%2014%2C%201948%2C%20David,nation%20on%20the%20same%20day.) by the British authorities who controlled the region at the time. During the foundation of Israel, it was decided that Hebrew would become the nation's official language. Before that time, Hebrew was a widely-spoken *second* language that many Jews learned to study holy texts, but almost no one spoke it as a *native* language. With over 5 million native speakers today, Hebrew stands as one of the most successful language revitalization projects in history.\n\nToday, there are an estimated [7.6 million people who identify as Jewish](https://jewishjournal.org/2021/04/22/new-brandeis-study-estimates-7-6-million-jews-living-in-u-s/) living in the US and up to 15 million people who have at least one Jewish grandparent, accounting for 2.4% and 4.5% of the US population, respectively. Many Jewish people live in urban areas, with the largest populations in New York City, Miami, Los Angeles, Philadelphia, Chicago, San Francisco, Boston, Baltimore, and Washington. [90-95%](https://en.wikipedia.org/wiki/American_Jews) of these people have Ashkenazi heritage.\n\n## Yiddish Words in English\n\nAshkenazis brought their Yiddish language with them when they settled in the US, and between [160,000 and 200,000](https://en.wikipedia.org/wiki/American_Jews#Language) people still speak Yiddish in the US today. The prevalence of Yiddish has influenced American English, as even non-Jewish Americans likely recognize a number of Yiddish words. We asked Deepgrammers with Jewish roots what their favorite Jewish words are in any language. The resulting list contains mostly Yiddish and Hebrew words, which reflects the fact that most American Jews-including those on the Deepgram team-have Ashkenazi heritage. Nonetheless, we celebrate other American-Jewish languages too.\n\n* **Bubbe & Zayde** - *Grandma* and *Grandpa*, both words of Slavic origin. Since many people's exposure to Yiddish came through their grandparents, these words have special meaning.\n* **Bupkis** - *Nothing*. This word is of Slavic origin and may have meant *goat droppings*, originally from a Slavic word for *bean \\[shaped object].* Often used as an interjection: \"Years of sweat and tears and what do I get in return, bupkis!\"\n* **Chutzpah** - *Nearly arrogant courage, audacity.* This word has a Hebrew origin, from an ancient word that meant *insolent.*\n* **Dreck** - *Dirt, crap.* From a Germanic word meaning *dung.* \"Dreck!\" is an exclamation of disgust or displeasure with something.\n* **Futz (around)** - *Waste time with frivolous things*, from a Germanic origin that translates to *fart.* \n* **Hak mir nicht kein tschainik** - *Don't bang my teapot.* An often-used expression by my own grandma in my childhood, which was used to tell me to be quiet and stop being difficult. The first four words are of Germanic origin (*don't bang my),* while the last word, *teapot,* is a loan from the Slavic languages. This expression shows how multilingual Yiddish speakers drew from several sources as they developed their language.\n* **Kibbitz** - *Chat*, *small talk, gossip*. Originally a Germanic name for a bird in imitation of its call, kibbitz can be used as a noun or a verb.\n* **Klutz** - *A clumsy, uncoordinated person*. Originally from a Germanic source meaning *wooden beam* and related to the word for *block, lump.*\n* **Kvetch** - *To* *whine, complain*. From a Germanic origin meaning *pinch,* possibly a reference to the facial expression of whining.\n* **L'Chaim** - *To life!* This is the most common phrase used as a toast in Ashkenazi culture. The phrase is drawn from Hebrew.\n* **Mazel tov** - *Congratulations*, literally *good luck*. A phrase drawn from Hebrew.\n* **Mentsh** - *A virtuous person*. This word is related to modern German *Mensch*, which means *person* with no connotation of virtuousness.\n* **Meshugge** - *Crazy, insane*, drawn from Hebrew. Also seen as *meshugass, meaning craziness, insanity*.\n* **Nosh** - *To snack*, from a Germanic word for *nibble.* \n* **Oy Vey! & Oy Gevalt!** - *Oh, God, enough already!* Both phrases come from Germanic words for *pain* and *violence*, respectively, as an expression of suffering.\n* **Shlemiel** - *A fool, someone who hurts people emotionally out of carelessness.* The origin is unknown.\n* **Shlep** - *To drag or lug something; to go on a tiresome errand.* Literally *to drag*, from a Germanic source. You might \"shlep your suitcase up the stairs\" or \"shlep all over town\" running errands.\n* **Shlimazel** - *A chronically unlucky person*. The first element *shlim* is from a Germanic source meaning *bad.* The second element *mazel* is from Hebrew meaning *luck*, as seen above in *mazel tov.*\n* **Shmutz** - *Dirt*, from a Germanic origin. Your Bubbe might say, \"You have shmutz on your face!\" \n* **Shpiel, Spiel**- *An opinionated, persuasive speech*. From a Germanic source meaning *performance.*\n* **Shvitz** - *Sweat*, from a Germanic origin. \"It's so hot, I'm shvitzing like crazy.\"\n* **Tchotchke** - *Trinket*, from a Slavic origin.\n* **Tuchis** - *Butt*, from a Hebrew origin. \"Don't fall on your tuchis!\"\n* **Verklempt** - *Overcome with emotion, usually in a positive way.* From a Germanic origin meaning *squeezed*, *inhibited*.\n* **Yenta** - *A matchmaker, usually also a busybody and gossiper*. Based on the name of a fictional character and popularized as a word in US English by *Fiddler on the Roof*.\n\n## Famous Jewish Linguists\n\nJewish American Heritage Month was first inaugurated in 2006, and its goal was to highlight the contributions that Jewish Americans have made to American culture and history. Given this post's focus on language issues, we'd be remiss to wrap up without mentioning some of Jewish linguists who've contributed to our understanding of how language works.\n\n* Perhaps the most famous is [Noam Chomsky](https://en.wikipedia.org/wiki/Noam_Chomsky), who defined a way of thinking about language that has influenced huge swathes of the field and led to a number of sub-theories and variations on his original schema.\n* [Steven Pinker](https://en.wikipedia.org/wiki/Steven_Pinker) is well known today as a popular science writer on various topics, but he started his career in psycholinguistics and cognitive psychology.\n* [Victoria Fromkin](https://en.wikipedia.org/wiki/Victoria_Fromkin) was an American linguist who studied slips of the tongue and language acquisition. She is perhaps best known for her work with [Genie](https://en.wikipedia.org/wiki/Genie_(feral_child)), a young girl raised in abusive conditions who did not acquire language as a normal child would.\n* [Edward Sapir](https://en.wikipedia.org/wiki/Edward_Sapir) is considered by many to be one of the founding scholars of American linguistics as we know it today, working on issues as diverse as [linguistic relativity](https://en.wikipedia.org/wiki/Linguistic_relativity) (does the language you speak influence how you think?) and documenting indigenous languages in North America.\n* [William Labov](https://en.wikipedia.org/wiki/William_Labov) essentially founded the field of sociolinguistics. He conducted seminal work on accents in New York, as well as [African American Vernacular English](https://en.wikipedia.org/wiki/African-American_Vernacular_English).\n* [Joseph Greenberg](https://en.wikipedia.org/wiki/Joseph_Greenberg) worked extensively on understanding how languages are related to each other as well as [linguistic typology](https://en.wikipedia.org/wiki/Linguistic_typology)—understanding how different features of language are instantiated in different languages.\n\n## Final Thoughts\n\nJewish Americans are a diverse bunch. Centuries of migration have brought Jewish people from many cultural traditions to the United States, where they have blended together into a new Jewish-American identity. This identity is held together less by ancestry or religion than it is by a shared history of learning, trial, tribulation, and joy. So to all the Jewish Americans and our friends out there, we offer a heartfelt *l'chaim!*";
						}
						async function compiledContent$3I() {
							return load$3I().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3I() {
							return (await import('./chunks/index.794093e4.mjs'));
						}
						function Content$3I(...args) {
							return load$3I().then((m) => m.default(...args));
						}
						Content$3I.isAstroComponentFactory = true;
						function getHeadings$3I() {
							return load$3I().then((m) => m.metadata.headings);
						}
						function getHeaders$3I() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3I().then((m) => m.metadata.headings);
						}

const __vite_glob_0_40 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3I,
  file: file$3I,
  url: url$3I,
  rawContent: rawContent$3I,
  compiledContent: compiledContent$3I,
  default: load$3I,
  Content: Content$3I,
  getHeadings: getHeadings$3I,
  getHeaders: getHeaders$3I
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3H = {"title":"Automatically Censor Profanity with Node.js","description":"Learn how to create censored audio files automatically.","date":"2021-11-04T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1635438532/blog/2021/11/censor-profanity-nodejs/automatically-censor-profanity-with-nodejs-blog%402x.png","authors":["kevin-lewis"],"category":"tutorial","tags":["nodejs"],"seo":{"title":"Automatically Censor Profanity with Node.js","description":"Learn how to create censored audio files automatically."},"shorturls":{"share":"https://dpgr.am/562fccf","twitter":"https://dpgr.am/f751da2","linkedin":"https://dpgr.am/b3df202","reddit":"https://dpgr.am/afc2f1d","facebook":"https://dpgr.am/d4e7bb9"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453797/blog/censor-profanity-nodejs/ograph.png"}};
						const file$3H = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/censor-profanity-nodejs/index.md";
						const url$3H = undefined;
						function rawContent$3H() {
							return "\nHere at Deepgram we run GRAM JAM - a series of internal hackathons to have Deepgrammers build cool projects using our own API. Sometimes the projects lead to product improvements, sometimes they get a laugh, and other times they are just super useful. This blog post is based on one of those projects - Bleepgram - built by the very interdisciplinary team of Faye Donnelley, Mike Stivaletti , Conner Goodrum, Claudia Ring, and Anthony Deschamps.\n\nSometimes we all let \"unprovoked or unintended utterances\" slip out of our mouth, and often it's the job of an editor to go through recordings and overlay a bleep so no one has to hear the original word. Historically this has been a manual process, but with Deepgram's Speech Recognition API we can work to censor them automatically.\n\nIf you want to look at the final project code you can find it at <a href=\"https://github.com/deepgram-devs/censor-audio\">https://github.com/deepgram-devs/censor-audio</a>.\n\n## Before We Start\n\nYou will need:\n\n*   Node.js installed on your machine - [download it here](https://nodejs.org/en/).\n*   A Deepgram API Key - [get one here](https://console.deepgram.com/signup?jump=keys).\n*   An audio file to censor - [here's one you can download](https://github.com/deepgram-devs/censor-audio-js/blob/main/input.m4a) and place in your new project directory.\n\nCreate a new directory and navigate to it with your terminal. Run `npm init -y` to create a `package.json` file and then install the following packages:\n\n    npm install @deepgram/sdk ffmpeg-static profane-words\n\nCreate an `index.js` file, and open it in your code editor.\n\n## Preparing Dependencies\n\nAt the top of your file require these packages:\n\n```js\nconst fs = require('fs')\nconst { exec } = require('child_process')\nconst { Deepgram } = require('@deepgram/sdk')\nconst profanities = require('profane-words')\nconst ffmpegStatic = require('ffmpeg-static')\n```\n\n*   `fs` is the built-in file system module for Node.js. It is used to read and write files which you will be doing a few times throughout this post.\n*   `exec` allows us to fire off terminal commands from our Node.js script.\n*   `profane-words` exports an array of, perhaps unsurprisingly, profane words.\n*   `ffmpeg-static` includes a version of FFmpeg in our node\\_modules directory, and requiring it returns the file path.\n\n[FFmpeg](https://ffmpeg.org) is a terminal-based toolkit for developers to work with audio and video files, which can include some quite complex manipulation. We'll be using `exec` to run it.\n\nInitialize the Deepgram client:\n\n```js\nconst deepgram = new Deepgram('YOUR DEEPGRAM KEY')\n```\n\n## Creating a Main Function\n\nSince Node.js 14.8 you can use `await` anywhere, even outside of an asynchronous function, if you are creating a module. For this blog post I'll assume that's not the case, so we'll create a `main()` function for our logic to sit in:\n\n```js\nasync function main() {\n  try {\n    // Logic goes here\n  } catch (error) {\n    console.error(error)\n  }\n}\n\nmain()\n```\n\n## Get Transcript and Profanity\n\nInside of our `main()` function get a transcript using the Deepgram Node.js SDK, and then find the profanities:\n\n```js\nconst transcript = await deepgram.transcription.preRecorded({\n  buffer: fs.readFileSync('./input.m4a'),\n  mimetype: 'audio/m4a',\n})\nconst words = transcript.results.channels[0].alternatives[0].words\nconst bleeps = words.filter((word) => profanities.find((w) => word.word == w))\nconsole.log(bleeps)\n```\n\nBleeps will return words that appear in the `profane-words` list. Test this code by running `node index.js` in your terminal and you should see a result like this:\n\n![A terminal showing an array with four items. Each has a word, start, end, and confidence. Each of the words is clearly profanity but has been edited to obscure the actual words.](https://res.cloudinary.com/deepgram/image/upload/v1635438533/blog/2021/11/censor-profanity-nodejs/profane-words.png)\n\nOnce you have done this, remove the `console.log()` statement.\n\n## Determine Clean Audio Timings\n\nNext, we want the inverse start and end times - where the audio is 'clean' and doesn't need bleeping. Add this to the `main()` function:\n\n```js\nconst noBleeps = [{ start: 0, end: bleeps[0].start }]\nfor (let i = 0; i < bleeps.length; i++) {\n  if (i < bleeps.length - 1) {\n    noBleeps.push({ start: bleeps[i].end, end: bleeps[i + 1].start })\n  } else {\n    noBleeps.push({ start: bleeps[i].end })\n  }\n}\n\nconsole.log(noBleeps)\n```\n\nRun this again with `node index.js` and you should have the following result:\n\n![A terminal showing an array of 5 objects, each with a start and end except the last which only has a start.](https://res.cloudinary.com/deepgram/image/upload/v1635438533/blog/2021/11/censor-profanity-nodejs/no-bleeps.png)\n\n## FFmpeg Complex Filters\n\nFFmpeg allows complex manipulation of audio files, and works by chaining smaller manipulations known as filters. We pass in audio by a variable name, do something, and export a new variable which we can then further chain. This might feel complex, so let's talk through what we will do.\n\n1.  Take the original audio file and drop the volume to 0 during times where we have profanity.\n2.  Generate a constant beep with a sine wave.\n3.  Make the constant beep end when the final profanity finishes.\n4.  Drop the volume of the beep to 0 whenever there is not profanity.\n5.  Mix the bleep and the vocals to one final track which at any point in time will have a bleep or vocals - never both.\n\nIn our `main()` function let's do this with code. Starting with dropping the volume wherever we have profanity:\n\n```js\nconst dippedVocals = `[0]volume=0:enable='${bleeps\n  .map((b) => `between(t,${b.start},${b.end})`)\n  .join('+')}'[dippedVocals]`\n```\n\n`dippedVocals` will now look something like:\n\n    [0]volume=0:enable='between(t,1.5777808,1.977219)+between(t,4.7732863,5.2732863)+between(t,5.3724437,5.8724437)+between(t,6.371039,6.7704773)'[dippedVocals]\n\nThis takes the provided file (which here is `[0]`), makes the volume 0 between the provided times, and makes this altered version available to future parts of this filter as `[dippedVocals]`\n\nDelete `dippedVocals` and create `filter` which contains all parts of our complex filter with the value of `dippedVocals` as the first item, and then creates a valid string for FFmpeg:\n\n```js\nconst filter = [\n  `[0]volume=0:enable='${bleeps\n    .map((b) => `between(t,${b.start},${b.end})`)\n    .join('+')}'[dippedVocals]`,\n  'sine=d=5:f=800,pan=stereo|FL=c0|FR=c0[constantBleep]',\n  `[constantBleep]atrim=start=0:end=${\n    noBleeps[noBleeps.length - 1].start\n  }[shortenedBleep]`,\n  `[shortenedBleep]volume=0:enable='${noBleeps\n    .slice(0, -1)\n    .map((b) => `between(t,${b.start},${b.end})`)\n    .join('+')}'[dippedBleep]`,\n  '[dippedVocals][dippedBleep]amix=inputs=2',\n].join(';')\n```\n\nThat's all five steps above built into one complex filter. The final filter looks like this:\n\n    [0]volume=0:enable='between(t,1.5777808,1.977219)+between(t,4.7732863,5.2732863)+between(t,5.3724437,5.8724437)+between(t,6.371039,6.7704773)'[dippedVocals];sine=d=5:f=800,pan=stereo|FL=c0|FR=c0[constantBleep];[constantBleep]atrim=start=0:end=6.7704773[shortenedBleep];[shortenedBleep]volume=0:enable='between(t,0,1.5777808)+between(t,1.977219,4.7732863)+between(t,5.2732863,5.3724437)+between(t,5.8724437,6.371039)'[dippedBleep];[dippedVocals][dippedBleep]amix=inputs=2\n\nYeah. We did it in an array for a reason.\n\n## Create Censored File\n\nThe very final step is to actually run FFmpeg via `exec` with the above filter. Add this line to the bottom of your `main()` function:\n\n```js\nexec(`${ffmpegStatic} -y -i input.m4a -filter_complex \"${filter}\" output.wav`)\n```\n\nAnd run your script with `node index.js`. Once completed, your `output.wav` file should be your original file with automatic transcription.\n\n## Wrapping Up\n\nA transcript is not always the final step in a project - you can use the structured data returned by Deepgram to do further processing or analysis, as demonstrated by this post. I hope you found it interesting.\n\nThe complete project is available at <a href=\"https://github.com/deepgram-devs/censor-audio\">https://github.com/deepgram-devs/censor-audio</a> and if you have any questions please feel free to reach out on Twitter - we're [@DeepgramDevs](https://twitter.com/DeepgramDevs).\n\n        ";
						}
						async function compiledContent$3H() {
							return load$3H().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3H() {
							return (await import('./chunks/index.acc94734.mjs'));
						}
						function Content$3H(...args) {
							return load$3H().then((m) => m.default(...args));
						}
						Content$3H.isAstroComponentFactory = true;
						function getHeadings$3H() {
							return load$3H().then((m) => m.metadata.headings);
						}
						function getHeaders$3H() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3H().then((m) => m.metadata.headings);
						}

const __vite_glob_0_41 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3H,
  file: file$3H,
  url: url$3H,
  rawContent: rawContent$3H,
  compiledContent: compiledContent$3H,
  default: load$3H,
  Content: Content$3H,
  getHeadings: getHeadings$3H,
  getHeaders: getHeaders$3H
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3G = {"title":"Chili Pepper","description":"Want to see Deepgrams founder eat a chili pepper? Not sure why you wouldn't. Check it out.","date":"2020-01-01T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981336/blog/chili-pepper/chili-pepper%402x.jpg","authors":["scott-stephenson"],"category":"dg-insider","tags":["fun"],"seo":{"title":"Chili Pepper","description":"Want to see Deepgrams founder eat a chili pepper? Not sure why you wouldn't. Check it out."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981336/blog/chili-pepper/chili-pepper%402x.jpg"},"shorturls":{"share":"https://dpgr.am/34c4678","twitter":"https://dpgr.am/50c551f","linkedin":"https://dpgr.am/cff6b95","reddit":"https://dpgr.am/db72e68","facebook":"https://dpgr.am/866a9f2"}};
						const file$3G = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/chili-pepper/index.md";
						const url$3G = undefined;
						function rawContent$3G() {
							return "I ate the world's hottest pepper to get you to respond to this email. Your move.\n\n<iframe class=\"wistia_embed\" title=\"Wistia video player\" src=\"https://fast.wistia.net/embed/iframe/275gfw0348?videoFoam=true\" name=\"wistia_embed\" width=\"600\" height=\"315\" frameborder=\"0\" scrolling=\"no\" allowfullscreen=\"allowfullscreen\"></iframe>\n\nIt hurts, a lot. Ready to get started? [Sign up for a free API key](https://console.deepgram.com/signup) or [contact us](https://deepgram.com/contact-us/) if you have questions.\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-how-deepgram-works\"></WhitepaperPromo>";
						}
						async function compiledContent$3G() {
							return load$3G().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3G() {
							return (await import('./chunks/index.5a3e411d.mjs'));
						}
						function Content$3G(...args) {
							return load$3G().then((m) => m.default(...args));
						}
						Content$3G.isAstroComponentFactory = true;
						function getHeadings$3G() {
							return load$3G().then((m) => m.metadata.headings);
						}
						function getHeaders$3G() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3G().then((m) => m.metadata.headings);
						}

const __vite_glob_0_42 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3G,
  file: file$3G,
  url: url$3G,
  rawContent: rawContent$3G,
  compiledContent: compiledContent$3G,
  default: load$3G,
  Content: Content$3G,
  getHeadings: getHeadings$3G,
  getHeaders: getHeaders$3G
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3F = {"title":"Start Chromium in Kiosk Mode on Raspberry Pi OS","description":"Launch directly into a fullscreen browser on your Raspberry Pi","date":"2022-01-19T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1642519931/blog/2022/01/chromium-kiosk-pi/Pi.png","authors":["kevin-lewis"],"category":"tutorial","tags":["raspberrypi","iot"],"seo":{"title":"Start Chromium in Kiosk Mode on Raspberry Pi OS","description":"Launch directly into a fullscreen browser on your Raspberry Pi"},"shorturls":{"share":"https://dpgr.am/f0d301c","twitter":"https://dpgr.am/3033ea8","linkedin":"https://dpgr.am/9cc98cb","reddit":"https://dpgr.am/47f10bd","facebook":"https://dpgr.am/70b90f5"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453839/blog/chromium-kiosk-pi/ograph.png"}};
						const file$3F = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/chromium-kiosk-pi/index.md";
						const url$3F = undefined;
						function rawContent$3F() {
							return "\r\nEarlier this month [I built a wearable transcription device](https://twitter.com/_phzn/status/1478504862170161152) using Deepgram and a Raspberry Pi. The project is a web application running in a fullscreen browser on the Pi. However, when the device first starts, it requires a fiddly set of touchscreen interactions to get it in a ready state - opening the browser, navigating to the correct URL, and then fullscreening the browser. In this quick guide, I will show you the steps I took to automate this on device launch.\r\n\r\n<Alert type=\"info\">This tutorial works for Raspberry Pi OS 10 - Buster</Alert>\r\n\r\nOpen your terminal and type the following:\r\n\r\n```bash\r\nsudo nano /etc/xdg/lxsession/LXDE-pi/autostart\r\n```\r\n\r\nThis will open a new text file which will be executed when the desktop environment (LXDE) launches. In the file type the following:\r\n\r\n```bash\r\n@lxpanel --profile LXDE-pi\r\n@pcmanfm --desktop --profile LXDE-pi\r\n\r\n@xset s off\r\n@xset -dpms\r\n@xset s noblank\r\n\r\n@chromium-browser --kiosk https://deepgram.com\r\n```\r\n\r\nClick **Control + X** to quit the app, and then **Y** to say 'yes' and save your file.\r\n\r\nThe first section sets up the environment and profile for the Pi, and the second section stops the Pi sleeping or starting the screensaver.\r\n\r\nThe final line is the most crucial - it starts Chromium (the built-in browser on which Google Chrome is based) in fullscreen mode at the specified URL. Kiosk mode also stops other user input outside of the browser - effectively locking the user into the browser.\r\n\r\nI hope this helps you build web-based Raspberry Pi projects. If you have any questions, please feel free to reach out on Twitter - we're [@DeepgramDevs](https://twitter.com/DeepgramDevs).\r\n\r\n        ";
						}
						async function compiledContent$3F() {
							return load$3F().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3F() {
							return (await import('./chunks/index.c7702c1a.mjs'));
						}
						function Content$3F(...args) {
							return load$3F().then((m) => m.default(...args));
						}
						Content$3F.isAstroComponentFactory = true;
						function getHeadings$3F() {
							return load$3F().then((m) => m.metadata.headings);
						}
						function getHeaders$3F() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3F().then((m) => m.metadata.headings);
						}

const __vite_glob_0_43 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3F,
  file: file$3F,
  url: url$3F,
  rawContent: rawContent$3F,
  compiledContent: compiledContent$3F,
  default: load$3F,
  Content: Content$3F,
  getHeadings: getHeadings$3F,
  getHeaders: getHeaders$3F
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3E = {"title":"Adding Live Captions To Your Classroom With Deepgram","description":"Make your lectures more accessible with live automatic captioning.","date":"2022-02-09T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1644272672/blog/2022/02/classroom-captioner/Adding-Live-Captions-To-Your-Classroom%402x.jpg","authors":["kevin-lewis"],"category":"tutorial","tags":["nodejs","education"],"seo":{"title":"Adding Live Captions To Your Classroom With Deepgram","description":"Make your lectures more accessible with live automatic captioning."},"shorturls":{"share":"https://dpgr.am/e0788f9","twitter":"https://dpgr.am/9c1aee5","linkedin":"https://dpgr.am/105bb6d","reddit":"https://dpgr.am/b5d0747","facebook":"https://dpgr.am/565f0de"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453858/blog/classroom-captioner/ograph.png"}};
						const file$3E = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/classroom-captioner/index.md";
						const url$3E = undefined;
						function rawContent$3E() {
							return "\nMany places of study offer accessibility accommodations for students who need them to understand and participate in lectures and seminars. However, the process for asking for and receiving this help can limit who can have it, and it really [isn't good enough](https://blog.deepgram.com/automatic-speech-recognition-education/).\n\n![A lecturer at the front of a classroom](https://res.cloudinary.com/deepgram/image/upload/v1644272678/blog/2022/02/classroom-captioner/iglt.jpg)\n\n*A photo of a large lecture hall at Goldsmiths, University of London, my alma mater. [Photo source](https://virtualtours.gold.ac.uk/map/learn/ian-gulland-lecture-theatre/)*\n\nThis project, Classroom Captioner, aims to alleviate the concerns of students who need or prefer a text representation of what's happening in a lecture. Most lecturers will present from a computer - either their own or one built into a podium. At the start of a session, lecturers can:\n\n1.  Open a browser tab to the application.\n2.  Create a new room and provide the lecture key needed to validate themselves as the lecturer.\n3.  Provide the room code to students to put in the same web application.\n4.  Speak as usual - the tab can be left in the background.\n\nIf you want to see the finished code and deploy your own version of this project in one click, visit <a href=\"https://github.com/deepgram-devs/classroom-captioner\">https://github.com/deepgram-devs/classroom-captioner</a>.\n\n## Before We Start\n\nYou will need:\n\n*   Node.js installed on your machine - [download it here](https://nodejs.org/en/).\n*   A Deepgram API Key and Project ID - [get them here](https://console.deepgram.com/signup?jump=keys). Make sure your API Key has either an admin or owner role.\n\nCreate a new directory and navigate to it with your terminal. Run `npm init -y` to create a `package.json` file and then install the following packages:\n\n    npm install @deepgram/sdk dotenv express socket.io\n\nCreate a file called `.env` and add the following to it:\n\n    DEEPGRAM_KEY=your-api-key\n    DEEPGRAM_PROJECT=your-project-id\n    LECTURE_KEY=any-passphrase\n\n<Alert type=\"warning\">Do not let others access your .env file as it contains sensitive values. If you share your code, omit this file.</Alert>\n\n## Setting Up Server\n\nThis application uses a combined express and socket.io server. Express is used to serve files, authenticate our lecture key, and generate temporary Deepgram API keys. Socket.io is used for realtime communication - sending completed transcriptions from the lecturer's view to their students.\n\nCreate an `index.js` file and add the following code to create this combined server and set up Deepgram for later use:\n\n```js\nrequire('dotenv').config()\nconst http = require('http')\nconst express = require('express')\nconst Socket = require('socket.io').Server\nconst { Deepgram } = require('@deepgram/sdk')\n\nconst app = express()\nconst server = http.createServer(app)\nconst io = new Socket(server)\nconst deepgram = new Deepgram(process.env.DEEPGRAM_KEY)\n\napp.use(express.static('public'))\napp.use(express.json())\n\n// Further code goes here\n\nconst PORT = process.env.PORT || 3000\nserver.listen(PORT, () => {\n  console.log(`listening on ${PORT} at ${new Date().toISOString()}`)\n})\n```\n\nFinally, create a `views` and `public` directory.\n\n## Landing Page\n\nThe first of three pages to build is our landing page. It will allow users to navigate to a room as either a lecturer or a student.\n\nWe must create a route handler to tell express which file to load when a user navigates to our page. While we are here, we will also create route handlers for other pages. In `index.js`:\n\n```js\napp.get('/', (req, res) => {\n  res.sendFile(__dirname + '/views/index.html')\n})\napp.get('/student', (req, res) => {\n  res.sendFile(__dirname + '/views/student.html')\n})\napp.get('/lecturer', (req, res) => {\n  res.sendFile(__dirname + '/views/lecturer.html')\n})\n```\n\nWe'll create the student and lecturer views later. For now, add an `index.html` page to your `views` directory, and open it in your code editor:\n\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n  <head>\n    <meta charset=\"UTF-8\" />\n    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\" />\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n    <title>Classroom Captioner</title>\n  </head>\n  <body>\n    <h1>Classroom Captioner</h1>\n\n    <h2>Join as a student</h2>\n    <form action=\"/student\">\n      <label for=\"id\">Room ID</label>\n      <input type=\"text\" id=\"id\" name=\"id\" />\n      <input type=\"submit\" value=\"Submit\" />\n    </form>\n\n    <h2>Create as a lecturer</h2>\n    <form action=\"/lecturer\">\n      <label for=\"id\">Room ID</label>\n      <input type=\"text\" id=\"id\" name=\"id\" />\n      <input type=\"submit\" value=\"Submit\" />\n    </form>\n  </body>\n</html>\n```\n\nStart your server by running `node index.js` and navigate to `http://localhost:3000`. Type a value into the first input and submit the form. You should be sent to `http://localhost:3000/student?id=TYPEDVALUE` (which should present an error as there is no file yet for that page). However, this confirms that our landing page can direct users to the student and lecturer pages.\n\n![Webpage with the title \"Classroom Captioner\" with two forms. The first form says \"join as a student\" and asks for a Room ID. The second form is the same except it starts \"create as a lecturer\".](https://res.cloudinary.com/deepgram/image/upload/v1644272676/blog/2022/02/classroom-captioner/landing-no-style.png)\n\n## Understanding Socket Rooms\n\nWhen using socket.io for realtime communication, there are two main concepts:\n\n1.  Sending (*broadcasting* or *emitting*) and receiving (*listening*) events with data.\n2.  All users (*clients*) connect to a *server*. Messages get sent to and from the server - you can think of the server as an intermediary between all other connections in this context.\n\nTypically, data sent from the server will be sent to all clients connected to it, except the sender, or to one specific client. However, this project needs to handle multiple ongoing rooms with many users in each, and that's where rooms come in.\n\nAny socket connection can be assigned to any number of rooms, which you can think of as groups. When emitting an event from the server, you can specify which rooms should be sent the data. All users in those rooms get it, and those not assigned do not - perfect!\n\nWhen a client establishes a new connection via socket.io, they are automatically assigned to a room with the name of their unique identifier. This means users are immediately in one room, and they will need to be added to the shared class-wide room when joining. Just keep this in mind as we move into the next section.\n\n## Lecturer View\n\nInside of the *views* directory, add `lecturer.html`:\n\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n  <head>\n    <meta charset=\"UTF-8\" />\n    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\" />\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n    <title>Class Captioner: Lecturer</title>\n  </head>\n  <body>\n    <h1>Room <span id=\"id\"></span></h1>\n    <form>\n      <label for=\"key\">Lecturer Key</label>\n      <input type=\"password\" id=\"key\" name=\"key\" />\n      <input type=\"submit\" value=\"Submit\" />\n    </form>\n\n    <p></p>\n\n    <script src=\"/socket.io/socket.io.js\"></script>\n    <script>\n      const url = new URL(location.href)\n      const search = new URLSearchParams(url.search)\n      const id = search.get('id')\n      document.querySelector('#id').textContent = id\n\n      const socket = io()\n      socket.emit('join', id)\n\n      // Further code goes here\n    </script>\n  </body>\n</html>\n```\n\nLet's talk about this page:\n\n1.  There is a form which is asking for a lecturer key. We'll use this shortly to validate the user.\n2.  We include the socket.io client file.\n3.  We get access to the room name from the URL, store it in a variable called `id`, and display it to the user.\n4.  We connect to our socket server, and immediately emit an event called `join` along with the `id` value.\n\n### Add User To Socket Room\n\nIt is now time to listen for, and handle, the `join` event. Just below the route handlers in `index.js`:\n\n```js\nio.on('connection', (socket) => {\n  socket.on('join', (roomId) => {\n    socket.join(roomId)\n    console.log(`${socket.id} joins ${roomId}`)\n  })\n})\n```\n\nRestart your server, and navigate to a room as a lecturer. Look at your terminal.\n\n![Terminal displays a log of a user joining the specified room name - here \"test\"](https://res.cloudinary.com/deepgram/image/upload/v1644272676/blog/2022/02/classroom-captioner/socket-user-joins-room.png)\n\n### Accessing Lecturer's Microphone\n\nThere are parts of this project which build on our [\"Get Live Speech Transcriptions In Your Browser\"](https://blog.deepgram.com/live-transcription-mic-browser/) blog post and video. I'll call these out, and go into less depth about the code. This is one of them - add this code to `lecturer.html` to get access to the user's mic:\n\n```js\nnavigator.mediaDevices\n  .getUserMedia({ audio: true })\n  .then((stream) => {\n    mediaRecorder = new MediaRecorder(stream)\n  })\n  .catch(() => alert('You must provide access to the microphone'))\n```\n\n### Validate Lecturer Key\n\nThe main visual difference between the lecturer and student views is the inclusion of a form that prompts for a \"lecturer key\". This value must be compared against the `LECTURE_KEY` in our `.env` file, and if it's correct, we must issue a temporary Deepgram API Key to allow transcription to begin. Finally, this new key will be used to establish a connection with Deepgram and begin transcription.\n\nTo build this validation system, add a route handler to the `index.js` file:\n\n```js\napp.post('/auth', async (req, res) => {\n  try {\n    const { id, key } = req.body\n    if (req.body.key != process.env.LECTURE_KEY)\n      return res.json({ error: 'Key is missing or incorrect' })\n    const newKey = await deepgram.keys.create(\n      process.env.DEEPGRAM_PROJECT,\n      'Temporary key - works for 10 secs',\n      ['usage:write'],\n      { timeToLive: 10 }\n    )\n    res.json({ deepgramToken: newKey.key })\n  } catch (error) {\n    res.json({ error })\n  }\n})\n```\n\nA new short-lived Deepgram API Key with minimal permissions will be generated and returned if the provided key is correct. If the provided key is wrong, or an error occurs, we will show this to the browser in the returned payload.\n\nWhen the form in `lecturer.html` is submitted, let's send a request to our new route handler:\n\n```js\ndocument.querySelector('form').addEventListener('submit', async (event) => {\n  event.preventDefault()\n  const key = document.querySelector('#key').value\n\n  const resp = await fetch('/auth', {\n    method: 'POST',\n    body: JSON.stringify({ id, key }),\n    headers: { 'Content-Type': 'application/json' },\n  })\n    .then((r) => r.json())\n    .catch((error) => alert(error))\n\n  if (resp.error) return alert(resp.error)\n\n  document.querySelector('form').remove()\n\n  // Further code here [1]\n})\n\n// Further code here [2]\n```\n\nErrors will be shown to users in a popup. Success will lead to the form disappearing. Restart your server and try it out!\n\n### Live Transcribe Lecturer\n\nNow there is a valid Deepgram API Key in our web page, immediately establish a connection with Deepgram. In the first annotated spot above, connect to Deepgram:\n\n```js\nws = new WebSocket('wss://api.deepgram.com/v1/listen', [\n  'token',\n  resp.deepgramToken,\n])\nws.onopen = start\nws.onmessage = handleResponse\n```\n\nIn the second annotated spot, add our event handlers for a connection being opened, and receiving data back from Deepgram. Take a look at [\"Get Live Speech Transcriptions In Your Browser\"](https://blog.deepgram.com/live-transcription-mic-browser/) for more explanation.\n\n```js\nfunction start() {\n  mediaRecorder.addEventListener('dataavailable', (event) => {\n    if (event.data.size > 0 && ws.readyState == 1) {\n      ws.send(event.data)\n    }\n  })\n  mediaRecorder.start(250)\n}\n\nfunction handleResponse(message) {\n  const data = JSON.parse(message.data)\n  const transcript = data.channel.alternatives[0].transcript\n  if (transcript && data.is_final) {\n    document.querySelector('p').textContent += ' ' + transcript\n    // Further code here\n  }\n}\n```\n\nRestart your server, and you should see transcripts displayed in the browser.\n\n![A webpage reads \"Hello I hope you've had a wonderful week so far and I'm very excited for today's class\"](https://res.cloudinary.com/deepgram/image/upload/v1644272676/blog/2022/02/classroom-captioner/lecturer-transcript.png)\n\n### Emit Socket Event With Transcript\n\nThe final step on the lecturer side is to emit a socket event with this transcript, so we can bring it into students' pages. Add the following line to the `handleResponse` function:\n\n```js\nsocket.emit('transcriptReady', transcript)\n```\n\nNow, as transcripts are displayed on the lecturer's page, a `transcriptReady` event will also be triggered.\n\n## Emit Transcript\n\nIn `index.js`, add a new listener to the socket right below where the `socket.on('join')` callback ends:\n\n```js\nsocket.on('transcriptReady', (message) => {\n  for (let room of socket.rooms) {\n    socket.to(room).emit('transcriptComplete', message)\n  }\n})\n```\n\nThis goes through all of the rooms this current socket belongs to (which includes the room they joined with the `join` event) and emits a `transcriptComplete` event with the transcript to just the sockets in those rooms.\n\nIf the room you navigated to in the browser is called \"my-awesome-room\" the `join` event will have added your connection to a socket room of the same name. If students join the same room, they will receive the transcripts too.\n\n## Student View\n\nThe student view is just a stripped-back version of the lecturer view. Create a `student.html` page in the `views` directory and add the following:\n\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n  <head>\n    <meta charset=\"UTF-8\" />\n    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\" />\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n    <title>Class Captioner</title>\n  </head>\n  <body>\n    <h1>Room <span id=\"id\"></span></h1>\n\n    <p></p>\n\n    <script src=\"/socket.io/socket.io.js\"></script>\n    <script>\n      const url = new URL(location.href)\n      const search = new URLSearchParams(url.search)\n      const id = search.get('id')\n      document.querySelector('#id').textContent = id\n\n      const socket = io()\n      socket.emit('join', id)\n\n      socket.on('transcriptComplete', (message) => {\n        document.querySelector('p').textContent += ' ' + message\n      })\n    </script>\n  </body>\n</html>\n```\n\nRestart your server, open the application in various browser windows, with one window acting as the lecturer and the others as students. You should see the transcript appear on all screens.\n\n## Adding Styling\n\nCreate a `style.css` file in your `public` directory with the following:\n\n```css\n@import url('https://fonts.googleapis.com/css2?family=Cairo&display=swap');\n* {\n  margin: 0;\n  padding: 0;\n  box-sizing: border-box;\n}\nbody {\n  padding: 2em;\n  font-family: 'Cairo', sans-serif;\n  background: #141e29;\n  color: white;\n}\nh2 {\n  margin-top: 1.5rem;\n}\ninput {\n  display: block;\n  font-size: 1em;\n  font-family: inherit;\n  padding: 0 0.5em;\n  width: 200px;\n}\ninput[type='submit'] {\n  background: #38edac;\n  color: #141e29;\n  border: none;\n  margin-top: 0.5em;\n}\n```\n\nThen, just before the `</head>` in all three `.html` files, add the following:\n\n```html\n<link rel=\"stylesheet\" href=\"style.css\" />\n```\n\nRestart your server one final time and your application should look like this:\n\n![Four browser windows. One is a broadcaster in room test1. Two are students in room test1. The final window is a student in room test 2. The first three have identical text displayed, and the other is empty](https://res.cloudinary.com/deepgram/image/upload/v1644272676/blog/2022/02/classroom-captioner/fin.png)\n\n## Run Your Own\n\nIf you want to see the finished code and deploy your own version of this project in one click, visit <a href=\"https://github.com/deepgram-devs/classroom-captioner\">https://github.com/deepgram-devs/classroom-captioner</a>.\n\n## Wrapping Up\n\nSharing knowledge as an educator feels wonderful, and now you can ensure all of your students have an equal experience in the classroom. If you have any questions, please feel free to reach out on Twitter - we're [@DeepgramDevs](https://twitter.com/DeepgramDevs).\n\n        ";
						}
						async function compiledContent$3E() {
							return load$3E().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3E() {
							return (await import('./chunks/index.99585ee5.mjs'));
						}
						function Content$3E(...args) {
							return load$3E().then((m) => m.default(...args));
						}
						Content$3E.isAstroComponentFactory = true;
						function getHeadings$3E() {
							return load$3E().then((m) => m.metadata.headings);
						}
						function getHeaders$3E() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3E().then((m) => m.metadata.headings);
						}

const __vite_glob_0_44 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3E,
  file: file$3E,
  url: url$3E,
  rawContent: rawContent$3E,
  compiledContent: compiledContent$3E,
  default: load$3E,
  Content: Content$3E,
  getHeadings: getHeadings$3E,
  getHeaders: getHeaders$3E
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3D = {"title":"How Closed Captioning is Enabled by ASR","description":"Closed captioning provides accessibility to people who have trouble hearing audio—and with ASR, it’s getting faster and more powerful.","date":"2022-08-10T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981430/blog/closed-captioning-companies-use-asr/how-closed-captioning-is-enabled-by-ASR-thumb-554x.png","authors":["chris-doty"],"category":"speech-trends","tags":["voice-strategy"],"seo":{"title":"How Closed Captioning is Enabled by ASR","description":"Closed captioning provides accessibility to people who have trouble hearing audio—and with ASR, it’s getting faster and more powerful."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981430/blog/closed-captioning-companies-use-asr/how-closed-captioning-is-enabled-by-ASR-thumb-554x.png"},"shorturls":{"share":"https://dpgr.am/d8cecee","twitter":"https://dpgr.am/1474e89","linkedin":"https://dpgr.am/289671c","reddit":"https://dpgr.am/d2c4efd","facebook":"https://dpgr.am/3318e59"}};
						const file$3D = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/closed-captioning-companies-use-asr/index.md";
						const url$3D = undefined;
						function rawContent$3D() {
							return "\r\nThere are many different ways that closed captioning is enabled by speech-to-text and automatic speech recognition (ASR). In this blog post, we'll discuss some of the most common of these use cases, including creating captions for live events, podcasts, and transcribing videos. We'll also explore the benefits that state-of-the-art, AI-powered speech-to-text solutions like Deepgram can provide to closed captioning companies. But before we start, let's define what exactly closed captioning is.\r\n\r\n## What is Closed Captioning?\r\n\r\nClosed captioning is a means of adding a transcript of what is said to video files. It's similar to subtitles, but while subtitles are usually intended for someone who doesn't speak or understand the language used in the video, closed captions are intended for those who might be [deaf or hard of hearing](https://blog.deepgram.com/asr-important-deaf-hoh-community/). However, [Verizon Media](https://www.streamingmedia.com/Articles/News/Online-Video-News/80-of-Video-Caption-Users-Arent-Hearing-Impaired-Finds-Verizon-131860.aspx?) found that 80% of closed caption users aren't hearing impaired, so this is a feature that's expanding in use. And, in case you're curious-\"closed\" here means that the captions aren't visible to the viewer until they turn them on.\r\n\r\n## Use Cases for Speech-to-Text in Closed Captioning\r\n\r\nIt might seem like there's just one use case for ASR in closed captioning-namely, providing a transcript of what was said-but we can think about several different domains where captions can be generated with speech-to-text solutions, and that have real advantages over using human transcriptionists.\r\n\r\n### Live Events\r\n\r\nSpeech-to-text for live captioning is one of the prime use cases for ASR solutions. These captions are especially important for live events because they allow people who are deaf or hard of hearing to follow along with what is happening and be included in the event. And, these captions can also help others-those too far away from the speaker to hear clearly, for example, can also benefit. This type of captioning can be done with or without human intervention, but it's important to have someone who is familiar with ASR monitoring the captioning process to ensure accuracy.\r\n\r\n### Live Television\r\n\r\nSimilar to live events, live television is another place where closed captions powered by AI speech-to-text can have a big impact. If you've ever tried to watch something live with closed captions turned on, you know that they're often delayed several seconds while humans transcribe what was said. But by using speech-to-text for captioning, transcriptions can be generated in real time, removing delays and lag.\r\n\r\n### Education and Training\r\n\r\nCaptioning can also be used to transcribe pre-recorded videos or podcasts. This is often done for educational or training videos, but it can also be used for other types of video content. ASR can be used to create a transcript of the video, which can then be used to create captions. This type of captioning is important for making sure that all viewers can access the information in the video, regardless of whether they are able to hear the audio.\r\n\r\n### Podcasts\r\n\r\nAlthough you might mostly associate captions with video content, they're also a critical component of accessibility for podcasts. Podcast content has exploded in recent years and has become a major type of media. But it's one that can be difficult or impossible for people who are deaf or hard-of-hearing to access without captions. These captions can help other people, too-non-native speakers, people listening with background noise, and those who'd rather read content than listen to it, to name a few. You can read more about the importance of captioning for podcasts at [Podcast Accessibility](https://podcast-accessibility.com/).\r\n\r\n<WhitepaperPromo whitepaper=\"latest\"></WhitepaperPromo>\r\n\r\n## Benefits of AI-Powered ASR for Closed Captioning\r\n\r\nIt should be clear from the above that closed captioning powered by AI speech recognition has a lot of potential use cases. But not all speech-to-text solutions can be used for real-time closed captioning. Older systems that rely on a legacy approach are typically too slow for live use cases. However, [end-to-end deep-learning ASR solutions like Deepgram](https://blog.deepgram.com/deep-learning-speech-recognition/) can turn around transcriptions in fractions of a second, creating a truly real-time experience. So what are some of the specific benefits that AI-powered speech-to-text tools can provide when compared to a human? Let's take a look.\r\n\r\n### Speed\r\n\r\nAs noted above, if you've ever tried to watch a sporting event in a bar, for example, it's very obvious that the captions are delayed-oftentimes so delayed it's hard to match them up to what's happening. With a speech-to-text system that runs in real time, these delays can be reduced to fractions of a second so that transcriptions more closely match what's being said in time.\r\n\r\n### Accuracy\r\n\r\nAnother issue you might have seen with live events is that the accuracy of the captions can suffer. This can be anything from small typos to misheard words to complete gibberish. AI-powered speech recognition systems can have accuracies of over 90%. And, with [custom model training](https://deepgram.com/product/custom-training/), you can use audio from your particular domain to further improve a model thanks to [transfer learning](https://blog.deepgram.com/transfer-learning-spanish-portuguese/)-something that's not possible with older speech-to-text systems.\r\n\r\n### Automatically Align Audio and Captions\r\n\r\nIf you're working with human-created transcripts on pre-recorded audio, someone-usually the transcriptionist-has to manually align each caption to the right part of a video, which can be a tedious and time-consuming process. Because ASR transcriptions output start and end times, it's much easier to correctly align captions with the audio.\r\n\r\n### Cost Savings\r\n\r\nPaying to have a video transcribed can be quite expensive. But with ASR, the cost savings over human transcriptionists can be substantial. And, with AI-powered solutions, you can run multiple audio streams at the same time without losing speed or accuracy, allowing for more things to be transcribed for less.\r\n\r\n## One Problem with Using ASR for Closed Captioning\r\n\r\nBefore we wrap up, it's worth noting that there's one issue that you can run into if you're trying to use only ASR for your transcriptions of things like TV shows or movies: even the most sophisticated system won't be able to tell which [door creaks] or [spooky whispering] should be included in the captions to help those who are deaf or hard-of-hearing understand what's happening on-screen. In these cases, you'd still want a human in the loop to make sure that any important, non-speech audio is included in the captions. But the ASR transcript can still be used as the base, providing many of the features above, like speed and time syncing, even if a human needs to be included.\r\n\r\n## Wrapping Up\r\n\r\nAnd there you have it-some of the main ways that ASR tools can deliver strong benefits for anyone who needs to generate closed captions. If you're curious how Deepgram's speech-to-text API can help your captioning use case, [give us a try and get $150 in free credits](https://console.deepgram.com/signup). Have questions? Reach out and we'll be happy to talk through your use case with you and see how we can help.\r\n";
						}
						async function compiledContent$3D() {
							return load$3D().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3D() {
							return (await import('./chunks/index.54bc8990.mjs'));
						}
						function Content$3D(...args) {
							return load$3D().then((m) => m.default(...args));
						}
						Content$3D.isAstroComponentFactory = true;
						function getHeadings$3D() {
							return load$3D().then((m) => m.metadata.headings);
						}
						function getHeaders$3D() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3D().then((m) => m.metadata.headings);
						}

const __vite_glob_0_45 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3D,
  file: file$3D,
  url: url$3D,
  rawContent: rawContent$3D,
  compiledContent: compiledContent$3D,
  default: load$3D,
  Content: Content$3D,
  getHeadings: getHeadings$3D,
  getHeaders: getHeaders$3D
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3C = {"title":"Remembering Cloud To Butt","description":"Who remembers the Cloud to Butt extension? Say no to buzzwords and use Deepgram's Find and Replace feature to make transcripts way more fun to read.","date":"2022-08-11T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1660159294/blog/2022/08/cloud-to-butt/ctb.jpg","authors":["kevin-lewis"],"category":"tutorial","tags":["terminal"],"seo":{"title":"Remembering Cloud To Butt","description":"Who remembers the Cloud to Butt extension? Say no to buzzwords and use Deepgram's Find and Replace feature to make transcripts way more fun to read."},"shorturls":{"share":"https://dpgr.am/3d72211","twitter":"https://dpgr.am/0588a0f","linkedin":"https://dpgr.am/a013353","reddit":"https://dpgr.am/1c11ab7","facebook":"https://dpgr.am/c24e20f"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454113/blog/cloud-to-butt/ograph.png"}};
						const file$3C = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/cloud-to-butt/index.md";
						const url$3C = undefined;
						function rawContent$3C() {
							return "\r\nEvery morning I cycle through the same few apps on my phone before I get up, and one of my stops is Facebook. While it isn't really a platform I actively use, I take a moment to go into today's 'memories' - all of the posts I posted on this date in previous years - and scrub old, embarrassing entries.\r\n\r\nOn the day I'm writing this, I was made to remember the [Cloud to Butt Chrome Extension](https://www.gizmodo.com.au/2014/08/a-chrome-extension-that-replaces-cloud-with-butts-wins-everything/) - which replaces instances of \"cloud\" with \"butt\". I'm all for slapping the wrists of those who lean into buzzwords, which in 2014, \"the cloud\" definitely was.\r\n\r\nChrome Extensions are so last month ([cough cough here's a blog post I wrote on them](https://blog.deepgram.com/transcribing-browser-tab-audio-chrome-extensions/)), so I thought it'd be fun to use the new Deepgram [Find and Replace feature](https://developers.deepgram.com/documentation/features/replace/) to get my nostalgia trip in 2022.\r\n\r\nI recorded some words based on [this 2014 talk from Maciej Cegłowski](https://idlewords.com/talks/internet_with_a_human_face.htm). [Here is the audio file we're going to use](http://lws.io/static/the-cloud.mp3).\r\n\r\n## Using cURL\r\n\r\nOpen your terminal, copy and paste the following (remembering to replace `YOUR_DEEPGRAM_API_KEY` with a real API Key generated in the [Deepgram Console](https://console.deepgram.com/signup?jump=keys)), and hit enter.\r\n\r\n```bash\r\ncurl \\\r\n  --request POST \\\r\n  --header 'Authorization: Token YOUR_DEEPGRAM_API_KEY' \\\r\n  --header 'Content-Type: application/json' \\\r\n  --data '{\"url\":\"http://lws.io/static/the-cloud.mp3\"}' \\\r\n  --url 'https://api.deepgram.com/v1/listen?replace=the%20cloud:my%20butt'\r\n```\r\n\r\nThe `replace` query parameter accepts the following format `original:new`. The value `%20` is a URL-encoded space. So `the%20cloud:my%20butt` replaces `the cloud` with `my butt`.\r\n\r\n## Adding jq\r\n\r\n[`jq`](https://stedolan.github.io/jq/) is an excellent command-line utility that allows you to display and manipulate json data. Once installed, try running this command to display just the transcript:\r\n\r\n```bash\r\ncurl \\\r\n  --request POST \\\r\n  --header 'Authorization: Token YOUR_DEEPGRAM_API_KEY' \\\r\n  --header 'Content-Type: application/json' \\\r\n  --data '{\"url\":\"http://lws.io/static/the-cloud.mp3\"}' \\\r\n  --url 'https://api.deepgram.com/v1/listen?replace=the%20cloud:my%20butt' | jq '.results.channels[0].alternatives[0].transcript'\r\n```\r\n\r\n> \"and then there's my butt my butt fascinates me because of the distance between what it promises and what it actually is my butt promises us complete liberation from the mundane world of hardware and infrastructure it invites us to soar into a plane of pure computation freed from the weary bonds of earth what my butt is is a big collection of buildings and computers that we actually know very little about run by a large American company notorious for being pretty terrible to its workers but who knows what angry admin looks inside my butt\"\r\n\r\nI hope you found this... insightful? And remember, if I can get paid to write posts like this for my job then anything's possible. Have a fantastic rest of your day!\r\n\r\n        ";
						}
						async function compiledContent$3C() {
							return load$3C().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3C() {
							return (await import('./chunks/index.00e880b8.mjs'));
						}
						function Content$3C(...args) {
							return load$3C().then((m) => m.default(...args));
						}
						Content$3C.isAstroComponentFactory = true;
						function getHeadings$3C() {
							return load$3C().then((m) => m.metadata.headings);
						}
						function getHeaders$3C() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3C().then((m) => m.metadata.headings);
						}

const __vite_glob_0_46 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3C,
  file: file$3C,
  url: url$3C,
  rawContent: rawContent$3C,
  compiledContent: compiledContent$3C,
  default: load$3C,
  Content: Content$3C,
  getHeadings: getHeadings$3C,
  getHeaders: getHeaders$3C
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3B = {"title":"Video: Coding a Website With Your Voice","description":"Deepgram community member Filip builds a project which creates websites through voice commands. Find out how.","date":"2022-02-18T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1646692163/blog/2022/02/coding-website-with-voice/cover.png","authors":["kevin-lewis"],"category":"project-showcase","tags":["javascript"],"seo":{"title":"Video: Coding a Website With Your Voice","description":"Deepgram community member Filip builds a project which creates websites through voice commands. Find out how."},"shorturls":{"share":"https://dpgr.am/7718cf2","twitter":"https://dpgr.am/9b4fcd7","linkedin":"https://dpgr.am/2c77018","reddit":"https://dpgr.am/143cc54","facebook":"https://dpgr.am/30931e1"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453860/blog/coding-website-with-voice/ograph.png"}};
						const file$3B = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/coding-website-with-voice/index.md";
						const url$3B = undefined;
						function rawContent$3B() {
							return "\r\nDeepgram community member Filip Grebowski recently built a really cool demo which allows him to build websites by dictating elements and their attributes.\r\n\r\n<YouTube id=\"rhFlRPz-AxQ\"></YouTube>\r\n\r\nIn Part 1, Filip expands on our [Get Live Speech Transcriptions In Your Browser](https://blog.deepgram.com/live-transcription-mic-browser/) demo to detect keywords and build a webpage.\r\n\r\n![Actions: add, delete, modify, save, structure. Elements: button, title, paragraph, input. Sub-actions: all, at index. Sub-category: name, palceholder, default.](https://res.cloudinary.com/deepgram/image/upload/v1646692165/blog/2022/02/coding-website-with-voice/keywords.png)\r\n\r\n<YouTube id=\"HgoUIIhjc2A\"></YouTube>\r\n\r\nIn Part 2, more functionality was introduced to add layout and styling to a webpage.\r\n\r\n        ";
						}
						async function compiledContent$3B() {
							return load$3B().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3B() {
							return (await import('./chunks/index.8701314c.mjs'));
						}
						function Content$3B(...args) {
							return load$3B().then((m) => m.default(...args));
						}
						Content$3B.isAstroComponentFactory = true;
						function getHeadings$3B() {
							return load$3B().then((m) => m.metadata.headings);
						}
						function getHeaders$3B() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3B().then((m) => m.metadata.headings);
						}

const __vite_glob_0_47 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3B,
  file: file$3B,
  url: url$3B,
  rawContent: rawContent$3B,
  compiledContent: compiledContent$3B,
  default: load$3B,
  Content: Content$3B,
  getHeadings: getHeadings$3B,
  getHeaders: getHeaders$3B
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3A = {"title":"Create Comic Books From Videos with yack!","description":"Developers built a video and restyle it as a classic comic book using Deepgram's Speech Recognition API and computer vision. See how here!","date":"2022-03-09T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1646083965/blog/2022/03/comic-books-videos-yack/yack.jpg","authors":["kevin-lewis"],"category":"project-showcase","tags":["computer-vision","hackathon"],"seo":{"title":"Create Comic Books From Videos with yack!","description":"Developers built a video and restyle it as a classic comic book using Deepgram's Speech Recognition API and computer vision. See how here!"},"shorturls":{"share":"https://dpgr.am/f0ae853","twitter":"https://dpgr.am/4bb8f13","linkedin":"https://dpgr.am/ed4384a","reddit":"https://dpgr.am/3530152","facebook":"https://dpgr.am/c7e98e3"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454016/blog/comic-books-videos-yack/ograph.png"}};
						const file$3A = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/comic-books-videos-yack/index.md";
						const url$3A = undefined;
						function rawContent$3A() {
							return "\r\nThe team behind yack! wanted to use Deepgram and computer vision to build a fun and novel project. What came out of it was an automatic video to comic generator, which has more complexity than you might think. I sat down with [Allan Zhang](https://github.com/WeixuanZ), [Andreas Economides](https://github.com/antroseco/), [Felix Chippendale](https://github.com/FChippendale), and [Tom Grant](https://github.com/DaveDuck321/) to ask them about their project.\r\n\r\nyack! takes a video and restyles it as a classic comic book using Deepgram's Speech Recognition API and computer vision. The output looks a bit like this:\r\n\r\n![A comic book with four panes. In each pane is Mark Zuckerberg. In the first and third is a speech box with text.](https://res.cloudinary.com/deepgram/image/upload/v1646083983/blog/2022/03/comic-books-videos-yack/screenshot.jpg)\r\n\r\nOnce a video is provided, yack! generates a transcript with Deepgram. Then, keyframes in the video are chosen. Frames are cropped, the image has some comic book styling applied, and captions are overlaid as speech bubbles. Finally, each 'tile' is placed in a dynamic SVG element which is rendered on the page.\r\n\r\nThat's... a lot.\r\n\r\n## How It Works\r\n\r\nThe team got Deepgram working within the first hour of building, which freed the team to focus on more complex parts of the project. To make the returned transcript as useful as possible, they used our [utterances](https://developers.deepgram.com/documentation/features/utterances/) feature to understand what keyframes to show and [diarization](https://developers.deepgram.com/documentation/features/diarize/) to color text when different speakers are detected.\r\n\r\nOnce a key frame is chosen, computer vision is used to detect a speaker's location in the frame. It is then cropped to ensure faces are seen, that there's enough space for text to be overlaid, and that the aspect ratio is roughly maintained. During development, the face detection algorithm was one of the slowest parts -- taking up to 20 seconds -- though the team managed to speed this up slightly.\r\n\r\nThe style transfer then took place -- a set of simple visual tricks to make a real-life image look more comic-like -- reducing colors, finding edges and making them darker/bolder, and stacking. This was by far the slowest bit of the overall processing time - accounting for around 60%. Given more time this could be done with machine learning.\r\n\r\nFinally, the text is overlaid, and a dynamic SVG is created. The placement of tiles is, in itself, an engineering challenge. The team used a block-claiming algorithm to have times 'claim' space on the page.\r\n\r\n## Try It Out\r\n\r\nThe yack! team built a website for users to interact with and a Docker image to create a portable, scalable, and easily-deployable server.\r\n\r\nYou can try out yack! at [yack.ml](https://yack.ml)\r\n\r\n        ";
						}
						async function compiledContent$3A() {
							return load$3A().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3A() {
							return (await import('./chunks/index.4393d450.mjs'));
						}
						function Content$3A(...args) {
							return load$3A().then((m) => m.default(...args));
						}
						Content$3A.isAstroComponentFactory = true;
						function getHeadings$3A() {
							return load$3A().then((m) => m.metadata.headings);
						}
						function getHeaders$3A() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3A().then((m) => m.metadata.headings);
						}

const __vite_glob_0_48 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3A,
  file: file$3A,
  url: url$3A,
  rawContent: rawContent$3A,
  compiledContent: compiledContent$3A,
  default: load$3A,
  Content: Content$3A,
  getHeadings: getHeadings$3A,
  getHeaders: getHeaders$3A
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3z = {"title":"The Complete Guide to Punctuation & Capitalization in Speech-to-Text","description":"Punctuation and capitalization make text more readable, but they aren’t a part of every ASR system. In this post, youll learn why they matter.","date":"2022-08-17T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981431/blog/complete-guide-punctuation-capitalization-speech-to-text/mplete-guide-to-punctuation-capitalization-in-spee.png","authors":["chris-doty"],"category":"speech-trends","tags":["language","word-error-rate"],"seo":{"title":"The Complete Guide to Punctuation & Capitalization in Speech-to-Text","description":"Punctuation and capitalization make text more readable, but they aren’t a part of every ASR system. In this post, youll learn why they matter."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981431/blog/complete-guide-punctuation-capitalization-speech-to-text/mplete-guide-to-punctuation-capitalization-in-spee.png"},"shorturls":{"share":"https://dpgr.am/9da2db0","twitter":"https://dpgr.am/2edede0","linkedin":"https://dpgr.am/7e36e39","reddit":"https://dpgr.am/240a42e","facebook":"https://dpgr.am/db5fb50"}};
						const file$3z = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/complete-guide-punctuation-capitalization-speech-to-text/index.md";
						const url$3z = undefined;
						function rawContent$3z() {
							return "Do you ever get frustrated when you're trying to dictate a text message or email and your phone keeps capitalizing the wrong words? Or adding extra periods at the end of your sentences? You're not alone! Automatic speech recognition for punctuation and capitalization can be tricky. In this blog post, we'll explore what punctuation and capitalization mean, how they're used, and some of the problems they present for speech-to-text solutions. We'll also explain what your best option for a speech recognition solution is if you need a transcript that's punctuated and capitalized correctly. To get started, let's take a look at what punctuation and capitalization are, how they're used, and how they different cross-linguistically.\n\n## What is Punctuation?\n\nPunctuation characters are symbols that are used to indicate the structure and organization of a text. In the West, [the tradition of punctuation dates from the 3rd Century BCE](https://www.bbc.com/culture/article/20150902-the-mysterious-origins-of-punctuation). Before that, texts in languages like Greek and Latin were written without any punctuation or capitalization at all-and even without spaces between words! Today, punctuation marks are used, among other things, to separate words and phrases and indicate when a sentence is ending. In English, the most common punctuation characters are the period ( . ), comma ( , ) question mark ( ? ), and exclamation point ( ! ), but there are many others, including the semicolon ( ; ), colon ( : ), dash ( - ), parentheses ( (...) ), and quotation marks ( \"...\" ). But other languages have different punctuation standards. In German, for example, quotation marks often appear as ( «...» ). Japanese punctuation, although somewhat inspired by how Western languages punctuate, uses its own symbols, using ( 。) instead of a period/full stop, and ( 、) for commas. Other languages have entirely separate traditions of punctuation, marking things that we wouldn't in English or other Western languages. If we look at Tibetan, for example, we find characters that mark the break between syllables ( ་ ), symbols for the end of a section of text ( ། ) and a larger topic ( ༎ ), and a character that marks the start of a text ( ༄ ).\n\n### What is the Purpose of Punctuation?\n\nThe main function of punctuation is to make a text more understandable. As mentioned above, prior to punctuation, words were written in a singlestreamofcharacterslikethis, which made texts challenging to read. Language is, first and foremost, a spoken or signed system of communication, and not a written one. That means we often need some help to make sure that we can understand what's being communicated in writing, and punctuation is one of the tools that we use to do that (along with other things like spelling according to the relevant standard). For example, if someone's speaking out loud, you're unlikely to confuse \"Let's eat, Grandma!\" and \"Let's eat Grandma!\"\n\nBut in writing, it's the comma that makes the difference. Punctuation helps by providing some sense of the intonation and pacing that would occur if a sentence was spoken out loud. For example, commas are used to mark brief pauses between words or phrases, while periods are used to mark the end of a sentence, what linguistically we might call an intonational unit. In both cases, these pauses have specific acoustic features that indicate what a speaker is doing when spoken out loud. Likewise, question marks and exclamation points can be used to show excitement or emphasize a point, again reflecting how a sentence would be pronounced and serve to influence the way that a sentence is read. By including punctuation marks like this in writing, the written word is brought closer to the spoken word.\n\nOrganization of a text is another purpose of punctuation. For example, semicolons and colons are often used to list items, while dashes can be used to separate parts of a sentence and quotation marks are used to set off dialogue or direct quotes from other sources. These features might not exactly match certain pauses or intonation in the same way that a question mark does, but they still serve to help make a text more understandable. For example, English has a particular intonation for lists-if I say \"I need three things: milk, bread, and eggs\" there's a pause at the colon, then rising intonation on \"milk\", then a pause, then rising intonation on \"bread\", then a pause, and then falling intonation on \"eggs\". This pattern helps our listeners understand that we're listing things off, but it's spread across several words, and doesn't occur only where the colon does.\n\n## What is Capitalization?\n\nCapitalization is the process of making a letter capital, or upper case. In English, we typically use capital letters to begin sentences and proper nouns, or for emphasis in casual writing. Proper nouns are the specific names of people, places, things, or organizations. For example, \"Susan,\" \"New York City,\" and \"Nintendo\" are all proper nouns.  Other languages have different standards for capitalization. In German, you capitalize *every* noun, not just proper nouns. And many languages don't have capital letters at all. Arabic, Hebrew, Japanese, Chinese, Hindi-no \"capital\" option exists in these languages.\n\nAlthough capitalization isn't found in all languages, it's still important to consider it along with punctuation when thinking about ASR. To some extent, this is because in many Western languages, the two go together-a period marks the end of one sentence, and a capitalized word marks the start of the next. Additionally, these have often been thought of as the same kind of problem and been treated together historically, so it makes sense to think about them together.\n\n## Why Punctuation and Capitalization Matter for Speech Recognition\n\nTypically, ASR systems don't output punctuation or capitalization-the ASR transcripts that you get just consist of lower-case words without any punctuation at all. If you're planning to use your transcripts as the input for machine learning, you might not need to worry about punctuation at all. Typically, these systems are happiest working with unformatted text. However, there are a couple of reasons why you might want your text to be formatted. The first is that, without this formatting, the texts can be hard for humans to read. Just take a look at the snippet below, from [Deepgram's transcription of NASA's first all-female spacewalk](https://deepgram.com/asr-comparison/compare-accuracy/select-alternative/), and you can see how hard it is to figure out what's happening without capitalization and punctuation.\n\n> and jessica and christina we are so proud of you i'm gonna do great today we'll be waiting for you here in a couple of hours when you get home i'm gonna hand you over to stephanie now have a great great eva drew thank you so much and our pleasure working with you this morning and i'm working on getting my ev hat open and i can report is open and stowed\n\nIf you want something that's readable by humans, capitalization and punctuation are necessary to make things clearer. You can see the same NASA text below, but with capitalization and punctuation (as well as [diarization](https://blog.deepgram.com/what-is-speaker-diarization/)-breaking up the transcript to isolate different speakers).\n\n> \\[SPEAKER 1:] ...and Jessica and Christina, we are so proud of you. I'm gonna do great today. We'll be waiting for you here in a couple of hours when you get home. I'm gonna hand you over to Stephanie now. Have a great great EVA.\n>\n> \\[SPEAKER 2:] Drew, thank you so much. And our pleasure working with you this morning, and I'm working on getting my EV hat open and I can report. Is open and stowed.\n\nIt's obvious, even at a glance, how much easier it is to read a text like this than it is to read the kind of stream-of-consciousness we see without punctuation and capitalization.  Another use for punctuation in speech-to-text relates to [sentiment analysis](https://blog.deepgram.com/sentiment-analysis-emotion-regulation-difference/). Punctuation marks can be used to divide a text into sections so that you can ask \"what's the sentiment in this particular chunk of the transcript?\" If you have a transcript from an hour-long call, it's more granular to look at sentiment in small chunks, rather than across the whole call, which punctuation makes easier to do. So how do we get ASR transcripts that have punctuation and capitalization? Let's take a look.\n\n## How Punctuation in Automatic Speech Recognition Works\n\nIf you need punctuation in your transcripts, what are your options? ASR providers typically have two ways of getting punctuation into a transcript. The first is a separate punctuation and capitalization model that runs after the text has already been generated by the speech-to-text model. Because punctuation is often a reflection of how something would be said out loud, this method, based only on the text, can create some less-than-useful outputs. This can be shown with a simple example. What is the correct punctuation for the sentence below?\n\n> sam ate dinner\n\nYou might have said \"a period\", which is probably what one of these *post hoc* models would have said. But is this a question or a statement? From the text alone, without any context, it's impossible to determine-it could be a statement or a question. If I played the audio for you, though, you'd immediately understand whether the sentence is a question or a statement based on the speaker's intonation. Determining the correct punctuation in this case is impossible with the words alone; it's much easier if you have access to the audio.\n\nThis leads us to the second way to add punctuation to a transcript. If you're using an [end-to-end deep learning system](https://blog.deepgram.com/deep-learning-speech-recognition/) as part of the process to generate your transcript, it's possible to have it output punctuation and capitalization at the same time as the words. A model that creates both text and punctuation can make decisions based on acoustic information, which is often the difference between good and bad punctuation. Let's consider another example. \n\n<WhitepaperPromo whitepaper=\"latest\"></WhitepaperPromo>\n\nConsider how the word \"huh\" might be used in speech:\n\n* Huh? = \"What did you say?\"\n* Huh!? = \"What are you talking about?!\"\n* Huh. = \"Oh, I see.\"\n\nAll of these are clearly different to a native speaker of English, and you'd have no trouble picking out which meaning was intended from speech (and the punctuation here probably helped you know how I intended these to be read). But in the context of an ASR transcript, knowing which meaning was intended would be very difficult, which is why using the audio is so critical. But even with access to the audio, automatic punctuation can still be tricky. Why? There are three main reasons. The first is that people don't always speak in sentences. If they're reading a speech they probably will, but if you listen to how people speak in meetings or casual settings, you'll find that they often don't speak in tidy ways.\n\nThis can make it difficult to decide how something should be punctuated since there aren't clear rules for punctuating text that's derived directly from spoken language. Second, people can disagree on punctuation. Some examples include when and where commas get used (I'm looking at you, [Oxford comma](https://thewritelife.com/is-the-oxford-comma-necessary/)), whether two sentences should be separated by a period or a semicolon, and if side thoughts should be put-inside em-dashes-or (inside parentheses). Because of these variations, it can be challenging to get punctuation to seem exactly right to everyone looking at a transcript (or even a written work like this blog post, for example). Finally, deep learning systems for speech recognition typically use small windows of audio to make predictions about what word is being said. This works well at the level of individual sounds, but can be tricky with intonation, which is one of the main things a system might look at to determine punctuation. That's because intonation patterns can be spread across multiple words within a phrase, meaning any given window might not have all the information it needs to make the best prediction possible.\n\nOne solution that some vendors have used to address this problem is having a second script that runs after the first model that doesn't seek to punctuate or capitalize the text, but rather to double-check that what the model output is correct. This can serve as a kind of spelling and grammar check after the fact to help make sure that the punctuation looks correct.\n\n## How to Evaluate Punctuation and Capitalization Accuracy\n\nSo how do you evaluate the accuracy of different punctuation and capitalization methods? You can do similar calculations that are used for [word error rate (WER)](https://blog.deepgram.com/what-is-word-error-rate/), a standard way of calculating the accuracy of ASR transcripts. For punctuation, we use the Punctuation Error Rate (PER). When calculating the PER, we care about how punctuation is placed relative to the words (its grammatical accuracy); PER is thus the number of punctuation errors (including misplacements relative to the truth words) divided by the number of truth punctuation tokens; i.e., number of real places that should have punctuation, as seen below. \n\n![](https://res.cloudinary.com/deepgram/image/upload/v1661976861/blog/complete-guide-punctuation-capitalization-speech-to-text/per_def.png) \n\nThe Capitalization Error (CapER) is used for capitalization accuracy, and has its own subtlety. If we were to compare capitalization across all words in a text, we'd have a very skewed metric because most words aren't capitalized. Rather than comparing the first letter of all words in the truth/predicted sequences, we compare just the aligned words which are capitalized in the truth and/or predicted text. \n\n![](https://res.cloudinary.com/deepgram/image/upload/v1661976861/blog/complete-guide-punctuation-capitalization-speech-to-text/caper_example.png) \n\nIn the example above, we see that there are two capitalization errors in a sentence with only two capitalized words in the truth text.\n\n## Deepgram's Punctuation and Capitalization for ASR Offerings\n\nAs we've seen, end-to-end deep learning systems like Deepgram that have access to acoustic information provide the best option for punctuating and capitalizing a text based on a spoken audio file. At Deepgram, we offer punctuation for all of our models, in any of the languages that we offer-all for free; capitalization is also included for relevant languages. Both punctuation and capitalization have been optimized for each language and use case model. [Check out our documentation for all of the details about how Deepgram punctuation and capitalization work](https://developers.deepgram.com/documentation/features/punctuate/). \n\n## Wrapping Up\n\nIf you'd like to give Deepgram's speech-to-text solution a try-whether you need punctuation or not-you can [sign up for a free trial and get $150 in free credits](https://console.deepgram.com/signup), or [reach out to a member of our team](https://deepgram.com/contact-us/) to discuss how Deepgram can help your business achieve its goals.";
						}
						async function compiledContent$3z() {
							return load$3z().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3z() {
							return (await import('./chunks/index.c2918103.mjs'));
						}
						function Content$3z(...args) {
							return load$3z().then((m) => m.default(...args));
						}
						Content$3z.isAstroComponentFactory = true;
						function getHeadings$3z() {
							return load$3z().then((m) => m.metadata.headings);
						}
						function getHeaders$3z() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3z().then((m) => m.metadata.headings);
						}

const __vite_glob_0_49 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3z,
  file: file$3z,
  url: url$3z,
  rawContent: rawContent$3z,
  compiledContent: compiledContent$3z,
  default: load$3z,
  Content: Content$3z,
  getHeadings: getHeadings$3z,
  getHeaders: getHeaders$3z
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3y = {"title":"How Contact-Center-as-a-Service Companies (CCaaS) Utilize ASR Solutions","description":"ASR solutions represent critical functionality for the contact-center-as-a-service industry. Learn more.","date":"2022-08-03T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981429/blog/contact-center-as-a-service-utilize-solutions/how-contact-center-as-a-service-companies-utilize-.png","authors":["aimie-ye"],"category":"speech-trends","tags":["call-analytics","contact-center","support-enablement"],"seo":{"title":"How Contact-Center-as-a-Service Companies (CCaaS) Utilize ASR Solutions","description":"ASR solutions represent critical functionality for the contact-center-as-a-service industry. Learn more."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981429/blog/contact-center-as-a-service-utilize-solutions/how-contact-center-as-a-service-companies-utilize-.png"},"shorturls":{"share":"https://dpgr.am/e82d0d3","twitter":"https://dpgr.am/58a4340","linkedin":"https://dpgr.am/634365a","reddit":"https://dpgr.am/2d19b42","facebook":"https://dpgr.am/80b9979"}};
						const file$3y = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/contact-center-as-a-service-utilize-solutions/index.md";
						const url$3y = undefined;
						function rawContent$3y() {
							return "Contact-center-as-a-service (CCaaS) platforms help businesses automate and ease the processing of customer inquiries. Critical to this functionality is automatic speech recognition, or ASR, which converts speech into written text so that a computer can understand what was said. In this blog post, we'll discuss how CCaaS companies use speech-to-text solutions and what benefits they provide, as well as how ASR tools fit in and the benefits they provide for CCaaS platforms. But before we get started-what exactly does \"contact center as a service\" mean?\n\n## What is a Contact Center as a Service (CCaaS)?\n\nContact center as a service refers to a SaaS-based application that provides management and analytics to contact centers. These tools are typically multichannel, providing ways of managing text chat, phone calls with agents, and even conversational AI solutions for customers to solve their own problems. These solutions provide intelligent contact center automation, built on a backbone of speech-to-text solutions. Using CCaaS platforms offers many benefits for business, including flexible delivery of customer service, analytics and insights based on industry best practices, and access to advanced contact center features and technologies that a company might not be able to provide if they built their own contact center software.\n\nOne of the most important technologies for CCaaS platforms is ASR, or automatic speech recognition. This technology allows contact center agents to quickly and easily handle customer inquiries without the need for human intervention. ASR solutions are the foundation of services that do things like route calls, provide customer support, and transcribe call recordings, which are commonly included as part of a CCaaS offering. Let's take a look at some of the key benefits that CCaaS solutions offer before turning to an understanding of how ASR can support them.\n\n## Why are CCaaS Solutions so Beneficial?\n\nAs noted above, there are a lot of benefits that companies can get from using CCaaS platforms.  Let's look in-depth at some of the major benefits of CCaaS solutions that are powered by ASR for contact centers.\n\n### Lower IT Costs\n\nLower IT costs are one of the main benefits of CCaaS providers. By effectively \"outsourcing\" contact center operations to a CCaaS software platform, businesses can avoid the high costs associated with setting up and maintaining their own contact center infrastructure. In addition, these platforms often offer state-of-the-art tools (like AI-powered ASR) that would be time-consuming and expensive for companies to build in-house.\n\n### Improved Customer Satisfaction\n\nCCaaS platforms allow companies to provide their customers with a better service experience by providing the ability to route calls more efficiently and provide accurate transcriptions of call recordings. For example, [Sharpen uses Deepgram's ASR solution](https://deepgram.com/case-study-sharpen/) to identify training and coaching opportunities for their agents, improving customer experience and satisfaction.\n\n### Call Tracking and Analytics\n\nCCaaS companies can also provide businesses with advanced call tracking and analytics. Businesses can get detailed information about their callers beyond just caller location, call duration, and call frequency. For example, [CallTrackingMetrics adds conversational analytics](https://deepgram.com/case-study-calltrackingmetrics/) data to these basic facts, providing more insight into calls. This information can be used to improve contact center operations, identify upselling opportunities, and train agents.\n\n### Easier Call Routing\n\nBy utilizing ASR tools, CCaaS companies can provide their customers with an easier way to route calls with intelligent contact center automation. This is because ASR solutions, when paired with natural language understanding, can help automatically route calls to the correct agent. This can save businesses time and improve customer satisfaction by lowering wait times and improving interactions with phone systems.\n\n## End-to-End Deep Learning Automatic Speech Recognition for CCaaS\n\nASR is one of the critical technologies that CCaaS platforms need to succeed. By turning speech into text, CCaaS can take action with more data, and in more ways, than if relied only on other metrics like call length, number of calls taken per agent, etc. But not all speech-to-text solutions work equally well for contact centers. Because of things like industry-specific jargon and poor call quality, old-school speech recognition models often don't provide sufficient accuracy to be truly effective.\n\nBut speech recognition solutions built on [end-to-end deep learning](https://blog.deepgram.com/deep-learning-asr-for-business/) are able to take advantage of [transfer learning](https://blog.deepgram.com/transfer-learning-spanish-portuguese/), which allows one accurate model to quickly and easily be retrained on new data to create a new model for a different use case. This technology has allowed Deepgram to offer both [a use-case model specifically for telephone calls](https://deepgram.com/product/use-cases/), as well as the ability to tailor a model with your own data if you need even higher accuracy. Overall, the specific speech recognition features of the top AI-powered speech-to-text systems that support CCaaS businesses can be divided into two categories: speech recognition and speech understanding. Let's look at each in turn.\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-make-application-voice-ready\"></WhitepaperPromo>\n\n### Speech Recognition\n\nSpeech recognition is the ability of the system to identify spoken words and convert them into text. This is a crucial capability for contact-center-as-a-service companies, as it can help to automate the process of handling customer inquiries, as well as the other benefits discussed above.\n\n#### Transcription\n\nTranscription is the ability of the system to generate a written transcript of a call recording. This can be extremely useful for businesses, as it can help them understand what was said during a call, and turn into training, improving services, gauging sentiment, and more.\n\n#### Languages\n\nOne of the benefits of using a speech recognition API like [Deepgram is that it supports multiple languages](https://deepgram.com/product/languages/). This can be extremely helpful for businesses that operate in multiple countries, or plan to in the future.\n\n#### Tailored Models\n\nAnother big benefit of using an end-to-end deep learning speech-to-text solution like Deepgram is that it offers tailored models via [transfer learning](https://blog.deepgram.com/transfer-learning-spanish-portuguese/). This means that businesses can create custom models that are specifically designed for their needs, based on their own data. This can be extremely helpful for businesses that have specific requirements or complicated audio data. For example, [NASA was able to use this ability to transcribe space-to-ground communications](https://deepgram.com/case-study-nasa/) more accurately than any other tool they tried.\n\n### Speech Understanding\n\nSpeech understanding is the ability of a system to interpret the *meaning* of spoken words. This is an important capability for CCaaS companies, as it can help businesses improve their contact center operations. Some of the most impactful features of speech understanding for CCaaS centers are discussed below.\n\n#### Diarization\n\nDiarization is the ability of the system to identify different speakers in a contact recording. This can be extremely helpful for businesses, as it can help them understand who said what during a call.\n\n#### Sentiment Analysis & Emotion Recognition\n\n[Sentiment analysis and emotion recognition](https://blog.deepgram.com/sentiment-analysis-emotion-regulation-difference/) refers to the ability of the system to identify how callers are feeling (positive or negative) as well as more specific emotions. This can be helpful for businesses as well as agents who might be dealing with difficult customers.\n\n#### Punctuation\n\nPunctuation is the ability of the system to add punctuation to a contact recording, making transcripts more readable and thus easier to process for humans.\n\n#### Capitalization\n\nCapitalization is the ability of the system to capitalize words in a contact recording. As with punctuation, this makes transcripts more readable to humans.\n\n#### Topic Detection\n\nTopic detection is the ability of the system to identify the topic of a contact recording. This tool lets CCaaS platforms identify what customers are asking about so they can be routed to the correct department.\n\n#### Summarization\n\nSummarization is the ability of the system to generate a summary of a call recording. This can be helpful for follow-ups, agent evaluation, and to determine the main reasons that customers are reaching out.\n\n#### Profanity detection\n\nProfanity detection is the ability of the system to identify-and potentially remove-profanity in a call recording. \n\n#### Speaker ID\n\nSpeaker ID is the ability of the system to identify different speakers in a contact recording. This can help track down who said what, who was on a call, and even provide an extra layer of security for callers.\n\n## What are the Benefits of Using Top ASR Tools with CCaaS Platforms?\n\nThere are many benefits of using ASR tools in your contact center. CCaaS platforms that use state-of-the-art end-to-end deep learning solutions like Deepgram can help in a variety of ways.\n\n### Faster Issue Resolution\n\nTranscripts provided by speech-to-text solutions allow more knowledge collection and knowledge transfer between the teams. Overall, CCaaS companies that use ASR tools can improve their customer service and save time.\n\n### Better Built-in Reporting\n\nOne of the best things about using ASR tools is that they provide transcripts that can easily be used to generate insightful reports, supported by some of the features discussed above like summarization and topic detection. This means that businesses can track the performance of their contact center at a glance and make necessary changes to improve operations. In addition, businesses can also use reports to track customer satisfaction levels.\n\n### Better Data Collection (and Customer Experience)\n\nAnother big benefit that contact-center-as-a-service companies get from using ASR tools is the ability to collect customer information and experience data. This is because when you have a system that can transcribe call recordings, you can easily go back and listen to what was said, or even search transcripts for specific information. This way, you can understand how your customers feel about your product or service, and can make changes accordingly.\n\n### Better Customer Service\n\nIn general, CCaaS companies that use ASR tools tend to provide better customer service. This is because they are able to quickly and easily handle customer inquiries. In addition, they can also offer accurate transcriptions of call recordings. This can help businesses improve their contact center operations and make the necessary changes to improve customer satisfaction.\n\n### Opportunities for Coaching\n\nAnother great benefit of using a CCaaS with ASR is that these platforms provide opportunities for agent coaching and training. This is because businesses can use call recordings to train new agents, as well as monitor them during their employment, identifying high performers and low performers.\n\n### Improved Compliance\n\nCCaaS platforms using ASR solutions can also help businesses improve compliance. This is because ASR tools can help businesses record calls and track contact data-transcripts can be mined for data, and topic detection, summarization, and diarization can all help to understand who is saying what and that all policies and procedures are being followed.\n\n### Easier Onboarding and Training\n\nOnboarding and training can also be supported by speech recognition by providing a record of previous calls so that trainers can understand what typical calls look like, what problems agents typically face, and more. By using transcripts of previous calls, CCaaS companies can quickly and easily provide their agents with the information they need to handle customer inquiries in an efficient manner.\n\n### Reduce Churn\n\nCCaaS tools with built in ASR technology can also help their customers reduce churn. This is because ASR tools can transcribe what's happening on calls, as well as customer sentiment and emotion. This information can be used to identify customer service issues, recommend best courses of action to try and save accounts, and ensure agents are doing everything they can to keep customers.\n\n### Increase Revenues\n\nFinally, CCaaS that leverage state-of-the-art ASR tools can also help businesses increase revenues. This is because top ASR tools can help find buying signals in what customers are saying. This information can be used to identify upselling and cross-selling opportunities, as well as help sales-team members suggest text steps.\n\n## Wrapping up\n\nOverall, contact-center-as-a-service companies that use advanced speech recognition APIs like Deepgram benefit their customers in many ways. These tools can help businesses automate the process of handling customer inquiries, improving contact center operations, and providing accurate transcriptions of contact recordings, all with the ultimate goal of delighting customers, reducing churn, and increasing revenues.\n\nIf you'd like to learn how Deepgram can help you transcribe your calls, reach out and [we'll be happy to help you get you started](https://deepgram.com/contact-us/). Or, you can [sign up for Console for free](https://console.deepgram.com/signup) and get $150 in credits to give us a try.";
						}
						async function compiledContent$3y() {
							return load$3y().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3y() {
							return (await import('./chunks/index.efec33c8.mjs'));
						}
						function Content$3y(...args) {
							return load$3y().then((m) => m.default(...args));
						}
						Content$3y.isAstroComponentFactory = true;
						function getHeadings$3y() {
							return load$3y().then((m) => m.metadata.headings);
						}
						function getHeaders$3y() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3y().then((m) => m.metadata.headings);
						}

const __vite_glob_0_50 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3y,
  file: file$3y,
  url: url$3y,
  rawContent: rawContent$3y,
  compiledContent: compiledContent$3y,
  default: load$3y,
  Content: Content$3y,
  getHeadings: getHeadings$3y,
  getHeaders: getHeaders$3y
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3x = {"title":"Creating Contextual Video Overlays with TomScottPlus","description":"How the team behind TomScottPlus used Deepgram to analyze YouTube videos in real-time and provide an overlay with Wikipedia links to read. Read more here.","date":"2022-03-17T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1646146501/blog/2022/03/contextual-video-overlay-tomscottplus/cover.jpg","authors":["kevin-lewis"],"category":"project-showcase","tags":["analysis","research"],"seo":{"title":"Creating Contextual Video Overlays with TomScottPlus","description":"How the team behind TomScottPlus used Deepgram to analyze YouTube videos in real-time and provide an overlay with Wikipedia links to read. Read more here."},"shorturls":{"share":"https://dpgr.am/bfad829","twitter":"https://dpgr.am/afe279a","linkedin":"https://dpgr.am/ced2dbb","reddit":"https://dpgr.am/76bdc62","facebook":"https://dpgr.am/0a64ab6"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454018/blog/contextual-video-overlay-tomscottplus/ograph.png"}};
						const file$3x = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/contextual-video-overlay-tomscottplus/index.md";
						const url$3x = undefined;
						function rawContent$3x() {
							return "\r\nThe team behind TomScottPlus used Deepgram to analyze YouTube videos in real-time and provide an contextual overlays with Wikipedia links to read. I sat down with Gwendolen Sellers, Harry Langford, [Maxwell Pettett](https://github.com/StolenCheese), and Tim McGilly to ask them about their project.\r\n\r\n[Tom is an English YouTuber](https://www.youtube.com/TomScottGo) who mostly makes videos about geography, history, science, technology, and linguistics. His style is 'talk to camera' as he explains various nerdy topics, sometimes with cutaways to other experts explaining a concept.\r\n\r\n<YouTube id=\"cdPymLgfXSY\"></YouTube>\r\n\r\nThe team took their inspiration from Tom's YouTube experience, where he shares interesting facts that inspire watchers to learn more. As they talked about learning through YouTube videos, they all agreed that it was cumbersome to learn more about topics mentioned in the videos. They found themselves often pausing videos, opening a browser tab, and searching for a mentioned topic for further reading. That's how the idea for TomScottPlus was born. TomScottPlus is a Chrome extension that aims to make this as seamless as possible by providing clickable overlay for videos with contextual Wikipedia article links in a video overlay as topics are mentioned in the video.\r\n\r\n![A frame from a playing video with Tom speaking. On the left side is a purple pane with a link to Wikipedia article \"Coins of the pound sterling\" with a short page summary underneath.](https://res.cloudinary.com/deepgram/image/upload/v1646146519/blog/2022/03/contextual-video-overlay-tomscottplus/screenshot.jpg)\r\n\r\nWhen a YouTube video is visited, the Chrome extension sends a request to a Python application which downloads the audio and gets a high-quality transcript using the [Deepgram Python SDK](https://developers.deepgram.com/sdks-tools/) and our [utterances](https://developers.deepgram.com/documentation/features/utterances/) feature.\r\n\r\nThe Python application then performed basic Natural Language Processing to look for contextually-relevant words and look for matching data points on Wikipedia (which took several API requests making this quite computationally expensive even with batching). Data points were filtered based on relevance and returned to the Chrome extension, which would display data over the video.\r\n\r\nYou can check out the code for this [project on GitHub](https://github.com/StolenCheese/hackathon2022).\r\n\r\n        ";
						}
						async function compiledContent$3x() {
							return load$3x().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3x() {
							return (await import('./chunks/index.afad21a8.mjs'));
						}
						function Content$3x(...args) {
							return load$3x().then((m) => m.default(...args));
						}
						Content$3x.isAstroComponentFactory = true;
						function getHeadings$3x() {
							return load$3x().then((m) => m.metadata.headings);
						}
						function getHeaders$3x() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3x().then((m) => m.metadata.headings);
						}

const __vite_glob_0_51 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3x,
  file: file$3x,
  url: url$3x,
  rawContent: rawContent$3x,
  compiledContent: compiledContent$3x,
  default: load$3x,
  Content: Content$3x,
  getHeadings: getHeadings$3x,
  getHeaders: getHeaders$3x
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3w = {"title":"How Conversational AI Platforms Utilize Top ASR Tools","description":"Conversational AI platforms are powering changes across industries. Learn more about their benefits and how ASR powers them.","date":"2022-07-25T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981427/blog/conversational-ai-platforms-utilize-top-asr-tools/how-conversational-ai-platforms-utilize-top-asr-to.png","authors":["aimie-ye"],"category":"speech-trends","tags":["call-analytics","conversational-ai"],"seo":{"title":"How Conversational AI Platforms Utilize Top ASR Tools","description":"Conversational AI platforms are powering changes across industries. Learn more about their benefits and how ASR powers them."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981427/blog/conversational-ai-platforms-utilize-top-asr-tools/how-conversational-ai-platforms-utilize-top-asr-to.png"},"shorturls":{"share":"https://dpgr.am/aa8a968","twitter":"https://dpgr.am/ea3ad02","linkedin":"https://dpgr.am/7d67587","reddit":"https://dpgr.am/8288262","facebook":"https://dpgr.am/0631bff"}};
						const file$3w = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/conversational-ai-platforms-utilize-top-asr-tools/index.md";
						const url$3w = undefined;
						function rawContent$3w() {
							return "[Conversational AI](https://deepgram.com/solutions/voicebots/) platforms are on the rise. As more and more businesses move towards conversational interfaces, the need for platforms that can handle these interactions grows. And their impact is being recognized. In this year's [State of Voice Technology 2022 report](https://deepgram.com/state-of-voice-technology-2022/), 54% of respondents said that conversational AI is the most impactful use case in speech tech today. In this blog post, we'll take a look at conversational AI platforms and how they utilize top automatic speech recognition (ASR) tools to delight customers with new experiences. We'll also explore some of the problems these platforms solve and some of the most common use cases for conversational AI, as well as how ASR powers them. Before we start looking at how speech-to-text tools can help power conversational AI, let's take a look at what exactly conversational AI is, and how it helps businesses.\n\n## What is Conversational AI?\n\nConversational AI is a type of artificial intelligence that enables computers to communicate with humans in natural language. Rather than forcing humans to learn what kind of commands the system can accept (like pushing a button for a specific service), conversational AI lets people speak normally, and replies in kind, just as quickly as another human would. Conversational AI is a direct evolution of interactive voice response (IVR) systems, where callers push 1, 2, or 3, or say basic information like their account number as part of their interaction with telephone trees. But conversational AI is far more advanced. By creating a voicebot or a virtual assistant, conversational AI allows the system to respond intuitively and naturally, rather than being constrained to following specific paths down a tree. These technologies allow customers to interact with a company to solve their own problems, rather than requiring that a customer service agent answer every call. Imagine if navigating through a phone tree were as easy as explaining what you needed-that's the goal of conversational AI.\n\n## Why is Conversational AI Beneficial?\n\nThere are a number of ways that conversational AI can benefit businesses today, especially given the complex financial environment. Although it can be challenging to [create a great conversational AI experience](https://blog.deepgram.com/what-makes-a-great-conversational-ai-experience/), the benefits make it absolutely worth the effort. Let's look at some of the benefits that conversational AI can have for customers as well as the direct benefits that it can have for enterprises.\n\n### Customer Benefits of Conversational AI\n\nHere are three of the biggest benefits for customers when it comes to using conversational AI platforms.\n\n#### 1. Shorter Wait Times\n\nOne of the biggest benefits for customers when interacting with a company that's using a conversational AI platform is shorter wait times for issue resolution. Because conversational AI platforms can handle multiple calls at once, and can solve many of the most common reasons people might call, they end up doing a lot of the work, significantly speeding up time to answer. And, even if you do end up needing to speak to a human agent, you'll likely to spend less time waiting there, too, since the conversational AI platform is dealing with lots of the calls that would otherwise bog down human customer service agents.\n\n#### 2. Choice for Engagement\n\nBeing able to contact a company on your terms is important to many customers. You might prefer a specific avenue, whether that's Twitter, documentation, help articles, chatbots, or text messaging. Conversational AI provides another avenue that's quick, easy, and doesn't require talking to another person-while also keeping human agents close at hand should they be needed to solve a complex problem.\n\n#### 3. Faster Issue Resolution\n\nBecause conversational AI platforms can be built to handle the most common issues-and because they can handle multiple calls at the same time-they result in faster resolution times for customers.\n\n### Enterprise Benefits of Conversational AI\n\nThe benefits of conversational AI extend beyond just what customers see, however. There are a number of direct business impacts that using a conversational AI platform can have on your business.\n\n#### 1. Cost Savings\n\nOne of the main benefits of conversational AI is that it allows businesses to automate tasks that would otherwise require human interaction, thus saving costs. This can free up employees to focus on other tasks and improve efficiency. Additionally, conversational AI can help businesses save money on customer service by providing a self-service option for customers.\n\n#### 2. Increased Productivity\n\nConversational AI platforms can power dramatic increases in productivity, as they allow companies to handle more calls, more quickly, with less human customer service agents. With the right infrastructure in place, conversational AI platforms can handle thousands of calls at the same time, allowing for the resolution of more issues more quickly with less human agents.\n\n#### 3. Increased Agent Satisfaction\n\nIn addition to increased productivity, using a conversational AI platform can also increase the satisfaction of the humans who work in a call center. Why? Because the conversational AI platform handles all of the simple, rote issues like taking payments and updating addresses, and leaves humans to deal with the more complex cases that are more challenging-and more satisfying-to solve.\n\n#### 4. Stronger Customer Service\n\nEffective customer service is one of the ways that companies can help retain customers-and, on the flip side, poor customer service can result in customers really disliking your company. By providing conversational AI solutions, customers will come away happier with their interaction with your business, for the reasons discussed in the section above.\n\n#### 5. Better Insights and Knowledge\n\nConversational AI platforms provide a way to capture the conversations that your customer service agents are having with customers, which can provide an excellent starting place for understanding what your customers want and need. By turning to this database, you might be able to identify patterns that help you improve how information is presented on your website, create training materials for human agents, or even build new automated workflows for your conversational AI platform to handle.\n\n<WhitepaperPromo whitepaper=\"latest\"></WhitepaperPromo>\n\n## Top Use Cases for Conversational AI\n\nConversational AI platforms are being used across industries for a variety of different use cases. The below examples are some of the most common, but they're far from the only places that conversational AI is having an impact in business today.\n\n### 1. Call Centers\n\nPerhaps not surprisingly, one of the most common use cases for conversational AI is in call centers. Using a conversational AI platform to help customers do tasks like change their addresses or make a payment without needing to talk to a human-and without needing to punch in numbers on a keypad. Companies like [Elerian AI](https://deepgram.com/case-study-elerian-ai/) have built a conversational AI platform that uses natural language understanding, alongside ASR, to let companies [create a voicebot](https://offers.deepgram.com/evolution-of-voicebots-panel-webinar-on-demand) to easily answer customer queries.\n\n### 2. Healthcare\n\nThe healthcare industry is using conversational AI to automate common tasks to help doctors, nurses, billing, and patients all stay in sync. For example, conversational AI platforms can be used to follow up with patients after a procedure to check in on how they're doing, if they're adhering to medication guidelines, scheduling needed follow-ups, as well as gathering any other information doctors might need. Conversational AI can also be used to track mental health metrics like stress and depression with a simple phone interview. [Outbound AI](https://www.outbound.ai/) is a company that's pioneering the use of conversational AI for healthcare, offering solutions that support medical billing, streamline systems to eliminate duplicate work, and bring conversational analytics to the healthcare industry.\n\n### 3. Virtual Shopping Assistants\n\nVirtual shopping assistants are another domain where conversational AI platforms are being used. Companies like [Vocinity](https://www.vocinity.com/) have created virtual shopping assistants that interact with customers just like a human employee would, whether online or in stores, providing a personalized shopping experience for customers.\n\n### 4. Food Ordering\n\nOne use case you might not have considered for conversational AI platforms is food ordering. For example, [Valyant AI](https://valyant.ai/) has created a platform that lets customers order in drive-thrus automatically. [Kea](https://kea.ai/) is a similar system that works via telephone calls to take orders and even upsell customers.\n\n## End-to-End Deep Learning Automatic Speech Recognition for Conversational AI Platforms\n\nIn order to understand and respond to humans, conversational AI platforms need some way to interpret human speech. This is where automatic speech recognition (ASR) comes in. ASR is a technology that enables computers to convert spoken language into text which, with the help of tools like natural language understanding, can be used to determine what a person is saying and respond in kind. There are a number of different ASR solutions available, each with its own advantages and disadvantages. The most important thing to consider when choosing an ASR solution is making sure you're using one that's well-suited for the task at hand.\n\nIn the case of conversational AI, that means using a speech-to-text solution that can provide responses in real time, and not every ASR provider is able to provide that kind of turnaround on transcription. If it takes seconds or minutes to reply to someone using a conversational AI platform, you've left the realm of anything that seems conversational. The best way to ensure that you're getting a fast turnaround is to use an ASR solution like Deepgram that's [built on end-to-end deep learning](https://blog.deepgram.com/deep-learning-asr-for-business/). Deep learning ASR systems are faster than other options, providing responses quickly enough that the interactions truly feel like a conversation.\n\nAnother advantage of end-to-end deep learning speech-to-text systems for conversational AI platforms is that it's quick and easy to train a new model based on your data. That means you have a model that knows exactly what your audio looks like, including things like background noise and specific jargon or industry terms, and can far exceed generic, out-of-the-box ASR, which [will never be good enough for conversational AI](https://blog.deepgram.com/generic-asr-will-never-be-accurate-enough-for-conversational-ai/).\n\n## Benefits of Top ASR Tools for Conversational AI\n\nThere are a number of benefits that the top ASR solutions for conversational AI can bring to your business. Overall, ASR can be a powerful tool for conversational AI platforms. It can help conversational AI platforms understand human speech, power other features of the platform, and improve the accuracy of the platform.\n\n### 1. Speech Understanding\n\nASR can help conversational AI platforms understand human speech. This is the most important thing for conversational AI platforms, as they need to be able to understand what users are saying in order to respond accordingly-in fact, this is the lynchpin that holds conversational AI platforms together, and it call comes from having an accurate speech-to-text system that will work for conversational AI.\n\n### 2. Call Transcription\n\nASR can be used to power other features of conversational AI platforms as well. For example, ASR can transcribe customer service calls or sales calls. This transcript can then be analyzed to help improve the quality of the conversation by identifying areas for improvement or training, or even provide opportunities for upselling.\n\n### 3. Handling Complex Audio\n\nASR solutions can help conversational AI platforms handle different accents, regional dialects, or noisy environments. This is important because conversational AI platforms need to be able to understand speech from a variety of different people. As noted above, using an end-to-end deep learning ASR solution can help you in this regard by letting you tailor a model for your specific audio data.\n\n### 4. Searchable Data\n\nUsing ASR with conversational AI also lets you create a database of transcribed audio of the conversations that you're having with your customers. This can be used to power internal initiatives, from additional training, to streamlining processing, to customer education-the options are nearly endless once you have a large database of these conversations to work with.\n\n### 5. Improved Accuracy\n\nASR tools can help improve the accuracy of conversational AI platforms by providing more transcribed data for the platform to use. And with the latest advances in ASR models, the accuracy can be high even for rare words. For example, Deepgram's enhanced model has a broader vocabulary than other speech-to-text models and can transcribe words it hasn't seen before.\n\n### 6. Custom Model Training\n\nAnother advantage of using an ASR solution that's built on end-to-end deep learning is that its flexibility lets you train a custom model quickly and easily thanks to [transfer learning](https://blog.deepgram.com/transfer-learning-spanish-portuguese/). Your business might have lots of unique vocabulary and acronyms that an off-the-shelf model struggles with. By training a custom model on your own data, you can drastically improve the quality of transcripts that the model outputs, as it now knows your lingo.\n\n## Wrapping up\n\nIf you're looking for an ASR solution to power your conversational AI platform-or looking for a speech-to-text tool for another project-please [reach out with any questions you might have](https://deepgram.com/contact-us/) and we can help you decide if Deepgram is the right fit for you. If you'd rather dive in and get started yourself, you can [sign up for Console](https://console.deepgram.com/signup) and get $150 in free credits to give us a try, or check out our [ASR comparison tool](https://deepgram.com/asr-comparison/) to get a sense of how we stack up against Big Tech-no signup needed.";
						}
						async function compiledContent$3w() {
							return load$3w().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3w() {
							return (await import('./chunks/index.7b29711d.mjs'));
						}
						function Content$3w(...args) {
							return load$3w().then((m) => m.default(...args));
						}
						Content$3w.isAstroComponentFactory = true;
						function getHeadings$3w() {
							return load$3w().then((m) => m.metadata.headings);
						}
						function getHeaders$3w() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3w().then((m) => m.metadata.headings);
						}

const __vite_glob_0_52 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3w,
  file: file$3w,
  url: url$3w,
  rawContent: rawContent$3w,
  compiledContent: compiledContent$3w,
  default: load$3w,
  Content: Content$3w,
  getHeadings: getHeadings$3w,
  getHeaders: getHeaders$3w
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3v = {"title":"Conversational Intelligence Podcast with Scott Stephenson","description":"Listen to insights from our CEO, Scott Stephenson, on a podcast discussing Conversational Intelligence","date":"2021-04-30T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981368/blog/conversational-intelligence-podcast-with-scott-stephenson/convo-intelligence-podcast%402x.jpg","authors":["call-tracking-metrics"],"category":"speech-trends","tags":["call-analytics","education"],"seo":{"title":"Conversational Intelligence Podcast with Scott Stephenson","description":"Listen to insights from our CEO, Scott Stephenson, on a podcast discussing Conversational Intelligence"},"shorturls":{"share":"https://dpgr.am/9a384df","twitter":"https://dpgr.am/7e03568","linkedin":"https://dpgr.am/fd5cb63","reddit":"https://dpgr.am/e5a6fec","facebook":"https://dpgr.am/930c351"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981368/blog/conversational-intelligence-podcast-with-scott-stephenson/convo-intelligence-podcast%402x.jpg"}};
						const file$3v = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/conversational-intelligence-podcast-with-scott-stephenson/index.md";
						const url$3v = undefined;
						function rawContent$3v() {
							return "\r\nOur CEO, Scott Stephenson, is featured on the [Smart Route](https://www.calltrackingmetrics.com/blog/resources-o/podcast/introducing-our-new-podcast-series-smart-route/) podcast, where he breaks down the concept of what is \"conversation intelligence\"-how it works, what kind of value it can add to your organization, and how businesses everywhere can start leveraging this technology to pull insights from their daily conversations with customers and understand, more intimately, what drives them in order to refine their marketing, sales, and service strategy. Thanks to our partner [CallTrackingMetrics](http://www.calltrackingmetrics.com) for inviting us. [Check it out!](https://anchor.fm/smartroute/episodes/What-is-Conversation-Intelligence-eveifk/a-a5b230b)\r\n";
						}
						async function compiledContent$3v() {
							return load$3v().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3v() {
							return (await import('./chunks/index.6a163209.mjs'));
						}
						function Content$3v(...args) {
							return load$3v().then((m) => m.default(...args));
						}
						Content$3v.isAstroComponentFactory = true;
						function getHeadings$3v() {
							return load$3v().then((m) => m.metadata.headings);
						}
						function getHeaders$3v() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3v().then((m) => m.metadata.headings);
						}

const __vite_glob_0_53 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3v,
  file: file$3v,
  url: url$3v,
  rawContent: rawContent$3v,
  compiledContent: compiledContent$3v,
  default: load$3v,
  Content: Content$3v,
  getHeadings: getHeadings$3v,
  getHeaders: getHeaders$3v
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3u = {"title":"How To Transcribe Your Podcast with Python","description":"Learn how to create lovely readable transcripts with Python for your podcasts, both before and after publishing.","date":"2022-08-24T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1660841102/blog/2022/08/create-readable-transcripts-for-podcasts/cover.jpg","authors":["kevin-lewis"],"category":"tutorial","tags":["python"],"seo":{"title":"How To Transcribe Your Podcast with Python","description":"Learn how to create lovely readable transcripts with Python for your podcasts, both before and after publishing."},"shorturls":{"share":"https://dpgr.am/f66d485","twitter":"https://dpgr.am/d6c2f7c","linkedin":"https://dpgr.am/9dfba57","reddit":"https://dpgr.am/ef300d4","facebook":"https://dpgr.am/e735fb2"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454116/blog/create-readable-transcripts-for-podcasts/ograph.png"}};
						const file$3u = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/create-readable-transcripts-for-podcasts/index.md";
						const url$3u = undefined;
						function rawContent$3u() {
							return "\nIf you have a podcast, or want to analyze podcasts, this is the post for you! We'll cover how to transcribe your local podcast recordings, those which are hosted online, and the latest episodes from podcast RSS feeds.\n\n## Before You Start\n\nYou must have Python installed on your machine - I'm using Python 3.10 at the time of writing. You will also need a Deepgram API Key - [get one here](https://console.deepgram.com/signup?jump=keys).\n\nCreate a new directory and navigate to it in your terminal. [Create a virtual environment](https://blog.deepgram.com/python-virtual-environments/) with `python3 -m venv virtual_env` and activate it with `source virtual_env/bin/activate`. Install dependencies with `pip install deepgram_sdk asyncio python-dotenv feedparser`.\n\nOpen the directory in a code editor, and create an empty `.env` file. Take your Deepgram API Key, and add the following line to `.env`:\n\n    DEEPGRAM_API_KEY=\"replace-this-bit-with-your-key\"\n\n## Dependency and File Setup\n\nCreate an empty `script.py` file and import the dependencies:\n\n```py\nimport asyncio\nimport os\nfrom dotenv import load_dotenv\nfrom deepgram import Deepgram\nimport feedparser\n```\n\nLoad values from the `.env` file and store the Deepgram key into a variable:\n\n```py\nload_dotenv()\nDEEPGRAM_API_KEY = os.getenv('DEEPGRAM_API_KEY')\n```\n\nFinally, set up a `main()` function that is executed automatically when the script is run:\n\n```py\nasync def main():\n    print('Hello world')\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n\n## Generate a Transcript\n\nDeepgram can transcribe both hosted and local files, and in the context of podcasting, files may also be contained within an RSS feed.\n\nInside of the `main()` function, initialize the Deepgram Python SDK with your API Key:\n\n```py\ndeepgram = Deepgram(DEEPGRAM_API_KEY)\n```\n\n### Option 1: Hosted Files\n\nTo transcribe a hosted file, provide a `url` property:\n\n```py\nurl = 'https://traffic.megaphone.fm/GLT8627189710.mp3?updated=1655947230'\nsource = { 'url': url }\ntranscription_options = { 'punctuate': True }\nresponse = await deepgram.transcription.prerecorded(source, transcription_options)\nprint(response)\n```\n\n### Option 2: RSS Feed\n\nTo transcribe the latest podcast episode, use `feedparser` and select the first returned item:\n\n```py\nrss = feedparser.parse('https://feeds.npr.org/510318/podcast.xml')\nurl = rss.entries[0].enclosures[0].href\nsource = { 'url': url }\ntranscription_options = { 'punctuate': True }\nresponse = await deepgram.transcription.prerecorded(source, transcription_options)\nprint(response)\n```\n\n### Option 3: Local File\n\n```py\nwith open('icymi.mp3', 'rb') as audio:\n    source = { 'buffer': audio, 'mimetype': 'audio/mp3' }\n    transcription_options = { 'punctuate': True }\n    response = await deepgram.transcription.prerecorded(source, transcription_options)\n    print(response)\n```\n\nNote that once you open the file, all further lines must be indented to gain access to the `audio` value.\n\n## Speaker Detection and Paragraphing\n\nThe generated transcript is pretty good, but Deepgram has two additional features which make a huge difference when creating podcast transcripts - [diarization (speaker detection)](https://developers.deepgram.com/documentation/features/diarize/) and [paragraphs](https://developers.deepgram.com/documentation/features/paragraphs/).\n\nUpdate your `transcription_options`:\n\n```py\ntranscription_options = { 'punctuate': True, 'diarize': True, 'paragraphs': True }\n```\n\nReplace `print(response)` with the following to access a nicely-formatted transcript:\n\n```py\ntranscript = response['results']['channels'][0]['alternatives'][0]['paragraphs']['transcript']\nprint(transcript)\n```\n\n![A terminal shows three lines of text which have space between them. Each is started by Speaker One, Two or Three. Then the words they said.](https://res.cloudinary.com/deepgram/image/upload/v1660841103/blog/2022/08/create-readable-transcripts-for-podcasts/final.png)\n\n## Saving Transcript to a File\n\nReplace `print(transcript)` with the following to save a new text file with the output:\n\n```py\nwith open('transcript.txt', 'w') as f:\n  f.write(transcript)\n```\n\n## Wrapping Up\n\nYou can find the full code snippet below. If you have any questions, feel free to get in touch.\n\n```py\nimport asyncio\nimport os\nfrom dotenv import load_dotenv\nfrom deepgram import Deepgram\nimport feedparser\n\nload_dotenv()\nDEEPGRAM_API_KEY = os.getenv('DEEPGRAM_API_KEY')\n\nasync def main():\n    print('Hello world')\n    deepgram = Deepgram(DEEPGRAM_API_KEY)\n\n    # Option 1: Hosted File\n    url = 'your-hosted-file-url'\n    source = { 'url': url }\n\n    # Option 2: Latest Podcast Feed Item\n    # rss = feedparser.parse('rss-feed-url')\n    # url = rss.entries[0].enclosures[0].href\n    # source = { 'url': url }\n\n    # Option 3: Local File (Indent further code)\n    # with open('florist.mp3', 'rb') as audio:\n    #     source = { 'buffer': audio, 'mimetype': 'audio/mp3' }\n\n    transcription_options = { 'punctuate': True, 'diarize': True, 'paragraphs': True }\n    response = await deepgram.transcription.prerecorded(source, transcription_options)\n\n    transcript = response['results']['channels'][0]['alternatives'][0]['paragraphs']['transcript']\n\n    with open('transcript.txt', 'w') as f:\n        f.write(transcript)\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n\n        ";
						}
						async function compiledContent$3u() {
							return load$3u().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3u() {
							return (await import('./chunks/index.4f65c9c9.mjs'));
						}
						function Content$3u(...args) {
							return load$3u().then((m) => m.default(...args));
						}
						Content$3u.isAstroComponentFactory = true;
						function getHeadings$3u() {
							return load$3u().then((m) => m.metadata.headings);
						}
						function getHeaders$3u() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3u().then((m) => m.metadata.headings);
						}

const __vite_glob_0_54 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3u,
  file: file$3u,
  url: url$3u,
  rawContent: rawContent$3u,
  compiledContent: compiledContent$3u,
  default: load$3u,
  Content: Content$3u,
  getHeadings: getHeadings$3u,
  getHeaders: getHeaders$3u
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3t = {"title":"Building a Cross Platform NuGet Package","description":"Learning to build a NuGet package by building a .NET SDK for the Deepgram API, while ensuring it's compatible with as many versions of the .NET Framework and as many platforms as possible.","date":"2021-12-23T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1639685643/blog/2021/12/cross-platform-nuget-dotnet/Building-Cross-Platform-NuGet-Package%402x.jpg","authors":["michael-jolley"],"category":"tutorial","tags":["dotnet","sdk"],"seo":{"title":"Building a Cross Platform NuGet Package","description":"Learning to build a NuGet package by building a .NET SDK for the Deepgram API, while ensuring it's compatible with as many versions of the .NET Framework and as many platforms as possible."},"shorturls":{"share":"https://dpgr.am/80c1eaf","twitter":"https://dpgr.am/63333a7","linkedin":"https://dpgr.am/22512d8","reddit":"https://dpgr.am/0688948","facebook":"https://dpgr.am/7764cea"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453827/blog/cross-platform-nuget-dotnet/ograph.png"}};
						const file$3t = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/cross-platform-nuget-dotnet/index.md";
						const url$3t = undefined;
						function rawContent$3t() {
							return "\r\nI love the .NET ecosystem. My career started writing classic ASP applications in\r\nVisual Basic and transitioned to C# with .NET 2.0. I remember building my first\r\nASP.NET MVC application and feeling like I had just performed some kind of magic.\r\n\r\nOnce I joined Deepgram, I was very excited about the prospect of building a\r\n.NET SDK from scratch. During the process, I realized that there are certain\r\nthings to consider when building a .NET library to make it as accessible as\r\npossible to developers building with different versions of the .NET Framework\r\nand various platforms.\r\n\r\n> Happy holidays! This post is a contribution to [C# Advent 2021](https://www.csadvent.christmas/). Be\r\n> sure to visit and read all the excellent content focused on C# and the .NET community.\r\n\r\n## Use Case\r\n\r\nBefore we get too deep in the how-to, let's talk about the need I was trying to\r\naddress. Today, Deepgram has two fully supported SDKs; Node.js & Python.\r\nLike .NET, both are great languages with solid ecosystems, but I wanted to\r\nprovide that first-class citizen experience to my beloved .NET developers. 😁\r\n\r\nAfter a bit of planning, I landed on the following requirements for the SDK:\r\n\r\n*   Enable access to all the publicly available endpoints of the [Deepgram API](https://developers.deepgram.com/api-reference/)\r\n*   Allow users to provide their own logging by using the [LoggerFactory](https://docs.microsoft.com/en-us/dotnet/api/microsoft.extensions.logging.loggerfactory) provided in the `Microsoft.Extensions.Logging` library\r\n*   Ensure the library was accessible to as many frameworks & platforms as reasonably practical\r\n\r\n## Calling the API\r\n\r\nMost of the Deepgram API is accessible via HTTP requests, so the library handles\r\nthose as you'd expect with an HTTPClient. Requests to transcribe audio in\r\nreal-time are handled via WebSockets. Creating a reusable and well-managed\r\nWebSocket client was more challenging because I couldn't find any real-world\r\nexamples in the documentation. In most cases, the documentation would show\r\nconnecting to a socket, sending a message, receiving a message, and then\r\ndisconnecting. In the real world, I needed a client that would connect, then\r\nsend & receive messages on-demand, and disconnect at a later time that I\r\ndecide.\r\n\r\n## Bring Your Own Logging\r\n\r\nLogging, like tests, are one of those features that developers like to bypass.\r\nFor years, my projects were scarce on logging and, when included, it was\r\noften added as an afterthought. That said, I was very impressed by one of my\r\ncolleagues, [Steve Lorello](https://twitter.com/slorello), at Vonage, who worked\r\non their .NET SDK. Not only did he do a great job with logging throughout the\r\nSDK, he utilized the `LoggerFactory` to provide the ability for developers to\r\nchoose their own logging solution. I contacted him as I was getting started to\r\nwarn him that I was blatantly plagiarizing his work. 😂\r\n\r\nLuckily, Steve was super gracious and offered to help with any questions.\r\nSeriously, if you aren't following Steve on [Twitter](https://twitter.com/slorello),\r\nyou should. He's doing outstanding work at Redis now.\r\n\r\n## Microsoft's Cross-Platform Recommendations\r\n\r\nMicrosoft recommends starting with a `netstandard2.0` target. Since we only plan\r\non supporting platforms & frameworks that can use .NET Standard 2.0 or later,\r\nI started reviewing any dependencies I had added intending to strip it down\r\nto only those compliant with the .NET Standard 2.0.\r\n\r\nI did notice in [Microsoft's recommendations](https://docs.microsoft.com/en-us/dotnet/standard/library-guidance/cross-platform-targeting)\r\nthat in some cases, you may have to shield your users depending on their platform\r\nand framework, as in the example below:\r\n\r\n```csharp\r\npublic static class GpsLocation\r\n{\r\n    // This project uses multi-targeting to expose device-specific APIs to .NET Standard.\r\n    public static async Task<(double latitude, double longitude)> GetCoordinatesAsync()\r\n    {\r\n#if NET461\r\n        return CallDotNetFramworkApi();\r\n#elif WINDOWS_UWP\r\n        return CallUwpApi();\r\n#else\r\n        throw new PlatformNotSupportedException();\r\n#endif\r\n    }\r\n\r\n    // Allows callers to check without having to catch PlatformNotSupportedException\r\n    // or replicating the OS check.\r\n    public static bool IsSupported\r\n    {\r\n        get\r\n        {\r\n#if NET461 || WINDOWS_UWP\r\n            return true;\r\n#else\r\n            return false;\r\n#endif\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nFortunately, our SDK didn't require these types of workarounds.\r\n\r\n## Publishing to Nuget with GitHub Actions\r\n\r\nBecause I created the library in Visual Studio 2022 using the new class library\r\ntemplates, the configuration for building a NuGet package was as painless as\r\nproviding details like the name, description, etc. of the package. I had already\r\ncreated a [GitHub Action](https://github.com/deepgram-devs/deepgram-dotnet-sdk/blob/main/.github/workflows/CI.yml)\r\nto perform CI tasks, so I decided to add another GitHub Action to deploy the\r\npackage to NuGet.org when a new version was released.\r\n\r\nThe Continuous Deployment (CD) action contains two jobs: `build` and `publish`.\r\nThe `build` job creates the NuGet package, while the `publish` job\r\nhandles uploading the generated package to NuGet.org. The `publish` job will\r\nonly run if the `build` job completes successfully. You can review the entire\r\nCD workflow file [here](https://github.com/deepgram-devs/deepgram-dotnet-sdk/blob/main/.github/workflows/CD.yml).\r\n\r\n### Triggering a New Release\r\n\r\nOnce we're ready to release a new version of the SDK, we create a new GitHub\r\nrelease. The CD action is triggered when that new release is published. Once\r\nit begins, we use the `actions/checkout@v2` to check out the code based on the\r\nsha associated with the release.\r\n\r\n### Restoring Dependencies\r\n\r\nOnce the repository is retrieved, we install .NET 6 and install any\r\nrequired dependencies from NuGet.\r\n\r\n### Identifying Version Number\r\n\r\nOnce the dependencies are installed, the next step pulls the version number from\r\nthe GitHub release and outputs that value so that subsequent steps can access\r\nit.\r\n\r\n### Building & Packaging the SDK\r\n\r\nNext, the action calls `dotnet pack` and passes various parameters to configure\r\nthe build and packing process to ensure we've got the cleanest output\r\npossible.\r\n\r\n#### --configuration\r\n\r\nThe `--configuration` parameter tells the build process to run in `Release`\r\nmode rather than `Debug` mode.\r\n\r\n#### --no-restore\r\n\r\nBecause we previously ran `dotnet restore` in the action, there's no need to\r\nrestore packages from Nuget during the build process. The `--no-restore`\r\nparameter tells the build process to skip this step to save time.\r\n\r\n#### --output\r\n\r\nOnce we build the SDK with the various targets, we want that clean output saved\r\nto a specific directory. In our case, the `./dist` directory.\r\n\r\n#### -p\r\n\r\nThe `-p` parameter is used to pass additional parameters to the build process.\r\nIn our case, we are sending a parameter called `Version` and set it to the value of\r\nthe `get_version` step, which returned our version number based on the GitHub\r\nrelease.\r\n\r\n### Archiving Packing Artifacts\r\n\r\nThe generated package should live in the ./dist directory when the build and\r\npacking process completes. We use the `actions/upload-artifact@v2` action to\r\nsave the contents of that directory as an artifact of the action with the name\r\n`dist`. We'll access this artifact in the next step of the process.\r\n\r\n### Publishing to NuGet\r\n\r\nWith the package archived as an artifact, the `publish` job will send it\r\nto NuGet.\r\n\r\n#### Downloading Artifacts\r\n\r\nThe publish job will first download the artifact named dist that was created in\r\nthe build job. These artifacts are downloaded to the `./dist` directory.\r\n\r\n#### Pushing the Package\r\n\r\nNext, the job calls `dotnet nuget push` to send any .nupkg file in the `./dist`\r\ndirectory to NuGet.org. This requires an access token that NuGet provides.\r\nFor securities sake, we store that token in the repositories secrets and access\r\nit via `${{secrets.NUGET_API_KEY}}`.\r\n\r\nWith that step complete, the action is finished and stops. NuGet will review\r\nthe uploaded package and release it to the marketplace automatically.\r\n\r\n## Announcing the Deepgram .NET SDK\r\n\r\nOf course, with all this work completed, we can announce the new\r\n[Deepgram .NET SDK](https://www.nuget.org/packages/Deepgram/). Try it out, and\r\nlet us know if it helps you get up and running with Deepgram even faster.\r\n\r\nAlso, the entire project has been\r\n[built in the open on GitHub](https://github.com/deepgram-devs/deepgram-dotnet-sdk),\r\nand we'd love your input, feedback, and contributions to make it even better!\r\nHappy building!\r\n\r\n        ";
						}
						async function compiledContent$3t() {
							return load$3t().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3t() {
							return (await import('./chunks/index.6a8cc963.mjs'));
						}
						function Content$3t(...args) {
							return load$3t().then((m) => m.default(...args));
						}
						Content$3t.isAstroComponentFactory = true;
						function getHeadings$3t() {
							return load$3t().then((m) => m.metadata.headings);
						}
						function getHeaders$3t() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3t().then((m) => m.metadata.headings);
						}

const __vite_glob_0_55 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3t,
  file: file$3t,
  url: url$3t,
  rawContent: rawContent$3t,
  compiledContent: compiledContent$3t,
  default: load$3t,
  Content: Content$3t,
  getHeadings: getHeadings$3t,
  getHeaders: getHeaders$3t
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3s = {"title":"How Randall-Reilly Uses Deepgram to Match Drivers to a Leading Ridesharing Company","description":"Randall-Reilly is the recruiting powerhouse for fast growing companies. Learn about how they use Deepgram here.","date":"2018-08-31T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981312/blog/customer-story-rideshare-smartrhino-deepgram/how-randall-reilly-uses-dg%402x.jpg","authors":["scott-stephenson"],"category":"dg-insider","tags":["customer-story"],"seo":{"title":"How Randall-Reilly Uses Deepgram to Match Drivers to a Leading Ridesharing Company","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981312/blog/customer-story-rideshare-smartrhino-deepgram/how-randall-reilly-uses-dg%402x.jpg"},"shorturls":{"share":"https://dpgr.am/ddf7163","twitter":"https://dpgr.am/176e9af","linkedin":"https://dpgr.am/9b0d80a","reddit":"https://dpgr.am/9a076a2","facebook":"https://dpgr.am/9598d32"}};
						const file$3s = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/customer-story-rideshare-smartrhino-deepgram/index.md";
						const url$3s = undefined;
						function rawContent$3s() {
							return "Randall-Reilly is *the* recruiting powerhouse for fast growing companies. They utilize more than 30 web properties, an assortment of strategic media channels, and in-house high-tech call center campaigns to rapidly find, promote, and assist candidates through the recruiting and partnering processes of innovative companies. By engaging the services of Randall-Reilly, one of the largest ridesharing companies can scale their partnering process without losing focus on the important things - finding the right people.\n\n## Challenge: Discovering that Unique Gem\n\nFinding the right people starts simply - often with a phone call. But when you handle an enormous amount of calls, you're faced with a massive challenge: sorting through millions of minutes of recorded audio in hopes of discovering a unique gem buried within complex and diverse conversations.\n\nRandell-Reilly analyzes millions of minutes of phone interviews in an effort to identify specific call characteristics consistent with the ridesharing company's quest to partner with high quality safe drivers.\n\n> \"They receive an enormous number of driver-partner applications per year. Analyzing their phone contact processes is key to finding amazing safe drivers. Working with Deepgram is the most effective way to do it.\" —*Dennis Evanson, Head of Quality Assurance, Randall-Reilly*\n\n## Better Speech Processing, Better Outcomes\n\n> \"We tried Google's Cloud Speech API and Nuance Dragon, and investigated several other products from companies including Amazon, Tethr, Prosodica and Zoom. Deepgram had the best accuracy and program by far.\" —Brett Evanson, CTO, Randall-Reilly\n\nAfter facing inaccurate results produced by leading providers, and struggling through cumbersome process designs, Randall-Reilly turned to Deepgram. They found that **last generation solutions simply couldn't offer the services, accuracy, design, or speed needed to review every single call**. Deepgram's neural networks, however, easily provided the scale, speed, and accuracy that they required to programmatically analyze phone interviews.\n\n> Before Deepgram, Randall-Reilly found that when it audited its speech-to-text query results from other vendors, only 80% of the search results produced confirmable matches. <mark>*Using Deepgram they were able to increase audit-confirmed results to levels exceeding 95% accuracy.*</mark>\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-state-of-voice-2022\"></WhitepaperPromo>\n\n## Faster, Better, and More Reliable Compliance\n\n> \"The reason we went with Deepgram is because it's a way that we can touch and have some level of QA assessment on every single call. **It gives us the ability to scale compliance operations without hiring a hundred people**. We're able to get more comfortable with compliance in a more flexible and efficient manner; rather than wading through speech-to-text and its inherent inaccuracies, Deepgram's approach is a faster and more accurate assessment.\"\n\nWhile speaking with potential driver-partner candidates, Randell-Reilly's agents guide potential drivers through rigorous approval process. Deepgram's fast and accurate assessments of those conversations provides an optimized method for Randall-Reilly to gain a suitable comfort level concerning both the depth and effectiveness of its compliance efforts. Being able to quickly and reliably measure how well clients do at meeting both client and regulatory standards puts them miles ahead of other standard assessment methods. Deepgram is a crucial part of their process in streamlining both regulatory and client-mandated processes.\n\n> \"As a company we are very concerned that we are doing and saying what our clients want, and doing it in the most efficient manner. Deepgram helps us track that effort efficiently and speedily. We love working with Deepgram. They're crucial in our automated analysis of every recorded interview, and in our capacity to make sure that \\[our client's] needs are being met.\" —Corbin McCabe, Randell-Reilly, General Manager\n\n[Find out](https://deepgram.com/) how you can achieve what Randell-Reilly did.";
						}
						async function compiledContent$3s() {
							return load$3s().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3s() {
							return (await import('./chunks/index.b8135a43.mjs'));
						}
						function Content$3s(...args) {
							return load$3s().then((m) => m.default(...args));
						}
						Content$3s.isAstroComponentFactory = true;
						function getHeadings$3s() {
							return load$3s().then((m) => m.metadata.headings);
						}
						function getHeaders$3s() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3s().then((m) => m.metadata.headings);
						}

const __vite_glob_0_56 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3s,
  file: file$3s,
  url: url$3s,
  rawContent: rawContent$3s,
  compiledContent: compiledContent$3s,
  default: load$3s,
  Content: Content$3s,
  getHeadings: getHeadings$3s,
  getHeaders: getHeaders$3s
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3r = {"title":"Stanford Moves Education Forward with Deepgram","description":"With Deepgram's help, researchers at Stanford's Graduate School of Education have embraced natural language processing and machine learning to elegantly speed up research and make it more useful for teachers and students.","date":"2018-10-19T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981936/blog/customer-story-stanford-moves-education-forward-with-deepgram/placeholder-post-image%402x.jpg","authors":["natalie-rutgers"],"category":"dg-insider","tags":["education"],"seo":{"title":"Stanford Moves Education Forward with Deepgram","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981936/blog/customer-story-stanford-moves-education-forward-with-deepgram/placeholder-post-image%402x.jpg"},"shorturls":{"share":"https://dpgr.am/7c2b833","twitter":"https://dpgr.am/f7d8118","linkedin":"https://dpgr.am/44aca79","reddit":"https://dpgr.am/2a1fb73","facebook":"https://dpgr.am/3bbec6a"}};
						const file$3r = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/customer-story-stanford-moves-education-forward-with-deepgram/index.md";
						const url$3r = undefined;
						function rawContent$3r() {
							return "\r\nEducational research is important for understanding how to enhance educational outcomes and experiences for everyone. To do this, quantitative and qualitative analyses can help researchers determine what goes into effective teaching. Unfortunately, quantitative and qualitative studies in classrooms are also notoriously time intensive and laborious. And, of course, the bigger the study, the longer it takes. Or, does it? With Deepgram's help, researchers at Stanford's Graduate School of Education have embraced natural language processing and machine learning to elegantly speed up research and make it more useful for teachers and students.\r\n\r\n### The impact of MOOCs\r\n\r\nIn recent years, teachers have increasingly turned to MOOCs, massive online open courses, as a way to \"[take responsibility for their own professional learning](https://www.forbes.com/sites/skollworldforum/2013/06/10/moocs-for-teachers-theyre-learners-too/#4a877ebf160c)\" and learn about new tools and techniques. However, it's difficult to quantify the impact of these professional development resources on teachers and their students. Are MOOCs actually useful for teacher education? To answer this question, a team of researchers at Stanford is asking K-12 science teachers in a professional development MOOC to audio-record their own teaching for a couple of months. This data will enable the research team to ask:\r\n\r\n*   When do teachers have opportunities to use tools and techniques presented in the MOOC?\r\n*   How do they fit these tools and techniques into their own repertoires of professional practices?\r\n\r\nThe answers to such questions could indicate what sort of impact MOOCs do (and don't) have as a medium for teacher education, and can help improve the design of future MOOCs to make them more useful for teachers.\r\n\r\n### An automated solution to a tedious task\r\n\r\nAttempting to answer those questions presents a time-consuming obstacle: transcribing hundreds of hours of classroom audio for analysis. Traditionally this task would be passed on to students for manual transcription or outsourced to a transcription service. However, this process has its disadvantages. As Quentin Sedlacek, a PhD student and one of the principal investigators on the study, explains:\r\n\r\n> \"We really want to be able to share our findings with the same teachers who are participating in the study. They're helping us out, and we want to help them--but **if it takes us six or seven months to complete our research, the school year's practically over. Teachers won't have time to actually use our findings**, at least not in time to benefit the specific cohort of students who participated in the study.\"\r\n\r\nHowever, <mark>advances in computational methods and machine learning have the potential to help resolve this conundrum</mark>. Aaron Alvero, a PhD student and member of the research team, has experience using such methods to quickly uncover hidden patterns in large corpora of educational data. To use such methods for this project, however, the team first needed a way to rapidly convert their audio-recordings into reliable transcriptions of text. Enter Klint Kanopka, another PhD student and research team member with expertise in natural language processing and computational text analysis. Kanopka believed he could find an automated solution to what he describes as the \"miserable task\" of transcribing and analyzing months' worth of audio recordings. After evaluating a number of [automatic speech recognition](https://blog.deepgram.com/what-is-asr/) services, Kanopka said \"<mark>Deepgram became a clear winner for what we wanted to do</mark>\". Given the sheer quantity of recordings his team hopes to collect, they needed a consistently accurate tool that can speed up the process of transcription.\r\n\r\n> \"Google dumped out a big, disgusting JSON file with quality that wasn't good enough. CMU Sphinx leaned heavily on a language model and totally distorted entire paragraphs. Nuance Dragon Dictation outputted unformatted blocks of text with big chunks where it didn't transcribe anything.\" <mark>**\"For our use case, Deepgram was first accuracy-wise and produced, by far, the easiest transcriptions to work with.\"**</mark>\r\n\r\n### Automating processes and saving money\r\n\r\nKanopka found that using Deepgram sped up research in a number of ways. Contrasted with Google, which was \"the hardest of any of them to actually get up and running\" and Nuance Dragon which was tied to a single machine, he said \"**Deepgram is _really_ easy to use**.\" Using both Google Cloud and PocketSphinx required him to write programs to interface with the APIs, but **Deepgram was functioning right out of the box**. That enables the team the freedom to interact with Deepgram's in-browser and API options without hassle and in any location.\r\n\r\n###### Whereas CMU Sphinx or Nuance Dragon took up to several hours for each file to transcribe on his local machine, Deepgram's cloud platform allows him to batch upload multiple files and be done in under a minute.\r\n\r\nBy choosing Deepgram, Kanopka not only significantly shortens transcription time but also reduces costs.\r\n\r\n> \"Saving us a bunch of money is a huge consideration because it makes it possible to recruit more teachers for the study.\"\r\n\r\nAs researchers at one of the top education schools in the country, Kanopka and his team have the potential to play a big role in how teachers go about securing the tools to improve their student's educations. With Deepgram, they can automate research processes and maintain focus on their goal of pushing education forward.\r\n\r\n> \"Finding technologies that make our research more seamless and easier to implement is incredibly exciting. It won't be long before we're able to conduct classroom research do analysis fast enough to share findings with teachers in the same school-year with the same set of students. Being able to use something like real-time speech recognition in a classroom setting will be a game-changer for research. We'll be using tools like this more and more as we move forward.\" _- Quentin Sedlacek, Stanford PhD Student and Researcher_\r\n";
						}
						async function compiledContent$3r() {
							return load$3r().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3r() {
							return (await import('./chunks/index.7c3436e9.mjs'));
						}
						function Content$3r(...args) {
							return load$3r().then((m) => m.default(...args));
						}
						Content$3r.isAstroComponentFactory = true;
						function getHeadings$3r() {
							return load$3r().then((m) => m.metadata.headings);
						}
						function getHeaders$3r() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3r().then((m) => m.metadata.headings);
						}

const __vite_glob_0_57 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3r,
  file: file$3r,
  url: url$3r,
  rawContent: rawContent$3r,
  compiledContent: compiledContent$3r,
  default: load$3r,
  Content: Content$3r,
  getHeadings: getHeadings$3r,
  getHeaders: getHeaders$3r
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3q = {"title":"Cześć! We’re Releasing a Base Polish (beta) Speech-to-Text Language Model","description":"We’re excited to now offer a speech-to-text language model for Polish! Read on for details, including how to get access.","date":"2022-09-30T13:03:57.664Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1664543332/2209-How-Accurate-is-OpenAI-Whisper-Speech-to-Text-Model-featured-1200x630_hhcr5l.png","authors":["katie-byrne"],"category":"product-news","tags":["polish","language"],"seo":{"canonical":"base-polish-speech-to-text-language-model-released","title":"We’re Releasing a Base Polish (beta) Speech-to-Text Language Model"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1664543344/2209-How-Accurate-is-OpenAI-Whisper-Speech-to-Text-Model-social-1200x628_jsyzug.png"}};
						const file$3q = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/cześć-we’re-releasing-a-base-polish-beta-speech-to-text-language-model/index.md";
						const url$3q = undefined;
						function rawContent$3q() {
							return "Did you know Polish is spoken by over fifty million people worldwide, and is the sixth most spoken language in the European Union (EU)? That's a lot of people! That’s why we’re thrilled to announce extended EU language support for the Deepgram platform for our customers with our beta Base Polish model.\n\n## Give Our Polish Language Model a Try\n\nIf you want to try out our Base Polish model you can quickly create an account on Deepgram [Console](https://console.deepgram.com/) and we’ll give you $150 in free credits. Simply select Polish from the language menu when trying out our APIs.\n\nIf you’re already a Deepgram customer, you can call the Base Polish model using the following arguments:\n\n* `model=general`\n* `version=beta`\n* `language=nl`\n\n### Example API call\n\nYou can see an example of a full API call using these arguments below.\n\n```\ncurl \\\n  --request POST \\\n  --header 'Authorization: Token YOUR_DEEPGRAM_API_KEY' \\\n  --header 'Content-Type: audio/wav' \\\n  --data-binary @youraudio.wav \\\n  --url 'https://api.deepgram.com/v1/listen?language=pl'\n```\n\n## What Can Developers Do with a Polish Language Model?\n\n* Pair it with a Phone Call model to transcribe recordings from your European call center.\n* Pair it with a Meetings model to understand the topics commonly discussed by customers.\n* Create an Agent Assist solution to increase the productivity of your European sales team.\n\n## Key Benefits of Deepgram Base Polish Language Model\n\n* Accurately transcribe Polish speakers with our Phone Call, Meeting, Voicemail, Conversational AI use case models.\n* Many developers see high accuracy, depending on their use case.\n* Available for pre-recorded and [streaming](https://deepgram.com/blog/all-about-transcription-for-real-time-audio-streaming/) ([getting started guide](https://developers.deepgram.com/documentation/getting-started/streaming/)).\n* Transcribe on-premises or through the Deepgram Cloud.\n* Use in conjunction with speech understanding features such as [Diarization](https://deepgram.com/blog/what-is-speaker-diarization/) ([docs](https://developers.deepgram.com/documentation/features/diarize/)), [Summarization](https://developers.deepgram.com/documentation/features/summarize/), Topic Detection and more.\n\n## More to Come!\n\nAs our customers leverage Deepgram’s world-class speech API to expand their markets, we work together with them to create new models for languages they need. We’ll continue to support the expansion of our EU language capabilities in the coming months to support our growing global customer base.\n\nTo learn more about the dozens of other languages and use cases Deepgram enables, please see the [Language](https://developers.deepgram.com/documentation/features/language/) page in our documentation. And if you'd like to speak with someone about your project for diving in, [please reach out](https://deepgram.com/contact-us/)! We welcome your feedback, please share it with us at [deepgram.com/feedback](https://www.deepgram.com/feedback).";
						}
						async function compiledContent$3q() {
							return load$3q().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3q() {
							return (await import('./chunks/index.d0cbc64a.mjs'));
						}
						function Content$3q(...args) {
							return load$3q().then((m) => m.default(...args));
						}
						Content$3q.isAstroComponentFactory = true;
						function getHeadings$3q() {
							return load$3q().then((m) => m.metadata.headings);
						}
						function getHeaders$3q() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3q().then((m) => m.metadata.headings);
						}

const __vite_glob_0_58 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3q,
  file: file$3q,
  url: url$3q,
  rawContent: rawContent$3q,
  compiledContent: compiledContent$3q,
  default: load$3q,
  Content: Content$3q,
  getHeadings: getHeadings$3q,
  getHeaders: getHeaders$3q
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3p = {"title":"Video: Add Live Transcriptions to a Daily Video Call With Deepgram","description":"How to set up a Daily video call with live transcriptions.","date":"2022-02-24T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1645718184/blog/2022/02/daily-video-live-transcription/finished.png","authors":["kevin-lewis"],"category":"tutorial","tags":["video","daily"],"seo":{"title":"Video: Add Live Transcriptions to a Daily Video Call With Deepgram","description":"How to set up a Daily video call with live transcriptions."},"shorturls":{"share":"https://dpgr.am/a5af3d0","twitter":"https://dpgr.am/7635d5c","linkedin":"https://dpgr.am/5601be2","reddit":"https://dpgr.am/27dad47","facebook":"https://dpgr.am/70e4580"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453862/blog/daily-video-live-transcription/ograph.png"}};
						const file$3p = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/daily-video-live-transcription/index.md";
						const url$3p = undefined;
						function rawContent$3p() {
							return "\r\n[Daily](https://www.daily.co) allows developers to build in video and audio calls into their applications. [Daily recently announced](https://www.daily.co/blog/add-live-transcription-to-a-daily-call-with-our-newest-api/) our partnership which brings accurate live transcriptions to video calls in just a few lines of code.\r\n\r\nIn their announcement, the team at Daily provide a super useful React.js demo and starter project. Today we're sharing a video tutorial on getting live transcription working, without a framework, in under 40 lines of code (including the HTML boilerplate).\r\n\r\nThis tutorial covers:\r\n\r\n1.  How to set up a Daily video call using Daily Prebuilt.\r\n2.  How to enable live transcriptions for your Daily domain.\r\n3.  How to start transcribing your Daily video calls.\r\n\r\n<YouTube id=\"nyDnC5iCU_I\"></YouTube>\r\n\r\nImportant links:\r\n\r\n*   [Get a free Deepgram account](https://console.deepgram.com/signup)\r\n*   [Get a free Daily account](https://dashboard.daily.co/signup)\r\n\r\nFurther reading:\r\n\r\n*   [Daily's announcement post with sample React app](https://www.daily.co/blog/add-live-transcription-to-a-daily-call-with-our-newest-api/)\r\n*   [React sample app](https://github.com/daily-demos/examples/tree/main/custom/live-transcription)\r\n*   [Daily `startTranscription()` Documentation](https://docs.daily.co/reference/daily-js/instance-methods/start-transcription)\r\n\r\nIf you have any questions, please feel free to reach out on Twitter - we're [@DeepgramDevs](https://twitter.com/DeepgramDevs).\r\n\r\n        ";
						}
						async function compiledContent$3p() {
							return load$3p().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3p() {
							return (await import('./chunks/index.67cfacfd.mjs'));
						}
						function Content$3p(...args) {
							return load$3p().then((m) => m.default(...args));
						}
						Content$3p.isAstroComponentFactory = true;
						function getHeadings$3p() {
							return load$3p().then((m) => m.metadata.headings);
						}
						function getHeaders$3p() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3p().then((m) => m.metadata.headings);
						}

const __vite_glob_0_59 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3p,
  file: file$3p,
  url: url$3p,
  rawContent: rawContent$3p,
  compiledContent: compiledContent$3p,
  default: load$3p,
  Content: Content$3p,
  getHeadings: getHeadings$3p,
  getHeaders: getHeaders$3p
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3o = {"title":"5 Reasons Deep Learning for Speech Recognition is Business-Ready Now","description":"Have you heard that deep learning for ASR only exists in the lab? Its not true! Here are five ways that deep learning is business-ready now.","date":"2022-02-15T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981406/blog/deep-learning-asr-for-business/blog-5-Reasons-DL-for-SR-Biz-Ready-Now-thumb-554x2.png","authors":["keith-lam"],"category":"ai-and-engineering","tags":["deep-learning"],"seo":{"title":"5 Reasons Deep Learning for Speech Recognition is Business-Ready Now","description":"Have you heard that deep learning for ASR only exists in the lab? Its not true! Here are five ways that deep learning is business-ready now."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981406/blog/deep-learning-asr-for-business/blog-5-Reasons-DL-for-SR-Biz-Ready-Now-thumb-554x2.png"},"shorturls":{"share":"https://dpgr.am/a45bacf","twitter":"https://dpgr.am/0556101","linkedin":"https://dpgr.am/e941a69","reddit":"https://dpgr.am/fb544c2","facebook":"https://dpgr.am/608b4ee"}};
						const file$3o = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deep-learning-asr-for-business/index.md";
						const url$3o = undefined;
						function rawContent$3o() {
							return "I'm frustrated. When I read from tech authors, advisors, and our competitor's blogs that End-to-End Deep Learning (E2EDL) Speech Recognition software is only being researched or not production-ready, I want to scream...\n\n> **\"Listen people! End-to-end deep learning speech recognition is ready and in production now, with customers running millions of hours of new audio transcribed per month.\"**\n\nDo these numbers make it sound like E2EDL is just a research project? Absolutely not. E2EDL has moved from research into stable production and shown the world that E2EDL is not just a pipe dream but a reality.  [In a previous post](https://blog.deepgram.com/deep-learning-speech-recognition/), we covered some of the technical differences between the traditional way of doing ASR-the one used by every company except Deepgram-and [using E2EDL for speech recognition](https://blog.deepgram.com/deep-learning-speech-recognition). But at this point, you might be saying, \"Who cares, as long as the transcript I get is accurate?\" If you only care about accuracy, I have good news for you-deep learning approaches to ASR ***are*** more accurate than traditional approaches. But I'd guess you care about more than accuracy. You want a technology that can enable real-time communications. You want something that's cost-effective while also being easy to maintain and ready to adapt to future challenges. If that's true, I have even more good news-**E2EDL approaches to speech recognition provide all of this and more.** Let's dive in and talk about five of the key ways that deep learning for voice recognition can support your business.\n\n## 5 Advantages of Deep Learning Voice Recognition for Businesses\n\nWhether you're most interested in lower costs, higher accuracy, faster turnaround, easier scaling, or a future-ready technology, deep learning is the way to go. \n\n### 1. Lower costs\n\nE2EDL technology is much harder to develop initially but costs less to use. That's because DNNs can utilize hardware acceleration and GPUs to do multiple things at the same time, rather than running things in sequence like a CPU. Overall, you need less computing power than traditional ASR that runs on CPUs. This means you pay for less computer usage time to get transcripts back from your model or from a speech recognition API. Plus, you also save time and money on the model maintenance side, as you only have to maintain one thing, rather than a [Franken-model](https://blog.deepgram.com/what-is-asr/) composed of multiple parts.\n\n### 2. Higher accuracy\n\nFor traditional ASR, \"you get what you get.\" E2EDL allows you to maintain context through the entire process because you're not going through independent steps or models and hence the accuracy of each word and sentence improves. For example, deep learning is much quicker to train to focus on the speakers and transcribe the audio to get the important keywords correct. That's because you only have to update a single model, rather than each step of a traditional ASR model. This makes it feasible to train a new model for specific use cases with very little effort, rather than having to tweak multiple, connected models to get the output you want.  No other architecture can quickly train use case-specific models.\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-how-deepgram-works\"></WhitepaperPromo>\n\n### 3. Faster speed\n\nAs mentioned above, E2EDL models are faster because it allows massive computing parallelization opportunities with GPUs compared to single threading on a CPU for the traditional ASR method. What does this mean for businesses? It means real-time transcription is possible, enabling conversation AI for use cases in call and contact centers. It also means that, even if you don't need low-latency transcription, transcripts of historical data can be turned around much more quickly than would be possible with traditional systems.\n\n### 4. Easier scale-up\n\nBecause of the massive parallelization of GPU resources, E2EDL can be vertically and horizontally scaled more easily at a more cost-effective level. E2EDL can run 450 concurrent streaming transcriptions on just one T4 NVIDIA GPU, with only a nominal increase in latency.  If you scale up a cloud service to process more data or use internal computing resources, you'll need to pay for a lot more computing power if you're using a traditional ASR system.\n\n### 5. Future Ready\n\nMost researchers agree that HMM-GMM has reached the limit of speed, accuracy, and overall improvement. HMM-DNN has some room for improvement left but must compromise speed, accuracy, or computing resources; i.e., you cannot get great accuracy at speed or high speed at a low computing resource cost. E2EDL, on the other hand, still has plenty of room to improve on accuracy, speed, and scale-up efficiency as we move into the future. E2EDL is tackling use cases that simply wouldn't be possible with older ways of doing ASR. For example, one customer is using us for transcriptions and IBM Watson for translations to create meeting translations and transcriptions, so everyone in a meeting can speak their own language while you can view the discussion in your language, in real-time!  The speed and accuracy can only be achieved with E2EDL.\n\n## Wrapping up\n\nAll of these features make deep learning the best speech recognition option available today for businesses of any size, from start-ups to enterprises. Production-ready E2EDL shouldn't be the best-kept secret out there. Discussions should be around how E2EDL can continually improve based on specific use cases and audio features, not on whether or not it's production-ready. In data science and machine learning, there's a truism that says you should go for the simplest algorithm or tool that gets you the results you need, even if it isn't the latest technology; sometimes, a simple linear regression model is more than enough. Deep learning ASR models are in the unique position of not only being the simplest option available-a single model that does everything, rather than a few different models strung together-but also being the most accurate and the most cost-effective. End-to-end deep learning for speech recognition is ready now! If you still don't believe me, you can try Deepgram out for free at console.deepgram.com or [contact our STT experts](https://deepgram.com/contact-us/) if you want to explore training a custom model for difficult audio situations.";
						}
						async function compiledContent$3o() {
							return load$3o().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3o() {
							return (await import('./chunks/index.6aa32753.mjs'));
						}
						function Content$3o(...args) {
							return load$3o().then((m) => m.default(...args));
						}
						Content$3o.isAstroComponentFactory = true;
						function getHeadings$3o() {
							return load$3o().then((m) => m.metadata.headings);
						}
						function getHeaders$3o() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3o().then((m) => m.metadata.headings);
						}

const __vite_glob_0_60 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3o,
  file: file$3o,
  url: url$3o,
  rawContent: rawContent$3o,
  compiledContent: compiledContent$3o,
  default: load$3o,
  Content: Content$3o,
  getHeadings: getHeadings$3o,
  getHeaders: getHeaders$3o
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3n = {"title":"Why Deep Learning is the Best Approach for Speech Recognition","description":"Most ASR systems rely on a combination of legacy systems that are slow, inaccurate, and inflexible. Learn why deep learning is a better approach.","date":"2022-02-01T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981403/blog/deep-learning-speech-recognition/why-dl-is-best-for-speech-recognition-thumb-554x22.png","authors":["sam-zegas"],"category":"ai-and-engineering","tags":["deep-learning"],"seo":{"title":"Why Deep Learning is the Best Approach for Speech Recognition","description":"Most ASR systems rely on a combination of legacy systems that are slow, inaccurate, and inflexible. Learn why deep learning is a better approach."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981403/blog/deep-learning-speech-recognition/why-dl-is-best-for-speech-recognition-thumb-554x22.png"},"shorturls":{"share":"https://dpgr.am/d6ee526","twitter":"https://dpgr.am/edca058","linkedin":"https://dpgr.am/49294ae","reddit":"https://dpgr.am/f9d4e6b","facebook":"https://dpgr.am/8255146"}};
						const file$3n = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deep-learning-speech-recognition/index.md";
						const url$3n = undefined;
						function rawContent$3n() {
							return "[Automatic speech recognition](https://blog.deepgram.com/what-is-asr/) isn't new. It has its origins in Cold War-era research with narrow military implementations, which was followed in the 1960s, 70s, and 80s by developments from leaders like Marvin Minsky and research funded by [DARPA](https://en.wikipedia.org/wiki/DARPA). However, it wasn't until the 1990s that researchers saw real progress thanks to government-funded projects like the Wall Street Journal Speech Dataset. Even then, these small datasets of around 30 hours of audio only yielded accuracies of about 30-50% in a research setting. Continued developments in speech technology have led to a variety of improvements and consumer use cases that we're all familiar with today-Alexa, Siri, telling the automated bank system that you need a PIN, etc. But if you've ever used any of these speech recognition tools, you know that they're far from perfect. That's because they rely on an old-fashioned way of doing speech recognition that has its roots back in those original experiments in the 1960s.\n\nIn this blog post, we'll walk through the old-fashioned way of doing speech recognition-because it's the one that's still used by most companies today-and then show why the new way, which relies on end-to-end deep learning to process speech, is far superior.\n\n## The Old Way: An Acoustic Model, a Pronunciation Model, and a Language Model-Oh my!\n\nThe smallest units of sound in spoken language are called phonemes. For example, \"cat\" has three phonemes: an initial \"k\" sound, a middle \"a\" vowel like in \"apple\", and a final \"t\" sound. In the old way of doing ASR, you start by identifying the phonemes in a recording, and then trying to assemble clumps of phonemes into possible words. Next, you look for how those possible words might fit together to make grammatical sense. Finally, you hack all those possibilities down to one 'transcript'. The components of this system are called the acoustic model, pronunciation model, and language model with beam search.\n\n* The **acoustic model** takes a representation of the audio signal-usually as a waveform or spectrogram-and tries to guess a phoneme probability distribution function over timeboxed windows of 10-80 ms throughout the entire recording. Essentially, the output is a huge lattice of possible phonemes as a function of time rather than simply a phonemic transcription.\n* The **pronunciation model** then takes the phoneme lattice as its input and tries to guess a *word* probability distribution function over time windows. The output of this step is a huge lattice of possible words as a function of time.\n* A **language model** is then used in conjunction with a beam search. The model takes the word lattice as its input and cuts down all the possibilities it thinks are less likely until it arrives at the final transcription. In addition, it uses a beam search: at every time step, the search throws away all possibilities below its cutoff (called the beam width), never to be seen or thought of again.\n\nAlthough this old way of building speech recognition models is intuitive to humans, and is motivated to some extent by how linguists think about language, it's highly lossy to a computer. At each step in this process, your models have to make simplifying assumptions to fit the computations in memory or finish within the lifetime of the universe-not kidding. There are just too many combinations and permutations for the models to return results if they consider all of the possibilities. This is why, for instance, the language model portions are typically very limited trigram language models. The *tri-* in trigram means \"three\" and indicates that the model only looks back two words to see if the current word makes sense in context. That might only be half of a sentence-or less! These simplifications are rampant and result in a performance-limited, pipelined approach for optimizing sub-problems at each step of the process, rather than an end-to-end approach that can simultaneously optimize across the entire problem domain. This creates three major problems with traditional methods.\n\n<WhitepaperPromo whitepaper=\"latest\"></WhitepaperPromo>\n\n### Problems with the Traditional Approach\n\nThere are three big problems with the old-fashioned method of speech recognition: it's slow, it's inaccurate, and it's brittle. The slowness makes it expensive and time consuming. The inaccuracy makes the traditional methods ineffective and frustrating to use, especially for enterprises and domains that require high levels of accuracy, like the health and legal fields. And the brittleness makes engineers afraid to change any code for fear that the house of cards will come crashing down.\n\n#### Slow\n\nThe traditional methods are slow because they rely on unoptimized heuristic approaches, which use compute and memory resources inefficiently. These approaches can only process around 0.5-2 streams per CPU core. This can lead to long turnaround times when it comes to providing results-often so long that some applications, like real-time chatbots, are simply not possible using these methods.\n\n#### Inaccurate\n\nThe old method is inaccurate because the models lack expressiveness and capacity. Expressiveness is a measure of how complicated of a world the systems can model while still maintaining accuracy. Capacity is a similar measure of how much knowledge a model can retain. Traditional systems are shallow in this sense. They have no hope of covering everything extremely well, so they either cover most areas with meager success or a narrow domain with some success.\n\n#### Inflexible\n\nThe old method is brittle because the systems are extremely complicated and inflexible. It takes a team of 20 engineers a year to set up a system that only starts to get adequate performance. So, they leave it alone and hope for the best. Attempting to modify or improve the system will only end in defeat as the surface area of the problem overcomes the team. This is why traditional speech recognition providers only serve one model (maybe two or three, but certainly not hundreds or thousands) and refuse to customize for their customers. The cost is too high with the old hydra methods (cut off one problem head and three grow back).\n\n## The best way: End-to-end deep learning for speech recognition\n\nThe good news, if you're looking for a speech recognition solution, is that it doesn't have to be this way! Although the old way of doing things is still used by most providers, there is an alternative that's fast, accurate, and flexible-an end-to-end deep learning (E2EDL) model.\n\n### Fast\n\nAn end-to-end model can be better optimized for run-time execution. Deep learning, specifically, utilizes the same set of mathematical operations ([tensor math](https://en.wikipedia.org/wiki/Tensor)) as implemented on graphics cards (GPUs). This means that E2EDL models are the very fastest implementations available. On the other hand, traditional speech stacks are composed of multiple sub-problems (less surface area for optimization) and cannot use accelerating computing resources (thus forcing them onto the general-purpose CPU).  E2EDL on GPUs achieves over 300 streams per GPU, meaning that results are returned much faster to customers-so much so that they are often surprised and delighted. Deepgram's customers often think they must have done something wrong, but no, it's just that fast.\n\n### Accurate\n\nE2EDL models have much greater capacity and enjoy a compression efficiency that allows all parts of the network to learn in cohesion, all as one organism. As a result, these models can optimize across the entire problem space at once-from the input audio features all the way through the transcript production. The result is an expert model that achieves much higher accuracy and continues to get better while training without \"topping out.\"\n\n### Flexible\n\nAt Deepgram, our E2EDL approach allows us to reach unprecedented levels of speech recognition accuracy at low cost. E2EDL-based automatic transcription systems dramatically shorten the time it takes to train and deploy new models. E2EDL models also continue to improve indefinitely from training on new data, in contrast to older hybrid systems, which see diminishing returns from training on data past a few thousand hours. These diminishing returns impose a cap on accuracy improvement for hybrid systems. Not so for E2EDL systems.  \n\nThe downside to the E2EDL approach for speech recognition is the complexity of building a truly data-driven E2EDL system hosted on GPUs. However, once the system is built, it is stable, efficient, fast, and accurate. Building it is, nonetheless, a massive endeavor. Think: building a rocket to put satellites into orbit. It is a highly complex undertaking that takes know-how, smart people, time, opportunity cost, and capital risk-but once a system has been refined, it can reliably perform highly valuable work. In the past, companies like Nuance, [Google](https://offers.deepgram.com/head-to-head-dg-vs-google-webinar-on-demand), and [Amazon](https://offers.deepgram.com/head-to-head-dg-vs-amazon-webinar-on-demand) didn't have the option to take the E2EDL approach because they didn't have the know-how when they started, and now they are stuck in a historical bind-backtracking is too expensive.\n\n## Conclusion\n\nAs you can see, E2EDL is the best option for speech recognition, while older approaches are too brittle and have sunk costs too high to efficiently leverage these new resources. And the differences in performance and flexibility are phenomenal. For example, Deepgram's technology can support 300 simultaneous audio streams on a single GPU, compared to the 1-2 streams per CPU core provided by old-fashioned solutions. And, because Deepgram uses E2EDL, models can be modified or repurposed easily and cheaply. New classifiers, novel architectures, and additional problem domains can be introduced with minimal labor, since the same training and inference processes still apply. In fact, trained models can often be re-applied to new tasks-a process called transfer learning-allowing brand new models or classifiers to benefit from previous training, even across different problem domains!\n\nDeepgram can leverage its E2EDL model to scale massively across custom models, new architectures, and advanced analytics. Compare this to traditional approaches, which require extensive, invasive overhauls to multiple unrelated components, leading to an avalanche of engineer tasking. Rather than 10 engineers and 1,600 work-hours of refactoring, Deepgram can perform the same feats of flexibility with 1 engineer in as few as 4 work-hours-all due to using an E2EDL solution. That's probably enough talk about why E2EDL systems are the best option for speech recognition. If you still don't believe me, [get your free API and give Deepgram a try](https://console.deepgram.com/). You'll see just how quick and easy it is to set up a speech recognition pipeline and get impactful results for your business.";
						}
						async function compiledContent$3n() {
							return load$3n().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3n() {
							return (await import('./chunks/index.5a21666a.mjs'));
						}
						function Content$3n(...args) {
							return load$3n().then((m) => m.default(...args));
						}
						Content$3n.isAstroComponentFactory = true;
						function getHeadings$3n() {
							return load$3n().then((m) => m.metadata.headings);
						}
						function getHeaders$3n() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3n().then((m) => m.metadata.headings);
						}

const __vite_glob_0_61 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3n,
  file: file$3n,
  url: url$3n,
  rawContent: rawContent$3n,
  compiledContent: compiledContent$3n,
  default: load$3n,
  Content: Content$3n,
  getHeadings: getHeadings$3n,
  getHeaders: getHeaders$3n
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3m = {"title":"Deepgram and Recall.ai Partner to Make it Easier for Developers to Extract Insights From Meeting Audio and Automate Tedious Workflows","description":"Developers can get started with Deepgram and Recall.ai with just a few lines of code, across variety of use cases","date":"2022-10-11T21:11:46.146Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1665522827/blog/Deepgram%20and%20Recall.ai%20Partnership/2210-dg-recallAI-announcement-featured-1200x630_qzjtq9.png","authors":["katie-byrne"],"category":"product-news","tags":["recall-ai"],"shorturls":{"share":"https://dpgr.am/e89f348","twitter":"https://dpgr.am/a2f0e18","linkedin":"https://dpgr.am/9293d5b","reddit":"https://dpgr.am/d6b861c","facebook":"https://dpgr.am/d0a8252"}};
						const file$3m = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-and-recall-ai-partner-to-make-it-easier-for-developers-to-extract-insights-from-meeting-audio-and-automate-tedious-workflows/index.md";
						const url$3m = undefined;
						function rawContent$3m() {
							return "\nDevelopers can get started with Deepgram and Recall.ai with just a few lines of code, across variety of use cases\n\nSan Francisco, CA – [Deepgram](http://www.deepgram.com), a leading AI speech recognition and understanding company, and [Recall.ai](https://www.recall.ai/), a real-time meeting API provider, today announced a partnership to streamline the process for developers to extract data from virtual meetings, making it easier to gain insights from voice data and automate tedious workflows to maximize speed, accuracy and scale, all while using minimal lines of code.  \n\nRecall.ai provides companies with a single Developer API to access real-time meeting data from a number of different platforms, including Zoom, Google Meet, Microsoft Teams, and more.  By adding a Recall.ai bot to a meeting, the company can observe what is happening in the background to provide real-time or on-demand data. The Recall.ai and Deepgram partnership enables efficient feedback flows for developers. \n\n“When working with voice data, having an accurate transcription is critical to leveraging the insights captured during a meeting,” said David Gu, CEO of Recall.ai. “Our partnership with Deepgram enables us to eliminate the pain points developers encounter when extracting insights from digital meeting spaces, and provide users with unparalleled accuracy, speed and scale for their meeting platforms, at a fraction of the cost and with minimal lines of code.”\n\nReal-time virtual meeting integrations can be difficult to implement at scale, but with Recall.ai’s unified framework, companies can benefit from a single API that makes it easier to trigger actions based on who is speaking, when people join or leave a meeting, and more. Because Recall.ai can integrate with every virtual meeting platform, it reduces the overall development time and ongoing costs. Through this partnership, users can perform the following tasks: \n\n*   Transcribe conversations of real-time meeting platforms\n*   Create a speaker-separated transcript with usernames \n*   Accurately transcribe live key phrases, named entities, numerals\n*   Calculate speaker turn counts and talk-time per speaker live\n*   Auto detect and classify languages spoken in meeting recordings\n\n“At Deepgram, we’re passionate about innovating technology that alleviates developers' workloads,” said Scott Stephenson, CEO of Deepgram. “Recall.ai’s partnership puts control back in the hands of developers, and that control will help catalyze the next generation of voice experiences for customers.” \n\nTo get started, users can create an account with [Recall.ai](https://www.recall.ai/) and [sign up for Deepgram](https://console.deepgram.com/signup). Once provided access to the API keys, developers can receive their first real-time meeting transcription in 10 minutes or less. \n\n**About Deepgram**\n\nDeepgram’s AI speech platform is revolutionizing the speech-to-text (STT) market and taking on the big guys. We’re redefining what companies can do with voice technology by offering a platform with AI architectural advantage, not legacy tech retrofitted with AI. We’ve raised over $37 million and have been recognized as an [Inc. Best Workplace (2022)](https://www.inc.com/best-workplaces/2022), a [Forbes Top 50 AI Company to Watch (2021)](https://www.forbes.com/sites/alanohnsman/2021/04/26/ai-50-americas-most-promising-artificial-intelligence-companies/?sh=9fcd89d77cf1), and a [CB Insights Top 100 AI Startup (2021)](https://www.cbinsights.com/research/report/artificial-intelligence-top-startups-2021/), among others. Our tech advantage is end-to-end deep learning, but our strength lies in our diversity of people, ideas, and experiences that allow our company to create amazing voice-enabled experiences for people who are true innovators in the field. We believe every voice should be heard—and understood—from our transcriptions to our customers to our employees. Come join our revolution to unlock the power of voice technology for everyone. We want to hear what you’ve got to say. \n\nPress Contact\n\n[deepgram @ inkhouse.com](mailto:deepgram@inkhouse.com)\n\n**About Recall.ai**\n\nRecall.ai provides a single API for accessing real-time meeting data from platforms like Zoom, Microsoft Teams, Google Meet, and more. Real-time meeting integrations require a lot of moving parts, so Recall.ai provides a unified framework that simplifies this process. Recall.ai makes it easy to trigger actions based on who is speaking, when people join or leave a meeting, and more, all with a single API for every platform which helps lower development time and upkeep costs. The API can be used with both audio and video streams, even for meeting platforms that do not have a publicly accessible API.\n\n";
						}
						async function compiledContent$3m() {
							return load$3m().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3m() {
							return (await import('./chunks/index.1a1fff1e.mjs'));
						}
						function Content$3m(...args) {
							return load$3m().then((m) => m.default(...args));
						}
						Content$3m.isAstroComponentFactory = true;
						function getHeadings$3m() {
							return load$3m().then((m) => m.metadata.headings);
						}
						function getHeaders$3m() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3m().then((m) => m.metadata.headings);
						}

const __vite_glob_0_62 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3m,
  file: file$3m,
  url: url$3m,
  rawContent: rawContent$3m,
  compiledContent: compiledContent$3m,
  default: load$3m,
  Content: Content$3m,
  getHeadings: getHeadings$3m,
  getHeaders: getHeaders$3m
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3l = {"title":"Deepgram Announces UniMRCP Integration to Power Modern Customer Experience","description":"Learn how organizations can increase efficiency, improve complaint resolution, and continue to provide a high standard of support today.","date":"2020-09-30T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981356/blog/deepgram-announces-unimrcp-integration-to-power-modern-customer-experience/dg-announces-unimrcp-integration%402x.jpg","authors":["katie-byrne"],"category":"product-news","tags":["call-analytics","voice-tech","voice-strategy"],"seo":{"title":"Deepgram Announces UniMRCP Integration to Power Modern Customer Experience","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981356/blog/deepgram-announces-unimrcp-integration-to-power-modern-customer-experience/dg-announces-unimrcp-integration%402x.jpg"},"shorturls":{"share":"https://dpgr.am/3305da7","twitter":"https://dpgr.am/365425f","linkedin":"https://dpgr.am/55b72ea","reddit":"https://dpgr.am/48aa7a5","facebook":"https://dpgr.am/475e9cb"}};
						const file$3l = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-announces-unimrcp-integration-to-power-modern-customer-experience/index.md";
						const url$3l = undefined;
						function rawContent$3l() {
							return "\r\nCOVID-19 has created a twofold challenge for customer service operations, inundating contact center employees with customer service queries while simultaneously requiring significant changes to accommodate a reduced and remote workforce. Even before the pandemic, organizations were increasing their focus on customer experience, and that focus has only heightened with less access to in-person customer service and a completely virtual holiday shopping season around the corner. How can organizations increase efficiency, improve complaint resolution and continue to provide a high standard of support under these circumstances?\r\n\r\n## **Clients can now implement state of the art speech recognition into their IVR solutions**\r\n\r\nWe believe speech recognition has an important role to play in powering the next generation of customer experience. That's why today we're excited to announce UniMRCP integration that will allow our customers to connect state of the art speech recognition with their existing Interactive Voice Response (IVR) solutions such as Genesys, Cisco, Avaya, and Nuance. \r\n\r\n## **But Doesn't IVR Already Do This?**\r\n\r\nIVR is an essential ingredient of all modern contact centers. The benefit of IVR is that customers are able to use their voice to navigate the menu versus using a touch-tone, which requires customers to listen to an automated menu and press multiple numbers before being routed to the correct customer service agent. It's great in theory, but not always in reality, because not all customer inquiries fit into a multiple-choice menu. The experience can easily turn from helpful to frustrating when the system struggles to accurately capture the reason customers are calling or the customer has to listen closely for the correct menu prompts instead of being able to simply state the reason for their call. In a world where customer experience can make or break your business, it's critical that the call center experience is quick and intuitive.   \r\n\r\nIntegrating Deepgram's [state-of-the-art speech recognition technology](https://deepgram.com/) into existing IVR solutions will help customers access the information they need faster and minimize frustration for first-time callers by ensuring that they are heard correctly the first time. Deepgram's solution provides its customers with unparalleled trained accuracy (over 90%), taking into account keywords that are important to your customers and automatically adjusting to noise pollution, meaning that the customer is heard the first time, regardless of background noise.   \r\n\r\n## **About Deepgram UniMRCP**\r\n\r\nMRCP stands for Media Resource Control Protocol, an industry-standard data and communication protocol commonly used for IVRs. UniMRCP is the open source cross-platform implementation of the MRCP client and server. \r\n\r\nOur UniMRCP offering allows enterprises to accurately capture the reason customers are calling by automatically adjusting to your customer's unique audio profile and by distinguishing between speakers and filtering out background noise. This enables enterprises to address customer pain points from the start, providing customers with a more positive customer experience overall. \r\n\r\nWith training, Deepgram can identify with unparalleled accuracy the keywords that are specific to your brand, and that matter most to your customers. This further streamlines the customer call center experience, routing customer calls to the correct support person based on their needs. UniMRCP makes it possible for companies to build a custom IVR dialogue workflow, as you have reliable transcriptions to build NLU models and automation off of.\r\n\r\n![](https://lh5.googleusercontent.com/J2p-CE-y-QEbP7khenTIc1yBAtptOLV1d9mi5TnCITD5PDxLha6aoZfkshm4ms-igkrQ16mYIhmH1ctOh5gn7bEHS1JI3SG32O04DHjIH4F-M8Pzos14kpAV7iDbWGJYdXHDvl_W)\r\n\r\n**The Next Generation of Customer Experience**\r\n\r\nDeepgram powers the next generation of customer experience-from [IVR and AI voice products](https://deepgram.com/solutions/software/), to [virtual customer assistants](https://deepgram.com/solutions/voicebots/), chatbots and agent productivity solutions. [Customer contact centers](https://deepgram.com/solutions/contact-centers/) have always been an important step in the customer journey, and with the shift to digital-first experiences, it is now a critical step to retaining customers. That's why innovative contact center solutions such as Agara, Active.ai, Observe.ai, Tethr, and Sharpen choose Deepgram to give them accurate transcription and customer experience foundation. \r\n\r\n_\"There could be hundreds of issues a customer is calling in about. Add to this complexity there is a distribution of words, specific to each of our customer's brands,\" said Arjun Maheswaran, CTO at Agara. \"We couldn't get these words right using Google, Amazon or Speechmatics, and are thrilled to finally reach our accuracy goal with Deepgram.\" _\r\n\r\n[Contact us](https://deepgram.com/contact-us/) today to learn how we can support your IVR solution.\r\n";
						}
						async function compiledContent$3l() {
							return load$3l().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3l() {
							return (await import('./chunks/index.1b4c6513.mjs'));
						}
						function Content$3l(...args) {
							return load$3l().then((m) => m.default(...args));
						}
						Content$3l.isAstroComponentFactory = true;
						function getHeadings$3l() {
							return load$3l().then((m) => m.metadata.headings);
						}
						function getHeaders$3l() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3l().then((m) => m.metadata.headings);
						}

const __vite_glob_0_63 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3l,
  file: file$3l,
  url: url$3l,
  rawContent: rawContent$3l,
  compiledContent: compiledContent$3l,
  default: load$3l,
  Content: Content$3l,
  getHeadings: getHeadings$3l,
  getHeaders: getHeaders$3l
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3k = {"title":"Building Diversity, Equity, and Inclusion at Deepgram: Every Voice. Heard and Understood","description":"This Black History Month, we’re rolling out a new DE&I program. Here’s what’s inside.","date":"2022-02-28T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981406/blog/deepgram-diversity-inclusion/building-diversity-equity-inclusion-at-DG-thumb-55.png","authors":["sam-zegas"],"category":"identity-and-language","tags":["inclusion"],"seo":{"title":"Building Diversity, Equity, and Inclusion at Deepgram: Every Voice. Heard and Understood","description":"This Black History Month, we’re rolling out a new DE&I program. Here’s what’s inside."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981406/blog/deepgram-diversity-inclusion/building-diversity-equity-inclusion-at-DG-thumb-55.png"},"shorturls":{"share":"https://dpgr.am/978c481","twitter":"https://dpgr.am/867c88e","linkedin":"https://dpgr.am/b584234","reddit":"https://dpgr.am/371d360","facebook":"https://dpgr.am/67075dc"}};
						const file$3k = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-diversity-inclusion/index.md";
						const url$3k = undefined;
						function rawContent$3k() {
							return "Happy Black History Month! This month, we honor the many contributions that Black people have made to the world we live in through their long history of struggles and victories. We are proud of the accomplishments of our Black team members at Deepgram and are taking a moment to reflect on the state of black wellness and the future of success on our team. \n\n## Deepgram's Diversity, Equity, and Inclusion Program\n\nPrivate companies have an important role to play in pushing for change. In recent years, many companies have launched Diversity, Equity, and Inclusion (DE&I) programs as a way to create a better work environment and make space for an ongoing conversation about these issues. Now, in celebration of Black History Month, we're happy to announce that we've rolled out a new DE&I program at Deepgram. This statement of our values serves as a foundation for the work ahead:\n\n> Everyone's voice is unique. At Deepgram, we thrive on a concert of voices. In order to build a more inclusive and understanding company, we are committed to seeking out and welcoming diversity of background, thought process, and experience. The belief that every voice deserves to be heard and understood is core to how we treat each other, advance the state of speech technology, and build a better world.\n\nThis statement is core to filling our purpose as a company. Our commitment to being **the speech company** means we need to include and represent *all* voices-and we are more excited than ever for the road ahead. We expect our DE&I program to evolve over time include a variety of events and resources for Deepgram team members as well as commitments to reporting on DE&I issues and to building a diverse, equitable, and inclusive team. In particular, we are committed to working with our Black team members to ensure their success and growth at Deepgram, and we applaud the impact they have already made to our shared achievements.\n\n## Why Now?\n\nWe are proud to release this statement during Black History Month, which is a time to reflect on Black Americans' struggle for freedom, recognition, and equality that has shaped the United States. The origin of Black History Month itself tells an inspiring story. Its roots trace back to the efforts of [Carter G. Woodson](https://naacp.org/find-resources/history-explained/civil-rights-leaders/carter-g-woodson), who founded the tradition of Negro History Week in 1926. Woodson was born in 1875 to former slaves who had little formal education. As a young child, he had to work to support his family while seeking out opportunities for education where he could find them. Much of what he learned through the high-school level was self-taught. But his dedication paid off. In 1912, he became the second Black American to earn a doctorate from Harvard after W. E. B. Du Bois, and his later writing on Black history made him a prominent historian. Woodson knew that Black history was often overlooked and ignored, and he wrote, \"If a race has no history, if it has no worthwhile tradition, it becomes a negligible factor in the thought of the world.\"\n\nIt was from that insight that Negro History Week and later Black History Month were born. Over the last 70 years since the beginning of the US Civil Rights Movement, Black campaigners for social change have dramatically shifted American society for the better. The struggles of Civil Rights leaders have led to significant improvements in the quality of life and political inclusion of Black people. Moreover, the campaigns of Black Civil Rights leaders have become a beacon to other minority groups seeking change and inclusion.\n\n## Looking to the Work Ahead\n\nA great deal of progress has been made, but Black History Month is a reminder to all Americans that more work remains to be done. American public policy has so far been able to grapple with forms of systemic racism that still impact Black Americans in serious ways. Social and political factors in our country still make [school segregation](https://www.pewresearch.org/fact-tank/2021/12/15/u-s-public-school-students-often-go-to-schools-where-at-least-half-of-their-peers-are-the-same-race-or-ethnicity/) a *de facto* reality in many areas. The [average wealth](https://www.federalreserve.gov/econres/notes/feds-notes/wealth-inequality-and-the-racial-wealth-gap-20211022.htm) of White families is 6x higher than that of Black families, and the wealth inequality index has risen over the last decade. Black Americans are 5x more likely to be [incarcerated](https://www.usnews.com/news/best-states/articles/2021-10-13/report-highlights-staggering-racial-disparities-in-us-incarceration-rates) than White Americans and other American ethnicities.\n\nInequities in [access to healthcare](https://www.commonwealthfund.org/publications/scorecard/2021/nov/achieving-racial-ethnic-equity-us-health-care-state-performance) by race are evident in all US states. Laws aimed at [restricting voting access](https://www.brennancenter.org/our-work/research-reports/impact-voter-suppression-communities-color) have been shown to disproportionately impact communities of color. Black Americans are more likely to be [killed by police](https://www.nature.com/articles/d41586-020-01846-z) than other Americans. These issues are just a few examples of the work that still lies ahead of us. The work of DE&I is never done. It is a mindset we embrace day by day to make our company and the world better. We look forward to the day when ***every voice is heard and understood***.";
						}
						async function compiledContent$3k() {
							return load$3k().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3k() {
							return (await import('./chunks/index.d76136ab.mjs'));
						}
						function Content$3k(...args) {
							return load$3k().then((m) => m.default(...args));
						}
						Content$3k.isAstroComponentFactory = true;
						function getHeadings$3k() {
							return load$3k().then((m) => m.metadata.headings);
						}
						function getHeaders$3k() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3k().then((m) => m.metadata.headings);
						}

const __vite_glob_0_64 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3k,
  file: file$3k,
  url: url$3k,
  rawContent: rawContent$3k,
  compiledContent: compiledContent$3k,
  default: load$3k,
  Content: Content$3k,
  getHeadings: getHeadings$3k,
  getHeaders: getHeaders$3k
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3j = {"title":"Deepgram Enables Developers To Build The Future Of Voice With Suite Of New Features And $10 Million In Free Speech Recognition","description":"Deepgram, the leading automatic speech recognition (ASR) provider, today announced a new startup program, in partnership with top accelerators such as Y Combinator, that puts the power of voice recognition in the hands of developers.","date":"2021-08-03T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981378/blog/deepgram-enables-developers-to-build-the-future-of-voice-with-suite-of-new-features-and-10-million-in-free-speech-recognition/dg-enables-developers%402x.jpg","authors":["katie-byrne"],"category":"dg-insider","tags":["announcements"],"seo":{"title":"Deepgram Enables Developers To Build The Future Of Voice With Suite Of New Features And $10 Million In Free Speech Recognition","description":"Deepgram, the leading automatic speech recognition (ASR) provider, today announced a new startup program, in partnership with top accelerators such as Y Combinator, that puts the power of voice recognition in the hands of developers."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981378/blog/deepgram-enables-developers-to-build-the-future-of-voice-with-suite-of-new-features-and-10-million-in-free-speech-recognition/dg-enables-developers%402x.jpg"},"shorturls":{"share":"https://dpgr.am/04231cb","twitter":"https://dpgr.am/aef7319","linkedin":"https://dpgr.am/9c76ba6","reddit":"https://dpgr.am/a48afb2","facebook":"https://dpgr.am/12d92c5"}};
						const file$3j = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-enables-developers-to-build-the-future-of-voice-with-suite-of-new-features-and-10-million-in-free-speech-recognition/index.md";
						const url$3j = undefined;
						function rawContent$3j() {
							return "\r\n## ASR leader launches developer tools, free speech recognition, enabling developers to put voice at the heart of their business\r\n\r\n**SAN FRANCISCO, Calif., August 3, 2021**- [Deepgram](https://deepgram.com/), the leading automatic speech recognition (ASR) provider, today announced a new program, in partnership with top accelerators such as Y Combinator, that puts the power of voice recognition in the hands of developers. The Deepgram Startup Program provides approved applicants up to $100,000 each in Deepgram speech recognition. The program will offer $10 million total in free speech recognition and is accepting applicants until the end of 2021\\. Deepgram also released new features to the company's category-defining speech recognition platform, including a new developer console, software development kits and new API documentation - all designed to create a frictionless developer experience. **The Rise of Voice** Voice is already an integrated component of customer service and call centers solutions, but with the shift to remote work and remote learning, it has become a cross-industry necessity for any organization to build strong relationships with customers, employees, students, or any other audience. Under this changing landscape, Deepgram is providing everything developers need - from early access to better documentation to free ASR transcription - so they can all go build a part of a voice-enabled future.\r\n\r\n> \"We believe in a world where every voice is heard and understood,\" said Scott Stephenson, CEO and co-founder of Deepgram. \"Developers playing and prototyping with our technology is one of the fastest paths to making voice a de facto mode for communications. We're just scratching the surface of how voice can transform business and can't wait to see how these awesome practitioners leverage automatic speech recognition in the real-world.\" \"Deepgram is insanely easy to use,\" said Hans Petter Eikemo, Co-Founder and CTO of Spot Meetings. \"I was extremely impressed with the simplicity of the system. No unnecessary options which allowed me to implement our integration in 1 day.\"\r\n\r\n**Voice for Everyone: Deepgram Startup Program** The Deepgram Startup Program is designed for entrepreneurs and developers to harness the power of speech recognition quickly and easily at no cost. As part of this announcement, Deepgram is offering $10 million in free speech recognition, with a specific focus on startups removing friction in education and employee experiences-two areas where innovation is needed as they become much more difficult with the shift from in-person to remote. Reasons entrepreneurs should apply to this program include:\r\n\r\n*   No membership fee or equity % required\r\n*   Ability to use in conjunction with their grant, seed, incubator or accelerator benefits\r\n*   Removes cost barrier to creating voice-enabled experiences\r\n*   Enables entrepreneurs to work closely with the team that is building state-of-the-art AI for communications\r\n\r\n**Fulfilling the Needs of the Developer Ecosystem: New Features** Deepgram also released new platform features, including a new developer console, software development kits for Python and Node.js to simplify onboarding and guarantee a more seamless user experience. New Platform features Include:\r\n\r\n*   **Developer Console:** The console is a graphical user interface (GUI) that enables users from solo developers to enterprise teams to access the full range of Deepgram ASR services. One of the console's key features, Missions, provides users with a learning path for getting started with Deepgram-with tasks to accomplish and rewards along the way. The console also simplifies usage and billing by offering promotional credits, automated billing and account management.\r\n*   **Software Development Kits:** The SDKs enable developers to easily start developing applications with Deepgram's speech-to-text platform to transcribe both real-time streaming and pre-recorded audio, as well as experiment with new applications to take advantage of Deepgram's millisecond transcription speed and high accuracy. Deepgram's new SDKs include libraries, documentation, code samples, processes and guides and are available in both Python and Node.js languages.\r\n\r\nLearn more by [creating an account](https://console.deepgram.com/signup) or apply for the [Deepgram Startup Program](https://deepgram.com/startup-program/) or [contact us](https://deepgram.com/contact-us/) today. **About Deepgram** Deepgram is the leader in enterprise automatic speech recognition (ASR) for call centers and software providers. With our patented end-to-end deep learning approach, data scientists get access to the industry's fastest, most accurate and highly scalable AI technology. We take the heavy lifting out of noisy, multi-speaker, hard to understand audio transcription, so you can focus on what you do best. To learn more visit deepgram.com or contact us to get started. **Contact** [press@deepgram.com](mailto:deepgram@inkhouse.com)\r\n";
						}
						async function compiledContent$3j() {
							return load$3j().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3j() {
							return (await import('./chunks/index.a2740b03.mjs'));
						}
						function Content$3j(...args) {
							return load$3j().then((m) => m.default(...args));
						}
						Content$3j.isAstroComponentFactory = true;
						function getHeadings$3j() {
							return load$3j().then((m) => m.metadata.headings);
						}
						function getHeaders$3j() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3j().then((m) => m.metadata.headings);
						}

const __vite_glob_0_65 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3j,
  file: file$3j,
  url: url$3j,
  rawContent: rawContent$3j,
  compiledContent: compiledContent$3j,
  default: load$3j,
  Content: Content$3j,
  getHeadings: getHeadings$3j,
  getHeaders: getHeaders$3j
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3i = {"title":"Deepgram Enters Strategic Investment Agreement with In-Q-Tel","description":"Announcing a strategic investment and technology development agreement with In-Q-Tel (IQT).","date":"2020-06-04T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981353/blog/deepgram-enters-strategic-investment-agreement-with-in-q-tel-2/dg-enters-strategic-investment-w-inqtel%402x.jpg","authors":["scott-stephenson"],"category":"dg-insider","tags":["voice-tech","voice-strategy"],"seo":{"title":"Deepgram Enters Strategic Investment Agreement with In-Q-Tel","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981353/blog/deepgram-enters-strategic-investment-agreement-with-in-q-tel-2/dg-enters-strategic-investment-w-inqtel%402x.jpg"},"shorturls":{"share":"https://dpgr.am/f0d45a9","twitter":"https://dpgr.am/accb5cd","linkedin":"https://dpgr.am/1333969","reddit":"https://dpgr.am/0450c97","facebook":"https://dpgr.am/87dcf92"}};
						const file$3i = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-enters-strategic-investment-agreement-with-in-q-tel-2/index.md";
						const url$3i = undefined;
						function rawContent$3i() {
							return "\r\nToday, we're happy to announce a strategic investment and technology development agreement with [In-Q-Tel](https://www.iqt.org/) (IQT). IQT is a non-profit, strategic investor that helps accelerate the development and delivery of cutting-edge technologies to U.S. government agencies. Established in 1999, IQT has the goal of identifying and partnering with startups that are developing innovative technologies that can better protect and preserve the United States' security. We're proud to partner with IQT to bring Deepgram's automatic speech recognition technology to our nation's government agencies. Our agreement with IQT will allow their government partners to securely utilize our end-to-end deep learning-based platform for efficient and accurate speech recognition and transcription. \"Deepgram's use of an AI enabled, neural network architecture leveraging custom speech recognition models trained on vast amounts of audio data allows them to rapidly achieve much more accurate transcriptions for non-standard audio environments vs solutions like Google Voice and Apple Siri\", offered George Hoyem, Managing Partner, Investments, IQT. \"Using state of the art transfer learning also enables Deepgram to quickly build speech to text capabilities for new and novel language variants on relatively small amounts of training data, resulting in huge time savings for our government partners.\" We are excited to be a part of the IQT portfolio and are looking forward to expanding our relationship with the organization and its government partners.\r\n";
						}
						async function compiledContent$3i() {
							return load$3i().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3i() {
							return (await import('./chunks/index.f54e6f9c.mjs'));
						}
						function Content$3i(...args) {
							return load$3i().then((m) => m.default(...args));
						}
						Content$3i.isAstroComponentFactory = true;
						function getHeadings$3i() {
							return load$3i().then((m) => m.metadata.headings);
						}
						function getHeaders$3i() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3i().then((m) => m.metadata.headings);
						}

const __vite_glob_0_66 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3i,
  file: file$3i,
  url: url$3i,
  rawContent: rawContent$3i,
  compiledContent: compiledContent$3i,
  default: load$3i,
  Content: Content$3i,
  getHeadings: getHeadings$3i,
  getHeaders: getHeaders$3i
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3h = {"title":"Deepgram Rises to #1 on G2 and Receives Stevie Award for Customer Service","description":"Two recent awards highlight both our overall impact and commitment to customer success. Read on to learn more.","date":"2022-03-23T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981412/blog/deepgram-g2-customer-service/DG-rises-to-1-G2-and-Stevie-thumb-554x220%402x.png","authors":["ralphette-english"],"category":"dg-insider","tags":["voicebots","recognition"],"seo":{"title":"Deepgram Rises to #1 on G2 and Receives Stevie Award for Customer Service","description":"Two recent awards highlight both our overall impact and commitment to customer success. Read on to learn more."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981412/blog/deepgram-g2-customer-service/DG-rises-to-1-G2-and-Stevie-thumb-554x220%402x.png"},"shorturls":{"share":"https://dpgr.am/4794fae","twitter":"https://dpgr.am/d59baa3","linkedin":"https://dpgr.am/6b22c9f","reddit":"https://dpgr.am/659175b","facebook":"https://dpgr.am/d183cdb"}};
						const file$3h = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-g2-customer-service/index.md";
						const url$3h = undefined;
						function rawContent$3h() {
							return "\r\nWe're pleased to announce that we've recently received recognition from two different venues. First, we're now number 1 on G2 in the [Voice Recognition Software](https://www.g2.com/categories/voice-recognition?tab=highest_rated) category, as well as being highlighted as a High Performer in G2's most recent quarterly report. Second, we won a [Silver Stevie Award](https://stevieawards.com/sales/2022-stevie-award-winners#Provider) in the Sales or Customer Service Solutions Technology Partner of the Year category. One of the driving forces for both of these recognitions is our focus on customer service and success. If you've ever tried to use Big Tech's automatic speech recognition (ASR) solutions, you know that if you run into trouble or need help from an actual human, you're pretty much out of luck. You just have to hope that whatever your problem is clearly documented.\r\n\r\n## Customer Success is at the Heart of Deepgram\r\n\r\nAt Deepgram, however, we aspire to go beyond these kinds of transactional business relationships with our customers and instead aim to build partnerships. We want to ensure that all of our customers are achieving their desired outcomes and seeing the value of their partnership with Deepgram. Our Customer Success team works hand-in-hand with each customer to have a clear view into what's important for the customer, what their target milestones are, and what a path for growing our partnership looks like. We strive to provide an exceptional customer experience throughout the entire customer journey, from dedicated technical resources for deploying our services to dedicated customer success managers to advocate on their behalf. Deepgram gives each of our customers access to various resources throughout our organization. Need help with scaling? We'll pair you with a Solutions Engineer. Need a tailored model? We'll pair you with a Research Engineer. We're committed to the success of each customer that partners with Deepgram. For example, you can [read](https://deepgram.com/case-study-elerian-ai/) how Elerian.ai developed a partnership with us to create voicebots that are up to 98% accurate in speech to text.\r\n\r\n<WhitepaperPromo whitepaper=\"latest\"></WhitepaperPromo>\r\n\r\n\r\n\r\n## Give Deepgram a Try\r\n\r\nIf you'd like to see the difference that stellar customer service makes-especially when paired with the best speed and accuracy on the market-[contact us](https://deepgram.com/contact-us/) or, if you're a developer, [sign up for a free API key](https://console.deepgram.com/) and try Deepgram for yourself.\r\n";
						}
						async function compiledContent$3h() {
							return load$3h().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3h() {
							return (await import('./chunks/index.bfe34c26.mjs'));
						}
						function Content$3h(...args) {
							return load$3h().then((m) => m.default(...args));
						}
						Content$3h.isAstroComponentFactory = true;
						function getHeadings$3h() {
							return load$3h().then((m) => m.metadata.headings);
						}
						function getHeaders$3h() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3h().then((m) => m.metadata.headings);
						}

const __vite_glob_0_67 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3h,
  file: file$3h,
  url: url$3h,
  rawContent: rawContent$3h,
  compiledContent: compiledContent$3h,
  default: load$3h,
  Content: Content$3h,
  getHeadings: getHeadings$3h,
  getHeaders: getHeaders$3h
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3g = {"title":"Deepgram Named High Performer by G2 for the Second Quarter Running","description":"We're pumped to announce that we're a High Performer for the second quarter running, and second overall for Voice Recognition Software!","date":"2021-12-14T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981403/blog/deepgram-g2-review-winter-2022/blog-G2-high-performer-winter-21-thumb-554x220%402x.png","authors":["chris-doty"],"category":"dg-insider","tags":["recognition"],"seo":{"title":"Deepgram Named High Performer by G2 for the Second Quarter Running","description":"Were pumped to announce that we're a High Performer for the second quarter running, and second overall for Voice Recognition Software!"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981403/blog/deepgram-g2-review-winter-2022/blog-G2-high-performer-winter-21-thumb-554x220%402x.png"},"shorturls":{"share":"https://dpgr.am/d931094","twitter":"https://dpgr.am/9736aa9","linkedin":"https://dpgr.am/a1b79fd","reddit":"https://dpgr.am/1f2ab25","facebook":"https://dpgr.am/f891928"}};
						const file$3g = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-g2-review-winter-2022/index.md";
						const url$3g = undefined;
						function rawContent$3g() {
							return "G2's quarterly report for Voice Recognition Software is out, and we're pleased to announce that we've been named a High Performer for the second quarter in a row! We've also risen to the number two spot in [Voice Recognition](https://blog.deepgram.com/what-is-asr/) Software on G2, based on reviews from our users. According to G2, **91% of Deepgram users think we're heading in the right direction,** and 89% of users said they'd be likely to recommend us to others looking for voice recognition solutions. So, why are we ranked so highly? Is it our speed, our accuracy, or our ease of use? Yes! But that's not the only reason our customers love us. Here's a sample of what some of our users had to say about using Deepgram in their G2 reviews:\n\n* ⚡️  [\"Powerful, accurate, scalable and blindingly fast!\"](https://www.g2.com/products/deepgram/reviews/deepgram-review-5142053)\n* 💬  [\"The Deepgram API covers the languages we need (and then some)...\"](https://www.g2.com/products/deepgram/reviews/deepgram-review-5143659)\n* ✅  [\"Deepgram's accuracy is hands-down the best aspect of their service.\"](https://www.g2.com/products/deepgram/reviews/deepgram-review-5139446)\n\nIf you'd like to check out all of the reviews and discover even more reasons why our users love us so much-or leave your own review of Deepgram if you're already a fan-[take a look at our page on G2](https://www.g2.com/products/deepgram/reviews). Still have questions about speech recognition and what it can do for you? Check out our [Definitive Guide to Speech Recognition](https://deepgram.com/the-definitive-guide-to-speech-recognition/), which covers everything from the basics of the technology to what you need to think about when implementing an ASR system.\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-how-deepgram-works\"></WhitepaperPromo>";
						}
						async function compiledContent$3g() {
							return load$3g().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3g() {
							return (await import('./chunks/index.60affbdc.mjs'));
						}
						function Content$3g(...args) {
							return load$3g().then((m) => m.default(...args));
						}
						Content$3g.isAstroComponentFactory = true;
						function getHeadings$3g() {
							return load$3g().then((m) => m.metadata.headings);
						}
						function getHeaders$3g() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3g().then((m) => m.metadata.headings);
						}

const __vite_glob_0_68 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3g,
  file: file$3g,
  url: url$3g,
  rawContent: rawContent$3g,
  compiledContent: compiledContent$3g,
  default: load$3g,
  Content: Content$3g,
  getHeadings: getHeadings$3g,
  getHeaders: getHeaders$3g
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3f = {"title":"How to Build a Speech-Enhanced Game with Godot and Deepgram","description":"Learn how to build a speech-enhanced game with Deepgram's ASR engine and the open-source Godot game engine.","date":"2022-03-09T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1646783141/blog/2022/03/deepgram-godot-tutorial/assets/Building-a-Game-w-Godot-Deepgram%402x.jpg","authors":["nikola-whallon"],"category":"tutorial","tags":["game-dev","godot"],"seo":{"title":"How to Build a Speech-Enhanced Game with Godot and Deepgram","description":"Learn how to build a speech-enhanced game with Deepgram's ASR engine and the open-source Godot game engine."},"shorturls":{"share":"https://dpgr.am/daeaf14","twitter":"https://dpgr.am/8870f2b","linkedin":"https://dpgr.am/220032e","reddit":"https://dpgr.am/79c753a","facebook":"https://dpgr.am/1794969"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454020/blog/deepgram-godot-tutorial/ograph.png"}};
						const file$3f = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-godot-tutorial/index.md";
						const url$3f = undefined;
						function rawContent$3f() {
							return "\nIn this post, we will be making a simplified version of the 2D game \"Spooky Speech Spells\" (which you can play [here](https://spookyspeechspells.deepgram.com))\r\nin the Godot game engine using an integration with the Deepgram automatic speech recognition (ASR) engine. Why Godot? Because it is an\r\neasy-to-learn, open-source alternative to the popular industry-standard Unity and can export directly to Mac, Windows, Linux, iOS,\r\nAndroid, and HTML5. Why Deepgram? Because it is the fastest, most accurate, cheapest, and easiest to use ASR engine out there! And why\r\nmake a speech-enhanced game? Well, because it's cool, and games are fun! Beyond that, adding nontraditional input devices can help make your games more accessible.\n\nThis tutorial is focusing on adding voice input to your game, but if you want to see more tutorials, you can find plenty on the\r\n[Godot website](https://docs.godotengine.org/en/stable/community/tutorials.html).\n\n## Pre-requisites\n\nYou will need:\n\n*   Godot installed on your machine - [download Godot here](https://godotengine.org/download). This tutorial was written with version `3.4.3`.\n*   A Deepgram API Key - [get an API Key here](https://console.deepgram.com/signup?jump=keys).\n\n## Try the Game\n\nTo run the game we are going to build and browse its files:\n\n*   Download [this repository](https://github.com/deepgram/SpeechSpells), open Godot, click \"Import\", and browse to and select the `project.godot` file from the repo.\n*   In the Godot editor, go to the \"FileSystem\" tab in the lower left, and navigate to and open `Scenes/Game.gd`.\n*   Edit `line 7` of `Scenes/Game.gd` and enter your Deepgram API Key.\n*   In the top right of the Godot editor, hit the \"Play\" (►) button.\n*   Move your character with `WASD` and say \"fire\" to cast fire spells!\n\n## Building the Game\n\nIn the following sub-sections, we will walk through step-by-step how to make a game where you can move\r\na character around the screen and have the character cast fireball spells by chanting \"fire!\" into your\r\nmicrophone.\n\n## Setting Up the Project\n\nOpen Godot and create a \"New Project\" in the project manager.\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1646455688/blog/2022/03/deepgram-godot-tutorial/assets/new_project.png\" alt=\"Create a new project.\" style=\"max-width: 1037px;display: block;margin-left: auto;margin-right: auto;\">\n\nNow click \"Browse\" and choose a directory on your filesystem to store the project, and in the \"Project Name\"\r\nfield, write \"SpeechSpells\" and hit \"Create Folder.\" Finally, hit \"Create and Edit,\" and we'll get to the Godot\r\nEditor, where we can build our game.\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1646455688/blog/2022/03/deepgram-godot-tutorial/assets/set_project_name.png\" alt=\"Set the name of the new project.\" style=\"max-width: 1038px;display: block;margin-left: auto;margin-right: auto;\">\n\nThe Godot Editor consists of several panels and tabs, and we will be going over the ones we need to interact\r\nwith to build \"Speech Spells\".\n\nLet's start with the \"Scene\" tab in the upper left panel. In this tab, we can put together \"Scenes,\" which,\r\ntogether with \"Nodes,\" form the basic building blocks of Godot games. Scenes and nodes are classes\r\nin object-oriented programming terminology, but the Godot Editor offers an intuitive way to create, edit,\r\nand manage our various game objects as scenes and nodes. Scenes can be almost anything - a collection\r\nof sprite nodes, a character with animations, attacks, and hit-boxes, or just a collection of scripts\r\nthat execute algorithms and can emit signals to other nodes (we will see this pattern further on). For now,\r\nwhere it says \"Create Root Node\" click \"2D Scene\" and double click the name of the resulting node and rename\r\nit to \"Game\". We will create a few other scenes for this game and add instances of those scenes to our Game\r\nscene.\n\nBefore saving the scene, let's head over to the lower right \"Filesystem\" tab. Right-click in the filesystem\r\nbrowser there and click \"New Folder...\" to create a folder called \"Scenes\" and then a folder called \"Assets\".\r\nYou may, of course, organize your projects however you like, but this is at least one way of doing it.\n\nClick on `Scene -> Save Scene` in the upper left corner of the editor to save this scene as `Game.tscn` in the \"Scenes\" directory that you just created.\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1646455688/blog/2022/03/deepgram-godot-tutorial/assets/create_folders.png\" alt=\"Set up the game directory structure.\" style=\"max-width: 249px;display: block;margin-left: auto;margin-right: auto;\">\n\nNow, go to `Project -> Project Settings` from the upper left bar, and we will set a couple of properties of our game.\r\nStart by navigating to `Rendering -> Environment` and change the \"Default Clear Color\" to black.\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1646455688/blog/2022/03/deepgram-godot-tutorial/assets/default_clear_color.png\" alt=\"Change the default clear color.\" style=\"max-width: 913px;display: block;margin-left: auto;margin-right: auto;\">\n\nNext, navigate to `Display -> Window`, uncheck \"Resizable\", set the \"Width\" to 320, the \"Height\" to 240,\r\nthe \"Test Width\" to 960, the \"Test Height\" to 720, the \"Stretch\" \"Mode\" to \"2d\" and the\r\nStretch \"Aspect\" to \"Keep\". This is setting us up to build a game\r\nwith a very low resolution close to 240p (common for 8-bit pixel games), but when we run the game it will\r\ndisplay in a nice big 960x720 window, effectively tripling the size of the pixels. Godot has many options\r\nunder `Display -> Window`, and understanding these can enable you to effortlessly build games that look\r\nfantastic at multiple resolutions and multiple aspect ratios targeting different devices without ever\r\nhaving to think about anything other than your base resolution (320x240 in our case)!\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1646549594/blog/2022/03/deepgram-godot-tutorial/assets/change_window_settings.png\" alt=\"Set the window settings.\" style=\"max-width: 986px;display: block;margin-left: auto;margin-right: auto;\">\n\nWe have one more setting to change - navigate to `Application -> Audio` and check \"Enable Audio Input\".\r\nA warning will appear saying you will need to restart the editor for this to take effect, click the button\r\nthat appears in the lower right (\"Save & Restart\") to do so.\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1646455688/blog/2022/03/deepgram-godot-tutorial/assets/enable_audio.png\" alt=\"Enable audio.\" style=\"max-width: 911px;display: block;margin-left: auto;margin-right: auto;\">\n\nFinally, click the \"Play\" button in the upper right (or press \"F5\") to start up the game - since this is the\r\nfirst time we are playing the game, you will be asked to tell Godot which scene you want to start when the\r\ngame is first opened - we only have `Game.tscn` so select that one. We now have a blank canvas ready to be filled!\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1646455688/blog/2022/03/deepgram-godot-tutorial/assets/press_play.png\" alt=\"Start the game.\" style=\"max-width: 310px;display: block;margin-left: auto;margin-right: auto;\">\n\n## Creating a Player\n\nNow that we have a running game in a window let's create a player to move around. In the top left of\r\nthe Godot editor, click `Scene -> New Scene`. Like when you first opened this project, the \"Scene\" tab\r\nwill give you some options for the root node. Click \"Other Node\" and navigate to, or use the search\r\nfield to find \"KinematicBody2D\" and click \"Create.\"\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1646548729/blog/2022/03/deepgram-godot-tutorial/assets/create_kinematic_body_2d.png\" alt=\"Create a KinematicBody2D.\" style=\"max-width: 913px;display: block;margin-left: auto;margin-right: auto;\">\n\n\"KinematicBody2D\" is one of the most reasonable types of objects to use for controllable characters.\r\nThis class/node offers convenient methods to make moving and handling collisions with various objects\r\neasy - we will discuss one of these methods shortly.\n\nNow, right-click the root node and click \"Add Child Node\" and create a \"Sprite\" node. Do this again\r\nand add a \"CollisionShape2D\" node. Your node structure should now look like the following:\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1646548729/blog/2022/03/deepgram-godot-tutorial/assets/player_node_structure.png\" alt=\"The node structure of the Player - the `KinematicBody2D` root node here contains a `Sprite` node and `CollisionShape2D` node as children.\" style=\"max-width: 250px;display: block;margin-left: auto;margin-right: auto;\">\n\nThe \"Sprite\" node will contain the image to use for our player. We will use a pixel art\r\nimage of a skull for our player; you can download it [here](https://res.cloudinary.com/deepgram/image/upload/v1646849910/blog/2022/03/deepgram-godot-tutorial/assets/skull.png). Drag the\r\nfile into the \"Assets\" directory in the \"Filesystem\" tab in the lower left, then click on the \"Import\"\r\ntab next to the \"Scene\" tab, uncheck \"Filter\" and click \"Reimport\":\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1646548728/blog/2022/03/deepgram-godot-tutorial/assets/import_skull.png\" alt=\"Import the skull image.\" style=\"max-width: 245px;display: block;margin-left: auto;margin-right: auto;\">\n\nThis is importing the image into our project and telling our project not to apply interpolation\r\nwhen scaling the image should the window size change from the base window size (which is a tiny\r\n320x240 for us). Most art assets ought to have some interpolation applied when scaling to smaller\r\n(or larger) resolutions, but this looks notoriously bad for pixel art, where one would expect\r\nthe sharp and blocky sprite to remain sharp and blocky at any resolution.\n\nNow, navigate back to the \"Scene\" tab, click on the \"Sprite\" node, and drag and drop `skull.png`\r\nfrom the \"Asset\" directory to the \"Texture\" field:\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1646548729/blog/2022/03/deepgram-godot-tutorial/assets/set_skull_texture.png\" alt=\"Set the Player's sprite texture.\" style=\"max-width: 1850px;display: block;margin-left: auto;margin-right: auto;\">\n\nAfter doing this, you should see an image in the scene's 2D view. Finally, some graphics!\r\nYou may want to play around with zooming in or out to get a better view.\n\nNow, click on the \"CollisionShape2D\" node, then click on the \"Shape\" field on the upper right\r\nand select \"RectangleShape2D\", and change the \"Extents\" \"x\" and \"y\" fields to both be 8:\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1646548729/blog/2022/03/deepgram-godot-tutorial/assets/add_rectangle_shape_2d.png\" alt=\"Setup the Player's CollisionShape2D.\" style=\"max-width: 1850px;display: block;margin-left: auto;margin-right: auto;\">\n\nThis is essentially defining the hit-box of the player. We won't utilize hit-boxes\r\nor collision detection in this tutorial, but it is good practice to learn how to\r\nset up a player as one typically would in a more involved game.\n\nRename the root node \"Player\" and then save the scene as `Player.tscn` in the \"Scenes\" directory.\r\nNow, with the root node (\"Player\") selected, hit the button which looks like a script with a green\r\nplus sign on it to create a script for this node:\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1646548728/blog/2022/03/deepgram-godot-tutorial/assets/attach_script.png\" alt=\"Attach a script to a node.\" style=\"max-width: 610px;display: block;margin-left: auto;margin-right: auto;\">\n\nThis will create a `Player.gd` script, that will help define how this node functions in the game.\r\n`.gd` is the file extension for GDScript source code. GDScript is one of two languages which Godot uses\r\nnatively (the other being a visual programming language which is quite fun!). GDScript is a lot\r\nlike Python, and has a comparatively easy learning curve, helped by the fact that the Godot engine does most of\r\nthe complicated stuff under the hood, leaving the scripting of game objects to be short and quick.\r\nMake the contents of `Player.gd` the following, and you may start to note how few lines of code it takes to perform some actions:\n\n    extends KinematicBody2D\r\n\r\n    export var speed = 100\r\n    var velocity = Vector2(0, 0)\r\n\r\n    func _physics_process(_delta):\r\n    \tif Input.is_key_pressed(KEY_W):\r\n    \t\tvelocity.y = -speed\r\n    \telif Input.is_key_pressed(KEY_S):\r\n    \t\tvelocity.y = speed\r\n    \telse:\r\n    \t\tvelocity.y = 0\r\n\r\n    \tif Input.is_key_pressed(KEY_A):\r\n    \t\tvelocity.x = - speed\r\n    \telif Input.is_key_pressed(KEY_D):\r\n    \t\tvelocity.x = speed\r\n    \telse:\r\n    \t\tvelocity.x = 0\r\n\r\n    \tvar _returned_velocity = move_and_slide(velocity, Vector2(0, 0), false, 4, 0, false)\r\n\r\n    \tif position.x < 0 - 16:\r\n    \t\tposition.x = 320 + 16\r\n    \tif position.x > 320 + 16:\r\n    \t\tposition.x = 0 - 16\r\n\r\n    \tif position.y < 0 - 16:\r\n    \t\tposition.y = 240 + 16\r\n    \tif position.y > 240 + 16:\r\n    \t\tposition.y = 0 - 16\n\nThe first line, `extends KinematicBody2D`, is telling us that our script is an extension of the \"KinematicBody2D\" class,\r\nmeaning we will be able to access any methods and variables that the \"KinematicBody2D\" class offers, plus any variables\r\nand methods that we introduce here in this file. We then define the variables `speed` and `velocity`. We will use `velocity`\r\nto mean the velocity of the player at any given time, and `speed` to mean the maximum horizontal or vertical speed of the\r\nplayer when we move the player.\n\nNext comes the method `_physics_process(_delta)`. This is a method accessible to many Godot nodes, and is executed\r\nroughly 60 times per second, allowing us to alter objects in a way that the physics engine can understand.\r\nThe \"delta\" argument is the amount of time that has passed since the last call to `_physics_process`, but here we\r\nare not using it, so we place an underscore in front of the argument name to avoid a warning.\n\nIn this method, we check if the `WASD` keys are pressed, and modify the player's velocity accordingly (\"W\" to move up,\r\n\"A\" to move left, \"S\" to move down, and \"D\" to move right). Note that to move up, we set the \"y\" velocity to `-speed` - this\r\nis because Godot, like many game engines, considers *down* to be the positive y-direction.\n\nAfter adjusting the player's velocity, we call the `move_and_slide` method, specifying the velocity as one of its arguments\r\n(the others don't matter for now). This method does a lot of logic internally and can handle collisions with static bodies\r\nand rigid bodies, of any shape or size, and correctly move the player along the surface of bodies instead of bouncing\r\noff of them (hence the \"slide\"). The method returns the resulting velocity of the player after any collisions/slides,\r\nbut since we won't be using this, we place an underscore to avoid a warning.\n\nLastly, we check and modify the position of the player to wrap around the window - i.e. if the player moves too far to the left,\r\nhave them wrap to the right side of the window.\n\nSave, and finally go back to the \"Game\" scene, and click the button at the top of the \"Scene\" tab to \"Instance Child Scene\":\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1646548728/blog/2022/03/deepgram-godot-tutorial/assets/instance_child_scene.png\" alt=\"Instance a child scene.\" style=\"max-width: 626px;display: block;margin-left: auto;margin-right: auto;\">\n\nSelect the `Player.tscn` we just created. Now our main \"Game\" scene has a \"Player\" scene as a child node! Click the \"Play\" button\r\nand now the game boots up with a skull in the corner - you can move the skull around with `WASD`! Feel free to reposition the skull\r\nin the \"Game\" scene - you can either click and drag the player in the 2D view, or you can directly enter the x and y position\r\nin `Node2D -> Transform -> Position` in the \"Inspector\" tab when the \"Player\" node is selected in the \"Game\" scene:\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1646785008/blog/2022/03/deepgram-godot-tutorial/assets/player_position_in_editor.png\" alt=\"Setting the Player's position.\" style=\"max-width: 1850px;display: block;margin-left: auto;margin-right: auto;\">\n\n## Creating a Fireball\n\nLet's make a fireball scene which we will use to have the player shooting fireballs across the screen.\r\nCreate a new scene, for the root node click \"Other Node\" and pick \"Area2D\". Rename this root node \"Fireball\".\r\nAdd two child nodes: an \"AnimatedSprite\" and a \"CollisionShape2D\".\n\nNext, import into your \"Assets\" directory [fireball\\_1.png](https://res.cloudinary.com/deepgram/image/upload/v1646850119/blog/2022/03/deepgram-godot-tutorial/assets/fireball_1.png)\r\nand [fireball\\_2.png](https://res.cloudinary.com/deepgram/image/upload/v1646850119/blog/2022/03/deepgram-godot-tutorial/assets/fireball_2.png).\r\nThe two sprites will make up our fireball animation. For each of the imported fireball sprites, go to the \"Import\" tab,\r\nuncheck \"Filter\" and click \"Reimport\" - just like for our pixel art player sprite, this will ensure that these sprites\r\nmaintain their blocky pixel form even on high-resolution displays.\n\nNow, click the \"AnimatedSprite\" node, and on the right in the \"Frames\" field, click where it says \"\\[empty]\" and select\r\n\"New SpriteFrames\". Then click the field again and you should be brought to an editor view where we can add our animation.\r\nClick and drag the `fireball_1.png` and `fireball_2.png` files from the \"Assets\" directory into the \"Animation Frames\" box:\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1646613234/blog/2022/03/deepgram-godot-tutorial/assets/fireball_animation.png\" alt=\"Create the fireball animation.\" style=\"max-width: 1850px;display: block;margin-left: auto;margin-right: auto;\">\n\nNext, click the \"AnimatedSprite\" node again to bring up the \"Inspector\" tab for this node again, check the box titled \"Playing\".\r\nIn the lower-left of the \"Animations\" tab, change the \"Speed\" field to \"12 FPS\". The fireball should now be animated in the editor:\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1646784180/blog/2022/03/deepgram-godot-tutorial/assets/fireball_editor_cropped_animated.gif\" alt=\"The fireball animation in the editor.\" style=\"max-width: 1850px;display: block;margin-left: auto;margin-right: auto;\">\n\nThere are certainly several tabs and fields to navigate through in this process, but I hope you find some of these operations intuitive!\r\nTo create animations, you drag the individual frames into the \"Animation Frames\"\r\nbox, you can then change the speed of the animation just left of this box, and you can set which animation plays by default\r\nin the \"Animation\" field of the \"Inspector\" tab for the \"AnimatedSprite\" node - here you can also set whether the animation\r\nis turned on or off with the \"Playing\" check box.\n\nNow, click the \"CollisionShape2D\" node, and in the \"Inspector\" tab for the \"Shape\" field select \"New CircleShape2D\". Then\r\nclick the \"CicleShape2D\" to edit it's properties and change its radius to 4:\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1646613234/blog/2022/03/deepgram-godot-tutorial/assets/fireball_hitbox.png\" alt=\"Setting the fireball's hit-box.\" style=\"max-width: 1850px;display: block;margin-left: auto;margin-right: auto;\">\n\nNow, save the scene as `Fireball.tscn` in the \"Scenes\" directory, and then attach a script to the root node. Make the contents\r\nof the script as follows:\n\n    extends Area2D\r\n\r\n    export var speed = 220\r\n    var direction = Vector2(0, 0)\r\n\r\n    func _physics_process(delta):\r\n    \tvar velocity = direction.normalized() * speed\r\n\r\n    \trotation = velocity.angle()\r\n    \tposition += velocity * delta\r\n\r\n    \tif position.x > 320 + 16:\r\n    \t\tget_tree().queue_delete(self)\r\n    \tif position.x < 0 - 16:\r\n    \t\tget_tree().queue_delete(self)\r\n    \tif position.y > 240 + 16:\r\n    \t\tget_tree().queue_delete(self)\r\n    \tif position.y < 0 - 16:\r\n    \t\tget_tree().queue_delete(self)\n\nLike with the \"Player\" scene, we are extending the root node's class, in this case an \"Area2D.\" We will give each fireball object\r\na speed and a direction. When `_physics_process` is called, we will update the fireball's position and angle according to\r\nthe fireball's direction and speed. If the fireball goes off-screen, we will destroy it using `get_tree().queue_delete(self)`.\n\nThat's all there is to the \"Fireball\" scene, but we haven't actually added any fireballs to our game. We could do this by instancing\r\na \"Fireball\" scene in our \"Game\" scene, but for these kinds of objects, there's a better way - we should spawn them via code!\n\n## Triggering the Fireball With Keystrokes\n\nGo to the \"Game\" scene and add a script to the root node, just like we did for the \"Player\" scene.\r\nThen edit `Game.gd` to have the following contents:\n\n    extends Node2D\r\n\r\n    var rng = RandomNumberGenerator.new()\r\n\r\n    func _ready():\r\n    \trng.randomize()\r\n\r\n    func _input(event):\r\n    \tif event is InputEventKey and event.pressed:\r\n    \t\tif event.scancode == KEY_F:\r\n    \t\t\tfor i in rng.randi_range(2, 5):\r\n    \t\t\t\tspawn_fireball()\r\n\r\n    func spawn_fireball():\r\n    \tvar fireball = load(\"res://Scenes/Fireball.tscn\").instance()\r\n    \tadd_child(fireball)\r\n\r\n    \tvar random_angle = rng.randf_range(0.0, 2 * PI)\r\n    \tfireball.direction = Vector2(cos(random_angle), sin(random_angle))\r\n    \tfireball.rotation = fireball.direction.angle()\r\n    \tfireball.position = $Player.position\n\nThe first line, `extends Node2D`, is essentially saying that this object is extending the \"Node2D\" class.\r\nThen, we create a global variable for this object called `rng` which will be used for random number generation.\r\nNext, we define the `_ready()` method which is called when an instance of this scene gets created - in this\r\nmethod we are initializing our random number generator.\n\nThe `_input(event)` method gets called every time there was an input event such as a keystroke, a mouse click,\r\na touch, a game-pad button press, etc. In our case, we are looking to see if the \"F\" key was pressed, and if\r\nso, we want to spawn 2-5 fireballs!\n\nThe logic handling the spawning of fireballs occurs in the method `spawn_fireball()`. Here we create an instance\r\nof our \"Fireball\" scene, add it as a child of the current scene, and then initialize the fireball's direction,\r\nrotation, and position. We are setting the fireball to spawn exactly where the player object is located,\r\nand we are setting the fireball's direction to be totally random.\n\nThe syntax `$Player` is syntax sugar\r\nfor `get_node(\"Player\")` and requires that our \"Game\" scene has a child node named \"Player\" (which it does!).\r\nHowever, since GDScript is very much like Python, the game will build and run just fine if one makes\r\na reference to a non-existent object - this will be caught only when the program reaches that line of code,\r\nand it will cause a crash. As Python developers are likely aware, this is sometimes one of the trade-offs of\r\nhaving a \"quick and easy\" language.\n\nYou should now be able to play the game, move the player around, and press \"F\" to fire off fireballs! Now\r\nthat the basic game is complete, let's add the juicy part by triggering the fireballs not with key presses,\r\nbut with your voice!\n\n## Triggering the Fireball With Your Voice\n\nFinally, let's do our Deepgram integration so that we can spawn fireballs by saying \"fire\" into the microphone instead\r\nof pressing a key. To do this, grab the `DeepgramIntegration` directory from `Scenes/DeepgramIntegration`\r\nfrom the [the SpeechSpells repository](https://github.com/deepgram/SpeechSpells) and place it in the `Scenes/` directory in the Godot editor.\n\nThis integration contains two Godot scenes with accompanying scripts: `MicrophoneInstance` and `DeepgramInstance`.\r\nWe won't go over the inner-workings of these scripts in detail, but feel free to have a look as they have\r\na fair amount of descriptive comments to help explain what is going on. In a nutshell, the `MicrophoneInstance`\r\ninterfaces with your device's microphone and streams the raw audio from the microphone via Godot signals\r\nto the `DeepgramInstance` which handles connecting to Deepgram via Websockets.\r\nThe `DeepgramInstance` then forwards the raw microphone audio to Deepgram, receives ASR results from Deepgram,\r\nand then forwards those results via Godot signals to some other node. In our case, this other node will be\r\nour \"Game\" scene's root node.\n\n***\n\n***Note***: A common issue which causes the microphone capture to fail on Mac is if Godot's audio sample rate is set to\r\nsomething different then the OS's audio sample rate. If you experience issues with microphone capture on Mac, you\r\ncan check your OS's audio sample rate under `Utilities -> Audio Midi Setup`.\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1646351903/blog/2022/03/deepgram-godot-tutorial/assets/mac_sample_rate_settings.png\" alt=\"Mac audio sample rate settings.\" style=\"max-width: 1850px;display: block;margin-left: auto;margin-right: auto;\">\n\n***\n\n***\n\n***Note***: This integration will not work for Godot's HTML5 builds out-of-the-box\r\nas authenticating websockets connections with headers is not supported for these builds due to\r\nsome authentication limitations of browsers. If you plan on making an HTML5 game with the Deepgram\r\nintegration, you will have to deploy a proxy server for authentication and make some minor adjustments\r\nof the `DeepgramInstance` scene. This may be a topic of a future guide!\n\n***\n\nIn your \"Game\" scene, add as a child an instance of the \"DeepgramInstance\" scene, then modify `Game.gd` as follows:\n\n    extends Node2D\r\n\r\n    var rng = RandomNumberGenerator.new()\r\n\r\n    func _ready():\r\n    \trng.randomize()\r\n    \t$DeepgramInstance.initialize(\"INSERT_YOUR_API_KEY_HERE\")\r\n\r\n    func _on_DeepgramInstance_message_received(message):\r\n    \tvar message_json = JSON.parse(message)\r\n    \tif message_json.error == OK:\r\n    \t\tif typeof(message_json.result) == TYPE_DICTIONARY:\r\n    \t\t\tif message_json.result.has(\"is_final\"):\r\n    \t\t\t\tif message_json.result[\"is_final\"] == true:\r\n    \t\t\t\t\tvar message_transcript = message_json.result[\"channel\"][\"alternatives\"][0][\"transcript\"]\r\n    \t\t\t\t\tprint(\"Transcript received: \" + message_transcript)\r\n    \t\t\t\t\tfor _i in message.count(\"fire\"):\r\n    \t\t\t\t\t\tspawn_fireball()\r\n\r\n    \telse:\r\n    \t\tprint(\"Failed to parse Deepgram message!\")\r\n\r\n    func spawn_fireball():\r\n    \tvar fireball = load(\"res://Scenes/Fireball.tscn\").instance()\r\n    \tadd_child(fireball)\r\n\r\n    \tvar random_angle = rng.randf_range(0.0, 2 * PI)\r\n    \tfireball.direction = Vector2(cos(random_angle), sin(random_angle))\r\n    \tfireball.rotation = fireball.direction.angle()\r\n    \tfireball.position = $Player.position\n\nRemember to replace `INSERT_YOUR_API_KEY_HERE` with your Deepgram API Key.\n\nNow, to finish up, click your \"Game\" scene's \"DeepgramInstance\" node and in the right panel click the \"Node\" tab (it should be right next\r\nto the \"Inspector\" tab). You should see a `message_received()` signal listed under `DeepgramInstance.gd` - double click this,\r\nmake sure that the \"Game\" node is highlighted, and click \"connect\".\n\nWhat is this signal doing? Signals are a useful way to organize the transmission of events in game engines.\r\nIn this particular case, the \"DeepgramInstance\" node is signaling to its parent \"Game\" node that it has received a message from Deepgram.\r\nIt passes this message with the signal, the parent \"Game\" node can then react to the signal to trigger further logic.\n\nIn our case, the \"Game\" node handles this signal in the `_on_DeepgramInstance_message_received` method where tries to parse\r\nthe JSON message from Deepgram into a data structure similar to a Dictionary using `JSON.parse(message)`.\n\nAfter verifying that the message successfully parsed as a Dictionary, it checks to see whether this is a final result or an interim result.\r\nTo understand the difference between final and interim results in the Deepgram realtime streaming API, check out [this page](https://developers.deepgram.com/documentation/features/interim-results/).\r\nFor our purposes, we are only considering final results. We then grab the transcript from the first alternative in the\r\nASR result, and count how many times the word \"fire\" appears in the transcript, spawning one fireball for each occurrence.\n\n## Build New Features\n\nSo now you can walk around and cast fireball spells with your voice! Here are some ideas for next steps\r\nyou may want to try out to make a more fully-featured game:\n\n*   Implement more spells! Try a thunder spell, or an ice beam spell.\n*   Add enemies and implement collision detection to destroy enemies when your spells hit them.\n*   Try using [interim results](https://developers.deepgram.com/documentation/features/interim-results/) instead of just final results - this should decrease the latency of the spells substantially, but you will need to watch out for\r\n    double-counting as multiple interim results will give transcripts for the same section of audio!\n*   Play around with Deepgram's `keyword`, `search`, and/or `phoneme` features to implement spells for out-of-vocab words.\n*   If you are feeling ambitious, work through some Godot networking tutorials to create a multiplayer game where you and your friends cast spells at each other!\n\n## Final Thoughts\n\nSpeech-enhanced games have actually been around for quite some time, with popular titles such as \"Hey You, Pikachu!\" and \"Seaman\",\r\nbut it hasn't been until more recently that ASR engines have really taken off and can start to offer gameplay experiences which are deeper\r\nthan the simple command-based games of the past which only understood a few phrases. With modern ASR engines like Deepgram's, understanding\r\nthousands of commands has become trivial, and much more interesting and complex uses for speech in games is now possible. I encourage anyone\r\ninterested to try things out and explore with us what the future of speech in games might look like! Off the top of my head, I want to lastly\r\nshare some game ideas to get the creative juices flowing in the community:\n\n*   An RPG where warriors fight with swords, archers fight with bows, and mages fight with... speech! Take the \"casting spells\" approach of this\r\n    tutorial and expand it to a game about incantations! (See \"In Verbis Virtus\" for even more inspiration here!)\n*   Speech-enhanced mini-games: imagine playing a Zelda-esque game where you walk into a town and go to the nearest mini-game building and have\r\n    to play word games ala \"Wheel of Fortune\" to win your next upgrade!\n*   Games fully controllable via speech: traditional turn-based games are a great candidate to integrate speech controls, and this could greatly\r\n    increase the accessibility of such games! This type of technology does exist, but it can likely be greatly improved with new powerful ASR engines\r\n    like Deepgram.\n*   Casual social games: imagine playing word and speech-based games in an environment like VR chat!\n*   AI-based visual novels: imagine playing a visual novel where instead of picking from a list of phrases to progress, you simply say what's on your mind!\n*   AAA AI-enhanced experiences: take the visual novel idea to the next level and enhance AI components of AAA titles by allowing your character to\r\n    talk to NPCs with a microphone! (Check out games like \"Phasmophobia\" for even more inspiration here!)\n\nIf you have any questions, please feel free to reach out on Twitter - we're @DeepgramDevs.\n\n        ";
						}
						async function compiledContent$3f() {
							return load$3f().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3f() {
							return (await import('./chunks/index.b9c40b02.mjs'));
						}
						function Content$3f(...args) {
							return load$3f().then((m) => m.default(...args));
						}
						Content$3f.isAstroComponentFactory = true;
						function getHeadings$3f() {
							return load$3f().then((m) => m.metadata.headings);
						}
						function getHeaders$3f() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3f().then((m) => m.metadata.headings);
						}

const __vite_glob_0_69 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3f,
  file: file$3f,
  url: url$3f,
  rawContent: rawContent$3f,
  compiledContent: compiledContent$3f,
  default: load$3f,
  Content: Content$3f,
  getHeadings: getHeadings$3f,
  getHeaders: getHeaders$3f
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3e = {"title":"A Deepgram Hackathon Recap: What makes a good submission?","description":"The Deepgram + Dev.to hackathon was an opportunity to build with Deepgram or share your coolest ideas, and we got some great ones. Check out what made them stand out.","date":"2022-05-10T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1652193528/blog/2022/05/deepgram-hackathon-recap/Devto-Hackathon-Winners%402x.jpg","authors":["bekah-hawrot-weigel"],"category":"best-practice","tags":["hackathon"],"seo":{"title":"A Deepgram Hackathon Recap: What makes a good submission?","description":"The Deepgram + Dev.to hackathon was an opportunity to build with Deepgram or share your coolest ideas, and we got some great ones. Check out what made them stand out."},"shorturls":{"share":"https://dpgr.am/3078aa3","twitter":"https://dpgr.am/f6eee4d","linkedin":"https://dpgr.am/27e71dc","reddit":"https://dpgr.am/1f9a0f9","facebook":"https://dpgr.am/0b92786"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454083/blog/deepgram-hackathon-recap/ograph.png"}};
						const file$3e = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-hackathon-recap/index.md";
						const url$3e = undefined;
						function rawContent$3e() {
							return "\r\nTo be perfectly honest, this was my first hackathon. Now, I didn’t get to participate, but I was able to answer questions, cheer on our participants, and read all of the posts. And what I learned is that there are some really inspiring people out there. One of my favorite parts of the hackathon was reading through all the posts and seeing the positive comments and interactions. It was hard to choose the top projects or posts in many cases, but you can find the [full list of winners here](https://dev.to/devteam/congrats-to-the-deepgram-hackathon-winners-586i). What stood out among these participants and the others we ranked high on our list? How can you make your hackathon submission stand out in future hackathons? That's what we're talking about today. Let's explore five ways to make your hackathon submission stand out.\r\n\r\n## Grab the Reader’s Attention\r\n\r\nWhen you’re submitting to a hackathon, you want to stand out. The quickest way to do that is to grab the reader's attention. This could be through the project's topic, usefulness, or playfulness. By pulling the reader in early in your submission, you’ve hooked them into reading more and invited them to go on the journey with you.\r\n\r\n## Don’t Shy Away from Storytelling with Your Hackathon Submission\r\n\r\nYou might remember from my [Technical Writing: A Developer’s Guide to Storytelling](https://blog.deepgram.com/technical-writing-a-developers-guide-to-storytelling/) that storytelling connects you to the reader on a more personal level. It takes it a step deeper than getting the reader’s attention. How will you stay in the reader’s memory? Think about what your reader needs--I wrote more about this in [Technical Writing: a Beginner’s Guide](https://blog.deepgram.com/technical-writing-a-beginners-guide/) if you want to learn more. Can you help solve their problem? When they read through your blog, will they say, “I could **really** use that!” If they do, you’ve pulled them into your story, and they’ll remember you.\r\n\r\n## Create a Clear Purpose and Explain your Project Approach\r\n\r\nHackathons are ultimately about solving a problem. The reader should know what problem you’re trying to solve. This is one of the reasons why writing a blog post along with your submissions can be helpful. You can clarify what you’re doing, and why you’re doing it and provide the reader with the context they need to understand your project.\r\n\r\n## Don’t Underestimate Creativity when Using the Product\r\n\r\nEach of the winners gave a unique take on a problem that was solved or improved with Deepgram’s Speech-to-Text Technology. If you read through all of our submissions, you’ll notice that some projects overlapped or were trying to solve the same problem. How can you differentiate yourself from similar projects?\r\n\r\n## Include a Live Preview or Video Demo\r\n\r\nHaving a way for the reader to understand what your project does- whether through a live demo or a video recording- helps tell the story of the code you’ve created. Although we did go through and run the code of other projects that ranked high on our list, seeing a project and interacting with it in some way helps grab the reader’s attention and remember the project.\r\n\r\nThere are a lot of benfits to entering hackathons beyond winning prizes. Entering hackathons is a great way to create a portfolio project, work on project-based learning, and see how others approach a problem.\r\n\r\nCongratulations to all of our participants. Thank you for sharing your stories and your projects with us. We took a lot of inspiration away from this event, and we’re looking forward to continuing to learn from you and sharing what we’ve learned. If you’re interested in hearing more from us, you can check out our [Community Conversations: Getting Started in DevRel](https://t.co/B55MR34gHn) meetup in a couple of weeks or follow us on Twitter at [@DeepgramDevs](https://twitter.com/DeepgramDevs) to catch our next Twitter Space.\r\n\r\n        ";
						}
						async function compiledContent$3e() {
							return load$3e().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3e() {
							return (await import('./chunks/index.08237a88.mjs'));
						}
						function Content$3e(...args) {
							return load$3e().then((m) => m.default(...args));
						}
						Content$3e.isAstroComponentFactory = true;
						function getHeadings$3e() {
							return load$3e().then((m) => m.metadata.headings);
						}
						function getHeaders$3e() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3e().then((m) => m.metadata.headings);
						}

const __vite_glob_0_70 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3e,
  file: file$3e,
  url: url$3e,
  rawContent: rawContent$3e,
  compiledContent: compiledContent$3e,
  default: load$3e,
  Content: Content$3e,
  getHeadings: getHeadings$3e,
  getHeaders: getHeaders$3e
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3d = {"title":"Deepgram is a Founding Member of CallMiner’s Open Voice Transcription Standard (OVTS)","description":"CallMiner has created a standard to provide flexibility in choosing the right speech recognition solution for your voice use case.","date":"2021-05-27T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981372/blog/deepgram-is-a-founding-member-of-callminers-open-voice-transcription-standard-ovts/dg-founding-member-callminer-OVTS%402x.jpg","authors":["keith-lam"],"category":"dg-insider","tags":["call-miner"],"seo":{"title":"Deepgram is a Founding Member of CallMiner’s Open Voice Transcription Standard (OVTS)","description":"CallMiner has created a standard to provide flexibility in choosing the right speech recognition solution for your voice use case."},"shorturls":{"share":"https://dpgr.am/705f9e2","twitter":"https://dpgr.am/075fc6b","linkedin":"https://dpgr.am/b7c4cd8","reddit":"https://dpgr.am/8ee00c7","facebook":"https://dpgr.am/17c6fe5"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981372/blog/deepgram-is-a-founding-member-of-callminers-open-voice-transcription-standard-ovts/dg-founding-member-callminer-OVTS%402x.jpg"}};
						const file$3d = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-is-a-founding-member-of-callminers-open-voice-transcription-standard-ovts/index.md";
						const url$3d = undefined;
						function rawContent$3d() {
							return "\r\nDeepgram is proud to be a founding member of [CallMiner's Open Voice Transcription Standard (OVTS)](https://callminer.com/news/press-releases/callminer-introduces-open-voice-transcription-standard-ovts). This standard programming framework enables CallMiner's customers to choose the best speech recognition provider for their use cases. The OVTS API allows an easier integration to any automatic speech recognition (ASR) solution to the CallMiner [Eureka](https://callminer.com/products/eureka) platform. The flexibility of this new standard provides organizations more options for ASR providers depending on use cases. Businesses are not locked into an ASR provider that is good for simple transcription summaries of phone calls but is poor at real-time transcriptions for call analytics and alerts. Now, they can use one ASR provider for phone call summaries and use a faster ASR provider for real-time call analytics and alerts.\r\n\r\n### **Best of Breed for Your Use Case**\r\n\r\nThis new standard follows the overall business trend away from the all-in-one platform solution to a flexible best-of-breed solution that can be tailored specifically to your business and use cases. These API-based best-of-breed solutions allow your organization to control your innovation roadmap.  You are no longer beholden to another company's innovation timeline, budget, or resources. You can now choose to upgrade or replace specific solutions on your timeline for new use case needs or to enter new markets.\r\n\r\n### **Real-time Speed and Domain-Specific Accuracy**\r\n\r\nFor CallMiner's customers, they can now easily implement Deepgram's End to End [AI Speech Platform](https://offers.deepgram.com/how-deepgram-works-whitepaper) with Eureka to get &lt;300-millisecond transcriptions for true real-time conversational analytics.  They can take advantage of Deepgram's [custom speech models](https://offers.deepgram.com/hubfs/Collateral/Deepgram-Speech%20Models-Product%20Sheet.pdf) to train a vertical, domain, and/or use the case-specific model that can reach 90%+ accuracy.  Deepgram's custom models are especially useful for keywords, terminology, jargon, and product or company names that are most important for conversational insights.  Finally, they can lower their overall Total Cost of Ownership (TCO) with our lower ASR cost and computing resource efficiency. With Deepgram, you don't need to compromise speed, accuracy, costs, or scalability. [Consider Deepgram](https://deepgram.com/contact-us) for your real-time ASR.\r\n";
						}
						async function compiledContent$3d() {
							return load$3d().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3d() {
							return (await import('./chunks/index.6068e62f.mjs'));
						}
						function Content$3d(...args) {
							return load$3d().then((m) => m.default(...args));
						}
						Content$3d.isAstroComponentFactory = true;
						function getHeadings$3d() {
							return load$3d().then((m) => m.metadata.headings);
						}
						function getHeaders$3d() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3d().then((m) => m.metadata.headings);
						}

const __vite_glob_0_71 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3d,
  file: file$3d,
  url: url$3d,
  rawContent: rawContent$3d,
  compiledContent: compiledContent$3d,
  default: load$3d,
  Content: Content$3d,
  getHeadings: getHeadings$3d,
  getHeaders: getHeaders$3d
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3c = {"title":"Deepgram Now Offers More than 20 Language and Dialect Speech Models ","description":"We're pleased to announce that we now offer more than 20 languages and dialect speech models—read on to learn more about what we offer.","date":"2022-03-29T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981413/blog/deepgram-language-speech-models/DG-offers-over-20-language-models-thumb-554x220%402x.png","authors":["keith-lam"],"category":"dg-insider","tags":["language"],"seo":{"title":"Deepgram Now Offers More than 20 Language and Dialect Speech Models ","description":"We pleased to announce that we now offer more than 20 languages and dialect speech models—read on to learn more about what we offer."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981413/blog/deepgram-language-speech-models/DG-offers-over-20-language-models-thumb-554x220%402x.png"},"shorturls":{"share":"https://dpgr.am/14da294","twitter":"https://dpgr.am/5de8ab3","linkedin":"https://dpgr.am/57f1aa3","reddit":"https://dpgr.am/c255159","facebook":"https://dpgr.am/3641d94"}};
						const file$3c = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-language-speech-models/index.md";
						const url$3c = undefined;
						function rawContent$3c() {
							return "One of our driving principles at Deepgram is to make every voice heard and understood. If your business has international locations or customers, you need a speech recognition solution that can deliver the same level of accuracy, regardless of language or dialect. We're thrilled to announce that we now offer more than 20 language and dialect models, all of which provide the level of speed and accuracy that sets Deepgram apart.  This announcement marks Deepgram's commitment to expanding to new regions and improving accuracy for as many users as possible. Adding this new suite of languages is a significant step towards delivering a global language experience that is on par with the success we've seen from our English model. Deepgram provides businesses with the fastest, most efficient language models from day one, and outperforms the other ASR providers in the space.\n\n## 20+ Language Models Now Available\n\nBy utilizing transfer learning processes, and backed by our [end-to-end deep learning](https://offers.deepgram.com/how-deepgram-works-whitepaper) architecture trained with real-world audio datasets, we've been able to double the number of models we offer in just one quarter. Continuously incorporating new language models like these allows our team to provide the most cost-effective, fast, and accurate speech-to-text system on the market in an ever-expanding number of languages, allowing our users to make incredible speech experiences available to their customers worldwide. The 24 language models that we currently offer are:\n\n* Dutch\n* English (unique models for Australian, US, British, New Zealand, and Indian Englishes)\n* French (unique models for continental and Canadian French)\n* German\n* Hindi\n* Indonesian\n* Italian\n* Japanese\n* Korean\n* Mandarin (simplified and traditional)\n* Portuguese (unique models for continental and Brazilian)\n* Russian\n* Spanish (unique models for continental and Latin American)\n* Swedish\n* Turkish\n* Ukrainian\n\nBy providing a best-of-breed, API-based solution, Deepgram is creating meaningful and data-rich voice experiences that excite developers and improve the overall experience for customers. These new language models are now available for you and your software teams to use at no extra charge. And don't miss out; the following languages are free to use for a limited time!\n\n* Dutch\n* French\n* German\n* Indonesian\n* Italian\n* Japanese\n* Korean\n* Mandarin (simplified and traditional)\n* Russian\n* Swedish\n* Ukrainian\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-how-deepgram-works\"></WhitepaperPromo>\n\n## Wrapping Up\n\nWe've got lots more planned for the year ahead. If you'd like to stay in the know about any new language releases, you can sign up for our newsletter below, or keep an eye on our [languages page](https://deepgram.com/product/languages/) for updates on the models we offer. And if you'd like to give these languages a try, sign up for a [free API key](https://console.deepgram.com/) or [contact us](https://deepgram.com/contact-us/) to see how we can help with your multilingual ASR needs.";
						}
						async function compiledContent$3c() {
							return load$3c().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3c() {
							return (await import('./chunks/index.a26245fa.mjs'));
						}
						function Content$3c(...args) {
							return load$3c().then((m) => m.default(...args));
						}
						Content$3c.isAstroComponentFactory = true;
						function getHeadings$3c() {
							return load$3c().then((m) => m.metadata.headings);
						}
						function getHeaders$3c() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3c().then((m) => m.metadata.headings);
						}

const __vite_glob_0_72 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3c,
  file: file$3c,
  url: url$3c,
  rawContent: rawContent$3c,
  compiledContent: compiledContent$3c,
  default: load$3c,
  Content: Content$3c,
  getHeadings: getHeadings$3c,
  getHeaders: getHeaders$3c
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3b = {"title":"Deepgram Named a High Performer for Voice Recognition in G2 Fall Report","description":"Deepgram is listed as one of the best solutions for Voice Recognition on G2, ranking #3 out of 11 solutions. Check it out!","date":"2021-09-09T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981380/blog/deepgram-named-a-high-performer-for-voice-recognition-software-in-g2-fall-report/G2-leader%402x.png","authors":["alexa-de-la-torre"],"category":"dg-insider","tags":["recognition"],"seo":{"title":"Deepgram Named a High Performer for Voice Recognition in G2 Fall Report","description":"Deepgram is listed as one of the best solutions for Voice Recognition on G2, ranking #3 out of 11 solutions. Check it out!"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981380/blog/deepgram-named-a-high-performer-for-voice-recognition-software-in-g2-fall-report/G2-leader%402x.png"},"shorturls":{"share":"https://dpgr.am/b2ec85d","twitter":"https://dpgr.am/7f995d1","linkedin":"https://dpgr.am/4cceee8","reddit":"https://dpgr.am/16a31d4","facebook":"https://dpgr.am/6c0eee6"}};
						const file$3b = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-named-a-high-performer-for-voice-recognition-software-in-g2-fall-report/index.md";
						const url$3b = undefined;
						function rawContent$3b() {
							return "Deepgram has been recognized as one of the best solutions for Voice Recognition based on its high levels of customer satisfaction and likeliness to recommend from real users on G2, a leading software solution review platform.  Here is what one of our customers had to say, \"*We analyzed 10 transcription vendors and DG was the highest performing and they have wonderful support.*\" What they liked best is Deepgram's \"*hands-on approach*\" to customer support and onboarding.  As a high performer in the [Grid Report for Voice Recognition](https://www.g2.com/categories/voice-recognition) for Fall 2021, Deepgram was ranked [\\#3 out of 11 Voice Recognition solutions](https://www.g2.com/categories/voice-recognition?tab=highest_rated).\n\n\"*Being recognized by our customers is an honor that we do not take lightly. We will continue to collect feedback from our customers to improve our products and services and keep our High Performing G2 status*\" said Shadi Baqleh, Chief Operating Officer, Deepgram. For inclusion in the report a product must have received ten or more reviews. \"*Rankings on G2 reports are based on data provided to us by real users,*\" said Michael Fauscette, Chief Research Officer, G2. \"*We are excited to share the achievements of the products ranked on our site because they represent the voice of the user and offer terrific insights to potential buyers around the world.*\"  Learn more about what real users have to say (or share your experience by reviewing Deepgram [here](https://www.g2.com/products/deepgram/reviews)).\n\n### Additional Resources\n\n* [2021 State of ASR](https://offers.deepgram.com/state-of-asr-report-2021)\n* [How Deepgram Works](https://offers.deepgram.com/how-deepgram-works-whitepaper)\n* [How to Vet an ASR Provider](https://offers.deepgram.com/how-to-vet-an-asr-provider-thank-you)\n* [How to Make Your Application Voice Ready](https://offers.deepgram.com/whitepaper-how-to-make-your-application-voice-ready)\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-how-deepgram-works\"></WhitepaperPromo>\n\n### About Deepgram\n\nDeepgram is a Speech Company whose goal is to have every voice heard and understood. We have revolutionized speech-to-text (STT) with an End-to-End Deep Learning platform. This AI architectural advantage means you don't have to compromise on speed, accuracy, scalability, or cost to build the next big idea in voice. Our easy-to-use SDKs and APIs allow developers to quickly test and embed our STT solution into their voice products. For more information, visit [https://deepgram.com](https://deepgram.com/).";
						}
						async function compiledContent$3b() {
							return load$3b().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3b() {
							return (await import('./chunks/index.cd3bcc45.mjs'));
						}
						function Content$3b(...args) {
							return load$3b().then((m) => m.default(...args));
						}
						Content$3b.isAstroComponentFactory = true;
						function getHeadings$3b() {
							return load$3b().then((m) => m.metadata.headings);
						}
						function getHeaders$3b() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3b().then((m) => m.metadata.headings);
						}

const __vite_glob_0_73 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3b,
  file: file$3b,
  url: url$3b,
  rawContent: rawContent$3b,
  compiledContent: compiledContent$3b,
  default: load$3b,
  Content: Content$3b,
  getHeadings: getHeadings$3b,
  getHeaders: getHeaders$3b
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3a = {"title":"Deepgram Pioneers Novel Training Approach Setting New Standard for AI Companies","description":"Deepgram sets a new standard for AI companies by pioneering novel training approach.","date":"2020-08-27T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981355/blog/deepgram-pioneers-novel-training-approach-setting-new-standard-for-ai-companies-2/dg-pioneers-novel-training%402x.jpg","authors":["scott-stephenson"],"category":"product-news","tags":["machine-learning"],"seo":{"title":"Deepgram Pioneers Novel Training Approach Setting New Standard for AI Companies","description":"Deepgram sets a new standard for AI companies by pioneering novel training approach."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981355/blog/deepgram-pioneers-novel-training-approach-setting-new-standard-for-ai-companies-2/dg-pioneers-novel-training%402x.jpg"},"shorturls":{"share":"https://dpgr.am/955cb45","twitter":"https://dpgr.am/d7c00e6","linkedin":"https://dpgr.am/a03d53c","reddit":"https://dpgr.am/f03b590","facebook":"https://dpgr.am/f96d435"}};
						const file$3a = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-pioneers-novel-training-approach-setting-new-standard-for-ai-companies-2/index.md";
						const url$3a = undefined;
						function rawContent$3a() {
							return "Artificial intelligence has made astonishing technological advances in recent years and more companies are turning to AI to improve internal functions and unlock the potential of enterprise datasets. [IDC](https://www.cio.com/article/3519273/key-to-sustained-digital-transformation-in-2020-people.html) has characterized AI as \"inescapable\" and estimates that by 2025, at least 90% of new enterprise apps will embed AI. But getting to the right models to effectively power AI is hard - and especially hard for speech. Building a model is tedious, requiring multiple stages of training and refinement, and deep learning expertise is hard to find. Compound that with the endless variations of speech, lack of high-quality training data, and astronomical computing costs, it's no wonder homegrown and off-the-shelf speech recognition has been slow to succeed.\n\n## **Introducing the first AutoML Model Training for Speech Recognition**\n\nThat's why today we're excited to announce Deepgram AutoML, a new training capability that streamlines AI model development, reducing manual cycles for data scientists while giving them the best accuracy humanly possible. With our approach organizations can deploy not only one, but 10's or 1000's of models trained to the needs of their specific company, target industries or largest customers in an automated way.\n\n## **Why AutoML?**\n\nAutoML is often referred to as \"AI creating other AI.\" Rather than relying on humans to painstakingly create and hand-tune a wide variety of AI models, AutoML is a mechanism by which new AI models can be constructed and tuned automatically. While AutoML exists for NLP, image and vision, it has never been deployed for [automatic speech recognition (ASR)](https://blog.deepgram.com/what-is-asr/)-until now. As the first company to offer this innovative technology for ASR, we're furthering our mission to be the de facto speech company, offering the world's fastest, most accurate and scalable speech solution. AutoML training capabilities are one of many ways Deepgram enables customers to extract value from their audio and deliver on the vision of an AI-enabled Enterprise.\n\nOur AutoML model training functionality is another proof point in how we continue to innovate and offer advanced solutions that far surpass what our competitors provide. With this, we're solving the challenges of building and training effective AI models, while delivering over 90% accuracy, 120 times faster delivery and at half the cost of Big Tech solutions. You can now get the best ASR solution with less hassle, time and money.\n\n## **About Deepgram AutoML**\n\nOur state-of-the-art AutoML for speech recognition is now available to engineers, data scientists and others looking to implement speech recognition or replace clunky ASR models that haven't worked. Deepgram AutoML utilizes GPU resources more effectively and automates processes so a speech recognition model is more effective with a smaller amount of effort. With Deepgram AutoML data scientists no longer have to:\n\n* Select input audio features to denoise audio\n* Tune hyperparameters of Hidden Markov Models or Neural Networks\n* Modify underlying algorithms or architectures to maintain a custom vocabulary list\n* Apply model ensembling with keyword boosting or stacking\n\nDeepgram AutoML reduces the time and effort needed to deploy speech recognition, enabling humans to spend more cycles on overall strategy and processes to successfully integrate AI into their organization. Humans have been, and always will be, an essential part of automating speech recognition as they are the only ones who can define what accuracy means, derive intuitions about their data, and create or curate new training data. Deepgram AutoML pushes the frontier of how AI helps humans evolve next generation AI.\n\n## **How Deepgram AutoML works**\n\nCustomers first begin by selecting a specific audio source. Next, they select a Deepgram base model to use: general, phone call or meeting. Then, customers select a training method and submit their model for training. After the model training process completes, customers review model performance (e.g., accuracy improvement). If additional gains are required, further training teaches models to recognize specific audio examples. Finally, customers select the top-performing model and with one click deploy it to cloud.\n\nAutoML is the next frontier for artificial intelligence to allow teams to reach unprecedented levels of accuracy needed to solve business problems. We could not be more excited to be the first to provide AutoML for ASR.\n\nGet started with [Deepgram](https://www.deepgram.com/) Beginner and Intermediate models by creating a [free account](https://try.deepgram.com/) or [contact us](https://deepgram.com/contact-us) to get started with AutoML!\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-how-deepgram-works\"></WhitepaperPromo>";
						}
						async function compiledContent$3a() {
							return load$3a().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3a() {
							return (await import('./chunks/index.1f225a58.mjs'));
						}
						function Content$3a(...args) {
							return load$3a().then((m) => m.default(...args));
						}
						Content$3a.isAstroComponentFactory = true;
						function getHeadings$3a() {
							return load$3a().then((m) => m.metadata.headings);
						}
						function getHeaders$3a() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3a().then((m) => m.metadata.headings);
						}

const __vite_glob_0_74 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3a,
  file: file$3a,
  url: url$3a,
  rawContent: rawContent$3a,
  compiledContent: compiledContent$3a,
  default: load$3a,
  Content: Content$3a,
  getHeadings: getHeadings$3a,
  getHeaders: getHeaders$3a
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$39 = {"title":"What's Next for AI in the Contact Center - Art Coombs, CEO, KomBea - Project Voice X","description":"What's Next for AI in the Contact Center presented by Art Coombs, CEO of KomBea, presented on day one of Project Voice X. ","date":"2021-12-09T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981390/blog/deepgram-projectvoicex-transcription-aicontactcenter-artcoombs/proj-voice-x-session-art-coombs-blog-thumb-554x220.png","authors":["claudia-ring"],"category":"speech-trends","tags":["contact-center","project-voice-x"],"seo":{"title":"Whats Next for AI in the Contact Center - Art Coombs, CEO, KomBea - Project Voice X","description":"Whats Next for AI in the Contact Center presented by Art Coombs, CEO of KomBea, presented on day one of Project Voice X. "},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981390/blog/deepgram-projectvoicex-transcription-aicontactcenter-artcoombs/proj-voice-x-session-art-coombs-blog-thumb-554x220.png"},"shorturls":{"share":"https://dpgr.am/26e3933","twitter":"https://dpgr.am/d1f9587","linkedin":"https://dpgr.am/fe49561","reddit":"https://dpgr.am/a392b91","facebook":"https://dpgr.am/e8a6636"}};
						const file$39 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-projectvoicex-transcription-aicontactcenter-artcoombs/index.md";
						const url$39 = undefined;
						function rawContent$39() {
							return "*This is the transcript for the session “What’s Next for AI in the Contact Center” presented by Art Coombs, CEO of KomBea, presented on day one of Project Voice X.* \n\n*The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was **94% accurate with only a 6% Word Error Rate (WER).**  Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram’s API.  If you want to see if Deepgram is right for your use case, [contact us](https://deepgram.com/contact-us/).*\n\n\\[Art Coombs:] Really quick. My name is Art Coombs. I thought… I wasn’t gonna do this, but I changed my mind. I’d like to tell you just the the very first time I ever heard the term artificial intelligence.\n\nIn nineteen eighty two, I was hired to be an… computer operator at HP Labs. Now the address of my office was fifteen o one Page Mill Road. If you’re a Silicon Valley savant, you’ll know that’s the headquarters of Hewlett-Packard. Bill Hewlett and David Packard… Packard’s offices were directly above me. I know I’m showing my age here. They were directly above me. While they didn’t come into the office all that much, they did come into the office. I work in the basement. In the basement, so just to set the scene, in the basement, we had a couple of colleagues from Stanford that would constantly come over to our our offices late in the evening, Sandy Lerner and Len Bosack. I don’t know if you know those names at all, but I remember vividly about nineteen eighty three, nineteen eighty four, Sandy and Len were working on a black box about the size of a lunch box. Maybe a little bit bigger, a bread box. And I said, Sandy, Len, what is that? And they said, this is a router, and I said, a router, what does a router do? And they said a router is gonna allow one computer to talk to another computer. The next week, we all went to a place called the Velvet Turtle. If you know Silicon Valley, that’s a very… it’s no longer there, but it used to be a a very nice restaurant, and they told us they announced that they incorporated Cisco that night. So Len and Sandy were the founders of Cisco. In that environment, there was a bunch of people working in the in the basement, and I remember the big excitement, and we all went over to their office, and there was one of the engineers who entered in a sentence on his computer in English and it read it back in Spanish. And we were all blown away, absolutely blown away. That was the very first time I heard the term artificial intelligence. Now, for a cocky twenty two year old at the time, I thought, what a stupid name. Artificial intelligence, that makes no sense whatsoever. How can you have artificial intelligence? That’s an oxymoron. It’s the lamest thing I’ve ever heard. Now here we are, forty years later, and we’ve been involved in some form of artificial intelligence for many, many years.\n\nIt helps if I turn this on too. So I’m gonna take this. I come from the call center industry. I don’t come from the AI industry. I come from the call center industry. So I’ve been in call centers, I know it’s scary for some of you, for almost forty years. And so I’m gonna take a look at it from that perspective, but I wanna I wanna sort of set the stage. I think it’s impossible for for us to truly understand where we’re going until we sorta look back at where we’ve been and what we’ve done to this point and some of the major events that have taken place. Here we are in this very simple, simple forum. We have a human agent on this side, human customer, and I know we’re going to have AI customers. I know. But for right now, let’s just stay human customer, and we have AI agents. For many, many years, this type of communication has been the only communication we’ve had. Now what the huma… what do customers want? This has not changed in fifty years. Whether you’re standing at a grocery store, getting your oil changed, going through and getting your clothes dry-cleaned, or calling customer service, there’s three things customers want, and three things only, respect my time, be professional, make it easy and accurate. That’s all consumers want. Now we adopt AI and technology very, very quickly as humans. If we are convinced that those three stools of the customer service chair are strong and met.\n\nLet me give you an example. When the first, I don’t know about you, but when the first grocery checkout self-serve stands happened, I don’t know about you, but I resisted them. I I kept going to the human checkout. Why? Because I had a bunch of vegetables and fruits and all sorts of stuff, and I knew that was gonna get all messed up, and I didn’t believe it was going to save me time, and it wasn’t gonna be easy. Now if I had a couple of boxes of cereal, a jug of milk, and I had barcodes… ok. Sure. I go over to self-service. But consumers are not gonna move over here until those three things are met. Now we start talking…\n\n> when we talk to customers, why do you wanna talk to a human being? Here are the adjectives we typically get. They are creative. They’re polite. They’re intuitive. They’re adaptive, empathetic, and flexible. Why do you wanna talk to technology? It’s accurate, consistent, fast, and easy.\n\nNow all of us in this room are trying to get some of these attributes over here. I don’t know if we can get ’em all over there. I know that there are some who believe that they can make a chatbot empathetic, but so far in our industry, in in our research… and we’ve done over, about twenty years, we’ve done over north of five hundred million calls, and we can go both ways. We can have the consumer believe they’re talking to a human, or we can have the consumer believe they’re talking to AI, because your brain switches. And I think that has already been addressed in this audience, but you literally think differently if you believe you’re talking to a human or if you believe you’re talking to an AI. But if you believe you’re talking to AI, having that AI be empathetic is condescending. So be very careful with how you’re designing your AI conversations, because you need to understand the dynamic of this human person over here and what they’re really after.\n\nNow let’s put in this… and, again, I’m not gonna go through this whole thing. We’ve only got just a few minutes, but Rockwell will tell you that the first call center came around in nineteen seventy two. They’ll tell you that they created the first ACD. That’s not the case. Call centers were around in the late fifties and early sixties. Now during the seventies and eighties, we were in the loop. We may not have called it in the loop, but we were in the loop. Agents were primarily talking and controlling the entire interaction with consumers, but we were assisted by technology, AI in a sense. In the two thousands, we started to get on the loop in a big, big way. I’ll give you an example of how we’ve done that. And two thousand eighteen, nineteen, twenty, twenty one, absolutely, we are getting out of the loop. Does everybody understand what in the loop, on the loop, and out of loop means? Yes. No. Ok. Everyone, pan-faced. I’m assuming you all do. In the loop, agents are in control, but AI is helping with that interaction. On the loop, it’s a combination of human and AI. Often, the human is in the background. AI is taking the front, unless the human needs to get involved. Out of a loop, the agents out of it all come… completely. Now, in the seventies, we had ACD, CRM, IVRs, knowledge bases. I know all of this stuff is is… you guys know all this, but these were major moves in the call center industry that changed the call center forever and changed how we interacted with consumers. The reality, though, is back in the seventies, eighties, and nineties, all of this was first done to reduce cost. Second, it was focused on CSAT.\n\nLet me give you a story here. I was about to start KomBea in nine… in two thousand one, two thousand two, and I wanted to get a feel as to what had happened for the last twenty years in our call center and how we had improved this product, the conversation. So if you think about a conversation with a consumer as a product, I wanted to get a gauge as to, had we moved the needle at all? Had we improved that product for you, the consumer? And I called my buddy, Jeff, who ran the American Express call center off two fifteen in Salt Lake City. He’s got about five hundred agents there, and I said, Jeff, can you tell me the average handle time of a lost American Express credit card in nineteen eighty five? And he said, yes. I can. And I said, what is that? And he said seven and a half minutes, about seven and a half minutes. Seven and a half minutes, it took a consumer to call to say, hey. Once I got through to an agent to say, hey. I lost my credit card and take care of that transaction. I said ok, Jeff, American Express has probably spent more money than just about any company on the planet on the best CRM, the best IVR, the knowledge bases. You guys are doing screen pops and skills-based routing, and you’re doing all sorts of stuff. Onshore, offshore, everywhere. In two thousand two, what is your average handle time for a lost credit card? There was a long pause, and he said, Art, it’s seven and a half minutes. And I was like, Jeff, isn’t that kinda depressing to you? Isn’t that kinda depressing? You’ve spent hundreds and hundreds of millions of dollars. We’ve spent twenty years with with, you know, Six Sigma analysts and all this stuff, and we haven’t moved the needle on saving customers’ time at all? And he said… he made a comment to me that I’ll never forget. And he said, Art, it’s more depressing than that. For the last decade, our CSAT has been dropping. He said, we have taken that seven and a half minute call in nineteen eighty five that cost just under thirty dollars, and in the year two thousand two, it costs just under six dollars. So while our shareholders are happy, our customers are unhappy.\n\nSo now let’s go… let’s keep going. So in two thousand, early two thousands, again, I know you guys all notice, smartphone, social media, consumer-friendly apps, they hit the market. They changed our lives forever. As soon as Steve Jobs held up that PDA, bang, the world changed in a big way. That created an explosion as to how we, as consumers, wanted to communicate with customers. Over the last decade, you have this explosion of cloud services, machine-learning tools, and powerful AI capabilities, which you guys are all involved with. These things are gonna change the customer service world even more radically than anything in the seventies, eighties, and nineties. The interesting thing here is based on what we see, when we talk to customers, is the first focus is on CSAT. Second, it’s on cost reduction. Now we just… I’m gonna… I wanna just play you one conversation of an agent communicating with a customer on the loop. Now this agent happens to be sitting in the Philippines, and it is a female. But I want you to listen to this interaction. If I turn up the sound… how do I turn up the sound? My sound’s all the way up. Bradley, SOS. Is there sound here I’m supposed to turn up? \n\n\\[SPEAKER 2:] Let’s try it again. \n\n\\[Art Coombs:] I’m not hearing it. You should hear that. Peter, it’s nice when you have your vice president of engineering in the audience. Am I supposed to… is this turn on and off? \n\n\\[SPEAKER 3:] I think.\n\n\\[Art Coombs:] Volume up? \n\n\\[SPEAKER 3:] It should be. Let me try to plug it back in there. \n\n\\[Art Coombs:] I hear I hear something happening. I don’t hear any volume. While he’s doing that, I’ll tell you a story as to why I started KomBea. I was working for a large BPO company in India. They had hired me not to own or manage a p and l, but what I learned was, I was literally supposed to be Yoda. I was the gray-haired guy, American, that was supposed to give our customers comfort with outsourcing to India. We had a customer, Blue Cross Blue Shield. I can say this. There’s no dawn disclosure or anything like that. But we had a customer, Blue Cross Blue Shield, and we were doing millions and millions of calls on their behalf. And we had a big meeting in London, and we… we’re all sitting around the table, and the vice president walked into the office… or into the conference room, and he said, I don’t want anyone to say a single word. Have you ever been to one of those meetings where the client walks in, he has a little attitude, and he says, essentially, everyone shut up?\n\n\\[SPEAKER 4:] Same time work for you, though? \n\n\\[Art Coombs:] I can hear it. I can hear it coming up. \n\n\\[SPEAKER 5:] I’ll be working on Wednesday at this time. Can you call me at nine AM?\n\n\\[SPEAKER 4:] — next… back at nine AM on Wednesday. Thanks again for calling, and we look forward —\n\n\\[Art Coombs:] So let’s see if we can do this again. You want me to finish the story? I’ll finish the story. So he called… he walked into the calls… into the conference room, and he says, I don’t want anyone to say a single word. So everyone hushes up, and he said, we’ve recorded randomly twenty introductions of your agents introducing themselves and starting the conversation with our Blue Cross Blue Shield customers, and I’d like to play the first five seconds of every conversation, but before I do that… he put on the screen, the intro that he wanted every agent to use, and it was benign. It was like, thank you for calling Blue Cross Blue Shield. My name is x. May I start by getting your verification number or something like that. It was really benign. He played all twenty of those intros. They were all different.\n\nNow if you were a QA person in a call center, you would think, that’s not a big deal. The person said, hi instead of hello. The person said Art Coombs, instead of Art. The persons forgot Blue Shield. They just said, hey. Thank you for calling Blue Cross, whatever, but it was benign in my opinion, but he made a really important point. He said, how in the world can we expect you to get our compliance statements right when you can’t get the intro right? And do you realize how big of a problem that is for our company when it comes to class action lawsuits? It was at that point that I said, we have to figure out a way to combine human intelligence and the consistency and accuracy of technology and sort of bring them together. So let’s see if we can do this again. And it’s gonna come up. I promise. Alright. One more time. I can see it, but you can’t. And I can’t hear it. Alright.\n\nOn that note, I encourage each of you… there’s three cost… three of my colleagues sitting right here. Take a moment and go listen to that conversation. And just remember. It’s a young Filipino lady handling multiple conversations at the same time on the loop, and you sit back and you decide if you were the customer, what do you think you’re experiencing? Again, we’ve taken over five hundred million calls. Most of them the… that human customer believes they’re talking to a human agent, and to our… the best of our ability, about one percent chime in and say, hey. Something is… something else is happening here. We can also go and make it sound like AI. So if we if we go back and look at the entire industry, and you look at the influence… come on now. Now it’s not… nothing’s working for me. If you look at the entire AI call center influence has been actually very, very little. Until about right here and depending on the analyst and what data you guys gather, if you’re talking strictly conversational AI, by the year two thousand twenty two, that’s in just a few months, about twenty percent of conversations that used to be between humans will now be done totally by AI. By the year two thousand thirty… and, again, we’re talking call centers.\n\nBy the year two thousand thirty, we estimate that’s gonna be north of eighty percent. Eighty percent of all conversations are gonna be handled by AI. Now what drove one of the major driving points, and we’ve heard this over and over again already, is COVID. Those companies that were in two thousand sixteen, seventeen, eighteen, and nineteen, that were already already working on AI, were in a better position to handle that kind of distraction. Humana is a great case study. If you’ve never… if you haven’t… if you don’t know what they accomplished, absolutely mesmerizing what they did. Their call volume went out the roof. It went up exponentially, as you can imagine with COVID, and yet, they had to send all their agents home instantly, creating chaos. Now I don’t know about you. I’m not trying to be a doomsdayer, but if you think this is the only pandemic we’re going to see in the next five to ten years, think again. I believe there’s going to be another pandemic. I believe there… and I’m I’m not trying to create panic, but I believe that our environment is going to create more and more situations that are going to make it difficult for us to have human beings talking to human beings all the time. We have to figure out a way to have interactions with our customers on a massive scale no matter where we are, no matter what’s happening on this globe. And, again, we’ve heard it over and over again.\n\nOne of my favorite sayings, technology has consumed our lives. We cannot get away from it. Just this morning… and I… this is true. While Jeff was talking about the garage door, I got a notification that my garage door was opening, and I looked, and my garage door was opening. And I could decide whether I close it or shut or open it. Technology has consumed our lives. We are right now in the process where AI is consuming technology. I’ll say it again. Technology has consumed our world, consumed our lives. Right now, AI is consuming technology. With that, I’m gonna open it up for questions real quick, and if there are no questions, then I’m gonna assume that it was completely understandable, and I don’t need to go into any AI. Thank you.";
						}
						async function compiledContent$39() {
							return load$39().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$39() {
							return (await import('./chunks/index.b7fc4a23.mjs'));
						}
						function Content$39(...args) {
							return load$39().then((m) => m.default(...args));
						}
						Content$39.isAstroComponentFactory = true;
						function getHeadings$39() {
							return load$39().then((m) => m.metadata.headings);
						}
						function getHeaders$39() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$39().then((m) => m.metadata.headings);
						}

const __vite_glob_0_75 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$39,
  file: file$39,
  url: url$39,
  rawContent: rawContent$39,
  compiledContent: compiledContent$39,
  default: load$39,
  Content: Content$39,
  getHeadings: getHeadings$39,
  getHeaders: getHeaders$39
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$38 = {"title":"Deepgram Has Received SOC 2 Type II Certification","description":"Deepgram has both SOC 2 Type I and Type II certifications, as defined by the American Institute of Certified Public Accountants (AICPA).","date":"2022-02-04T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981390/blog/deepgram-reached-soc-2-type-1-certification/DG-reached-SOC2-type1-thumb-554x220%402x.png","authors":["ehab-el-ali"],"category":"product-news","tags":["privacy","security"],"seo":{"title":"Deepgram Has Received SOC 2 Type II Certification","description":"Deepgram has both SOC 2 Type I and Type II certifications, as defined by the American Institute of Certified Public Accountants (AICPA)."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981390/blog/deepgram-reached-soc-2-type-1-certification/DG-reached-SOC2-type1-thumb-554x220%402x.png"},"shorturls":{"share":"https://dpgr.am/03c00c5","twitter":"https://dpgr.am/d9cb702","linkedin":"https://dpgr.am/9fb7248","reddit":"https://dpgr.am/7758006","facebook":"https://dpgr.am/cd17324"}};
						const file$38 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-reached-soc-2-type-1-certification/index.md";
						const url$38 = undefined;
						function rawContent$38() {
							return "Deepgram received a SOC 2 Type II \"clean bill of health\" from [Cyberguard Compliance](https://www.cgcompliance.com/). Their six-month audit of Deepgram's security, availability, and confidentiality found no exceptions to the SOC 2 Type II industry standards as defined by the American Institute of Certified Public Accountants (AICPA). Their audit was conducted from July to December 2021.\n\nThe SOC 2 Type II report is the gold standard in SOC audits and provides customers the confidence that a SaaS company has security controls in place, monitors the controls, and optimizes security for their customers. We've also previously achieved SOC 2 Type I certification. Type I describes a vendor's systems and whether their design is suitable to meet relevant trust principles, while Type II details the operational effectiveness of those systems.\n\n## What does SOC 2 mean to our customers and prospects?\n\nThe SOC 2 certifications mean that an independent auditor has examined our systems and processes and determined that we have best practices in place for securely managing data to protect the interests and privacy of our customers. We have been a trusted partner for many SaaS and Enterprise companies and this certification confirms our systems are using industry best practices for security, confidentiality, and privacy. If you are engaged with any SaaS company, they should meet SOC 2 Type I certification requirements *at a minimum* to ensure their customer's data is secure, available, and private. SOC 2 has five main principles:\n\n1. **Security** - Best practices are implemented to prevent unauthorized access to a company's systems.\n2. **Availability** - A company can meet its service level agreements (SLAs) for accessibility.\n3. **Confidentiality** - The use, access, and protection of information as stipulated in customer contracts can be met.\n4. **Process Integrity** - The company and its systems can achieve their stated purpose.\n5. **Privacy** - The disclosure and disposal of data are in line with the privacy notice of the company and the generally accepted principles of the American Institute of CPAs.\n\nIf you need a copy of our SOC 2 Type I or Type II attestation or report, please contact us at [security@deepgram.com](mailto:deepgram.comnull). This report is valid until February 2023. You can also read more about our [data privacy compliance](https://developers.deepgram.com/documentation/security/data-privacy/).\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-how-deepgram-works\"></WhitepaperPromo>";
						}
						async function compiledContent$38() {
							return load$38().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$38() {
							return (await import('./chunks/index.176aa9ab.mjs'));
						}
						function Content$38(...args) {
							return load$38().then((m) => m.default(...args));
						}
						Content$38.isAstroComponentFactory = true;
						function getHeadings$38() {
							return load$38().then((m) => m.metadata.headings);
						}
						function getHeaders$38() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$38().then((m) => m.metadata.headings);
						}

const __vite_glob_0_76 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$38,
  file: file$38,
  url: url$38,
  rawContent: rawContent$38,
  compiledContent: compiledContent$38,
  default: load$38,
  Content: Content$38,
  getHeadings: getHeadings$38,
  getHeaders: getHeaders$38
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$37 = {"title":"We Raised $12 Million to Solve Speech Recognition in the Enterprise","description":"At Deepgram, we're working to change the speech recognition game from the ground up. Learn about our Series A here.","date":"2020-03-18T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981352/blog/deepgram-series-a/we-raised-12M%402x.jpg","authors":["scott-stephenson"],"category":"dg-insider","tags":["recognition"],"seo":{"title":"We Raised $12 Million to Solve Speech Recognition in the Enterprise","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981352/blog/deepgram-series-a/we-raised-12M%402x.jpg"},"shorturls":{"share":"https://dpgr.am/a68bc67","twitter":"https://dpgr.am/c4ce8c9","linkedin":"https://dpgr.am/198c56f","reddit":"https://dpgr.am/8eeb6ea","facebook":"https://dpgr.am/366d594"}};
						const file$37 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-series-a/index.md";
						const url$37 = undefined;
						function rawContent$37() {
							return "The excitement around speech recognition is real: it has the potential to power the next wave of modern applications and give businesses and vendors a competitive advantage. But, with excitement comes misaligned expectations. Speech recognition is a messy, tough and persistent problem for enterprises, one that has languished under existing technology providers for decades. At [Deepgram](https://www.deepgram.com/) we have been working to change that by rebuilding speech recognition from the ground up. Today, we celebrate a key milestone on our path with a $12 million Series A round led by Wing VC, with participation from NVIDIA, Y Combinator, Compound and SAP.iO.\n\n## Why Speech Recognition?\n\nToday, getting actionable information from recorded phone conversations and meetings is time and resource-intensive, costly and cumbersome. Audio recordings don't play by the same rules as text or data. They're messy and idiosyncratic and go far beyond the short pre-programmed phrases that Siri and Alexa rely on. There's no silver bullet to speech recognition, especially when it comes to speed, scale, accuracy and reliability.\n\n> \"Enterprise communication amounts to trillions of minutes annually, but making sense of what's being said is technically challenging and cost-prohibitive,\" said Zach DeWitt, Partner at Wing VC.\n>\n> \"Deepgram is building mission-critical infrastructure to empower enterprises to accurately transcribe and analyze their calls and meetings in real-time, across multiple accents and languages. Deepgram is helping companies better serve their customers and employees by unlocking unutilized voice datasets.\"\n\nYou can read [Zach's blog post](https://medium.com/wing-vc/wings-investment-in-deepgram-296e5de46dd1) about the need for Deepgram and the importance that companies utilize speech recognition to better interpret customer needs and serve their employees.\n\n## Rebuilding Speech From the Ground up\n\nThe idea for Deepgram began while I was a PhD student at University of Michigan. My cofounder and I were researching the detection of dark matter two miles underground and in the hours not devoted to research, we life-logged (we made devices that recorded backup copies of the audio surrounding us, 24/7). When we tried to go back and find key conversations and specific moments in those audio files, we felt the very real pain of not having a good tool available to help process the recordings and pinpoint valuable timestamps. That was the spark that created Deepgram.\n\nDeepgram has taken an entirely new approach to speech recognition, replacing what hasn't worked-heuristics-based speech processing-with fully end-to-end deep learning. Audio recordings are complex and infinitely varied, meaning there is no one quick-fix to speech recognition. That's why we train speech models to learn and adapt under complex, real-world conditions with customers' unique vocabularies, accents, product names and acoustic environments. Companies dealing with challenging audio from conference calls or call centers previously struggled to make speech scalable, precise and fast enough. With Deepgram, they can transform their speech data into an enterprise asset. Our speech recognition reliably acts as a foundational layer within the next generation of business applications, allowing companies to build something with speech that actually works. \n\n![](https://res.cloudinary.com/deepgram/image/upload/v1661976832/blog/deepgram-series-a/Deepgram_SpeechStacks_v5-01.jpg)\n\n## Making Speech Work In The Enterprise\n\nSince going to market, we've amassed customers across the call center, retail and tech industries, and partnered with some of the leading large-scale communication and conferencing providers. Developers, data scientists, product managers and CIOs at these companies all trust Deepgram because our unique approach delivers a high-level of accuracy quickly, and at scale. Our customers and partners work with us because of our vision, our team and our commitment to continually refining and innovating our product. As part of that product innovation, along with our Series A, we're also announcing two new features of our platform:\n\n* **Real-Time Streaming**: an industry-first advancement in speech recognition that lets our customers analyze and transcribe speech as words are being spoken. More complex use cases are long running real-time transcription for meeting platforms or powering real-time agent assist for call center agents to achieve more effective customer service. A simple use case is \"command and control\" interactions like dictating doctor's notes or ordering takeout from your favorite restaurant chain.\n* **On-Premises Deployment**: Deepgram On-Premises Deployment provides a private, deployable instance of the Deepgram platform for speech recognition use cases involving confidential, regulated, or otherwise sensitive audio data in enterprise. It delivers the same scalable, high-performance, high-accuracy speech recognition capability as the Deepgram cloud, while allowing enterprises to manage the solution on-premises.\n\n> \"One of the next big frontiers in the AI revolution is conversational intelligence,\" said Jeff Herbst, Vice President of Business Development at NVIDIA.\n>\n> \"Deepgram is doing groundbreaking work in this field, and we are delighted to be working closely with them. Their world class GPU-accelerated speech recognition enables faster, more accurate natural language processing that will make an important impact on a range of industries.\"\n>\n> \"As SAP drives toward combining experience data with operational data, Deepgram's unique ability to automate transcription and intent recognition from voice conversations with high accuracy enables companies to provide a high quality customer experience,\" said Ram Jambunathan, Managing Director of SAP.iO. \"We're excited about Deepgram's potential to enable rich, voice based insights for SAP customers.\"\n\nWe're so excited about what's next. The speech recognition opportunity is huge, and the endorsement from these amazing investors validates that we have the team, technology and vision to crack it. We strive to become the de facto speech company by unlocking valuable voice data for our customers, giving them a competitive advantage in their industry. This round is going to help us do just that.";
						}
						async function compiledContent$37() {
							return load$37().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$37() {
							return (await import('./chunks/index.0915c09f.mjs'));
						}
						function Content$37(...args) {
							return load$37().then((m) => m.default(...args));
						}
						Content$37.isAstroComponentFactory = true;
						function getHeadings$37() {
							return load$37().then((m) => m.metadata.headings);
						}
						function getHeaders$37() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$37().then((m) => m.metadata.headings);
						}

const __vite_glob_0_77 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$37,
  file: file$37,
  url: url$37,
  rawContent: rawContent$37,
  compiledContent: compiledContent$37,
  default: load$37,
  Content: Content$37,
  getHeadings: getHeadings$37,
  getHeaders: getHeaders$37
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$36 = {"title":"How to Use Deepgram with Next.js Using StepZen","description":"Do you want to learn how to convert Deepgram's API into your own GraphQL API? This post is for you!","date":"2022-06-09T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1654877200/blog/2022/06/deepgram-stepzen-collaboration/stepzen-cover.png","authors":["bekah-hawrot-weigel"],"category":"tutorial","tags":["graphQL","nextjs","stepzen"],"seo":{"title":"How to Use Deepgram with Next.js Using StepZen","description":"Do you want to learn how to convert Deepgram's API into your own GraphQL API? This post is for you!"},"shorturls":{"share":"https://dpgr.am/52c40e7","twitter":"https://dpgr.am/f697111","linkedin":"https://dpgr.am/33bf612","reddit":"https://dpgr.am/e15cf04","facebook":"https://dpgr.am/4974e33"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454093/blog/deepgram-stepzen-collaboration/ograph.png"}};
						const file$36 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-stepzen-collaboration/index.md";
						const url$36 = undefined;
						function rawContent$36() {
							return "\r\nOur friends over at StepZen have a blog post up about how to use Deepgram, StepZen, and Next.js. In this post, they'll show you how to convert the Deepgram API into your own GraphQL API, translate English audio to French text, and display the results with Next.js.\r\n\r\nYou can read [\"Consuming the Deepgram API: The GraphQL Edit\"](https://stepzen.com/blog/consuming-the-deepgram-api-the-graphql-edit) on the StepZen Blog now. If you have any questions about how to use Deepgram or want to learn how to add more features to your Deepgram transcript, please reach out to us on Twitter [@DeepgramDevs](https://twitter.com/DeepgramDevs). We’re happy to help!\r\n\r\n        ";
						}
						async function compiledContent$36() {
							return load$36().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$36() {
							return (await import('./chunks/index.13d06666.mjs'));
						}
						function Content$36(...args) {
							return load$36().then((m) => m.default(...args));
						}
						Content$36.isAstroComponentFactory = true;
						function getHeadings$36() {
							return load$36().then((m) => m.metadata.headings);
						}
						function getHeaders$36() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$36().then((m) => m.metadata.headings);
						}

const __vite_glob_0_78 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$36,
  file: file$36,
  url: url$36,
  rawContent: rawContent$36,
  compiledContent: compiledContent$36,
  default: load$36,
  Content: Content$36,
  getHeadings: getHeadings$36,
  getHeaders: getHeaders$36
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$35 = {"title":"Deepgram Announces Final Speaker Lineup and Sessions at Inaugural What’s Next in Voice Summit [on demand]","description":"Check out the speakers for our inaugural Voice Summit—and watch on demand!","date":"2021-11-18T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981387/blog/deepgram-summit-speaker-lineup-2021/final-speaker-lineup-at-dg-summit-blog-thumb-554x2.png","authors":["katie-byrne"],"category":"dg-insider","tags":["voice-strategy","voice-tech"],"seo":{"title":"Deepgram Announces Final Speaker Lineup and Sessions at Inaugural What’s Next in Voice Summit [on demand]","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981387/blog/deepgram-summit-speaker-lineup-2021/final-speaker-lineup-at-dg-summit-blog-thumb-554x2.png"},"shorturls":{"share":"https://dpgr.am/36672ea","twitter":"https://dpgr.am/c84598c","linkedin":"https://dpgr.am/9ac4aac","reddit":"https://dpgr.am/6850667","facebook":"https://dpgr.am/ac4f1cb"}};
						const file$35 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-summit-speaker-lineup-2021/index.md";
						const url$35 = undefined;
						function rawContent$35() {
							return "\r\n_Executives from Volley, Red Box and other leading companies will discuss latest industry trends, cutting-edge use cases._ **SAN FRANCISCO, Calif.,  November** **18****, 2021** - [Deepgram](https://deepgram.com/), the leading automatic speech recognition (ASR) provider, today announced the final speaker lineup for its inaugural What's Next in Voice Summit. The virtual event will take place from 11:00 a.m. to 1:30 p.m. PT on Thursday, November 18\\. The summit will highlight emerging voice startups and shine a light on the importance of voice across industries - including retail, finance, healthcare and space.  The speaker lineup includes the brightest minds from both legacy enterprise companies and the venture-backed startup community. Confirmed speakers include:\r\n\r\n*   [Terry Chen](https://www.linkedin.com/in/techchen/), CTO at Modulate.ai \r\n*   [Pete Ellis](https://www.linkedin.com/in/pete-ellis-redbox/), Chief Product Officer at Red Box\r\n*   [Joe Wilson](https://www.linkedin.com/in/brandnewfeel/), Head of Product at Volley\r\n*   [Venky B](https://www.linkedin.com/in/bevenky/), CEO and co-founder at Plivo\r\n*   [Zachary DeWitt](https://www.linkedin.com/in/zachary-dewitt-a5a8b816/), Partner at Wing Venture Capital\r\n*   [Craig Akal](https://www.linkedin.com/in/craig-akal/), Co-founder at Elerian.ai\r\n*   [April Joyner](https://www.linkedin.com/in/apriljoyner/), VC/Startups correspondent at Business Insider\r\n*   [Dan Miller](https://www.linkedin.com/in/danmiller/), Lead Analyst and Founder at Opus Research\r\n*   [Ted McKenna](https://www.linkedin.com/in/ted-mckenna-9a093a3/), SVP of Research and Innovation at Tethr\r\n*   [Scott Stephenson](https://www.linkedin.com/in/scott-stephenson-/), CEO and co-founder at Deepgram\r\n*   [Shadi Baqleh](https://www.linkedin.com/in/shadibaqleh/), COO at Deepgram\r\n*   [Minh Le](https://www.linkedin.com/in/minhisin/), AI Solutions Engineer at Deepgram\r\n*   [Clinton May](https://www.linkedin.com/in/clinton-may-83437447/), Federal Sales Lead at Deepgram\r\n\r\n\"We're excited to showcase some of the greatest cutting-edge startups and use cases in the voice industry,\" said Scott Stephenson, CEO and co-founder of Deepgram. \"Voice technology has the power to change the way we build and create modern enterprise applications and customer experiences. The future of voice is now, and we're thrilled to host this event to continue meaningful conversations about what's next in voice technology.\" The summit will cover a wide range of topics and verticals, including the following sessions:\r\n\r\n*   **The Resurgence of Voice Technology**: The buzz around voice technology is real-it has the potential to power the next wave of modern applications and give businesses a competitive advantage. But speech technology has historically been a challenging problem for enterprises, one that has languished under legacy tech providers for decades. The pandemic brought this problem to the forefront, with enterprises relying on a heavy volume of video conferencing calls full of team members speaking over each other, background noise, diverse accents, and more. This sudden shift helped businesses realize that audio conversations hold a treasure trove of data and insights that can unlock valuable employee and customer insights. In this opening keynote, Deepgram CEO and co-founder Scott Stephenson will kick off What's Next in Voice-breaking down the resurgence of voice technology with recent innovations and what lies ahead.\r\n\r\n*   **Voice is the Next Interface for All Companies:** As the pandemic raced across the U.S., in-person experiences became largely impossible, upending all internal and external business interactions and placing emphasis on touchless interactions. A year and a half later, voice technology, including conferencing platforms and improved call center technology, has emerged as a way for businesses and customers to stay connected and better understand each other through the voice data produced.  During this panel moderated by Dan Miller at Opus Research, Shadi Baqleh, chief operating officer at Deepgram, Craig Akal at Elerian ai, Pete Ellis, chief product officer at Red Box and Venky B, CEO and co-founder at Plivo will discuss the untapped potential hidden within voice data and how to successfully implement voice interfaces moving forward.\r\n\r\n*   **Fireside Chat: Using Voice at NASA And Beyond:** As the global leader in space exploration, NASA deploys hundreds of space missions in any given year. During those missions, space-to-ground communication is essential to a mission's success and can be filled with crucial communication insights. In 2020, NASA turned to Deepgram to capture voice data and observations to help inform future missions and problem areas to look out for. In this fireside chat, we will discuss the various ways NASA uses voice technology within the organization, especially as it relates to these critical space missions. We will share a few real-life examples of observations they were able to capture through voice technology and touch on the importance of it for organizations everywhere.\r\n\r\n*   **Future of AI and Voice Technology:** Consumers began widely accepting and adopting voice technology in the 2010s with the emergence of Alexa, Google and Siri voice assistance. At the same time, companies began adding voice technology into their company product suite to better support customers and employees. As voice technology has evolved over the past decade, in particular, we are moving past command and response (Siri, Alexa, etc.) into more sophisticated use cases including conversational AI, call intelligence, real-time sales enablement, entertainment, and community safety. This panel discussion will highlight the innovative startups leading this resurgence of voice technology.  During the panel discussion moderated by April Joyner at Business Insider, Joe Wilson, Head of Product at Volley, Terry Chen, Vice President of Audio at Modulate.ai and Zachary DeWitt, Partner at Wing Capital will discuss the important role that voice will play in our future, and the steps they are taking to improve voice for the enterprise.\r\n\r\nAlthough the event has passed, you can still watch recordings of the session on demand at [Deepgram Summit: What's Next in Voice](https://deepgram.com/deepgram-summit-on-demand/). **About Deepgram** Deepgram is the leader in enterprise automatic speech recognition (ASR) for call centers and software providers. With our patented end-to-end deep learning approach, data scientists get access to the industry's fastest, most accurate and highly scalable AI technology. We take the heavy lifting out of noisy, multi-speaker, hard-to-understand audio transcription, so you can focus on what you do best. To learn more visit deepgram.com or [contact us](https://deepgram.com/contact-us/) to get started. **Contact** [deepgram@inkhouse.com](mailto:deepgram@inkhouse.com)\r\n";
						}
						async function compiledContent$35() {
							return load$35().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$35() {
							return (await import('./chunks/index.0bf00a68.mjs'));
						}
						function Content$35(...args) {
							return load$35().then((m) => m.default(...args));
						}
						Content$35.isAstroComponentFactory = true;
						function getHeadings$35() {
							return load$35().then((m) => m.metadata.headings);
						}
						function getHeaders$35() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$35().then((m) => m.metadata.headings);
						}

const __vite_glob_0_79 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$35,
  file: file$35,
  url: url$35,
  rawContent: rawContent$35,
  compiledContent: compiledContent$35,
  default: load$35,
  Content: Content$35,
  getHeadings: getHeadings$35,
  getHeaders: getHeaders$35
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$34 = {"title":"Transcribe Twilio Voice Calls in Real-Time with Rust and Deepgram","description":"Learn how to transcribe Twilio Voice calls with Deepgram using real-time speech-to-text in Rust.","date":"2022-06-07T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1654025615/blog/2022/06/deepgram-twilio-streaming-rust/Transcribing-Twilio-Rust-Calls-Real-Time-w-Deepgram%402x.jpg","authors":["nikola-whallon"],"category":"tutorial","tags":["twilio","rust"],"seo":{"title":"Transcribe Twilio Voice Calls in Real-Time with Rust and Deepgram","description":"Learn how to transcribe Twilio Voice calls with Deepgram using real-time speech-to-text in Rust."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454095/blog/deepgram-twilio-streaming-rust/ograph.png"},"shorturls":{"share":"https://dpgr.am/eefb3c8","twitter":"https://dpgr.am/e4fdda8","linkedin":"https://dpgr.am/817df91","reddit":"https://dpgr.am/37913ea","facebook":"https://dpgr.am/ee177cb"}};
						const file$34 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-twilio-streaming-rust/index.md";
						const url$34 = undefined;
						function rawContent$34() {
							return "\r\nIn a [previous blog post](https://blog.deepgram.com/deepgram-twilio-streaming/), we showed how to build an integration\r\nbetween Deepgram and Twilio for real-time, live transcription using Python. In this post, we will revisit this integration and implement\r\nit in Rust. The [Rust programming language](https://www.rust-lang.org/) is a favorite among Deepgram engineers, and is known for its\r\ntype safety, performance, and powerful memory management achieved via a strict ownership system which eliminates entire categories of bugs!\r\n\r\nWe will be building our Twilio streaming app using the [Axum web framework](https://docs.rs/axum/latest/axum/)\r\nwhich is built on top of the powerful and popular asynchronous [Tokio crate](https://tokio.rs/). Using Rust with\r\nan efficient asynchronous runtime like Tokio is a good choice for reliable and performant web app backends.\r\n\r\n## Pre-requisites\r\n\r\nYou will need:\r\n\r\n*   a [Twilio account](https://www.twilio.com/try-twilio) with a Twilio number (the free tier will work)\r\n*   a Deepgram API Key - [get an API Key here](https://console.deepgram.com/signup)\r\n*   [Rust installed](https://www.rust-lang.org/tools/install)\r\n*   *(optional)* [ngrok](https://ngrok.com/) to let Twilio access a local server\r\n\r\n## Setting Up a TwiML Bin\r\n\r\nWe will use TwiML Bins to make Twilio fork audio data from phone calls to a server that we will write.\r\nIn the Twilio Console, search for TwiML Bin, and click \"Create TwiML Bin.\"\r\n\r\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1654025616/blog/2022/06/deepgram-twilio-streaming-rust/assets/find_twiml_bin.png\" alt=\"Navigate to your TwiML Bins.\" style=\"max-width: 606px;display: block;margin-left: auto;margin-right: auto;\">\r\n\r\nGive the TwiML Bin a \"Friendly Name\" and enter the following as the the contents of the TwiML Bin:\r\n\r\n```xml\r\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<Response>\r\n  <Start>\r\n    <Stream url=\"wss://INSERT_YOUR_SERVER_URL/twilio\" track=\"both_tracks\"/>\r\n  </Start>\r\n  <Say voice=\"woman\" language=\"en\">\"This call may be monitored or recorded for quality purposes.\"</Say>\r\n  <Dial>+11231231234</Dial>\r\n</Response>\r\n```\r\n\r\nIn the `Dial` section, enter your phone number. Where it says `INSERT_YOUR_SERVER_URL` insert the URL where you will be hosting the server.\r\nWithout having to spin up and configure a cloud instance, you can use `ngrok` to expose a port on localhost. To do this for, say,\r\nport 5000, run:\r\n\r\n    ngrok http 5000\r\n\r\n`ngrok` will then generate a public URL which forwards requests to your computer at `localhost:5000`. This URL may look something like:\r\n`c52e-71-212-124-133.ngrok.io` - enter this URL in your TwiML Bin.\r\n\r\nNow the last thing to do on the Twilio Console before hopping over to write our server code is to hook up one of your Twilio numbers to this TwiML Bin.\r\nGo to the \"Develop\" tab on the left side of the Twilio Console, navigate to `Phone Numbers -> Manage -> Active numbers`, and click on your Twilio number in the list.\r\nThen, under the field \"A Call Comes In\", click the drop-down and select \"TwiML Bin\"; for the field directly next to this one, click the drop-down and select\r\nthe TwiML Bin you just created.\r\nClick \"Save\" at the bottom of the Twilio Console.\r\n\r\n## The Twilio Proxy Server\r\n\r\nThe system that we will be building is illustrated here:\r\n\r\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1654025618/blog/2022/06/deepgram-twilio-streaming-rust/assets/deepgram_twilio_diagram.png\" alt=\"The big picture.\" style=\"display: block;margin-left: auto;margin-right: auto;\">\r\n\r\nWe want audio from phone calls going through Twilio's server to be forked to the proxy server we will be writing. The proxy server then buffers\r\nand processes the audio, sends it to Deepgram, and receives transcripts back from Deepgram. The proxy server also accepts client connections which\r\nsubscribe to ongoing calls, and whenever the server receives transcripts from Deepgram for those calls, it broadcasts those transcripts to all subscribers.\r\nThis will all be done via WebSockets at near-real-time! Typical latencies for this system hover around 500 ms.\r\n\r\nDownload the code from [this repository](https://github.com/deepgram-devs/deepgram-twilio-streaming-rust).\r\n\r\nBelow we will go through creating this project from scratch, but this will also act as a comprehensive code-tour of the repository.\r\nIf you are keen on trying the server out right away and perusing the code\r\nmore at your leisure, feel free to skip to the [Running the Server and Testing with websocat](#running-the-server-and-testing-with-websocat) section!\r\n\r\n## Setup the Rust Project and `main.rs`\r\n\r\nCreate a new Rust project using `cargo new`:\r\n\r\n```\r\ncargo new deepgram-twilio-streaming-rust\r\n```\r\n\r\nGo into the project directory and edit the `Cargo.toml` file, giving it the following contents:\r\n\r\n```toml\r\n[package]\r\nname = \"deepgram-twilio-streaming-rust\"\r\nversion = \"0.1.0\"\r\nedition = \"2021\"\r\n\r\n[dependencies]\r\naxum = { version = \"0.5.1\", features = [\"ws\"] }\r\naxum-server = { version = \"0.4.0\", features = [\"tls-rustls\"] }\r\nbase64 = \"0.13.0\"\r\nfutures = \"0.3.21\"\r\nhttp = \"0.2.6\"\r\nserde = { version = \"1.0.136\", features = [\"derive\"] }\r\nserde_json = \"1.0.79\"\r\ntokio = { version = \"1.17.0\", features = [\"macros\", \"rt\", \"rt-multi-thread\"] }\r\ntokio-tungstenite = { version = \"0.15.0\", features = [\"native-tls\"] }\r\ntungstenite = \"0.14.0\"\r\n```\r\nNow let's modify `src/main.rs`. Let's begin by adding the `use` statements we will need, and defining some modules:\r\n\r\n```rust\r\nuse axum::{routing::get, Extension, Router};\r\nuse axum_server::tls_rustls::RustlsConfig;\r\nuse futures::lock::Mutex;\r\nuse std::{collections::HashMap, sync::Arc};\r\n\r\nmod audio;\r\nmod handlers;\r\nmod message;\r\nmod state;\r\nmod twilio_response;\r\n```\r\n\r\nThe modules we declared are: `audio`, `handlers`, `message`, `state`, and `twilio_response`.\r\nWe will go over each one, but briefly these will be for the following:\r\n\r\n*   `audio`: handle processing of audio data from Twilio\r\n*   `handlers`: handlers for the websocket endpoints `/twilio` and `/client`\r\n*   `message`: a helper module to convert between `axum` and `tungstenite` websocket messages\r\n*   `state`: will contain the definition for the global state of the server\r\n*   `twilio_response`: will contain definitions for Twilio's websocket message shape\r\n\r\nNow, let's start defining our `main` function and set up the state to be shared among the handlers:\r\n\r\n```rust\r\n#[tokio::main]\r\nasync fn main() {\r\n    let proxy_url = std::env::var(\"PROXY_URL\").unwrap_or_else(|_| \"127.0.0.1:5000\".to_string());\r\n\r\n    let deepgram_url = std::env::var(\"DEEPGRAM_URL\")\r\n        .unwrap_or_else(|_| \"wss://api.deepgram.com/v1/listen?encoding=mulaw&sample_rate=8000&channels=2&multichannel=true\".to_string());\r\n\r\n    let api_key =\r\n        std::env::var(\"DEEPGRAM_API_KEY\").expect(\"Using this server requires a Deepgram API Key.\");\r\n\r\n    let cert_pem = std::env::var(\"CERT_PEM\").ok();\r\n    let key_pem = std::env::var(\"KEY_PEM\").ok();\r\n\r\n    let config = match (cert_pem, key_pem) {\r\n        (Some(cert_pem), Some(key_pem)) => Some(\r\n            RustlsConfig::from_pem_file(cert_pem, key_pem)\r\n                .await\r\n                .expect(\"Failed to make RustlsConfig from cert/key pem files.\"),\r\n        ),\r\n        (None, None) => None,\r\n        _ => {\r\n            panic!(\"Failed to start - invalid cert/key.\")\r\n        }\r\n    };\r\n\r\n    let state = Arc::new(state::State {\r\n        deepgram_url,\r\n        api_key,\r\n        subscribers: Mutex::new(HashMap::new()),\r\n    });\r\n```\r\n\r\nOur `main` function is set up to be asynchronous via the use of the `#[tokio::main]` macro.\r\n`main` and every async function that `main` then calls will be executed by\r\nthe Tokio runtime. Inside `main` we grab the following environment variables:\r\n\r\n*   `PROXY_URL`: the URL that this server will run on - by default it will use localhost and port 5000\r\n*   `DEEPGRAM_URL`: the URL of Deepgram's streaming endpoint, including query parameters (Twilio audio uses the `mulaw` encoding with\r\n    a sample rate of 8000, and we will be streaming stereo (2 channel) audio)\r\n*   `DEEPGRAM_API_KEY`: your Deepgram API Key\r\n*   `CERT_PEM`: an optional environment variable pointing to a `cert.pem` file used for TLS\r\n*   `KEY_PEM`: an optional environment variable pointing to a `key.pem` file used for TLS\r\n\r\nWe use these environment variables to construct an `Arc<State>` object to store the global server state.\r\n\r\nNow, let's finish filling in our `main` function by configuring our routes and spinning up the `axum` server to serve these routes:\r\n\r\n```rust\r\n    let app = Router::new()\r\n        .route(\"/twilio\", get(handlers::twilio::twilio_handler))\r\n        .route(\"/client\", get(handlers::subscriber::subscriber_handler))\r\n        .layer(Extension(state));\r\n\r\n    match config {\r\n        Some(config) => {\r\n            axum_server::bind_rustls(proxy_url.parse().unwrap(), config)\r\n                .serve(app.into_make_service())\r\n                .await\r\n                .unwrap();\r\n        }\r\n        None => {\r\n            axum_server::bind(proxy_url.parse().unwrap())\r\n                .serve(app.into_make_service())\r\n                .await\r\n                .unwrap();\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nThe `axum` server is spun up with or without TLS support depending on whether\r\nor not the `CERT_PEM` and `KEY_PEM` environment variables are set.\r\n\r\nThat's all there is to `main.rs`! The bulk of the application logic will live in the websocket endpoint handlers, but before diving into them\r\nlet's go over some of the objects the server will use.\r\n\r\n## `state.rs`, `twilio_response.rs`, and `message.rs`\r\n\r\nCreate the file `src/state.rs` and give it the following contents:\r\n\r\n```rust\r\nuse axum::extract::ws::WebSocket;\r\nuse futures::lock::Mutex;\r\nuse std::collections::HashMap;\r\n\r\npub struct State {\r\n    pub deepgram_url: String,\r\n    pub api_key: String,\r\n    pub subscribers: Mutex<HashMap<String, Vec<WebSocket>>>,\r\n}\r\n```\r\n\r\nThis will represent the global state of the server. The server will need to know the URL of Deepgram's streaming endpoint and a Deepgram API Key to use as authentication when connecting to this endpoint. Additionally, the server will contain\r\na `HashMap` of websocket handlers for subscribers, one for each incoming connection from Twilio. These websocket handlers will be accessed\r\nvia the `callsid` of the Twilio call, and wrapped in a `Mutex` to handle concurrency.\r\n\r\nNext, create the file `src/twilio_response.rs` and give it the following contents:\r\n\r\n```rust\r\n    //! Definitions for the Twilio messages we need to parse\r\n\r\nuse serde::{Deserialize, Serialize};\r\n\r\n#[derive(Serialize, Deserialize, Default, Debug)]\r\n#[serde(rename_all = \"camelCase\")]\r\npub struct Event {\r\n    pub event: String,\r\n    pub sequence_number: String,\r\n    #[serde(flatten)]\r\n    pub event_type: EventType,\r\n    pub stream_sid: String,\r\n}\r\n\r\n#[derive(Serialize, Deserialize, Debug)]\r\n#[serde(rename_all = \"camelCase\")]\r\npub enum EventType {\r\n    Start(EventStart),\r\n    Media(EventMedia),\r\n}\r\n\r\nimpl Default for EventType {\r\n    fn default() -> Self {\r\n        EventType::Media(Default::default())\r\n    }\r\n}\r\n\r\n#[derive(Serialize, Deserialize, Default, Debug)]\r\n#[serde(rename_all = \"camelCase\")]\r\npub struct EventStart {\r\n    pub account_sid: String,\r\n    pub stream_sid: String,\r\n    pub call_sid: String,\r\n    pub tracks: Vec<String>,\r\n    pub media_format: MediaFormat,\r\n}\r\n\r\n#[derive(Serialize, Deserialize, Default, Debug)]\r\n#[serde(rename_all = \"camelCase\")]\r\npub struct MediaFormat {\r\n    pub encoding: String,\r\n    pub sample_rate: u32,\r\n    pub channels: u32,\r\n}\r\n\r\n#[derive(Serialize, Deserialize, Default, Debug)]\r\npub struct EventMedia {\r\n    pub track: String,\r\n    pub chunk: String,\r\n    pub timestamp: String,\r\n    pub payload: String,\r\n}\r\n```\r\n\r\nThese are just basic structs defining the shape of the messages Twilio will send our server. Feel free to checkout\r\n[Twilio's documentation](https://www.twilio.com/docs/voice/twiml/stream#websocket-messages-from-twilio) for more details.\r\n\r\nFinally, create the file `src/message.rs` and give it the following contents:\r\n\r\n```rust\r\n#[derive(Clone)]\r\npub enum Message {\r\n    Text(String),\r\n    Binary(Vec<u8>),\r\n    Ping(Vec<u8>),\r\n    Pong(Vec<u8>),\r\n    Close(Option<tungstenite::protocol::CloseFrame<'static>>),\r\n}\r\n\r\nimpl From<axum::extract::ws::Message> for Message {\r\n    fn from(item: axum::extract::ws::Message) -> Self {\r\n        match item {\r\n            axum::extract::ws::Message::Text(text) => Message::Text(text),\r\n            axum::extract::ws::Message::Binary(binary) => Message::Binary(binary),\r\n            axum::extract::ws::Message::Ping(ping) => Message::Ping(ping),\r\n            axum::extract::ws::Message::Pong(pong) => Message::Pong(pong),\r\n            // will deal with this later\r\n            axum::extract::ws::Message::Close(_) => Message::Close(None),\r\n        }\r\n    }\r\n}\r\n\r\nimpl From<tokio_tungstenite::tungstenite::Message> for Message {\r\n    fn from(item: tokio_tungstenite::tungstenite::Message) -> Self {\r\n        match item {\r\n            tokio_tungstenite::tungstenite::Message::Text(text) => Message::Text(text),\r\n            tokio_tungstenite::tungstenite::Message::Binary(binary) => Message::Binary(binary),\r\n            tokio_tungstenite::tungstenite::Message::Ping(ping) => Message::Ping(ping),\r\n            tokio_tungstenite::tungstenite::Message::Pong(pong) => Message::Pong(pong),\r\n            // will deal with this later\r\n            tokio_tungstenite::tungstenite::Message::Close(_) => Message::Close(None),\r\n        }\r\n    }\r\n}\r\n\r\nimpl From<Message> for axum::extract::ws::Message {\r\n    fn from(item: Message) -> axum::extract::ws::Message {\r\n        match item {\r\n            Message::Text(text) => axum::extract::ws::Message::Text(text),\r\n            Message::Binary(binary) => axum::extract::ws::Message::Binary(binary),\r\n            Message::Ping(ping) => axum::extract::ws::Message::Ping(ping),\r\n            Message::Pong(pong) => axum::extract::ws::Message::Pong(pong),\r\n            // will deal with this later\r\n            Message::Close(_) => axum::extract::ws::Message::Close(None),\r\n        }\r\n    }\r\n}\r\n\r\nimpl From<Message> for tokio_tungstenite::tungstenite::Message {\r\n    fn from(item: Message) -> tokio_tungstenite::tungstenite::Message {\r\n        match item {\r\n            Message::Text(text) => tokio_tungstenite::tungstenite::Message::Text(text),\r\n            Message::Binary(binary) => tokio_tungstenite::tungstenite::Message::Binary(binary),\r\n            Message::Ping(ping) => tokio_tungstenite::tungstenite::Message::Ping(ping),\r\n            Message::Pong(pong) => tokio_tungstenite::tungstenite::Message::Pong(pong),\r\n            // will deal with this later\r\n            Message::Close(_) => tokio_tungstenite::tungstenite::Message::Close(None),\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nThis is also a straightforward module which creates our own websocket `Message` type which can\r\nbe used to convert to and from `axum` websocket messages and `tungstenite` websocket messages.\r\n\r\n## The WebSocket Endpoint Handlers\r\n\r\nNow let's get into the core logic of the server. We need to define functions to handle client/subscriber\r\nconnections to `/client` and Twilio connections to `/twilio`. Let's start with the client handler.\r\n\r\nStart by creating `src/handlers/mod.rs` with the following contents:\r\n\r\n```rust\r\npub mod subscriber;\r\npub mod twilio;\r\n```\r\n\r\nThis simply declares the modules we will use to handle the client/subsriber and Twilio websocket connections.\r\n\r\nThen, create the file `src/handlers/subscriber.rs` with the following contents:\r\n\r\n```rust\r\nuse crate::message::Message;\r\nuse crate::state::State;\r\nuse axum::{\r\n    extract::ws::{WebSocket, WebSocketUpgrade},\r\n    response::IntoResponse,\r\n    Extension,\r\n};\r\nuse std::sync::Arc;\r\n\r\npub async fn subscriber_handler(\r\n    ws: WebSocketUpgrade,\r\n    Extension(state): Extension<Arc<State>>,\r\n) -> impl IntoResponse {\r\n    ws.on_upgrade(|socket| handle_socket(socket, state))\r\n}\r\n\r\nasync fn handle_socket(mut socket: WebSocket, state: Arc<State>) {\r\n    let mut subscribers = state.subscribers.lock().await;\r\n    // send these keys (which will be twilio callsids) to the client\r\n    let keys = subscribers.keys().map(|key| key.to_string()).collect();\r\n    socket\r\n        .send(Message::Text(keys).into())\r\n        .await\r\n        .expect(\"Failed to send callsids to client.\");\r\n\r\n    // wait for the first message from the client\r\n    // and interpret it as the callsid to subscribe to\r\n    if let Some(Ok(msg)) = socket.recv().await {\r\n        let msg = Message::from(msg);\r\n        if let Message::Text(callsid) = msg {\r\n            let callsid = callsid.trim();\r\n            if let Some(subscribers) = subscribers.get_mut(callsid) {\r\n                subscribers.push(socket);\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nAs we saw in `main.rs`, `subscriber_handler` is the function which will be called when a client tries to connect to the\r\n`/client` endpoint of our server. From there, we perform an upgrade from HTTP to websockets. Then, we try to obtain the\r\nsubscribers `HashMap` from our server's global state and send to the client a list of the `callsid`s of all ongoing\r\nTwilio calls that the server is handling. The server then waits for a single message back from the client, and it interprets\r\nthis message as the `callsid` to subscribe to. If the server receives a valid `callsid`, it will push the websocket handle\r\ninto the subscribers `HashMap`. When the Twilio handler receives a transcript for that `callsid`, it will broadcast it to all\r\nsubscribers, including the one we just pushed. That's it for `subscriber.rs`!\r\n\r\nNow let's look at the bulkier `twilio.rs`. Create `src/handlers/twilio.rs`. Let's build this module\r\npiece by piece, starting with some `use` statements:\r\n\r\n```rust\r\nuse crate::audio;\r\nuse crate::message::Message;\r\nuse crate::state::State;\r\nuse crate::twilio_response;\r\nuse axum::{\r\n    extract::ws::{WebSocket, WebSocketUpgrade},\r\n    response::IntoResponse,\r\n    Extension,\r\n};\r\nuse futures::channel::oneshot;\r\nuse futures::{\r\n    sink::SinkExt,\r\n    stream::{SplitSink, SplitStream, StreamExt},\r\n};\r\nuse std::{convert::From, sync::Arc};\r\nuse tokio::net::TcpStream;\r\nuse tokio_tungstenite::{connect_async, MaybeTlsStream, WebSocketStream};\r\n\r\nThen, add the following functions:\r\n\r\npub async fn twilio_handler(\r\n    ws: WebSocketUpgrade,\r\n    Extension(state): Extension<Arc<State>>,\r\n) -> impl IntoResponse {\r\n    ws.on_upgrade(|socket| handle_socket(socket, state))\r\n}\r\n\r\nasync fn handle_socket(socket: WebSocket, state: Arc<State>) {\r\n    let (_this_sender, this_receiver) = socket.split();\r\n\r\n    // prepare the connection request with the api key authentication\r\n    let builder = http::Request::builder()\r\n        .method(http::Method::GET)\r\n        .uri(&state.deepgram_url);\r\n    let builder = builder.header(\"Authorization\", format!(\"Token {}\", state.api_key));\r\n    let request = builder\r\n        .body(())\r\n        .expect(\"Failed to build a connection request to Deepgram.\");\r\n\r\n    // connect to deepgram\r\n    let (deepgram_socket, _) = connect_async(request)\r\n        .await\r\n        .expect(\"Failed to connect to Deepgram.\");\r\n    let (deepgram_sender, deepgram_reader) = deepgram_socket.split();\r\n\r\n    let (callsid_tx, callsid_rx) = oneshot::channel::<String>();\r\n\r\n    tokio::spawn(handle_to_subscribers(\r\n        Arc::clone(&state),\r\n        callsid_rx,\r\n        deepgram_reader,\r\n    ));\r\n    tokio::spawn(handle_from_twilio(\r\n        Arc::clone(&state),\r\n        callsid_tx,\r\n        this_receiver,\r\n        deepgram_sender,\r\n    ));\r\n}\r\n```\r\n\r\nIncoming Twilio connections hitting `/twilio` will be first directed to the function\r\n`twilio_handler` where the websocket upgrade will be performed. Then `handle_socket` will split the websocket connection\r\ninto a receiver and a sender, open up an entirely new websocket connection to Deepgram, split the Deepgram websocket\r\nconnection into a receiver and a sender, and spawn tasks which call the functions `handle_to_subscribers` and\r\n`handle_from_twilio` which take these receivers and senders as arguments. A oneshot channel is also set up so that\r\n`handle_from_twilio` can send the `callsid` of the Twilio call to `handle_to_subscribers` in a thread-safe manner -\r\nthe `callsid` is not yet known when these initial websocket connections are established, it only becomes available\r\nwhen Twilio sends this information in a Twilio `start` event websocket message.\r\n\r\nLet's now define the `handle_to_subscribers` function:\r\n\r\n```rust\r\nasync fn handle_to_subscribers(\r\n    state: Arc<State>,\r\n    callsid_rx: oneshot::Receiver<String>,\r\n    mut deepgram_receiver: SplitStream<WebSocketStream<MaybeTlsStream<TcpStream>>>,\r\n) {\r\n    let callsid = callsid_rx\r\n        .await\r\n        .expect(\"Failed to receive callsid from handle_from_twilio.\");\r\n\r\n    while let Some(Ok(msg)) = deepgram_receiver.next().await {\r\n        let mut subscribers = state.subscribers.lock().await;\r\n        if let Some(subscribers) = subscribers.get_mut(&callsid) {\r\n            // send the message to all subscribers concurrently\r\n            let futs = subscribers\r\n                .iter_mut()\r\n                .map(|subscriber| subscriber.send(Message::from(msg.clone()).into()));\r\n            let results = futures::future::join_all(futs).await;\r\n\r\n            // if we successfully sent a message then the subscriber is still connected\r\n            // other subscribers should be removed\r\n            *subscribers = subscribers\r\n                .drain(..)\r\n                .zip(results)\r\n                .filter_map(|(subscriber, result)| result.is_ok().then(|| subscriber))\r\n                .collect();\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nThis function first waits to receive the `callsid`\r\nfrom `handle_from_twilio` and then proceeds to read messages off the Deepgram websocket receiver, broadcasting all\r\nmessages that it obtains to all subscribers to that `callsid`.\r\n\r\nNow let's define `handle_from_twilio` as follows:\r\n\r\n```rust\r\nasync fn handle_from_twilio(\r\n    state: Arc<State>,\r\n    callsid_tx: oneshot::Sender<String>,\r\n    mut this_receiver: SplitStream<WebSocket>,\r\n    mut deepgram_sender: SplitSink<\r\n        WebSocketStream<MaybeTlsStream<TcpStream>>,\r\n        tokio_tungstenite::tungstenite::Message,\r\n    >,\r\n) {\r\n    let mut buffer_data = audio::BufferData {\r\n        inbound_buffer: Vec::new(),\r\n        outbound_buffer: Vec::new(),\r\n        inbound_last_timestamp: 0,\r\n        outbound_last_timestamp: 0,\r\n    };\r\n\r\n    // wrap our oneshot in an Option because we will need it in a loop\r\n    let mut callsid_tx = Some(callsid_tx);\r\n    let mut callsid: Option<String> = None;\r\n\r\n    while let Some(Ok(msg)) = this_receiver.next().await {\r\n        let msg = Message::from(msg);\r\n        if let Message::Text(msg) = msg {\r\n            let event: Result<twilio_response::Event, _> = serde_json::from_str(&msg);\r\n            if let Ok(event) = event {\r\n                match event.event_type {\r\n                    twilio_response::EventType::Start(start) => {\r\n                        // the \"start\" event only happens once, so having our oneshot in here is kosher\r\n                        callsid = Some(start.call_sid.clone());\r\n\r\n                        // sending this callsid on our oneshot will let `handle_to_subscribers` know the callsid\r\n                        if let Some(callsid_tx) = callsid_tx.take() {\r\n                            callsid_tx\r\n                                .send(start.call_sid.clone())\r\n                                .expect(\"Failed to send callsid to handle_to_subscribers.\");\r\n                        }\r\n\r\n                        // make a new set of subscribers for this call, using the callsid as the key\r\n                        state\r\n                            .subscribers\r\n                            .lock()\r\n                            .await\r\n                            .entry(start.call_sid)\r\n                            .or_default();\r\n                    }\r\n                    twilio_response::EventType::Media(media) => {\r\n                        if let Some(mixed) = audio::process_twilio_media(media, &mut buffer_data) {\r\n                            // send the audio on to deepgram\r\n                            if deepgram_sender\r\n                                .send(Message::Binary(mixed).into())\r\n                                .await\r\n                                .is_err()\r\n                            {\r\n                                break;\r\n                            }\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n\r\n    // close and remove the subscribers, if we have a callsid\r\n    if let Some(callsid) = callsid {\r\n        let mut subscribers = state.subscribers.lock().await;\r\n        if let Some(subscribers) = subscribers.remove(&callsid) {\r\n            for mut subscriber in subscribers {\r\n                // we don't really care if this succeeds or fails as we are closing/dropping these\r\n                let _ = subscriber.send(Message::Close(None).into()).await;\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nThis function begins by setting up an object to help handle the audio buffers\r\nfrom the inbound and outbound callers. We then start reading websocket messages from the Twilio websocket receiver.\r\nWhen we obtain the Twilio `start` event message, we can grab the `callsid`, use it to set up subscribers\r\nto this call, and send it off to the `handle_to_subscribers` task via the oneshot channel we set up earlier. Subsequent\r\nTwilio media events are then processed via `audio::process_twilio_media`, and when a buffer of mixed stereo audio is\r\nready, we send it to Deepgram via the Deepgram websocket sender.\r\n\r\nFinally, when Twilio closes the connection to our server (or some error occurs), we must remember to remove all subscribers\r\nfrom the subscriber `HashMap` and close the connections to those subscribers.\r\n\r\n## Processing the Audio in `audio.rs`\r\n\r\nWhen discussing the Twilio websocket handler, the processing of Twilio media events was delegated to `audio::process_twilio_media`.\r\nWe will define this function in `src/audio.rs`. Make `src/audio.rs` with the following contents:\r\n\r\n```rust\r\nuse crate::twilio_response;\r\n\r\nconst MULAW_SILENCE: u8 = 0xff;\r\nconst MULAW_BYTES_PER_MS: usize = 8;\r\nconst TWILIO_MS_PER_CHUNK: usize = 20;\r\nconst MIN_TWILIO_CHUNKS_TO_MIX: usize = 20;\r\n\r\npub struct BufferData {\r\n    pub inbound_buffer: Vec<u8>,\r\n    pub outbound_buffer: Vec<u8>,\r\n    pub inbound_last_timestamp: u32,\r\n    pub outbound_last_timestamp: u32,\r\n}\r\n\r\nfn pad_with_silence(buffer: &mut Vec<u8>, current_timestamp: u32, previous_timestamp: u32) {\r\n    let time_lost = if current_timestamp < previous_timestamp + TWILIO_MS_PER_CHUNK as u32 {\r\n        // here we have received a timestamp that is less than TWILIO_MS_PER_CHUNK = 20 ms ahead of the previous timestamp\r\n        // this occasionally occurs and is unexpected behavior from Twilio\r\n        0\r\n    } else {\r\n        current_timestamp - (previous_timestamp + TWILIO_MS_PER_CHUNK as u32)\r\n    };\r\n    let silence = std::iter::repeat(MULAW_SILENCE).take(MULAW_BYTES_PER_MS * time_lost as usize);\r\n    buffer.extend(silence);\r\n}\r\n\r\n/// (1) decodes twilio media events\r\n/// (2) pads inbound and outbound buffers with silence if needed\r\n/// (3) if there is more than MIN_TWILIO_CHUNKS_TO_MIX * TWILIO_MS_PER_CHUNK = 400 ms\r\n///     of audio in both inbound and outbound audio buffers, drains as much audio from\r\n///     both buffers as can be mixed together, mixes and returns this audio\r\npub fn process_twilio_media(\r\n    media: twilio_response::EventMedia,\r\n    mut buffer_data: &mut BufferData,\r\n) -> Option<Vec<u8>> {\r\n    // NOTE: when Twilio sends media data, it should send TWILIO_MS_PER_CHUNK = 20 ms audio chunks\r\n    // at a time, where each ms of audio is MULAW_BYTES_PER_MS = 8 bytes\r\n    let media_chunk = base64::decode(media.payload).unwrap();\r\n    let media_chunk_size = media_chunk.len();\r\n    if media_chunk_size != TWILIO_MS_PER_CHUNK * MULAW_BYTES_PER_MS {\r\n        // here, the Twilio media chunk size is not the expected size of TWILIO_MS_PER_CHUNK * MULAW_BYTES_PER_MS bytes\r\n        // this occasionally occurs and is unexpected behavior from Twilio\r\n    }\r\n    // NOTE: There are rare cases where the timestamp is less than TWILIO_MS_PER_CHUNK = 20 ms ahead of the previous chunk\r\n    let timestamp = media.timestamp.parse::<u32>().unwrap();\r\n\r\n    // pad the inbound or outbound buffer with silence if needed depending on timestamp info\r\n    // and then add the audio data from the twilio media message to the buffer\r\n    if media.track == \"inbound\" {\r\n        pad_with_silence(\r\n            &mut buffer_data.inbound_buffer,\r\n            timestamp,\r\n            buffer_data.inbound_last_timestamp,\r\n        );\r\n        buffer_data.inbound_buffer.extend(media_chunk);\r\n        buffer_data.inbound_last_timestamp = timestamp;\r\n    } else if media.track == \"outbound\" {\r\n        pad_with_silence(\r\n            &mut buffer_data.outbound_buffer,\r\n            timestamp,\r\n            buffer_data.outbound_last_timestamp,\r\n        );\r\n        buffer_data.outbound_buffer.extend(media_chunk);\r\n        buffer_data.outbound_last_timestamp = timestamp;\r\n    }\r\n\r\n    // we will return mixed audio of MIN_TWILIO_CHUNKS_TO_MIX * TWILIO_MS_PER_CHUNK = 400 ms (or more)\r\n    // corresponding to MIN_TWILIO_CHUNKS_TO_MIX = 20 twilio media messages (or more)\r\n    let minimum_chunk_size = MIN_TWILIO_CHUNKS_TO_MIX * TWILIO_MS_PER_CHUNK * MULAW_BYTES_PER_MS;\r\n    let mixable_data_size = std::cmp::min(\r\n        buffer_data.inbound_buffer.len(),\r\n        buffer_data.outbound_buffer.len(),\r\n    );\r\n    if mixable_data_size >= minimum_chunk_size {\r\n        let mut mixed = Vec::with_capacity(mixable_data_size * 2);\r\n        let inbound_buffer_segment = buffer_data.inbound_buffer.drain(0..mixable_data_size);\r\n        let outbound_buffer_segment = buffer_data.outbound_buffer.drain(0..mixable_data_size);\r\n\r\n        for (inbound, outbound) in inbound_buffer_segment.zip(outbound_buffer_segment) {\r\n            mixed.push(inbound);\r\n            mixed.push(outbound);\r\n        }\r\n        Some(mixed)\r\n    } else {\r\n        None\r\n    }\r\n}\r\n```\r\n\r\nTwilio sends its audio data as 8000 Hz `mulaw` data, independently for inbound and outbound callers. Additionally, sometimes Twilio\r\n(or the phones which use Twilio) will drop packets of audio. The function `process_twilio_media`, then, handles inserting silence\r\nshould there be dropped packets or timing issues, and mixes together the inbound and outbound audio into a valid stereo audio stream\r\nwhich we can then send to Deepgram. Several of the finer details are explained in the comments in this file.\r\n\r\n## Running the Server and Testing with websocat\r\n\r\nLet's use websocat to quickly test our server.\r\n\r\nRun the server with the following:\r\n\r\n    DEEPGRAM_API_KEY=INSERT_YOUR_DEEPGRAM_API_KEY cargo run\r\n\r\nreplacing `INSERT_YOUR_DEEPGRAM_API_KEY` with your Deepgram API Key.\r\n\r\nThis server will need to be accessible to Twilio, and this is set up in the TwiML Bin you created in the previous [Setting Up a TwiML Bin](#setting-up-a-twiml-bin) section. If you are using `ngrok`, make sure your TwiML Bin\r\nis updated with the current `ngrok` URL.\r\n\r\nNow, call your Twilio number with one phone, and answer the call on the phone your Twilio number forwards to.\r\nThen, latch onto the proxy server via the client endpoint with websocat:\r\n```\r\nwebsocat ws://localhost:5000/client\r\n```\r\nWebsocat should immediately send you a message containing a list of the `callsid`s of all active calls (which in this case should be one).\r\nReply to the message with the `callsid` by copy/pasting and hitting enter:\r\n\r\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1654132200/blog/2022/06/deepgram-twilio-streaming-rust/assets/connect_to_callsid.png\" alt=\"Subscribe to the call via the `callsid`.\" style=\"display: block;margin-left: auto;margin-right: auto;\">\r\n\r\nYou should start to see transcription results appear in your websocat session in real time:\r\n\r\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1654132200/blog/2022/06/deepgram-twilio-streaming-rust/assets/websocat_streaming_asr_results.png\" alt=\"Websocat streaming ASR results.\" style=\"display: block;margin-left: auto;margin-right: auto;\">\r\n\r\nFeel free to try setting up multiple Twilio numbers, and multiple client sessions!\r\n\r\n## Making a Docker Image for the Server\r\n\r\nLet's go through the process of building a Docker image so that this server can be portably deployed. We'll start by making a `rust-toolchain` file with the following contents:\r\n\r\n    1.61\r\n\r\n(quite the simple file!). This will ensure that when you run `cargo build` (either manually, or as part of building a Docker image), the same version of Rust will be used\r\nevery time.\r\n\r\nNow, let's create a Dockerfile called `Dockerfile` and give it the following contents:\r\n```\r\nFROM ubuntu:22.04 as builder\r\n\r\nLABEL maintainer=\"YOUR_INFO\"\r\n\r\nENV DEBIAN_FRONTEND=noninteractive\r\n\r\nRUN apt-get update && \\\r\n    apt-get install -y --no-install-recommends \\\r\n        ca-certificates \\\r\n        clang \\\r\n        curl \\\r\n        libpq-dev \\\r\n        libssl-dev \\\r\n        pkg-config\r\n\r\nCOPY rust-toolchain /rust-toolchain\r\nRUN curl https://sh.rustup.rs -sSf | sh -s -- -y --default-toolchain $(cat /rust-toolchain) && \\\r\n    . $HOME/.cargo/env\r\n\r\nCOPY . /deepgram-twilio-streaming-rust\r\n\r\nRUN . $HOME/.cargo/env && \\\r\n    cargo install --path /deepgram-twilio-streaming-rust --root /\r\n\r\nFROM ubuntu:22.04\r\n\r\nLABEL maintainer=\"YOUR_INFO\"\r\n\r\nENV DEBIAN_FRONTEND=noninteractive\r\n\r\nRUN apt-get update && \\\r\n    apt-get install -y --no-install-recommends \\\r\n        ca-certificates \\\r\n        libpq5 \\\r\n        libssl3 && \\\r\n    apt-get clean\r\n\r\nCOPY --from=builder /bin/deepgram-twilio-streaming-rust /bin/deepgram-twilio-streaming-rust\r\n\r\nENTRYPOINT [\"/bin/deepgram-twilio-streaming-rust\"]\r\nCMD [\"\"]\r\n```\r\n\r\nReplace `YOUR_INFO` with your name and email address (for me, for example, this would be `Nikola Whallon <nikola@deepgram.com>`).\r\nThe key bits to take away are:\r\n\r\n*   we start with an Ubuntu 22.04 image\r\n*   we install several dependencies via `apt`\r\n*   we use the `rust-toolchain` and build+install our executable with `cargo install`\r\n*   we set the `ENTRYPOINT` to `/bin/deepgram-twilio-streaming-rust`, with no command-line arguments (`CMD`)\r\n\r\nNow with the Dockerfile written, build the Docker image with:\r\n\r\n```\r\ndocker build -t your-docker-hub-account/deepgram-twilio-streaming-rust:0.1.0 -f Dockerfile .\r\n```\r\nIf you will be pushing this image to Docker Hub so that the image can be pulled from a remote server (like an AWS instance),\r\nreplace `your-docker-hub-account` with your Docker Hub account. For local testing, simply using the image name `deepgram-twilio-streaming-rust:0.1.0`\r\n(or whatever you would like) will work. You are also free to pull and use `deepgram/deepgram-twilio-treaming-rust:0.1.0`!\r\n\r\nNow you can run the Docker image in a container locally via:\r\n```\r\ndocker run -e PROXY_URL=0.0.0.0:5000 -e DEEPGRAM_API_KEY=INSERT_YOUR_DEEPGRAM_API_KEY \\\r\n    -p 5000:5000 your-docker-hub-account/deepgram-twilio-streaming-rust:0.1.0\r\n```\r\nreplacing `INSERT_YOUR_DEEPGRAM_API_KEY` with your Deepgram API Key, and make sure the Docker image name matches what you built. This will\r\nrun the image in a container in your current terminal, but you can include a `-d` to detach the process to run in the background. If you do this,\r\nyou will need to keep track of whether or not it is running with `docker ps` and similar commands.\r\n\r\nRefer to the [Docker CLI documentation](https://docs.docker.com/engine/reference/commandline/cli/) for more info.\r\n\r\nNow that the Twilio proxy server should be running in a Docker container, feel free to give your Twilio number a call, and subscribe to the call\r\nwith websocat by doing:\r\n\r\n```\r\n websocat ws://localhost:5000/client\r\n```\r\nand replying to the server with the `callsid` it sends you.\r\n\r\nYou should be all set to push this Docker image to your Docker Hub (or use ours: `deepgram/deepgram-twilio-treaming-rust:0.1.0`), and pull\r\nand use it on your cloud server! You will need to provide the additional environment variables `CERT_PEM` and `KEY_PEM` to do this, making\r\nsure those files are accessible to the Docker continer by using `-v`, and you may need to specify\r\nthe port as `443` in the `PROXY_URL` and use `-p 443:443` among other subtle changes.\r\nYou should refer to your cloud server provider's documentation on setting up an https/wss enabled server with certificates. As an example,\r\nhere's how I spun up the server app on an AWS Ubuntu 20.04 instance:\r\n\r\n```\r\ndocker run -v /home/ubuntu:/foo -p 443:443 -d \\\r\n    -e PROXY_URL=0.0.0.0:443 -e DEEPGRAM_API_KEY=INSERT_YOUR_DEEPGRAM_API_KEY \\\r\n    -e CERT_PEM=/foo/cert.pem -e KEY_PEM=/foo/key.pem \\\r\n    deepgram/deepgram-twilio-streaming-rust:0.1.0\r\n```\r\n## Further Development\r\n\r\nThis should get you up and running with an almost-production-ready Twilio-Deepgram proxy server, written in Rust. There are a few pieces\r\nthat have been left out, for the sake of brevity and for the sake of being agnostic to the needs of your desired system. For example, calls to the\r\n`/client` endpoint are currently entirely unauthenticated, and indeed calls to `/twilio` are also unauthenticated (see\r\n[these Twilio docs](https://www.twilio.com/docs/usage/security) for more details). For a fully-production-ready service, you should\r\ntake authentication into consideration. Also, no logging or telemetry is presented in the proxy server.\r\n\r\nFinally, you will very likely need to build a front-end to interact with the server and properly parse the JSON messages being streamed.\r\nwebsocat is great for testing, but is not a reasonable final solution for subscribing to calls!\r\n\r\nIf you have any questions, please feel free to reach out on Twitter - we're [@DeepgramDevs](https://twitter.com/DeepgramDevs).\r\n\r\n        ";
						}
						async function compiledContent$34() {
							return load$34().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$34() {
							return (await import('./chunks/index.fed0ba3a.mjs'));
						}
						function Content$34(...args) {
							return load$34().then((m) => m.default(...args));
						}
						Content$34.isAstroComponentFactory = true;
						function getHeadings$34() {
							return load$34().then((m) => m.metadata.headings);
						}
						function getHeaders$34() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$34().then((m) => m.metadata.headings);
						}

const __vite_glob_0_80 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$34,
  file: file$34,
  url: url$34,
  rawContent: rawContent$34,
  compiledContent: compiledContent$34,
  default: load$34,
  Content: Content$34,
  getHeadings: getHeadings$34,
  getHeaders: getHeaders$34
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$33 = {"title":"Transcribing Twilio Voice Calls in Real-Time with Deepgram","description":"In this tutorial, learn how to transcribe Twilio Voice calls with Deepgram with real-time speech-to-text.","date":"2022-04-07T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1649267223/blog/2022/04/deepgram-twilio-streaming/Transcribing-Twilio-Calls-Real-Time-w-Deepgram%402x.jpg","authors":["nikola-whallon"],"category":"tutorial","tags":["twilio","python"],"seo":{"title":"Transcribing Twilio Voice Calls in Real-Time with Deepgram","description":"In this tutorial, learn how to transcribe Twilio Voice calls with Deepgram with real-time speech-to-text."},"shorturls":{"share":"https://dpgr.am/650fd92","twitter":"https://dpgr.am/6458d1e","linkedin":"https://dpgr.am/273ef64","reddit":"https://dpgr.am/2fc8f2d","facebook":"https://dpgr.am/80b16d1"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454061/blog/deepgram-twilio-streaming/ograph.png"}};
						const file$33 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-twilio-streaming/index.md";
						const url$33 = undefined;
						function rawContent$33() {
							return "\nTwilio is a very popular voice platform, and Deepgram is a great automatic speech recognition (ASR) solution, so there\r\nis great value in integrating the two. This tutorial will guide you through building an integration that allows\r\nmultiple client subscribers to watch live transcripts from ongoing Twilio calls. The code for this tutorial\r\nis located [here](https://github.com/deepgram-devs/deepgram-twilio-streaming-python).\n\n## Pre-requisites\n\nYou will need:\n\n*   A [Twilio account](https://www.twilio.com/try-twilio) with a Twilio number (the free tier will work).\n*   A Deepgram API Key - [get an API Key here](https://console.deepgram.com/signup?jump=keys).\n*   *(Optional)* [ngrok](https://ngrok.com/) to let Twilio access a local server.\n\n## Setting Up A TwiML Bin\n\nWe need to tell Twilio to fork audio data from calls going to your Twilio number to the server we are going to write.\r\nThis is done via \"TwiML Bin\"s. In the Twilio Console, search for TwiML Bin, and click \"Create TwiML Bin.\"\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1648782300/blog/2022/04/deepgram-twilio-streaming/assets/find_twiml_bin.png\" alt=\"Navigate to your TwiML Bins.\" style=\"max-width: 606px;display: block;margin-left: auto;margin-right: auto;\">\n\nGive the TwiML Bin a \"Friendly Name\" - something like \"Streaming\" or \"Deepgram Streaming,\" and then make the contents of the TwiML Bin the following:\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<Response>\r\n  <Start>\r\n    <Stream url=\"wss://INSERT_YOUR_SERVER_URL/twilio\" track=\"both_tracks\"/>\r\n  </Start>\r\n  <Say voice=\"woman\" language=\"en\">\"This call may be monitored or recorded for quality purposes.\"</Say>\r\n  <Dial>+11231231234</Dial>\r\n</Response>\n```\n\nReplace the number in the `Dial` section with the phone number you want incoming calls to be forwarded to (this should not be\r\nyour Twilio number, it should be the number of a real phone in your possession!). Then, where it says `INSERT_YOUR_SERVER_URL`\r\ntype in the URL where you will be running your server. Without having to setup an AWS or DigitalOcean server, you can use\r\n`ngrok` to expose a local server. To expose port 5000 on your computer, you can use ngrok as follows:\n\n```bash\nngrok http 5000\n```\n\n`ngrok` will then tell you the public URL which points to your `localhost:5000`. Your URL may\r\nend up looking something like: `c52e-71-212-124-133.ngrok.io`.\n\nNow, we need to connect this TwiML Bin to your Twilio phone number. Go to the \"Develop\" tab on the left side\r\nof the Twilio Console, navigate to `Phone Numbers -> Manage -> Active numbers`, and click on your Twilio number\r\nin the list. Then, under the field \"A Call Comes In,\" click the drop-down and select \"TwiML Bin\"; for the field\r\ndirectly next to this one, click the drop-down and select \"Streaming\" (or whatever your TwiML Bin's \"Friendly Name\" is).\r\nFinally, click \"Save\" at the bottom of the Twilio Console. Everything on Twilio's side should now be set up, and we\r\nare ready to move on to the Deepgram integration server!\n\n## The Twilio Proxy Server\n\nLet's take a look at the system we are building here:\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1648783494/blog/2022/04/deepgram-twilio-streaming/assets/deepgram_twilio_diagram.png\" alt=\"The big picture.\" style=\"max-width: 2096px;display: block;margin-left: auto;margin-right: auto;\">\n\nWe have pairs of callers, inbound and outbound, and, for each call passing through Twilio's servers, Twilio is able to fork the audio from the call\r\nto our proxy server via websockets. Our server then has to do some light processing of that audio, forward it on to Deepgram, receive transcripts\r\nback from Deepgram, and forward those transcripts on to potentially multiple clients who are subscribed to watch the call's transcripts. So in order\r\nto view real-time transcripts in a client application, our backend server must maintain a minimum of three websockets connections - we can see how\r\nthis can get complicated, especially when dealing with many concurrent Twilio calls and subscribed clients!\n\nDownload the code from [this repository](https://github.com/deepgram-devs/deepgram-twilio-streaming-python). It contains a single file, `twilio.py`!\n\nLet's look at the code (make sure to replace `INSERT_YOUR_DEEPGRAM_API_KEY` with your Deepgram API Key):\n\n```python\nimport asyncio\r\nimport base64\r\nimport json\r\nimport sys\r\nimport websockets\r\nimport ssl\r\nfrom pydub import AudioSegment\r\n\r\nsubscribers = {}\r\n\r\ndef deepgram_connect():\r\n extra_headers = {\r\n  'Authorization': 'Token INSERT_YOUR_DEEPGRAM_API_KEY'\r\n }\r\n deepgram_ws = websockets.connect('wss://api.deepgram.com/v1/listen?encoding=mulaw&sample_rate=8000&channels=2&multichannel=true', extra_headers = extra_headers)\r\n\r\n return deepgram_ws\r\n\r\nasync def twilio_handler(twilio_ws):\r\n audio_queue = asyncio.Queue()\r\n callsid_queue = asyncio.Queue()\r\n\r\n async with deepgram_connect() as deepgram_ws:\r\n\r\n  async def deepgram_sender(deepgram_ws):\r\n   print('deepgram_sender started')\r\n   while True:\r\n    chunk = await audio_queue.get()\r\n    await deepgram_ws.send(chunk)\r\n\r\n  async def deepgram_receiver(deepgram_ws):\r\n   print('deepgram_receiver started')\r\n   # we will wait until the twilio ws connection figures out the callsid\r\n   # then we will initialize our subscribers list for this callsid\r\n   callsid = await callsid_queue.get()\r\n   subscribers[callsid] = []\r\n   # for each deepgram result received, forward it on to all\r\n   # queues subscribed to the twilio callsid\r\n   async for message in deepgram_ws:\r\n    for client in subscribers[callsid]:\r\n     client.put_nowait(message)\r\n\r\n   # once the twilio call is over, tell all subscribed clients to close\r\n   # and remove the subscriber list for this callsid\r\n   for client in subscribers[callsid]:\r\n    client.put_nowait('close')\r\n\r\n   del subscribers[callsid]\r\n\r\n  async def twilio_receiver(twilio_ws):\r\n   print('twilio_receiver started')\r\n   # twilio sends audio data as 160 byte messages containing 20ms of audio each\r\n   # we will buffer 20 twilio messages corresponding to 0.4 seconds of audio to improve throughput performance\r\n   BUFFER_SIZE = 20 * 160\r\n   # the algorithm to deal with mixing the two channels is somewhat complex\r\n   # here we implement an algorithm which fills in silence for channels if that channel is either\r\n   #   A) not currently streaming (e.g. the outbound channel when the inbound channel starts ringing it)\r\n   #   B) packets are dropped (this happens, and sometimes the timestamps which come back for subsequent packets are not aligned)\r\n   inbuffer = bytearray(b'')\r\n   outbuffer = bytearray(b'')\r\n   inbound_chunks_started = False\r\n   outbound_chunks_started = False\r\n   latest_inbound_timestamp = 0\r\n   latest_outbound_timestamp = 0\r\n   async for message in twilio_ws:\r\n    try:\r\n     data = json.loads(message)\r\n     if data['event'] == 'start':\r\n      start = data['start']\r\n      callsid = start['callSid']\r\n      callsid_queue.put_nowait(callsid)\r\n     if data['event'] == 'connected':\r\n      continue\r\n     if data['event'] == 'media':\r\n      media = data['media']\r\n      chunk = base64.b64decode(media['payload'])\r\n      if media['track'] == 'inbound':\r\n       # fills in silence if there have been dropped packets\r\n       if inbound_chunks_started:\r\n        if latest_inbound_timestamp + 20 < int(media['timestamp']):\r\n         bytes_to_fill = 8 * (int(media['timestamp']) - (latest_inbound_timestamp + 20))\r\n         # NOTE: 0xff is silence for mulaw audio\r\n         # and there are 8 bytes per ms of data for our format (8 bit, 8000 Hz)\r\n         inbuffer.extend(b'\\xff' * bytes_to_fill)\r\n       else:\r\n        # make it known that inbound chunks have started arriving\r\n        inbound_chunks_started = True\r\n        latest_inbound_timestamp = int(media['timestamp'])\r\n        # this basically sets the starting point for outbound timestamps\r\n        latest_outbound_timestamp = int(media['timestamp']) - 20\r\n       latest_inbound_timestamp = int(media['timestamp'])\r\n       # extend the inbound audio buffer with data\r\n       inbuffer.extend(chunk)\r\n      if media['track'] == 'outbound':\r\n       # make it known that outbound chunks have started arriving\r\n       outbound_chunked_started = True\r\n       # fills in silence if there have been dropped packets\r\n       if latest_outbound_timestamp + 20 < int(media['timestamp']):\r\n        bytes_to_fill = 8 * (int(media['timestamp']) - (latest_outbound_timestamp + 20))\r\n        # NOTE: 0xff is silence for mulaw audio\r\n        # and there are 8 bytes per ms of data for our format (8 bit, 8000 Hz)\r\n        outbuffer.extend(b'\\xff' * bytes_to_fill)\r\n       latest_outbound_timestamp = int(media['timestamp'])\r\n       # extend the outbound audio buffer with data\r\n       outbuffer.extend(chunk)\r\n     if data['event'] == 'stop':\r\n      break\r\n\r\n     # check if our buffer is ready to send to our audio_queue (and, thus, then to deepgram)\r\n     while len(inbuffer) >= BUFFER_SIZE and len(outbuffer) >= BUFFER_SIZE:\r\n      asinbound = AudioSegment(inbuffer[:BUFFER_SIZE], sample_width=1, frame_rate=8000, channels=1)\r\n      asoutbound = AudioSegment(outbuffer[:BUFFER_SIZE], sample_width=1, frame_rate=8000, channels=1)\r\n      mixed = AudioSegment.from_mono_audiosegments(asinbound, asoutbound)\r\n\r\n      # sending to deepgram via the audio_queue\r\n      audio_queue.put_nowait(mixed.raw_data)\r\n\r\n      # clearing buffers\r\n      inbuffer = inbuffer[BUFFER_SIZE:]\r\n      outbuffer = outbuffer[BUFFER_SIZE:]\r\n    except:\r\n     break\r\n\r\n   # the async for loop will end if the ws connection from twilio dies\r\n   # and if this happens, we should forward an empty byte to deepgram\r\n   # to signal deepgram to send back remaining messages before closing\r\n   audio_queue.put_nowait(b'')\r\n\r\n  await asyncio.wait([\r\n   asyncio.ensure_future(deepgram_sender(deepgram_ws)),\r\n   asyncio.ensure_future(deepgram_receiver(deepgram_ws)),\r\n   asyncio.ensure_future(twilio_receiver(twilio_ws))\r\n  ])\r\n\r\n  await twilio_ws.close()\r\n\r\nasync def client_handler(client_ws):\r\n client_queue = asyncio.Queue()\r\n\r\n # first tell the client all active calls\r\n await client_ws.send(json.dumps(list(subscribers.keys())))\r\n\r\n # then recieve from the client which call they would like to subscribe to\r\n # and add our client's queue to the subscriber list for that call\r\n try:\r\n  # you may want to parse a proper json input here\r\n  # instead of grabbing the entire message as the callsid verbatim\r\n  callsid = await client_ws.recv()\r\n  callsid = callsid.strip()\r\n  if callsid in subscribers:\r\n   subscribers[callsid].append(client_queue)\r\n  else:\r\n   await client_ws.close()\r\n except:\r\n  await client_ws.close()\r\n\r\n async def client_sender(client_ws):\r\n  while True:\r\n   message = await client_queue.get()\r\n   if message == 'close':\r\n    break\r\n   try:\r\n    await client_ws.send(message)\r\n   except:\r\n    # if there was an error, remove this client queue\r\n    subscribers[callsid].remove(client_queue)\r\n    break\r\n\r\n await asyncio.wait([\r\n  asyncio.ensure_future(client_sender(client_ws)),\r\n ])\r\n\r\n await client_ws.close()\r\n\r\nasync def router(websocket, path):\r\n if path == '/client':\r\n  print('client connection incoming')\r\n  await client_handler(websocket)\r\n elif path == '/twilio':\r\n  print('twilio connection incoming')\r\n  await twilio_handler(websocket)\r\n\r\ndef main():\r\n # use this if using ssl\r\n# ssl_context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\r\n# ssl_context.load_cert_chain('cert.pem', 'key.pem')\r\n# server = websockets.serve(router, '0.0.0.0', 443, ssl=ssl_context)\r\n\r\n # use this if not using ssl\r\n server = websockets.serve(router, 'localhost', 5000)\r\n\r\n asyncio.get_event_loop().run_until_complete(server)\r\n asyncio.get_event_loop().run_forever()\r\n\r\nif __name__ == '__main__':\r\n sys.exit(main() or 0)\n```\n\nThis server uses the Python `websocket` library to connect to Twilio, Deepgram, and client applications, and the `asyncio` library to handle\r\nconcurrent connections. The server has two routes: `/twilio` and `/client`. As we have configured in our TwiML Bin, Twilio will be connecting\r\nto and sending audio data to the `/twilio` endpoint, and we will use the `/client` endpoint for client applications which will watch the\r\nstreaming transcripts.\n\nThe server uses a dictionary, called `subscribers`, to handle concurrent connected clients. Specifically, `subscribers` is a dictionary\r\nwhose keys are Twilio `callSid`s which uniquely identify calls, and whose values are a list of queues for clients who are \"subscribed\"\r\nto those calls (i.e. watching for streaming transcripts from those calls).\n\nTo dive into the code, let's look at the `client_handler` function. When a client connects to the `/client` endpoint, the `client_handler`\r\nfunction will first send a websocket message to the client listing the `callSid`s of all currently streaming calls. The function then waits\r\nto receive a websocket message which it expects to be the `callSid` of the call that the client wants to view live transcripts for\r\n(and if the function does not receive a valid `callSid`, it will bail). Having received a valid `callSid`, the function then inserts\r\nthis client's queue into the `subscribers` dictionary and starts an async task which reads from this queue, sending transcription\r\nresults back to the client via websocket messages, or gracefully closing the websocket connection if the message \"close\" was received on the queue.\n\nNow let's jump into the more involved `twilio_handler` function. This function handles incoming websocket connections from Twilio,\r\nand begins by setting up a queue for audio data, and a queue to handle passing the incoming `callSid` between async tasks.\r\nIt then connects to Deepgram and sets up three async tasks: `deepgram_receiver`, `deepgram_sender`, and `twilio_receiver` (we will\r\nnever send websocket messages back to Twilio, hence no \"twilio\\_sender\" task).\n\nThe `twilio_receiver` task handles incoming [websocket messages](https://www.twilio.com/docs/voice/twiml/stream#websocket-messages-from-twilio) from Twilio.\r\nBefore Twilio sends audio data, it will send some metadata as part of a `start` event. One of these pieces of metadata is the `callSid`\r\nof the call, and we will pass that on to the `deepgram_receiver` task via a queue. Then, when Twilio starts streaming `media` (i.e. audio)\r\nevents, we will perform some logic to buffer and mix this audio. In particular, Twilio will stream audio in via separate `inbound`\r\nand `outbound` audio tracks; we must make sure we mix these two audio tracks together as correct stereo audio to pass on to Deepgram.\r\nSome issues arise if call packets are dropped from one of these tracks, and logic is implemented with ample comments to deal with this\r\nwithout having the two channels in the mixed stereo audio get out of sync. Finally, with correctly mixed audio buffers prepared,\r\n`twilio_receiver` will pass this audio on to the `deepgram_sender` task via a queue. The `deepgram_sender` task then simply passes\r\nthis audio on to Deepgram via the Deepgram websocket handle.\n\nFinally, we get to the `deepgram_receiver` task. In order to pass transcripts from Deepgram on to subscribed clients, we must first\r\nknow the `callSid` of the call, so the first thing `deepgram_receiver` does is wait to obtain this from the `twilio_receiver` via\r\na queue. Once the `callSid` is obtained, the `deepgram_receiver` is then able to forward on all transcription results from Deepgram\r\nto all clients subscribed to that `callSid`. It does this via another queue, which is handled by the async task defined in `client_handler`,\r\nand thus we come full circle.\n\n## Running the Server and Testing with WebSocat\n\nTo run the server, first `pip3 install` the `websockets`, `pydub`, and `asyncio` libraries, and then run:\n\n```bash\npython3 twilio.py\n```\n\nIf you are running this on your own cloud server, make sure port 5000 is accessible. If you followed the optional\r\nsuggestion of using `ngrok`, this should be all set up simply by running `ngrok http 5000` on a separate terminal.\n\nTo quickly test the integration, start a call to your Twilio number - this call will be forwarded to the phone number\r\nin the `Dial` section of your TwiML Bin, so you will need two phones (so feel free to grab a friend, or set up\r\na Google Voice account or something similar!).\n\nAfter the phone call has started, use a tool like [websocat](https://github.com/vi/websocat#installation) to connect\r\nto `ws://localhost:5000/client`. Upon connecting, the server should output a list of the `callSid`s of ongoing calls\r\n(it should be a list of exactly one call at this point); reply to the server with one of these `callSid`s and watch\r\nthe Deepgram transcription responses roll in! You can start multiple clients and have them all subscribe to the\r\nsame `callSid` to see how a concurrent system could work.\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1649269367/blog/2022/04/deepgram-twilio-streaming/assets/websocat_screenshot.png\" alt=\"Using websocat to view the transcripts.\" style=\"max-width: 1623px;display: block;margin-left: auto;margin-right: auto;\">\n\n## Further Development\n\nThe Deepgram-Twilio integration design presented here is slightly opinionated, in the interest of getting\r\na reasonably complete demo up and running. You may want to factor in authentication, as the `/client` endpoint\r\nexplained here is completely unauthenticated. You also may want to find an alternate way of labelling calls\r\nto subscribe to - instead of grabbing `callSid`s, one could subscribe directly to Twilio numbers, but this\r\nwould require extra Twilio API integration to look up the status of calls to your Twilio numbers.\n\nAnother clear next step would be to develop a proper client application. Programs like `websocat` are fantastic\r\nfor testing, but you will likely want to design a front-end application which handles selecting `callSid`s\r\nto subscribe to, parses and formats the Deepgram transcription response, and possibly other features.\n\nIf you have any questions, please feel free to reach out on Twitter - we're [@DeepgramDevs](https://twitter.com/DeepgramDevs).\n\n        ";
						}
						async function compiledContent$33() {
							return load$33().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$33() {
							return (await import('./chunks/index.2d907542.mjs'));
						}
						function Content$33(...args) {
							return load$33().then((m) => m.default(...args));
						}
						Content$33.isAstroComponentFactory = true;
						function getHeadings$33() {
							return load$33().then((m) => m.metadata.headings);
						}
						function getHeaders$33() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$33().then((m) => m.metadata.headings);
						}

const __vite_glob_0_81 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$33,
  file: file$33,
  url: url$33,
  rawContent: rawContent$33,
  compiledContent: compiledContent$33,
  default: load$33,
  Content: Content$33,
  getHeadings: getHeadings$33,
  getHeaders: getHeaders$33
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$32 = {"title":"How to Add Deepgram Speech Recognition to Your Unity Game","description":"In this tutorial, learn how to integrate Deepgram's automatic speech recognition engine into your Unity game.","date":"2022-03-16T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1647451573/blog/2022/03/deepgram-unity-tutorial/assets/Building-a-Game-w-Unity-Deepgram%402x.jpg","authors":["nikola-whallon"],"category":"tutorial","tags":["game-dev","unity"],"seo":{"title":"How to Add Deepgram Speech Recognition to Your Unity Game","description":"In this tutorial, learn how to integrate Deepgram's automatic speech recognition engine into your Unity game."},"shorturls":{"share":"https://dpgr.am/0692b8d","twitter":"https://dpgr.am/fb3493d","linkedin":"https://dpgr.am/da35424","reddit":"https://dpgr.am/6adc409","facebook":"https://dpgr.am/3ce4691"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454022/blog/deepgram-unity-tutorial/ograph.png"}};
						const file$32 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-unity-tutorial/index.md";
						const url$32 = undefined;
						function rawContent$32() {
							return "\r\nIn this tutorial, we will be making a simple physics simulation in Unity with actions triggered by Deepgram's\r\nautomatic speech recognition (ASR) engine. Why use Deepgram with Unity? Well, Unity is an industry standard when it comes to game development,\r\nand while speech-enhanced games have been around for decades, high performance, easy-to-use ASR is relatively new, and Deepgram represents the cream-of-the-crop.\r\nSo if you are looking to try something new with your games, perhaps for a more immersive or accessible experience, I highly recommend trying this out!\r\n\r\nThis tutorial assumes no prior experience with Unity or C# (the language Unity uses for scripting). However, its\r\nfocus is on getting you quickly set up to integrate Deepgram in your Unity\r\nproject, and does not cover Unity game development or the C# language in depth. There are a plethora of resources out there to learn game development with Unity,\r\nand I highly recommend [checking out their learning website](https://learn.unity.com/) as a starting point!\r\n\r\nFinally, for an example of a simple game built with Deepgram ASR, you can check out [Spooky Speech Spells](https://spookyspeechspells.deepgram.com).\r\n\r\n## Pre-requisites\r\n\r\nYou will need:\r\n\r\n*   Unity installed on your machine - [download Unity here](https://unity.com/download). This tutorial was written with the Unity Editor version `2020.3.30f1`.\r\n*   *(Optional)* [VS Code](https://code.visualstudio.com/docs/other/unity) or [Script Inspector 3](https://assetstore.unity.com/packages/tools/visual-scripting/script-inspector-3-3535) for editing C# Unity scripts (you can use any text editor, but these are highly recommended for their Unity integration).\r\n*   A Deepgram API Key - [get an API Key here](https://console.deepgram.com/signup?jump=keys).\r\n\r\n## Try the Demo\r\n\r\nTo run the demo we are going to build and browse its files:\r\n\r\n*   Download [this repository](https://github.com/deepgram/UnityDeepgramDemo), open Unity, click \"Open\", and browse to and select the directory `UnityDeepgramDemo`.\r\n*   Edit the script `UnityDeepgramDemo/Assets/DeepgramInstance.cs` and replace the string `INSERT_YOUR_API_KEY` with your Deepgram API key.\r\n*   In the bottom left \"Project\" tab open `Assets -> Scenes` and double click \"SampleScene\" to load this scene.\r\n*   In the top center of the Unity editor, hit the \"Play\" (►) button.\r\n*   Say \"left,\" \"right,\" \"up,\" and \"down\" to move the ball around!\r\n\r\n## Building the Demo\r\n\r\nIn the following sections, we will walk through step-by-step how to make this demo where you can move\r\na ball around a simulated physics environment just by commanding it via your computer's microphone.\r\n\r\n## Setting Up the Project\r\n\r\nOpen Unity and click \"New project\".\r\n\r\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1647377747/blog/2022/03/deepgram-unity-tutorial/assets/new_project.png\" alt=\"Create a new project.\" style=\"max-width: 2272px;display: block;margin-left: auto;margin-right: auto;\">\r\n\r\nYou will be presented with a list of templates - choose \"2D\", and under the \"Project Settings\" panel name the project \"UnityDeepgramDemo\" (or whatever you'd like!)\r\nand choose a location for the project on your filesystem.\r\nThen click \"Create project.\"\r\n\r\n![Create the project from the \"2D\" template.](https://res.cloudinary.com/deepgram/image/upload/v1647261812/blog/2022/03/deepgram-unity-tutorial/assets/create_the_project.png)\r\n\r\nWe are now in the Unity Editor. Our demo will rely on one external package to help us handle the websocket connection to Deepgram - [Native WebSockets](https://github.com/endel/NativeWebSocket).\r\nTo install Native WebSockets, first, open the Package Manager from `Window -> Package Manager`.\r\n\r\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1647377747/blog/2022/03/deepgram-unity-tutorial/assets/open_package_manager.png\" alt=\"Open the Package Manager.\" style=\"max-width: 466px;display: block;margin-left: auto;margin-right: auto;\">\r\n\r\nThen click the \"+\" drop-down and click \"Add package from git URL...\".\r\n\r\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1647377746/blog/2022/03/deepgram-unity-tutorial/assets/add_package_from_git_url.png\" alt=\"Add a package from a git URL.\" style=\"max-width: 400px;display: block;margin-left: auto;margin-right: auto;\">\r\n\r\nEnter the URL: https://github.com/endel/NativeWebSocket.git#upm and click \"Add.\"\r\n\r\nWe are now ready to start putting the demo together!\r\n\r\n## Creating Physics Objects\r\n\r\nIn the center of the Unity Editor are the \"Scene\" and \"Game\" tabs. We will spend most of our time in the \"Scene\" tab, though when we play the game, we will be shifted over to the \"Game\"\r\ntab. On the left side of the Unity Editor is the \"Hierarchy\" tab - this is where we will be adding our game objects.\r\n\r\nLet's add some physics objects to our scene - we are going to create a box out of 4 static (non-movable) walls, and add a dynamic (movable) ball in the center.\r\n\r\nIn the \"Hierarchy\" tab, right-click and select `2D Object -> Physics -> Static Sprite`.\r\n\r\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1647377747/blog/2022/03/deepgram-unity-tutorial/assets/add_static_sprite.png\" alt=\"Add a Static Sprite.\" style=\"max-width: 1166px;display: block;margin-left: auto;margin-right: auto;\">\r\n\r\nWe now have a static square sprite in the center of our scene. Let's move this square to the left and make it a vertical rectangle to start building our box.\r\nTo do this, go to the \"Inspector\" tab on the right. Under \"Transform,\" change the \"X\" \"Position\" to -5 and the \"Y\" \"Scale\" to 5. This will stretch the square\r\ninto a vertical rectangle 5 units tall, and place it 5 units to the left.\r\n\r\n![Create a static wall.](https://res.cloudinary.com/deepgram/image/upload/v1647404529/blog/2022/03/deepgram-unity-tutorial/assets/wall_left.png)\r\n\r\nNow, right-click the \"Static Sprite\" object in the \"Hierarchy\" tab, click \"Rename,\" and rename it \"WallLeft.\" Let's do the same procedure to make a \"WallRight,\" \"WallUp,\" and \"WallDown.\"\r\n\r\nFor \"WallRight,\" change the \"X\" \"Position\" to 5 and the \"Y\" \"Scale\" to 5.\r\nFor \"WallUp,\" change the \"Y\" \"Position\" to 3 and the \"X\" \"Scale\" to 9.\r\nAnd for the \"WallDown,\" change the \"Y\" \"Position\" to -3 and the \"X\" \"Scale\" to 9.\r\nYou should now have a box which looks like this:\r\n\r\n![Our completed box.](https://res.cloudinary.com/deepgram/image/upload/v1647404529/blog/2022/03/deepgram-unity-tutorial/assets/complete_box.png)\r\n\r\nLet's add a dynamic ball inside the box. Right-click inside the \"Hierarchy\" tab and select `2D Object -> Physics -> Dynamic Sprite` and name the object \"Ball.\"\r\nDynamic sprites have gravity applied to them by default, and this can be changed if desired in the \"Gravity Scale\" field of the \"Rigidbody 2D\" node in the \"Inspector\" tab.\r\nFor now, let's leave it at the default value. Feel free to press \"Play\" (►) to start the game! You should see the ball fall to the bottom of the box - not much going on yet.\r\n(Note, however, that you will not be able to do certain edits on the game until you stop playing the game by pressing again on the \"Play\" (►) button.)\r\n\r\nFinally, let's attach a script to our \"Ball\" object. In the bottom panel, select the \"Project\" tab and open the \"Assets\" directory. Right-click in the \"Assets\"\r\npanel and select `Create -> C# Script`. Name the script `Ball` (on your OS's filesystem, the file will exist as `Ball.cs`). Open this script with your text\r\neditor of choice, and paste the following contents:\r\n\r\n```\r\n    using System.Collections;\r\n    using System.Collections.Generic;\r\n    using UnityEngine;\r\n\r\n    public class Ball : MonoBehaviour\r\n    {\r\n        public int forceFactor = 300;\r\n\r\n        void Start()\r\n        {\r\n\r\n        }\r\n\r\n        void Update()\r\n        {\r\n\r\n        }\r\n\r\n        public void PushLeft()\r\n        {\r\n            Rigidbody2D rigidBody = GetComponent<Rigidbody2D>();\r\n            rigidBody.AddForce(Vector2.left * forceFactor);\r\n        }\r\n        public void PushRight()\r\n        {\r\n            Rigidbody2D rigidBody = GetComponent<Rigidbody2D>();\r\n            rigidBody.AddForce(Vector2.right * forceFactor);\r\n        }\r\n        public void PushUp()\r\n        {\r\n            Rigidbody2D rigidBody = GetComponent<Rigidbody2D>();\r\n            rigidBody.AddForce(Vector2.up * forceFactor);\r\n        }\r\n        public void PushDown()\r\n        {\r\n            Rigidbody2D rigidBody = GetComponent<Rigidbody2D>();\r\n            rigidBody.AddForce(Vector2.down * forceFactor);\r\n        }\r\n    }\r\n```\r\n\r\nThis script defines the class `Ball`, which inherits from Unity's `MonoBehavior` class. The class has one member variable, `forceFactor`, and defines\r\n4 methods that can be used to move the object, `PushLeft`, `PushRight`, `PushUp`, and `PushDown`. We will not use these methods yet, but when we\r\nimplement our Deepgram integration, we will trigger these methods when you say the words \"left,\" \"right,\" \"up,\" and \"down.\"\r\n\r\nSo now we have a \"Ball\" script and a \"Ball\" object, but they are not coupled yet! To attach the script to the object, click the object in the \"Hierarchy\"\r\ntab, and drag the \"Ball\" script to the \"Add Component\" button found at the bottom of the \"Inspector\" tab.\r\n\r\n## Implementing the Deepgram Integration\r\n\r\nTo implement the Deepgram integration, we will need to create an object which handles microphone input and an object which handles the websocket\r\nconnection to Deepgram.\r\n\r\nLet's start with the websocket handler. Right-click in the \"Hierarchy\" tab and select \"Create Empty\" - name this object \"DeepgramObject.\" Now,\r\ncreate a new script and name it \"DeepgramInstance.\" Edit the script and add the following contents:\r\n\r\n```\r\n    using System;\r\n    using System.Collections;\r\n    using System.Collections.Generic;\r\n    using UnityEngine;\r\n    using UnityEditor;\r\n    using System.Text.RegularExpressions;\r\n\r\n    using NativeWebSocket;\r\n\r\n    [System.Serializable]\r\n    public class DeepgramResponse\r\n    {\r\n        public int[] channel_index;\r\n        public bool is_final;\r\n        public Channel channel;\r\n    }\r\n\r\n    [System.Serializable]\r\n    public class Channel\r\n    {\r\n        public Alternative[] alternatives;\r\n    }\r\n\r\n    [System.Serializable]\r\n    public class Alternative\r\n    {\r\n        public string transcript;\r\n    }\r\n\r\n    public class DeepgramInstance : MonoBehaviour\r\n    {\r\n        WebSocket websocket;\r\n\r\n        public Ball _ball;\r\n\r\n        async void Start()\r\n        {\r\n            var headers = new Dictionary<string, string>\r\n            {\r\n                { \"Authorization\", \"Token INSERT_YOUR_API_KEY\" }\r\n            };\r\n            websocket = new WebSocket(\"wss://api.deepgram.com/v1/listen?encoding=linear16&sample_rate=\" + AudioSettings.outputSampleRate.ToString(), headers);\r\n\r\n            websocket.OnOpen += () =>\r\n            {\r\n                Debug.Log(\"Connected to Deepgram!\");\r\n            };\r\n\r\n            websocket.OnError += (e) =>\r\n            {\r\n                Debug.Log(\"Error: \" + e);\r\n            };\r\n\r\n            websocket.OnClose += (e) =>\r\n            {\r\n                Debug.Log(\"Connection closed!\");\r\n            };\r\n\r\n            websocket.OnMessage += (bytes) =>\r\n            {\r\n                var message = System.Text.Encoding.UTF8.GetString(bytes);\r\n                Debug.Log(\"OnMessage: \" + message);\r\n\r\n                DeepgramResponse deepgramResponse = new DeepgramResponse();\r\n                object boxedDeepgramResponse = deepgramResponse;\r\n                EditorJsonUtility.FromJsonOverwrite(message, boxedDeepgramResponse);\r\n                deepgramResponse = (DeepgramResponse) boxedDeepgramResponse;\r\n                if (deepgramResponse.is_final)\r\n                {\r\n                    var transcript = deepgramResponse.channel.alternatives[0].transcript;\r\n                    Debug.Log(transcript);\r\n                    int leftCount = new Regex(Regex.Escape(\"left\")).Matches(transcript).Count;\r\n                    int rightCount = new Regex(Regex.Escape(\"right\")).Matches(transcript).Count;\r\n                    int upCount = new Regex(Regex.Escape(\"up\")).Matches(transcript).Count;\r\n                    int downCount = new Regex(Regex.Escape(\"down\")).Matches(transcript).Count;\r\n                    for (int i = 0; i < leftCount; i++)\r\n                    {\r\n                        _ball.PushLeft();\r\n                    }\r\n                    for (int i = 0; i < rightCount; i++)\r\n                    {\r\n                        _ball.PushRight();\r\n                    }\r\n                    for (int i = 0; i < upCount; i++)\r\n                    {\r\n                        _ball.PushUp();\r\n                    }\r\n                    for (int i = 0; i < downCount; i++)\r\n                    {\r\n                        _ball.PushDown();\r\n                    }\r\n                }\r\n            };\r\n\r\n            await websocket.Connect();\r\n        }\r\n        void Update()\r\n        {\r\n        #if !UNITY_WEBGL || UNITY_EDITOR\r\n            websocket.DispatchMessageQueue();\r\n        #endif\r\n        }\r\n\r\n        private async void OnApplicationQuit()\r\n        {\r\n            await websocket.Close();\r\n        }\r\n\r\n        public async void ProcessAudio(byte[] audio)\r\n        {\r\n            if (websocket.State == WebSocketState.Open)\r\n            {\r\n                await websocket.Send(audio);\r\n            }\r\n        }\r\n    }\r\n```\r\n\r\nInsert your Deepgram API key where the script says \"INSERT\\_YOUR\\_API\\_KEY,\" then\r\nattach this script to the \"DeepgramObject\", and in the \"Inspector\" tab click the \"Ball\" field and select the \"Ball\" object we created earlier.\r\n\r\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1647404524/blog/2022/03/deepgram-unity-tutorial/assets/add_ball_to_deepgram_object.png\" alt=\"Adding a Ball object reference to the DeepgramObject.\" style=\"max-width: 556px;display: block;margin-left: auto;margin-right: auto;\">\r\n\r\nOk, so what's going on here? Well, first the script defines the classes `DeepgramResponse`, `Channel`, and `Alternative` which we will use to deserialize\r\nthe Deepgram ASR response, which is in JSON format. Then the script defines the class `DeepgramInstace` which has two member variables: a `WebSocket` object,\r\ndefined by `NativeWebSocket`, and a `Ball` object, defined by us in the \"Ball\" script.\r\n\r\nWhen the object that this script is attached to gets created, the `Start` method gets called. Inside `Start`, we create a new websocket connection to Deepgram\r\nand define functions that need to get executed when that connection opens, closes, receives an error, and receives a message. When the websocket connection\r\nreceives a message, we first parse it as a string, and then use `EditorJsonUtility` to parse the string as a JSON object, deserializing it as a `DeepgramResponse`\r\nobject. We can then directly access the transcript contained in this Deepgram message, count how many times the words \"left,\" \"right,\" \"up,\" and \"down\" were spoken,\r\nand for each time these words were spoken, we call the `PushLeft`, `PushRight`, `PushUp`, and `PushDown` methods on our `Ball` object!\r\n\r\nNear the end of the script is one more method of note: `ProcessAudio`. This method will be called by our microphone object, which will pass in raw audio. `ProcessAudio`\r\nwill then check to see if the websocket connection is open, and if it is, pass the audio along to Deepgram.\r\n\r\nNow let's create an object to handle the microphone input. Right-click in the \"Hierarchy\" tab, select `Audio -> AudioSource`, and name this object \"MicrophoneObject.\"\r\nThen create a new script called \"MicrophoneInstance\" and make its contents the following:\r\n\r\n```\r\n    using System.Collections;\r\n    using System.Collections.Generic;\r\n    using UnityEngine;\r\n    using UnityEngine.Audio;\r\n\r\n    [RequireComponent (typeof (AudioSource))]\r\n    public class MicrophoneInstance : MonoBehaviour\r\n    {\r\n        AudioSource _audioSource;\r\n        int lastPosition, currentPosition;\r\n\r\n        public DeepgramInstance _deepgramInstance;\r\n\r\n        void Start()\r\n        {\r\n            _audioSource = GetComponent<AudioSource> ();\r\n            if (Microphone.devices.Length > 0)\r\n            {\r\n                _audioSource.clip = Microphone.Start(null, true, 10, AudioSettings.outputSampleRate);\r\n            }\r\n            else\r\n            {\r\n                Debug.Log(\"This will crash!\");\r\n            }\r\n\r\n            _audioSource.Play();\r\n        }\r\n\r\n        void Update()\r\n        {\r\n            if ((currentPosition = Microphone.GetPosition(null)) > 0)\r\n            {\r\n                if (lastPosition > currentPosition)\r\n                    lastPosition = 0;\r\n\r\n                if (currentPosition - lastPosition > 0)\r\n                {\r\n                    float[] samples = new float[(currentPosition - lastPosition) * _audioSource.clip.channels];\r\n                    _audioSource.clip.GetData(samples, lastPosition);\r\n\r\n                    short[] samplesAsShorts = new short[(currentPosition - lastPosition) * _audioSource.clip.channels];\r\n                    for (int i = 0; i < samples.Length; i++)\r\n                    {\r\n                        samplesAsShorts[i] = f32_to_i16(samples[i]);\r\n                    }\r\n\r\n                    var samplesAsBytes = new byte[samplesAsShorts.Length * 2];\r\n                    System.Buffer.BlockCopy(samplesAsShorts, 0, samplesAsBytes, 0, samplesAsBytes.Length);\r\n                    _deepgramInstance.ProcessAudio(samplesAsBytes);\r\n\r\n                    if (!GetComponent<AudioSource>().isPlaying)\r\n                        GetComponent<AudioSource>().Play();\r\n                    lastPosition = currentPosition;\r\n                }\r\n            }\r\n        }\r\n\r\n        short f32_to_i16(float sample)\r\n        {\r\n            sample = sample * 32768;\r\n            if (sample > 32767)\r\n            {\r\n                return 32767;\r\n            }\r\n            if (sample < -32768)\r\n            {\r\n                return -32768;\r\n            }\r\n            return (short) sample;\r\n        }\r\n    }\r\n```\r\n\r\nAttach this script to the \"MicrophoneObject\", and in the \"Inspector\" tab click the \"DeepgramInstance\" field and select the \"DeepgramObject\" object we created earlier.\r\n\r\nIn this script, we define the `MicrophoneInstance` class which contains an `AudioSource` member variable, a `DeepgramInstance` member variable, and 2 integer member\r\nvariables which help to keep track of where we are in the microphone's audio stream. The `Start` method will set up the microphone to stream audio data into\r\nthe `clip` of the `AudioSource` object, and will start playback of the `AudioSource`.\r\n\r\nThe `Update` method of a `MonoBehavior` class gets called by the under-the-hood game loop every frame and is the typical place to handle game logic.\r\nIn our case, it gets the current position of the microphone's audio stream, compares it to the last position of the microphone's audio stream to\r\ncreate a buffer, `samples`, of floats the right size to store all of the new audio data since the last time `Update` was called, grabs that audio data via the method `GetData`\r\nand stores it in `samples`. Then, these `f32` samples are converted to `i16` samples, and then converted to raw bytes, and finally passed to the `DeepgramInstance`'s `ProcessAudio`\r\nmethod which, as we mentioned before, will then pass that audio on to Deepgram to get transcribed!\r\n\r\nNow, we are *almost* ready to try out our demo. There is one more task to do to make the demo reasonably playable, and if you try to play the demo now, you may see what it is!\r\nCurrently, in order to get the microphone's audio data, the microphone's audio stream must play. However, having the microphone's audio play through your computer's speakers\r\ncan be problematic and lead to feedback issues (plus, hearing your voice in the game isn't the point). To fix this issue, go to the \"Assets\" panel, right-click,\r\nand select `Create -> Audio Mixer`. Name the mixer \"Mixer.\"\r\n\r\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1647377747/blog/2022/03/deepgram-unity-tutorial/assets/add_audio_mixer.png\" alt=\"Add a Mixer.\" style=\"max-width: 1176px;display: block;margin-left: auto;margin-right: auto;\">\r\n\r\nDouble click \"Mixer\" in the \"Assets\" panel, then next to \"Groups\" click the \"+\" button and name the group \"Microphone.\"\r\nNow go to the fader for this group and turn it all the way down.\r\n\r\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1647377746/blog/2022/03/deepgram-unity-tutorial/assets/turn_fader_down.png\" alt=\"Silence the Microphone group.\" style=\"max-width: 1090px;display: block;margin-left: auto;margin-right: auto;\">\r\n\r\nNow, click the \"MicrophoneObject\" in the \"Hierarchy\" tab and then click on the field for \"Output\" in the \"Audio Source\" node in the \"Inspector\" tab and select \"Microphone (Mixer).\"\r\nThis will ensure the microphone audio can stream its data without being directed to your speakers!\r\n\r\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1647377746/blog/2022/03/deepgram-unity-tutorial/assets/send_audio_source_to_mixer_group.png\" alt=\"Silence the Microphone group.\" style=\"max-width: 568px;display: block;margin-left: auto;margin-right: auto;\">\r\n\r\nYou should now be able to press \"Play\" (►) and make the ball jump around by saying \"left,\" \"right,\" \"up,\" or \"down\" in the microphone!\r\n\r\n## Build New Features\r\n\r\nControlling a single ball with commands has a noticeable amount of latency. This can be alleviated somewhat\r\nby using [interim results](https://developers.deepgram.com/documentation/features/interim-results/), however,\r\nthe same transcribed word might be present in subsequent interim results, so logic would have to be added\r\nto avoid double counting commands. Still, the use of interim results can vastly reduce latency, so I strongly\r\nsuggest trying it out! In addition, here are a few more things to think about and try out with this demo before diving into\r\na full-on speech-enhanced game:\r\n\r\n*   Change the [Physics Material](https://docs.unity3d.com/Manual/class-PhysicMaterial.html) of the ball to make it bounce instead of stick to walls.\r\n*   Add more balls to the box - try making just one of them, several of them, or all of them react to the speech commands.\r\n*   Give the balls different colors, and implement logic to control each group (like \"red, left!\", \"blue, up!\").\r\n*   Remove gravity for balls and exchange the simple box with a sprawling level of walls.\r\n\r\nIf you try out the above ideas, you may come to a fun idea for a game - how about a game along the lines of Pikmin, where you command\r\ndifferent groups of creatures to move to different parts of the map to accomplish objectives? Instead of using a complex user interface\r\nof buttons, mice, and/or a keyboard, the game could simply require you to dictate commands!\r\n\r\n## Final Thoughts\r\n\r\nSpeech-enhanced games are not necessarily new, but until recently they have mostly centered around a handful of command words.\r\nWith today's ASR engines like Deepgram, supporting thousands of command words out of the box has become trivial, indeed transcribing entire\r\nconversations and complex commands is now easily within reach and has the power to enrich games. I'll end with a few ideas which\r\nmay give you some inspiration!\r\n\r\n*   Along the lines of the example mentioned in the previous section, build an RTS where you control units with your voice. There may be more\r\n    latency than you get with a mouse click, but this could be turned into a core game mechanic - maybe you are communicating with your units\r\n    via a radio, and any network + transcription latency would feel like latency of your units receiving your message.\r\n*   Also following up on this idea, make a game where you primarily control a single player, but have the option to give commands to other\r\n    AI players on your team - these commands could be conveyed with your voice without having to swap to a complex menu, interrupting the control of\r\n    your primary character. (For a concrete example, think Hyrule Warriors.)\r\n*   For MOBAs where voice chat is not easily available (think Pokémon Unite), allow for command phrases to be selected via ASR instead of clunky menus.\r\n*   To avoid issues with latency entirely, make speech-enhanced sections of your game occur during context-sensitive situations where the main action\r\n    of the game pauses so that reaction time becomes a non-issue. As an example, in many Zelda games, the game freezes when you pull out an instrument,\r\n    and playing the right notes on the instrument can cause events to occur - imagine this but with ASR and specific phrases.\r\n*   For in-game voice chat, write a plugin that performs ASR for each player in their preferred\r\n    language (see Deepgram's supported languages [here](https://developers.deepgram.com/documentation/features/language/), and then passes\r\n    the resulting transcripts through a translator to display everyone's speech to everyone else in their preferred language.\r\n\r\nSpeech-in-games is a relatively untapped area, so this list of ideas and suggestions is far from exhaustive, but I sincerely hope this helps\r\non the journey towards making games more immersive, interesting, and inclusive!\r\n\r\nIf you have any questions, please feel free to reach out on Twitter - we're @DeepgramDevs.\r\n\r\n        ";
						}
						async function compiledContent$32() {
							return load$32().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$32() {
							return (await import('./chunks/index.36620639.mjs'));
						}
						function Content$32(...args) {
							return load$32().then((m) => m.default(...args));
						}
						Content$32.isAstroComponentFactory = true;
						function getHeadings$32() {
							return load$32().then((m) => m.metadata.headings);
						}
						function getHeaders$32() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$32().then((m) => m.metadata.headings);
						}

const __vite_glob_0_82 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$32,
  file: file$32,
  url: url$32,
  rawContent: rawContent$32,
  compiledContent: compiledContent$32,
  default: load$32,
  Content: Content$32,
  getHeadings: getHeadings$32,
  getHeaders: getHeaders$32
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$31 = {"title":"5 Reasons Amazon and Google are Losing Customers to Deepgram","description":"Amazon and Google might be your first thought for ASR, but theyre not the best game in town. Learn why people are switching to Deepgram.","date":"2022-01-18T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981404/blog/deepgram-versus-amazon-google/5-reasons-amazon-google-losing-customers-thumb-554.png","authors":["chris-doty"],"category":"dg-insider","tags":["deep-learning","education"],"seo":{"title":"5 Reasons Amazon and Google are Losing Customers to Deepgram","description":"Amazon and Google might be your first thought for ASR, but theyre not the best game in town. Learn why people are switching to Deepgram."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981404/blog/deepgram-versus-amazon-google/5-reasons-amazon-google-losing-customers-thumb-554.png"},"shorturls":{"share":"https://dpgr.am/ab57f9a","twitter":"https://dpgr.am/9c8b0b6","linkedin":"https://dpgr.am/30f38c0","reddit":"https://dpgr.am/e1c5629","facebook":"https://dpgr.am/409db12"}};
						const file$31 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-versus-amazon-google/index.md";
						const url$31 = undefined;
						function rawContent$31() {
							return "[Amazon](https://offers.deepgram.com/head-to-head-dg-vs-amazon-webinar-on-demand) and [Google](https://offers.deepgram.com/head-to-head-dg-vs-google-webinar-on-demand) are often people's first thought when it comes to speech recognition systems, but if you've ever tried to use these [ASR](https://blog.deepgram.com/what-is-asr/) tools, you know that the big name doesn't help you get accurate, timely results. In fact, people's first encounter with Big Tech speech recognition often leaves them wanting more, and sends them searching for a better solution to power their voice-driven future. According to an upcoming survey conducted by Opus Research, **only 1% of respondents didn't think that voice-enabled experiences were going to be a critical part of their company's future enterprise strategy.** That means that you need fast, accurate ASR that's easy to use if you're going to keep pace with your peers and competitors.\n\n## Top 5 Reasons to Switch to Deepgram\n\nSo what are five of the top reasons that companies are switching from Big Tech to Deepgram for their automated speech recognition needs? And why are they [delighted with Deepgram](https://blog.deepgram.com/deepgram-g2-review-winter-2022/) when they've made the switch? Let's take a look.\n\n### 1. Speed\n\nReal talk: Amazon and Google are slow. Their legacy way of doing speech recognition means that incoming audio has to go through multiple stages during processing, which in turn means that any real-time applications like interactive chatbots or truly real-time captioning is out of the question. Deepgram's end-to-end, hardware-accelerated deep learning ASR system, on the other hand, returns results in a fraction of the time-up to 60x faster than Big Tech-even while processing multiple audio streams.\n\n### 2. Accuracy\n\nMost speech recognition systems are \"good enough\"-you can listen to some audio, look at a transcript of it, and figure out how the ASR system got from A to B. But if you've ever used a device or product from Big Tech that relies on speech recognition to turn on your lights or get directions, you know that good enough often isn't actually good enough. For critical business functions like customer service, sales, and operations, you can end up giving yourself more work instead of less if you're constantly fighting with crummy transcripts. Deepgram's ASR system consistently outperforms Amazon, Google, and other Big Tech providers out of the box (all while also being faster, as noted above).\n\n### 3. Model Tailoring\n\nBut what if that out-of-the-box accuracy isn't good enough? What if it's still driving extra work while you try to figure out what to do with confusing transcriptions? If you're working with Big Tech, you're out of luck; the out-of-the-box accuracy is the only accuracy.  But with Deepgram, you can quickly and easily train a tailored model that understands the words you need it to, driving gains and accuracy and thus ROI. Have jargon-filled phone calls about widget manufacturing processes? No problem, we can tailor a model that knows those words as well as it does the rest of the English (or any of the other languages we offer).\n\n### 4. Implementation and Support Systems\n\nIf you encounter a problem while using one of Big Tech's solutions, hopefully it's something that's addressed by the documentation or by another user on Stack Overflow. If not, you'll quickly discover it's all but impossible to get any kind of customer support.  Not so with Deepgram-we want you to succeed, not just toss your money at us and make your API calls. We've got a full-fledged set of developer tools and a developer community to help you get the most out of your voice technology implementations. We also have live, expert humans who can help you optimize your voice technology. Developers have said that [signing up and connecting to our APIs](https://console.deepgram.com/) was the fastest and easiest process they've ever used.\n\n### 5. Cost Savings\n\nDid you know that both Amazon and Google charge in 15 second increments for their speech-to-text services? That might not sound like a big deal, but if you're processing a lot of audio-especially a lot of short requests-you're going to end up paying for a lot of time that you didn't even use. At Deepgram, we charge you for the length of your audio.You only pay for what you use, with no rounding up to pad our pockets.\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-how-deepgram-works\"></WhitepaperPromo>\n\n## Thinking About Making the Switch?\n\nIf you're curious about the details, you can see how Deepgram stacks up to Amazon Transcribe or Google Text-to-Speech with the [ASR Comparison Tool](https://deepgram.com/asr-comparison/). But the real proof is, as they say, in the pudding-or, in our case, the transcripts. If you want to see what it's like to switch to a solution that provides faster, more accurate transcriptions-no matter what your customers or employees are talking about-[contact us](https://deepgram.com/contact-us) to set up a time to chat, or, if you're a developer, get a [free API key](https://console.deepgram.com) and start seeing the difference that fast, accurate speech-to-text can have on your business.";
						}
						async function compiledContent$31() {
							return load$31().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$31() {
							return (await import('./chunks/index.069870be.mjs'));
						}
						function Content$31(...args) {
							return load$31().then((m) => m.default(...args));
						}
						Content$31.isAstroComponentFactory = true;
						function getHeadings$31() {
							return load$31().then((m) => m.metadata.headings);
						}
						function getHeaders$31() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$31().then((m) => m.metadata.headings);
						}

const __vite_glob_0_83 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$31,
  file: file$31,
  url: url$31,
  rawContent: rawContent$31,
  compiledContent: compiledContent$31,
  default: load$31,
  Content: Content$31,
  getHeadings: getHeadings$31,
  getHeaders: getHeaders$31
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$30 = {"title":"Speech Recognition with Vonage and Python","description":"Use Deepgram's speech-to-text features with Python and Vonage's API to transcribe real-time audio such as incoming phone calls.","date":"2022-05-09T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1651757346/blog/2022/05/deepgram-with-vonage/Speech-Analytics-Real-Time-Audio-w-Vonage-Python%402x.jpg","authors":["tonya-sims"],"category":"tutorial","tags":["python","vonage"],"seo":{"title":"Speech Recognition with Vonage and Python","description":"Use Deepgram's speech-to-text features with Python and Vonage's API to transcribe real-time audio such as incoming phone calls."},"shorturls":{"share":"https://dpgr.am/ef1d65f","twitter":"https://dpgr.am/f818563","linkedin":"https://dpgr.am/d1c4cf8","reddit":"https://dpgr.am/a051411","facebook":"https://dpgr.am/15a2bac"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454085/blog/deepgram-with-vonage/ograph.png"}};
						const file$30 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-with-vonage/index.md";
						const url$30 = undefined;
						function rawContent$30() {
							return "\r\nImagine having the ability to transcribe voice calls. Look no further because we’ll learn how to do that in this article by combining Vonage with Deepgram.\r\n\r\nWith Vonage, we can use one of their phone numbers to receive and record incoming calls and get a transcript using the Deepgram Speech Recognition API. We’ll use the Deepgram Python SDK in this example.\r\n\r\nHere’s a snapshot of what we’ll see in the browser after making the phone call and using Deepgram voice-to-text.\r\n\r\n![Deepgram voice-to-text with Vonage](https://res.cloudinary.com/deepgram/image/upload/v1651778132/blog/2022/05/deepgram-with-vonage/deepgram-vonage-transcript.png)\r\n\r\n## Getting Started\r\n\r\nBefore we start, it’s essential to generate a Deepgram API key to use in our project. We can go to the [Deepgram console](https://console.deepgram.com/signup?jump=keys). We'll make sure to copy it and keep it in a safe place, as we won’t be able to retrieve it again and will have to create a new one. In this tutorial, we’ll use Python 3.10, but Deepgram supports some earlier versions of Python.\r\n\r\nThen we'll make sure to go to [Vonage](https://dashboard.nexmo.com/sign-up?icid=tryitfree_api-developer-adp_nexmodashbdfreetrialsignup_nav#_ga=2.180701287.1184321093.1651679801-1860855498.1651679801) and sign up for an account. We’ll need to purchase a phone number with voice capabilities of type mobile.\r\n\r\nWe’ll also need two phones to make the outgoing call and another to receive a call.\r\n\r\nIn the project, we’ll use Ngrok, which provides a temporary URL that will act as the webhook in the application. Ngrok will forward requests to the application that is running locally. We can download it [here](https://ngrok.com/).\r\n\r\nNext, we'll make a directory anywhere we’d like.\r\n```bash\r\nmkdir deepgram-vonage\r\n```\r\nThen we'll change into that directory so we can start adding things to it.\r\n```bash\r\ncd deepgram-vonage\r\n```\r\nWe’ll also need to set up a virtual environment to hold the project and its dependencies. We can read more about those [here](https://blog.deepgram.com/python-virtual-environments/) and how to create one. It’s recommended in Python to use a virtual environment so the project can be installed inside a container rather than installing it system-wide.\r\n\r\nWe need to ensure the virtual environment is activated because we’ll install dependencies inside. If the virtual environment is named `venv`, then we'll need to activate it.\r\n```bash\r\nsource venv/bin/activate\r\n```\r\nWe'll install the dependencies for the project by running the below `pip` installs from the terminal inside the virtual environment.\r\n```bash\r\npip install deepgram-sdk\r\npip install vonage\r\npip install python-dotenv\r\npip install Flask\r\npip install 'flask[async]'\r\npip install pysondb\r\n```\r\nWe now can open up an editor and create a file called `deepgram-vonage-call.py`.\r\n\r\n## The Code\r\n\r\nNow to the fun part! Open the script called `deepgram-vonage-call.py` and add the following code to make sure the Flask application runs without errors:\r\n\r\n```python\r\nfrom flask import Flask\r\n\r\napp = Flask(__name__)\r\n\r\n@app.get(\"/\")\r\ndef hello():\r\n    return \"Hello World!\"\r\n\r\nif __name__ == \"__main__\":\r\n    app.run(port=5000)\r\n```\r\n\r\nWe'll run the Flask application by typing this into the terminal `python deepgram-vonage-call.py`.\r\n\r\nThen we'll pull up the browser window by going to `http://127.0.0.1:5000/` and we should see the text `Hello World`.\r\n\r\nAt the same time the application is running, we'll open a new terminal window and type:\r\n```bash\r\nngrok http 127.0.0.1:5000\r\n```\r\nHere's a snapshot of the terminal running with ngrok:\r\n\r\n![ngrok terminal with python flask](https://res.cloudinary.com/deepgram/image/upload/v1651757357/blog/2022/05/deepgram-with-vonage/ngrok-terminal-with-python-flask-deepgram.png)\r\n\r\nWe'll create a Vonage application in the Vonage API Dashboard by going to `Applications -> Create a new application`.\r\n\r\nWe'll give the application a friendly name that's meaningful and easy to remember. We'll call it `Deepgram Vonage`.\r\n\r\nWe'll also need to generate a private key by clicking the button `Generate public and private key`. Add\r\nthe private key to the same level directory as the `python deepgram-vonage-call.py` file.\r\n\r\nNext, under the section `Capabilities` toggle on the `Voice` option. We'll add the following webhooks, with the ngrok url\r\nand endpoints to the `Answer URL` and the `Event URL`. Please note that everyone has a different ngrok url.\r\n\r\n```\r\nhttp://a11f-104-6-9-133.ngrok.io/webhooks/answer\r\nhttp://a11f-104-6-9-133.ngrok.io/webhooks/event\r\n\r\n```\r\n\r\n![vonage webhooks](https://res.cloudinary.com/deepgram/image/upload/v1651757357/blog/2022/05/deepgram-with-vonage/vonage-answer-event-webhooks.png)\r\n\r\nWe’ll implement the endpoints in a few.\r\n\r\nLeave both terminals running as those are needed to run the application and receive the phone call.\r\n\r\nThen we'll store the environment variables in a `.env` file with the following:\r\n```bash\r\nDEEPGRAM_API_KEY=['DEEPGRAM_API_KEY']\r\nVONAGE_NUMBER=['VONAGE_NUMBER']\r\nRECIPIENT_NUMBER=['RECIPIENT_NUMBER']\r\nVONAGE_APPLICATION_ID=['VONAGE_APPLICATION_ID']\r\nVONAGE_APPLICATION_PRIVATE_KEY_PATH=['PATH_TO_PRIVATE_KEY']\r\n```\r\nReplace `DEEPGRAM_API_KEY` with the API key we received from signing up in the Deepgram console, and the `RECIPIENT_NUMBER` is the phone number we would like to receive the call.\r\n\r\nWe'll replace the code in `deepgram-vonage-call.py` with the following:\r\n\r\n```python\r\nimport json\r\nimport os\r\nimport vonage\r\nfrom flask import Flask, request, jsonify, render_template\r\nfrom deepgram import Deepgram\r\nfrom pysondb import db\r\nfrom dotenv import load_dotenv\r\n\r\nload_dotenv()\r\n\r\napp = Flask(__name__)\r\n\r\ncalls_db=db.getDb('calls.json')\r\n\r\nRECIPIENT_NUMBER = os.getenv(\"RECIPIENT_NUMBER\")\r\nVONAGE_NUMBER = os.getenv(\"VONAGE_NUMBER\")\r\nVONAGE_APPLICATION_ID = os.getenv(\"VONAGE_APPLICATION_ID\")\r\nVONAGE_APPLICATION_PRIVATE_KEY_PATH = os.getenv(\"VONAGE_APPLICATION_PRIVATE_KEY_PATH\")\r\nDEEPGRAM_API_KEY = os.getenv(\"DEEPGRAM_API_KEY\")\r\n\r\nclient = vonage.Client(\r\n    application_id=VONAGE_APPLICATION_ID,\r\n    private_key=VONAGE_APPLICATION_PRIVATE_KEY_PATH,\r\n)\r\n\r\n@app.get(\"/webhooks/answer\")\r\ndef answer_call():\r\n    ncco = [\r\n        {\r\n            \"action\": \"talk\",\r\n            \"text\": \"Hi, we will shortly forward your call. This call is recorded for quality assurance purposes.\"\r\n        },\r\n        {\r\n            \"action\": \"record\",\r\n            \"eventUrl\": [\"http://a11f-104-6-9-133.ngrok.io/webhooks/recordings\"]\r\n        },\r\n        {\r\n            \"action\": \"connect\",\r\n            \"eventUrl\": [\"http://a11f-104-6-9-133.ngrok.io/webhooks/event\"],\r\n            \"from\": VONAGE_NUMBER,\r\n            \"endpoint\": [\r\n                {\r\n                    \"type\": \"phone\",\r\n                    \"number\": RECIPIENT_NUMBER\r\n                }\r\n            ]\r\n        }\r\n    ]\r\n    return jsonify(ncco)\r\n```\r\n\r\nHere we are importing the libraries and creating a new instance of a Flask application. Then we create a new database named `calls`. We are using a lightweight JSON database called [PysonDB](https://dev.to/fredysomy/pysondb-a-json-based-lightweight-database-for-python-ija).\r\n\r\nWe create the `/webhooks/answer` endpoint, which allows us to make a voice call, connect to the Vonage number and record the call.\r\n\r\nNext, in the `/webhooks/recordings` route below, we tap into Deepgram’s speech-to-text feature by getting the recording of the call and using speech recognition to transcribe the audio. We check if `results` is in the response and format it by using a list comprehension and storing the results in `utterances`. We then add the `utterances` to the `calls` database. We return an empty dictionary in the `/webhooks/event` endpoint.\r\n\r\n```python\r\n@app.post(\"/webhooks/recordings\")\r\nasync def recordings():\r\n    print(\"recordings endpoint\")\r\n    deepgram = Deepgram(DEEPGRAM_API_KEY)\r\n    data = request.get_json()\r\n\r\n    response = client.get_recording(data['recording_url'])\r\n\r\n    source = {'buffer': response, 'mimetype': 'audio/mp3'}\r\n    transcript_data = await deepgram.transcription.prerecorded(source, {'punctuate': True,\r\n    'utterances': True,\r\n       'model': 'phonecall',\r\n        'multichannel': True \r\n})\r\n\r\n    if 'results' in transcript_data:\r\n        utterances = [\r\n            {\r\n                'channel': utterance['channel'],\r\n                'transcript': utterance['transcript']\r\n            } for utterance in transcript_data['results']['utterances']\r\n        ]\r\n\r\n        calls_db.addMany(utterances)\r\n\r\n        return json.dumps(utterances, indent=4)\r\n\r\n    return \"webhook received\"\r\n\r\n\r\n@app.get(\"/webhooks/event\")\r\ndef on_event():\r\n    return jsonify({})\r\n```\r\n\r\nWe can see how the utterances will look after they’re formatted:\r\n```\r\n    [{'channel': 0, 'transcript': 'Hello?', 'id': 288397603074461838},\r\n    {'channel': 1, 'transcript': 'Hey. How's it going?', 'id': 109089630999017748},\r\n    {'channel': 0, 'transcript': 'Thank you for using Deepgram.', 'id': 124620676610936565},\r\n    {'channel': 1, 'transcript': 'Have a good day.', 'id': 182036969834868158}]\r\n```\r\nLastly, we'll add the `/transcribe` route and a templates folder with an `index.html` file that will display the phone speech-to-text transcript.\r\n\r\nIn the Python file, we'll add the following code to get the voice-to-text transcript from the database and render them in the HTML template.\r\n\r\n```python\r\n@app.get(\"/transcribe\")\r\ndef transcribe_call():\r\n    context = calls_db.getAll()\r\n    return render_template(\"index.html\", context=context )\r\n\r\nif __name__ == '__main__':\r\n    app.run(port=5000)\r\n```\r\n\r\nWe'll create a folder in the project directory called `templates` and add an `index.html` file. In that file, we'll add the following HTML and Jinja code:\r\n\r\n```html\r\n<!DOCTYPE html>\r\n<html lang=\"en\">\r\n  <head>\r\n    <meta charset=\"UTF-8\" />\r\n    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\" />\r\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\r\n    <title>Transcript</title>\r\n  </head>\r\n  <body>\r\n    {% for c in context %} {{ c.transcript }} <br />\r\n    {% endfor %}\r\n  </body>\r\n</html>\r\n```\r\n\r\nThen we'll loop through every transcript and display it on the screen.\r\n\r\nFinally, we'll try making a phone call and using the non-Vonage phone to initiate a phone conversation with the phone number we provided in the environment variable `VONAGE_NUMBER`. We should be able to receive a call and engage in a conversation. After we hang up, the transcript will appear in the browser when we navigate to `http://127.0.0.1:5000/transcribe`.\r\n\r\nCongratulations on building a speech-to-text Python project with Vonage and Deepgram! If you have any questions, please feel free to reach out to us on Twitter at [@DeepgramDevs](https://twitter.com/DeepgramDevs).\r\n\r\n        ";
						}
						async function compiledContent$30() {
							return load$30().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$30() {
							return (await import('./chunks/index.91b032e9.mjs'));
						}
						function Content$30(...args) {
							return load$30().then((m) => m.default(...args));
						}
						Content$30.isAstroComponentFactory = true;
						function getHeadings$30() {
							return load$30().then((m) => m.metadata.headings);
						}
						function getHeaders$30() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$30().then((m) => m.metadata.headings);
						}

const __vite_glob_0_84 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$30,
  file: file$30,
  url: url$30,
  rawContent: rawContent$30,
  compiledContent: compiledContent$30,
  default: load$30,
  Content: Content$30,
  getHeadings: getHeadings$30,
  getHeaders: getHeaders$30
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2$ = {"title":"Why Deepgram's Speech-to-Text API is #1 for Developers on G2","description":"Take a look at some of the reasons that developers love using Deepgram's speech-to-text API for their speech recognition needs.","date":"2022-07-11T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981426/blog/deepgrams-speech-to-text-api-number-1-for-developers-g2/G2-summer-2022-report-blog-thumb-554x220%402x.png","authors":["alexa-de-la-torre"],"category":"dg-insider","tags":["deep-learning","nlp"],"seo":{"title":"Why Deepgrams Speech-to-Text API is #1 for Developers on G2","description":"Take a look at some of the reasons that developers love using Deepgrams speech-to-text API for their speech recognition needs."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981426/blog/deepgrams-speech-to-text-api-number-1-for-developers-g2/G2-summer-2022-report-blog-thumb-554x220%402x.png"},"shorturls":{"share":"https://dpgr.am/ee8145c","twitter":"https://dpgr.am/c859c97","linkedin":"https://dpgr.am/59f280f","reddit":"https://dpgr.am/bb818c5","facebook":"https://dpgr.am/5144716"}};
						const file$2$ = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgrams-speech-to-text-api-number-1-for-developers-g2/index.md";
						const url$2$ = undefined;
						function rawContent$2$() {
							return "Automatic speech recognition (ASR) has become less of a \"nice to have\" and more of a requirement as accessibility and a positive user experience have become more core to customer loyalty. But the ways that ASR, especially in API form, can be integrated into an application or multiple applications are endless. This means that finding an API that checks all the boxes-accuracy, speed, latency for real-time, deployment models for cloud and on-prem, and scalability-and has great documentation and support are key to success. We're happy to say that Deepgram has been reviewed and recognized as [the leader](https://www.g2.com/categories/voice-recognition?tab=highest_rated) in the [G2 Summer Grid® Report](https://www.g2.com/reports/grid-report-for-voice-recognition-summer-2022?secure%5Bpid%5D=77169&secure%5Brid%5D=2999985&secure%5Btoken%5D=68581f65247c290b4e9b0f8cb3a3d262be18bcc421e819e448c5579b93cda711) for Voice Recognition software by checking all those boxes and then some! Here we compiled some of the reasons that developers said make Deepgram the best automatic speech-to-text API available.\n\n## Highly Accurate Speech-to-Text Helps Solve Real-World Problems\n\nThe following are just a few of the use cases developers mentioned they were using the Deepgram API for:\n\n1. **Real-time transcription for call analytics:** Quick identification of mandatory or banned keywords\n2. **Audio file and podcast transcription:** Fast turnaround for compliance and service value-add\n3. **Building conversational voice bots:** Low latency for shorter response times\n4. **Real-time live stream transcription and captioning:** Broader accessibility for hearing-impaired viewers\n5. **Online classroom lecture transcription and meeting summarization:** Speaker ID and building action summarization for easy review\n\n## Top Reasons Developers Love Deepgram\n\nWhen reviewing Deepgram, our users were asked what they liked most about the product. These are several things they mentioned that had a positive impact on their experience.\n\n### **1. Ease** **of Use**\n\nOur easy-to-use API makes generating your first transcript a breeze (get a free API key, copy your sample script of choice and get your first transcript in less than 10 minutes). It also includes all the [features](https://developers.deepgram.com/documentation/features/) necessary for building amazing voice experiences ranging from diarization and multichannel to punctuation, redaction, utterances, and more.\n\n> **\"Very easy to use speech to text api, setup in minutes and great results.\"** — \"This was one of the easiest APIs that I ever used. There were examples on Deepgram that worked on the first try. I was testing the API within minutes of discovering it. The speech-to-text results were also accurate, comparable, or better than other APIs I was testing. There was a [$150 free credit](https://console.deepgram.com/signup) for users which allowed me to test the APIs without commitment.\"\n>\n> \"**The Speech Recognition API for 99% of Projects.\"** — \"It's ridiculously fast to get set up and going. As in - you sign up, write 5 lines of code, and you're done - kind of fast. And it just works! Its accuracy is really good, it's fast, and it has some fancy extra features to top it all off. And if you have some specialized audio type that most recognition services perform poorly on, they've got you covered too. And for the developers out there, the docs are seriously great.\"\n\n### **2. Accuracy**\n\nThe proprietary architecture of Deepgram's out-of-the-box deep learning speech models has enabled customers to achieve 90%+ transcription accuracy. Self-service customers can easily get started with Enhanced and Base [models](https://developers.deepgram.com/documentation/features/model/). Otherwise, if your use case requires transcribing unique words, industry jargon, or other specifics, we can train a model to learn your language, accents, or terminology in just a few weeks.\n\n> \"**Extremely Accurate Transcription API, and Developer Friendly Python SDK.\"** — \"We have tested a number of transcription APIs, and Deepgram has consistently come out as the most accurate for our use case whilst offering a nice Python interface for batch operations. The API schemas are also excellent.\"\n\n### **3. Documentation**\n\nWe are on a mission to help developers implement AI-enabled speech recognition into their products more easily. This starts with user-friendly [documentation](https://developers.deepgram.com/documentation/) where users can easily reference how to build with the Deepgram API. Here are a few examples of what developers had to say about it:\n\n> **\"An Automated Speech API with Intuitive Documentation**.\" — \"My favorite part about using Deepgram was the ease of learning. The API documentation is complete and intuitive, and the tutorials in the console left me feeling confident that I could use the API and [SDK in either Node or Python](https://developers.deepgram.com/sdks-tools/) projects.\"\n>\n> **\"API: Good Product, Good Documentation, Great Support.\"** — \"The Deepgram API covers the languages we need (and then some), integrates easily with our audio source, is accurate enough, and delivers results quickly. The documentation made it easy to design our code, and the very helpful support engineers were quick to respond to questions and to help us debug our initial efforts.\"\n\n### **4. Speed**\n\nDeepgram provides the fastest transcription on the market, with a 120x real-time speed for batch processing (i.e., transcribe one hour of audio in 30 seconds), and has less than a 300 millisecond lag on real-time streaming. Use cases where real-time streaming can be particularly useful include Conversational AI, sales and support agent enablement, and real-time compliance monitoring to name a few.\n\n> **\"Great speech-to-text results in seconds.\"** — \"As a software developer, there is plenty to like about Deepgram - complete and easy to follow documentation; easy-to-use API that allows for quick language-independent implementation; great follow-up support; multiple models including one specifically for telephone-based dictation; not only one of the best but also one of the least expensive speech rec services available; a generous free number of credits are provided at sign-up - plenty enough for experimentation and testing of your application.\"\n>\n> **\"The fastest Speech to Text service I've ever used!\"** \"The low latency of the response with high accuracy from the websocket connection is the most distinguishing feature from other providers. If this feature was not there then it's yet another Speech to Text service. I really love the community around it and the team which is driving it, kudos to the DevRel team.\"\n\nWe would like to thank our amazing developer community. The honest feedback we have received has allowed us to continue to improve our product to better serve their needs. As a result, Deepgram continues to [rank as the #1 solution](https://www.g2.com/categories/voice-recognition?tab=highest_rated) on G2 for the second consecutive quarter. Most notably, in [G2 Summer Grid® Report](https://www.g2.com/reports/grid-report-for-voice-recognition-summer-2022?secure%5Bpid%5D=77169&secure%5Brid%5D=2999985&secure%5Btoken%5D=68581f65247c290b4e9b0f8cb3a3d262be18bcc421e819e448c5579b93cda711), Deepgram received a 96 satisfaction rating and scored above the average across ease of use (90%), ease of set up (89%), quality of support (92%), and more.\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-how-deepgram-works\"></WhitepaperPromo>\n\n## Only the Beginning\n\nDeepgram is on a mission to become *the* speech company and help our customers solve their problems with better speech-to-text. A great example of this is the latest release of the [Enhanced Model](https://deepgram.com/changelog/introducing-new-enhanced-model/), our newest and most powerful ASR model to date. Based on our next-generation deep learning speech model and architecture, this new model has significantly higher accuracy and better word recognition (19% more accurate compared to our previous model). It also has increased effective vocabulary and can handle long-tail vocabulary (uncommon words) significantly better. In the last few months, we also added a [new suite of languages](https://siliconangle.com/2022/03/29/exclusive-quotes-tkdeepgram-adds-23-new-languages-dialects-voice-recognition-engine/) in an effort to deliver a global language experience for our customers. If you are thinking of building your next project with Deepgram, you can [sign up here](https://console.deepgram.com/signup) or check out our [quickstart guides](https://developers.deepgram.com/documentation/getting-started/). Or, if want to keep exploring, check out the latest [tutorials](https://blog.deepgram.com/) or [projects](https://blog.deepgram.com/categories/project-showcase/) from the Deepgram developer community for more inspiration. Happy building!";
						}
						async function compiledContent$2$() {
							return load$2$().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2$() {
							return (await import('./chunks/index.9b7b7102.mjs'));
						}
						function Content$2$(...args) {
							return load$2$().then((m) => m.default(...args));
						}
						Content$2$.isAstroComponentFactory = true;
						function getHeadings$2$() {
							return load$2$().then((m) => m.metadata.headings);
						}
						function getHeaders$2$() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2$().then((m) => m.metadata.headings);
						}

const __vite_glob_0_85 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2$,
  file: file$2$,
  url: url$2$,
  rawContent: rawContent$2$,
  compiledContent: compiledContent$2$,
  default: load$2$,
  Content: Content$2$,
  getHeadings: getHeadings$2$,
  getHeaders: getHeaders$2$
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2_ = {"title":"Democratizing Speech Analytics: Deepgram + Callbi","description":"Callbi has partnered with Deepgram to provide speech analytics in call centers with thousands of agents around the world. Learn more.","date":"2022-06-21T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981424/blog/democratizing-speech-analytics-deepgram-callbi/deepgram-callbi-thumb-554x220%402x.png","authors":["keith-lam"],"category":"dg-insider","tags":["contact-center"],"seo":{"title":"Democratizing Speech Analytics: Deepgram + Callbi","description":"Callbi has partnered with Deepgram to provide speech analytics in call centers with thousands of agents around the world. Learn more."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981424/blog/democratizing-speech-analytics-deepgram-callbi/deepgram-callbi-thumb-554x220%402x.png"},"shorturls":{"share":"https://dpgr.am/517f490","twitter":"https://dpgr.am/e544cbc","linkedin":"https://dpgr.am/95207e4","reddit":"https://dpgr.am/ab7519b","facebook":"https://dpgr.am/1487945"}};
						const file$2_ = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/democratizing-speech-analytics-deepgram-callbi/index.md";
						const url$2_ = undefined;
						function rawContent$2_() {
							return "[Callbi](https://callbi.io/), based in South Africa, has expanded the availability of speech analytics by offering an economical speech analytics solution for contact centers with from twenty agents to thousands of agents. Now they are expanding to the UK, US, Western Europe, and Australia, powered by Deepgram's highly accurate automatic speech recognition solution. Callbi will be able to take advantage of our [24 languages and dialects](https://deepgram.com/product/languages/) and our continuous improvements in accuracy and [feature enhancements](https://deepgram.com/product/speech-to-text-api-overview/). For speech analytics, accuracy is the key. Inaccurate audio transcriptions mean poor insights and outcomes.\n\nDeepgram provides Callbi with the highest accuracy plus transcription speeds that allow contact centers to easily transcribe all their calls within the day for faster insights to make critical decisions. Cost has been a barrier to many smaller contact centers that want to implement speech analytics. With this partnership, we are lowering the cost of speech analytics, so large and small contact centers can achieve the benefits that were normally reserved for large enterprises. We both believe that access to new technology should be simple, fast, and easy. See how fast, accurate, and cost-effective, speech analytics can be with Callbi and Deepgram.\n\n## **About Callbi Speech Analytics**\n\nCallbi is a global, easy to use, cloud-based SaaS contact center speech analytics solution that dominates the South African market. Callbi is recorder agnostic and needs no API integration with call recorder platforms, enabling non-technical users to get started within hours rather than months. It is a low-cost, highly effective solution that enables contact centers to maximize revenue, improve efficiencies and effectiveness, to reduce costs and to mitigate business risks. Callbi is a member of [Alphawave Group](https://alphawave.co.za), with over 350 employees specializing in the development and deployment of leading-edge technologies used all over the world. Callbi is easy to deploy, easy to learn, and easy to drive fast ROI. For more information or to arrange a demonstration please visit https://callbi.io.\n\n## **About Deepgram** \n\nDeepgram is the leader in scalable speech recognition. With our easy-to-use API, developers easily convert audio to text and build experiences that increase revenues and maximize employee productivity. Unlike previous generations, Deepgram has taken an entirely new approach to speech recognition, ditching brittle methods-heuristics-based speech processing- for an end-to-end deep learning AI architecture. With this patented approach, users can access the industry's fastest, most accurate and highly scalable AI technology with a simple API call. Deepgram takes the heavy lifting out of noisy, multi-speaker, hard-to-understand audio transcriptions, so companies can focus on what they do best.\n\n<WhitepaperPromo whitepaper=\"latest\"></WhitepaperPromo>";
						}
						async function compiledContent$2_() {
							return load$2_().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2_() {
							return (await import('./chunks/index.49db92f1.mjs'));
						}
						function Content$2_(...args) {
							return load$2_().then((m) => m.default(...args));
						}
						Content$2_.isAstroComponentFactory = true;
						function getHeadings$2_() {
							return load$2_().then((m) => m.metadata.headings);
						}
						function getHeaders$2_() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2_().then((m) => m.metadata.headings);
						}

const __vite_glob_0_86 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2_,
  file: file$2_,
  url: url$2_,
  rawContent: rawContent$2_,
  compiledContent: compiledContent$2_,
  default: load$2_,
  Content: Content$2_,
  getHeadings: getHeadings$2_,
  getHeaders: getHeaders$2_
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2Z = {"title":"Detect Non-Inclusive Language with Retext and Node.js","description":"Learn how to use the retext natural language tool to identify non-inclusive language with JavaScript.","date":"2022-09-01T18:51:41.053Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661982184/blog/2022/09/uninclusive-language-retext/cover.jpg","authors":["kevin-lewis"],"category":"tutorial","tags":["javascript"],"shorturls":{"share":"https://dpgr.am/fabf193","twitter":"https://dpgr.am/c1cae38","linkedin":"https://dpgr.am/62941ab","reddit":"https://dpgr.am/549cebe","facebook":"https://dpgr.am/7d895a9"}};
						const file$2Z = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/detect-non-inclusive-language-with-retext-and-node-js/index.md";
						const url$2Z = undefined;
						function rawContent$2Z() {
							return "\nPersonal language usage is a journey of learning and adapting, which certainly extends to terms you may not yet realize are non-inclusive or potentially profane to others. By detecting and pointing out some potentially-problematic language, you can work towards being more considerate and kind to others.\n\n[alex](https://alexjs.com) is a lovely command-line tool that takes in text or markdown files and, using [`retext-equality`](https://github.com/retextjs/retext-equality/blob/main/rules.md) and [`retext-profanities`](https://github.com/retextjs/retext-profanities/blob/main/rules.md), highlights suggestions for improvement. alex checks for gendered work titles, gendered proverbs, ableist language, condescending or intolerant language, profanities, and much more.\n\nIn this short tutorial, we'll cover how to use the `retext` libraries `alex` depends on to check your Deepgram transcript [utterances](https://developers.deepgram.com/documentation/features/utterances/) for suggestions.\n\n## Before You Start\n\nBefore we start, you will need a Deepgram API Key - [get one here](https://console.deepgram.com/signup?jump=keys).\n\nCreate a new directory and open it in your code editor, navigate to it in your terminal, create a new `package.json` file by running `npm init -y`, and install dependencies:\n\n    npm install retext retext-equality retext-profanities vfile-reporter-json @deepgram/sdk\n\nThe `retext` packages require an ES6 project. The easiest way to do this without needing to compile code with a tool like Babel is to add the following property to your `package.json` file:\n\n    \"type\": \"module\"\n\nCreate and open an `index.js` file in your code editor.\n\n## Generating a Transcript With Deepgram\n\nThe Deepgram Node.js is a CommonJS module, but can be imported via the default export. Because of this, our import will go from this in CommonJS:\n\n```js\nconst { Deepgram } = require('@deepgram/sdk')\n```\n\nTo this in ES6 (`DG` can be anything as long as it's the same in both uses):\n\n```js\nimport DG from '@deepgram/sdk'\nconst { Deepgram } = DG\n```\n\nThen, generate a transcript. Here I am using a recording of my voice reading out the `alex` sample phrase for demonstration.\n\n```js\nconst deepgram = new Deepgram('YOUR_DEEPGRAM_API_KEY')\nconst url = 'http://lws.io/static/inconsiderate.mp3'\nconst { results } = await deepgram.transcription.preRecorded({ url }, { utterances: true })\nconsole.log(results)\n```\n\nAs the [utterances](https://developers.deepgram.com/documentation/features/utterances/) feature is being used, an array will be provided with each utterance (spoken phrase) along with when it was spoken.\n\nTest it out! Run the file with `node index.js`, and you should see a payload in your terminal. Once you know it works, remove the `console.log()`.\n\n## Setting Up the Language Checker\n\nAt the very top of `index.js`, include the dependencies required to set up `retext` and then report issues found from it:\n\n```js\nimport { reporterJson } from 'vfile-reporter-json'\nimport { retext } from 'retext'\nimport retextProfanities from 'retext-profanities'\nimport retextEquality from 'retext-equality'\n```\n\nThen, create the following reusable function:\n\n```js\nasync function checkText(text) {\n    const file = await retext()\n        .use(retextProfanities)\n        .use(retextEquality)\n        .process(text)\n    const outcome = JSON.parse(reporterJson(file))\n    const warnings = outcome[0].messages.map(r => r.reason)\n    return warnings\n}\n```\n\nThis function processes the provided text through the specified plugins (here, `retextProfanities` and `retextEquality`). The `outcome` is actually quite a large amount of data:\n\n```js\n{\n    reason: '`man` may be insensitive, use `people`, `persons`, `folks` instead',\n    line: 1,\n    column: 9,\n    position: {\n        start: { line: 1, column: 9, offset: 8 },\n        end: { line: 1, column: 12, offset: 11 }\n    },\n    ruleId: 'gals-man',\n    source: 'retext-equality',\n    fatal: false,\n    stack: null\n},\n```\n\nThe `warnings` map in the reusable `checkText` function extracts only the `reason` and returns an array of these strings. Try it out by temporarily adding this line:\n\n```js\nconst testSuggestions = await checkText('He is a butthead.')\nconsole.log(testSuggestions)\n```\n\nThe result should be:\n\n```js\n[\n    'Don’t use `butthead`, it’s profane',\n    '`He` may be insensitive, use `They`, `It` instead'\n]\n```\n\nOnce you know it works, remove the `console.log()`.\n\n## Checking Each Utterance's Language\n\nAdd the following to your `index.js` file below where you generate Deepgram transcripts:\n\n```js\nlet suggestions = []\n\nfor(let utterance of results.utterances) {\n    const { transcript, start } = utterance\n\n    // Get array of warning strings\n    let warnings = await checkText(transcript)\n\n    // Alter strings to be objects including the utterance transcript and start time\n    warnings = warnings.map(warning => ({ warning, transcript, start }))\n\n    // Append to end of array\n    suggestions = [...suggestions, ...warnings]\n}\n\nconsole.log(suggestions)\n```\n\nYour terminal should show all of the suggestions presented by the two `retext` plugins:\n\n![A terminal showing an array of objects. Each object is one suggestion including the original text, a warning, and the utterance start time.](https://res.cloudinary.com/deepgram/image/upload/v1660574296/blog/2022/09/uninclusive-language-retext/final-output.png)\n\n## Wrapping Up\n\nThis full snippet (below) is a great place to start identifying and changing usage and non-inclusive language patterns. You may quickly realize that the `retext` plugins lack nuance and sometimes make suggestions on false-positive matches. Don't consider the suggestions as \"must-dos\", but rather points for consideration and thought.\n\nThere's a whole host of [other `retext` plugins](https://github.com/retextjs/retext/blob/main/doc/plugins.md) which you can process text with, including those that handle assumptions, cliches, passive voice, repetition, overly-complex words, and more. Enjoy!\n\n```js\nimport { reporterJson } from 'vfile-reporter-json'\nimport { retext } from 'retext'\nimport retextProfanities from 'retext-profanities'\nimport retextEquality from 'retext-equality'\nimport DG from '@deepgram/sdk'\nconst { Deepgram } = DG\nconst deepgram = new Deepgram(process.env.DG_KEY)\n\nconst url = 'http://lws.io/static/inconsiderate.mp3'\nconst { results } = await deepgram.transcription.preRecorded({ url }, { utterances: true })\n\nasync function checkText(text) {\n    const file = await retext()\n        .use(retextProfanities)\n        .use(retextEquality)\n        .process(text)\n    const outcome = JSON.parse(reporterJson(file))\n    const warnings = outcome[0].messages.map(r => r.reason)\n    return warnings\n}\n\nlet suggestions = []\nfor(let utterance of results.utterances) {\n    const { transcript, start } = utterance\n    let warnings = await checkText(transcript)\n    warnings = warnings.map(warning => ({ warning, transcript, start }))\n    suggestions = [...suggestions, ...warnings]\n}\n\nconsole.log(suggestions)\n```\n\n";
						}
						async function compiledContent$2Z() {
							return load$2Z().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2Z() {
							return (await import('./chunks/index.78193d99.mjs'));
						}
						function Content$2Z(...args) {
							return load$2Z().then((m) => m.default(...args));
						}
						Content$2Z.isAstroComponentFactory = true;
						function getHeadings$2Z() {
							return load$2Z().then((m) => m.metadata.headings);
						}
						function getHeaders$2Z() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2Z().then((m) => m.metadata.headings);
						}

const __vite_glob_0_87 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2Z,
  file: file$2Z,
  url: url$2Z,
  rawContent: rawContent$2Z,
  compiledContent: compiledContent$2Z,
  default: load$2Z,
  Content: Content$2Z,
  getHeadings: getHeadings$2Z,
  getHeaders: getHeaders$2Z
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2Y = {"title":"Detecting and Reducing Bias in Speech Recognition","description":"Machine learning bias is top of mind for many people. This blog post will teach you how to identify ASR bias and what to do about it.","date":"2022-03-09T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981409/blog/detecting-and-reducing-bias-in-speech-recognition/Detecting-Reducing-Bias-in-Speech-thumb-554x220%402x.png","authors":["chris-doty"],"category":"ai-and-engineering","tags":["bias"],"seo":{"title":"Detecting and Reducing Bias in Speech Recognition","description":"Machine learning bias is top of mind for many people. This blog post will teach you how to identify ASR bias and what to do about it."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981409/blog/detecting-and-reducing-bias-in-speech-recognition/Detecting-Reducing-Bias-in-Speech-thumb-554x220%402x.png"},"shorturls":{"share":"https://dpgr.am/4911de4","twitter":"https://dpgr.am/d10214e","linkedin":"https://dpgr.am/a82d026","reddit":"https://dpgr.am/5852ee4","facebook":"https://dpgr.am/3da44b9"}};
						const file$2Y = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/detecting-and-reducing-bias-in-speech-recognition/index.md";
						const url$2Y = undefined;
						function rawContent$2Y() {
							return "Bias in machine learning has been a hot topic in the news lately, and bias in ASR is no exception. In the recent [State of Voice Technology 2022 report](https://deepgram.com/state-of-voice-technology-2022/), we saw that 92% of companies believe voice technology bias has an important impact on their customers, including in the domains of gender, race, age, and accent (both native and non-native speaker). So how do you detect bias in your automated speech recognition (ASR) systems? And what do you do if you find it? We've got answers below. But before we dive in, let's start with a definition of what \"bias\" is to make sure we're all on the same page.\n\n## Two Kinds of Bias\n\nIt's important to understand that when people talk about bias and machine learning, they might be talking about two different things. On the one hand, they might mean **real-world bias** related to things like race, gender, age, etc. But the term bias is also used in **machine learning** when a model is overpredicting or underpredicting the probability for a given category. That's true even if that category is something that we might not apply the term \"bias\" to in the real world. For example, a model developed by a brewery to track when its vats are at risk of overheating might sound the alarm frequently, even when there is no risk of overheating-a problem of machine learning bias, but not real-world bias.\n\nIn many cases, though, when people talk about \"machine learning bias\" generally-especially in the media-they're referring to the intersection of real world and machine learning bias: a case where a model is overpredicting or underpredicting in terms of something like gender or race. With these definitions out of the way, let's turn to examining where bias comes from before moving on to how you might figure out if you've got bias in your ASR system, as well as what your next steps are if you've found it.\n\n## Where Does Bias Come From?\n\nMachine learning bias often comes from data-biased data leads to a biased model. If you've taken a statistics course, you're familiar with **sampling bias,** which is when data that is supposed to represent the true range of possibilities is instead skewed in some way. If you use data like this to train a model, the model is going to be skewed in the same way as the data. This sampling bias can show up in your model as machine learning or real-world bias because many organizations who want to create models rely on their own past decisions. If your company has a history of making biased decisions, any model trained on that data will likewise be biased. \n\nFor example, if a company that trains a model to help make promotion decisions has a poor track record of promoting women, it's likely that their model will make the same kinds of biased decisions if it's trained on the company's past behavior. In some cases, however, a model might be biased towards a certain group of people for less nefarious reasons. For example, an initial ASR model might simply be biased because it was trained on speech that was easy for the creator to collect-from themself, their family, and their friends, who likely all speak similarly. Here, the issue isn't biased past decisions, per se, but simply a lack of data to cover different possibilities.\n\n## How to Detect Bias in ASR Systems\n\nSo how do you know if you've got bias on your ASR system? Detecting it is relatively easy on its face. For example, you might ask if your transcripts for some speakers have significantly higher [word error rates (WER)](https://blog.deepgram.com/what-is-word-error-rate/) than for other speakers. But even the answers to a simple question like this can sometimes be difficult to understand on their face. Language is infinitely varied, and even though we speak of, for example, \"American English\", this actually represents a wide range of linguistic variation, from Massachusetts to North Dakota to Alabama. And in other places around the world, the variation between dialects is sometimes even greater than what we see in the US.\n\nWhen you're thinking about how well your ASR system is performing, you're always going to encounter edge cases that it might have trouble understanding. For example, your model might encounter a non-native accent that it's never heard before-say, a native speaker of Polish who moved to South Africa at 16 and learned English there. To figure out if you've found bias in your ASR system, you need to be very nuanced in how you're looking at the data. For example, if you're finding a few sparse cases of poor transcripts in your ASR system, it could be that you're simply finding people with accents, like the Polish person mentioned above. In most cases, trying to chase down these edge cases and figure out how to fix them isn't going to be worth the time or effort, and isn't evidence that there's a systematic problem with your model.\n\nBut what if the bias that you've detected seems to be for an entire group of people? Maybe you notice that your call center in the Southern US has transcripts that are consistently of poorer quality than call centers in other parts of the country. Because this looks like a systematic problem, rather than a one-off problem, you'll probably want to take some kind of action to address this bias.\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-how-deepgram-works\"></WhitepaperPromo>\n\n## How to Reduce Bias in ASR Systems\n\nBecause of the wide-ranging variation in human speech, it's unrealistic to think that you're going to completely remove bias (in the machine learning sense) from ASR systems. Human beings, after all, sometimes have trouble understanding speakers of other dialects, or with non-native accents, and it's not realistic to expect that AI and ML is going to do better than this-at least, not yet. So the question becomes-when you find that your model is not doing a good job for a specific class of people, what do you do? **You can use that information to make your model better.** And you have two options for how to go about doing this.\n\n### Option 1: Add Data to the Model\n\nThe first option is to add more data to your model. In this case, you retrain with a wider range of sample data to also include the varieties of speech that you've discovered your model doesn't perform well on. This can work to improve the model for everyone, especially if the varieties are relatively similar.  However, in some cases, this can be a mistake. If we take the example above, if you're a company in the US, it's unlikely that adding data for Polish-accented South African English is a good choice, even if you're dealing with a community of speakers. That's because doing so might end up making your model perform *worse* for the majority of speakers you want it to handle. So what do you do if this is a population you want to cover, but you think adding data might reduce the model's performance overall? That's where the second option comes in.\n\n### Option 2: Create Distinct Models\n\nThe second option is to create a new model specifically for the newly identified area where the main model is struggling. For example, a bank that has a lot of customers in Guam might discover that Guamanian English is challenging for the general model, and that if you add Guamanian speakers to your training data, the model's performance suffers for speakers in North America. The solution in these cases is to have two distinct models. If you've got a call center in Guam, for example, you could just use a Guam-specific model in that call center only, while you let your more general model run elsewhere. The prevents data from different varieties degrading the quality of a model, while also making sure speech of all relevant groups is transcribed correctly.\n\n## Wrapping Up\n\nAs you can see, bias is far from straightforward, and there isn't a simple, one-size-fits all means of identifying bias in ASR systems and addressing it. It all depends on what you're seeing in your data, what populations you expect the models to work well on, and how similar the speech of those populations is to one another. But hopefully this post will serve as a guide for you as you're thinking through these issues. If you'd like to learn more about bias in speech recognition, and how it might even be able to help your ASR efforts, watch our on-demand webinar [When is Speech Recognition Bias a Good Thing?](https://offers.deepgram.com/when-is-speech-recognition-bias-a-good-thing-webinar-on-demand)";
						}
						async function compiledContent$2Y() {
							return load$2Y().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2Y() {
							return (await import('./chunks/index.07c01acc.mjs'));
						}
						function Content$2Y(...args) {
							return load$2Y().then((m) => m.default(...args));
						}
						Content$2Y.isAstroComponentFactory = true;
						function getHeadings$2Y() {
							return load$2Y().then((m) => m.metadata.headings);
						}
						function getHeaders$2Y() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2Y().then((m) => m.metadata.headings);
						}

const __vite_glob_0_88 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2Y,
  file: file$2Y,
  url: url$2Y,
  rawContent: rawContent$2Y,
  compiledContent: compiledContent$2Y,
  default: load$2Y,
  Content: Content$2Y,
  getHeadings: getHeadings$2Y,
  getHeaders: getHeaders$2Y
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2X = {"title":"Whats the Difference Between a Language and a Dialect?","description":"Whats a language, what's a dialect? And for that matter, what's an accent? We've got answers—sort of. It's complicated.","date":"2022-06-14T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981423/blog/difference-between-language-dialect/difference-between-Language-n-dialect-thumb-554x22.png","authors":["chris-doty"],"category":"linguistics","tags":["language"],"seo":{"title":"Whats the Difference Between a Language and a Dialect?","description":"Whats a language, what's a dialect? And for that matter, what's an accent? We've got answers—sort of. It's complicated."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981423/blog/difference-between-language-dialect/difference-between-Language-n-dialect-thumb-554x22.png"},"shorturls":{"share":"https://dpgr.am/678720d","twitter":"https://dpgr.am/ae1a193","linkedin":"https://dpgr.am/4f80ef0","reddit":"https://dpgr.am/87e6e72","facebook":"https://dpgr.am/5170d87"}};
						const file$2X = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/difference-between-language-dialect/index.md";
						const url$2X = undefined;
						function rawContent$2X() {
							return "There's a long-standing joke that a language is just a dialect with an army and a navy. Part of the reason this phrase has stuck around is because it's pithy, but it's also true in some cases. The language variety considered as a standard is typically that spoken by the leaders, or people who live in the capital (whether the actual political capital or a cultural capital). In the case of \"the Queen's English\", that's even part of the name of the particular variety.\n\nAlthough this might seem to address the issue of language versus dialect, it's actually quite a bit trickier to define these two terms and to group different language varieties accordingly. In this post, I'm going to walk you through how linguists think about this issue, the usual definition used in linguistics, and some of the problems with that definition. But before we dive into that, let's take a moment to define our terms.\n\n## Some Quick Definitions\n\nBefore we get started, it's worth thinking about how linguists define a language and a dialect in the abstract.\n\n### Language\n\n**A language** is a spoken or signed system of communication used by humans. Human languages have certain features that set them apart from the communication systems used by other animals, including [recursion](https://en.wikipedia.org/wiki/Recursion#In_language), [displacement](https://en.wikipedia.org/wiki/Displacement_(linguistics)), and the ability of language to talk about itself (as we're doing in this post). Although the communication systems of other animals have some features in common with human language, no system (that we know of, anyway) exhibits all of these features. If you want my bet on which comes closest, it wouldn't be chimps, or dolphins, or [vervet monkeys](https://en.wikipedia.org/wiki/Vervet_monkey#Alarm_calls_and_offspring_recognition)—but [prairie dogs](https://blogs.scientificamerican.com/running-ponies/catch-the-wave-decoding-the-prairie-doge28099s-contagious-jump-yips/).\n\n### Dialect\n\n**A dialect** is a particular form of language, usually defined geographically or socially, that has unique features of grammar and pronunciation. In the US, we often talk casually of southern dialects, New York, Boston, etc. A similar situation exists in the UK for English, and, in fact, most countries have a number of dialects in addition to the standard form you might learn in the classroom.\n\n### Accent\n\nThe term accent often gets mixed into the conversations about what a language or a dialect is, but is actually something more specific. At a high level, we can think of an accent as meaning one of two things. In the first, which is most germane to our conversation here, **accent** refers to the pronunciation features of a particular language or dialect-for example, dropping the /r/ at the ends of some words are features of some dialects of English.\n\nThat is to say, an accent is one aspect of a dialect, the one related to how things are pronounced. But as noted above, dialects also have unique grammatical features that set them apart, so accent does not equal dialect, although it can be one of the most salient aspects of one. The second meaning of accent refers to a **non-native accent,** such as a French or Polish accent when someone is speaking English. In this case, the term is referring to a kind of interference from the pronunciation of the native language that's showing up in the non-native language.\n\n### Variety\n\nFinally, the term **variety** is a neutral term that linguists use when they're talking about a particular form of language without making any claims about whether it's a language or a dialect. You'll see this term used throughout this post.\n\n## The linguistic standard: Mutual intelligibility\n\nWith those definitions out of the way, let's turn to the question at hand. How do linguists decide if two different varieties constitute separate languages or dialects of the same language? The usual test is referred to as **mutual intelligibility**-can speakers of the two varieties in question understand each other? If so, we say they speak the same language, or different dialects of the same language. If they can't, then they speak two different languages. Sounds easy, right? If only that were true...\n\n### Challenges for mutual intelligibility\n\nMutual intelligibility works great for some cases-if you're comparing speakers of English and Mandarin, for example. It's unlikely they'd understand *anything* that the other person says, except in the rare case of a borrowed word. But what about in other cases, where languages are more similar? This is, after all, what we're interested in. English and Mandarin are obviously different languages.\n\n#### How intelligible is intelligible?\n\nFor example, what if you're comparing Swedish, Norwegian, and Danish which are all very similar to one another? You can see an example sentence of each of these three languages from a Wikipedia article below to see just how similar these languages are.\n\n##### Swedish\n\n> År 1877 lämnade Brandes Köpenhamn och bosatte sig i Berlin. Hans politiska åsikter gjorde emellertid det obehagligt för honom att uppehålla sig i Preussen och år 1883 återvände han till Köpenhamn, där han mötte en helt ny grupp av författare och tänkare, som var ivriga att anta honom som sin ledare.\n\n##### Norwegian *(Nynorsk*)*\n\n> I 1877 forlet Brandes København og busette seg i Berlin. Dei politiske synspunkta hans gjorde likevel at det vart ubehageleg for han å opphalde seg i Preussen, og i 1883 vende han tilbake til København, der han vart møtt av ei heil ny gruppe forfattarar og tenkjarar, som var ivrige etter å få han som leiaren sin.\n\n##### Danish\n\n> I 1877 forlod Brandes København og bosatte sig i Berlin. Hans politiske synspunkter gjorde dog, at Preussen blev ubehagelig for ham at opholde sig i, og han vendte i 1883 tilbage til København, hvor han blev mødt af en helt ny gruppe af forfattere og tænkere, der var ivrige efter at modtage ham som deres leder.\n\n##### English\n\n> In 1877 Brandes left Copenhagen and took up residence in Berlin. However, his political views made Prussia an uncomfortable place to live, and in 1883 he returned to Copenhagen, where he was met by a completely new group of writers and thinkers, who were eager to accept him as their leader.\n\nAs should be obvious, these languages are extremely similar, but not identical. So what do we do with mutual intelligibility if someone understands 70% of what someone says? Are they speaking the same language? What if they understand 50%? 30%? Where is the cut-off to decide that the varieties are dialects of the same language, or two different languages? I once worked with a linguistic consultant from Bhutan. I was trying to get a sense of how similar his variety was to others spoken in the country (since, another problem with mutual intelligibility-I didn't have speakers of all of them to do experiments and see who could understand who), and I asked him if he could understand the people who lived in the next valley over from where he grew up. He thought for a moment and then described it as a \"three-day dialect\", meaning that he needed about three days of living around people who spoke the language before he could understand what people there were saying. Are these two varieties of the same language, or two different languages?\n\n<WhitepaperPromo whitepaper=\"latest\"></WhitepaperPromo>\n\n#### Asymmetrical intelligibility\n\nThere are also cases of asymmetrical intelligibility, where it's easier for speakers of one language to understand speakers of another than vice versa. For example, speakers of Portuguese often report being able to understand quite a bit of Spanish, but Spanish speakers typically can't understand much Portuguese at all. A similar situation exists in Scandinavia, where Danish speakers typically have an easier time understanding Swedish and Norwegian than the other way around. (If you're curious, in both cases, this is due to some changes in the pronunciation of words that affected Portuguese but not Spanish, and Danish but not Swedish or Norwegian.)\n\n#### Who's answering?\n\nAnd, to further complicate the issue of mutual intelligibility, who are we asking if they can understand something? I'd guess that I, as someone who has studied linguistics and lived abroad several times, would be better at understanding different English dialects than someone who doesn't have that kind of experience. Or consider trying to compare, say, Polish and Russian. If you ask someone who's older, who grew up in the USSR and was exposed to Russian extensively, they're probably likely to view Polish and Russian as more similar than a younger person who has less exposure to Russian on a regular basis.\n\n## So, how do you know what's a language and what's a dialect?\n\nAs we've seen, linguists do have a clear definition of what constitutes a language versus a dialect-but that definition, on the ground, is a lot more complicated than it might appear at first glance. Unfortunately, that means I can't leave you with a clear, surefire way of determining what's a language and what's a dialect. As mentioned at the outset, a lot of what we refer to today as languages are groups of dialects, often based on geopolitics, with one of those dialects seen as the standard version.\n\nAll of this is to say that, if language versus dialect is important to what you're working on-say, a speech-to-text model-it's important to ask questions of your provider to make sure that you understand how they're referring to languages and dialects and how their models will work for what you need.\n\n- - -\n\n\\* Norwegian has two dialects-*bokmål* (literally, 'book language') and *nynorsk* (literally, 'new Norwegian'), I chose *nynorsk* here because it's the most different from Swedish and Danish, but you can still see how similar it is.";
						}
						async function compiledContent$2X() {
							return load$2X().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2X() {
							return (await import('./chunks/index.0bc6aaf1.mjs'));
						}
						function Content$2X(...args) {
							return load$2X().then((m) => m.default(...args));
						}
						Content$2X.isAstroComponentFactory = true;
						function getHeadings$2X() {
							return load$2X().then((m) => m.metadata.headings);
						}
						function getHeaders$2X() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2X().then((m) => m.metadata.headings);
						}

const __vite_glob_0_89 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2X,
  file: file$2X,
  url: url$2X,
  rawContent: rawContent$2X,
  compiledContent: compiledContent$2X,
  default: load$2X,
  Content: Content$2X,
  getHeadings: getHeadings$2X,
  getHeaders: getHeaders$2X
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2W = {"title":"Diving Into Vue 3 - Getting Started","description":"Compare how setting up a project in Vue 3 differs from setting one up in Vue 2, and take a look at what Vue 3 gives out of the box.","date":"2022-01-28T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1643297450/blog/2022/01/diving-into-vue-3-getting-started/dive-into-vue-3%402x.jpg","authors":["sandra-rodgers"],"category":"tutorial","tags":["vuejs","javascript"],"seo":{"title":"Diving Into Vue 3 - Getting Started","description":"Compare how setting up a project in Vue 3 differs from setting one up in Vue 2, and take a look at what Vue 3 gives out of the box."},"shorturls":{"share":"https://dpgr.am/7f7a2e9","twitter":"https://dpgr.am/d1d8f9b","linkedin":"https://dpgr.am/2b4d230","reddit":"https://dpgr.am/af0682a","facebook":"https://dpgr.am/af881e5"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453841/blog/diving-into-vue-3-getting-started/ograph.png"}};
						const file$2W = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/diving-into-vue-3-getting-started/index.md";
						const url$2W = undefined;
						function rawContent$2W() {
							return "\nVue is a front-end JavaScript framework for building UIs (User Interfaces) and SPAs (Single-Page-Applications). Vue 3 has been out for a while, but until now, I hadn't taken the time to jump into it since it's a little daunting to have to learn something new.\n\nBut no more excuses. I'm diving into Vue 3, and I'm going to write my impressions of how it compares to Vue 2. If, like me, you have questions such as:\n\n*   How do I set up component files in Vue 3?\n*   What is this new `setup()` method?\n*   What is the Composition API (Vue 3) versus the Options API (Vue 2)?\n*   Are props, events, and lifecycle hooks basically the same? Or will I encounter big changes?\n*   How do I even get started with Vue 3?\n\nThen this series of posts will be valuable to you! Read on if you are interested in getting started in Vue 3.\n\n## CLI - What we get out of the box\n\nLet's start by comparing Vue 2 and Vue 3 project setup using the Vue CLI.\n\nVue CLI helps you bootstrap a new Vue project, giving you all the files you need to get up and running.\n\nWith Vue 2, all you had to do was run the following command in your terminal (as long as you had Vue installed first), and the CLI would guide you through setup.\n\n```js\nvue create YOUR-PROJECT-NAME\n```\n\nGood news! With Vue 3, it's almost entirely the same. After you have [installed Vue 3](https://v3.vuejs.org/guide/installation.html#npm), you can just type that command, and you will see a list to choose from. You can select either Vue 2 or Vue 3 for your new project.\n\n![Vue CLI choices](https://res.cloudinary.com/deepgram/image/upload/v1642091717/blog/2022/01/diving-into-vue-3-getting-started/vue-presets.png)\n\nI love this because it means I don't have to fully commit to going over to Vue 3 if it turns out it's not to my liking. I can easily bootstrap a project in either Vue 2 or Vue 3 after upgrading to Vue 3.\n\nI'll select Vue 3 this time, so I can see how this scaffolded project compares to what I would get out of the box with Vue 2. I'll start by looking at the files I expect I would need.\n\n## Project Files\n\nAs a Vue user, I know that three of the important files that make Vue work and render onto the page are:\n\n*   **index.html**\n*   **main.js**\n*   **App.vue**.\n\nI'll examine those files first to see if there are any noticeable changes between Vue 2 and Vue 3. Notice in the screenshot that the file structure of a project created by the CLI is identical for both Vue 2 and Vue 3. However, if we dig in and examine the three files I called out in the list above, we'll find some changes.\n\n![File trees Vue 2 vs Vue 3](https://res.cloudinary.com/deepgram/image/upload/v1642091717/blog/2022/01/diving-into-vue-3-getting-started/files-comparison.png)\n\n### index.html\n\nWhen building even the most basic webpage (not just Vue projects), the **index.html** file is the file that serves as the default starting page, giving the basic skeleton of what we see on the page (the basic HTML structure).\n\nHere is the **index.html** file that is created with a Vue 2 project. And interestingly, the file is exactly the same in a Vue 3 project.\n\n```html\n<!DOCTYPE html>\r\n<html lang=\"\">\r\n  <head>\r\n    <meta charset=\"utf-8\" />\r\n    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\" />\r\n    <meta name=\"viewport\" content=\"width=device-width,initial-scale=1.0\" />\r\n    <link rel=\"icon\" href=\"<%= BASE_URL %>favicon.ico\" />\r\n    <title><%= htmlWebpackPlugin.options.title %></title>\r\n  </head>\r\n  <body>\r\n    <noscript>\r\n      <strong\r\n        >We're sorry but <%= htmlWebpackPlugin.options.title %> doesn't work\r\n        properly without JavaScript enabled. Please enable it to\r\n        continue.</strong\r\n      >\r\n    </noscript>\r\n    <div id=\"app\"></div>\r\n    <!-- built files will be auto injected -->\r\n  </body>\r\n</html>\n```\n\nThis file is extremely important, which is why it is my first instinct to check it out and see if anything has changed between Vue 2 and Vue 3.\n\nThe **index.html** file contains a `div` element with `id=\"app\"`, and that is where Vue knows to look so it can mount the application into the DOM.\n\n```html\n<div id=\"app\"></div>\n```\n\nVue uses this `div` as an entry point for where to inject all the Vue files. Within those Vue files, the Vue magic happens, but it is that first entry point `div` that gets the Vue code into the main HTML page.\n\nSince this file is identical in both Vue 2 and Vue 3 projects, this tells me that at least in this aspect, both Vue 2 and Vue 3 use the same approach to putting Vue files in the DOM - they search for a `div` in the **index.html** file, and when the `div` is found, the Vue app attaches to that particular element.\n\n### main.js\n\nThe **main.js** file in a Vue project is where the Vue instance itself gets initialized. (A Vue application is a Javascript object under the hood - each unique Vue project is an instance object of the Vue object. It inherits properties and methods that make Vue work the way it does.)\n\nIn Vue 2, app initialization looks like this:\n\n```js\nimport Vue from 'vue'\r\nimport App from './App.vue'\r\n\r\nnew Vue({\r\n  render: (h) => h(App),\r\n}).$mount('#app')\n```\n\nBut in Vue 3, it looks like this:\n\n```js\nimport { createApp } from 'vue'\r\nimport App from './App.vue'\r\n\r\ncreateApp(App).mount('#app')\n```\n\nThis is clearly a big change. I want to understand both, so I'll first explain what's happening in Vue 2 and then compare it with Vue 3.\n\n##### Vue 2\n\nFirst, I'll take a look at what's going on in Vue 2. I notice these two important statements:\n\n```js\nimport Vue from 'vue'\r\nimport App from './App.vue'\n```\n\nThe **Vue** constructor function has to be imported from Vue so that it can be used to create a *new instance of Vue*. **App** is also imported (it is the **App.vue** file in the project) because I need a *root Vue component* where all my Vue code will live. Any children components I later build will come into the project through that App file.\n\nNext, I see this in the Vue 2 **main.js** file:\n\n```js\nnew Vue({\r\n  render: (h) => h(App),\r\n}).$mount('#app')\n```\n\nThis is the Vue constructor function being used. `new Vue()` creates a new instance of the Vue object. And an object is passed into the constructor function `Vue({...})` which is known as the **options** object. This is why Vue 2 is sometimes referred to as the **Options API** (even though the Options API is just a *part* of Vue 2). The options are those properties like data, methods, mounted, computed, and so on.\n\nOn the next line, I see `render: h => h(App)`. The render property tells Vue to render the component as HTML (`render` is actually a function that is part of the Vue options API). You can read more in [this article](https://css-tricks.com/what-does-the-h-stand-for-in-vues-render-method/) about why the `h` is used if you are curious. The code in the **App.vue** component file is passed into that render function, so it gets built out as HTML (and there is other stuff going on to make Vue do its reactivity magic).\n\nThe [$mount](https://vuejs.org/v2/api/#vm-mount) method is a built-in Vue method that manually starts the mounting of the Vue instance.\n\n##### Vue 3\n\nIn Vue 3, I see that the **main.js** file looks very different:\n\n```js\nimport { createApp } from 'vue'\r\nimport App from './App.vue'\r\n\r\ncreateApp(App).mount('#app')\n```\n\nThis seems like a lot less code. Instead of importing a constructor function to create a Vue object instance, I see a function called `createApp` being imported. It's striking to me that initialization is happening with just one line of code, `createApp(App).mount('#app')`.\n\nEven though it's not apparent here, the `createApp` function is actually still using the `new Vue()` constructor inside the function. The difference now is that it *makes a copy* of the Vue instance. Instead of using `new Vue()` directly (and directly adding configuration options which will affect all uses of that Vue instance, resulting in mutations to global state), the `createApp` makes a copy that can be separately configured. By encapsulating this within a function, it gives me the ability to create separate Vue instances, and configuration to each instance won't affect other instances.\n\nThis is an improvement because now it's easier to create two Vue objects with separate configurations if needed. Here is an example of a setup that is doing that:\n\n```js\nimport GlobalApp from './App.vue'\r\n\r\n//Create one Vue app using same global app file and add unique configuration\r\nconst app1 = Vue.createApp(GlobalApp)\r\n\r\napp.component('SearchInput', SearchInputComponent)\r\napp.directive('focus', FocusDirective)\r\napp.use(LocalPlugin)\r\n\r\n//Create second Vue app using same global app file and add unique configuration\r\nconst app2 = Vue.createApp(GlobalApp)\r\n\r\napp.component('Modal', ModalComponent)\r\napp.directive('toolip', TooltipDirective)\r\napp.use(DifferentPlugin)\n```\n\nWhy is this helpful? I can only imagine possible situations. Perhaps if different teams using Vue want to build off the same main App file but add their own options, plugins, features, etc. Or maybe so that I could build something as a core but then branch off on different paths to see how different configurations work.\n\nWhatever the reason, just having that ability has made Vue better.\n\n### App.vue\n\nThe last file I want to look at to see if there are any differences out of the box is the **App.vue** file.\n\nThe **App.vue** file is the root component, the one that is the starting point for rendering the Vue code when the application gets mounted to that DOM element in the **index.html** file.\n\nAfter comparing **App.vue** in Vue 2 and **App.vue** in Vue 3, I see only one difference, and it's in the template block.\n\nHere is the Vue 2 **App.vue** template:\n\n```html\n<template>\r\n  <div id=\"app\">\r\n    <img alt=\"Vue logo\" src=\"./assets/logo.png\" />\r\n    <HelloWorld msg=\"Welcome to Your Vue.js App\" />\r\n  </div>\r\n</template>\n```\n\nAnd here is the Vue 3 **App.vue** template:\n\n```html\n<template>\r\n  <img alt=\"Vue logo\" src=\"./assets/logo.png\" />\r\n  <HelloWorld msg=\"Welcome to Your Vue.js App\" />\r\n</template>\n```\n\nI'm surprised to see that there is no longer the need for the surrounding root div in the template. We are no longer required to have a single root element, like so:\n\n```html\n<template>\r\n  <div id=\"app\">...</div>\r\n</template>\n```\n\nIn Vue 2, we had to provide a single root element as the direct child of the template, but now in Vue 3, we can have many direct child elements, i.e., multiple root elements. This eliminates all the extra divs that showed up around the HTML code for each component.\n\nHowever, it means that if you put a non-prop attribute on the component, and you use multi-root elements rather than a single root element like in Vue 2, that attribute won't show up in your component unless you explicitly [bind an element to those attributes](https://v3.vuejs.org/guide/component-attrs.html#disabling-attribute-inheritance). Just something to be aware of.\n\n## Conclusion\n\nSo far, I'm not too intimidated by these changes I see in Vue 3 versus Vue 2, but I haven't really gotten into the big stuff yet.\n\nIn my next post in this series, I'll examine the `setup() ` function, which is probably the most important change to understand. It will be a good opportunity for comparing the Composition API (which comes with Vue 3) versus the Options API (which was the Vue 2 way of doing things).\n\nPlease follow me on [Twitter](https://twitter.com/sandra_rodgers_) if you want to dive into more Vue 3 with me!\n\n        ";
						}
						async function compiledContent$2W() {
							return load$2W().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2W() {
							return (await import('./chunks/index.13570bbe.mjs'));
						}
						function Content$2W(...args) {
							return load$2W().then((m) => m.default(...args));
						}
						Content$2W.isAstroComponentFactory = true;
						function getHeadings$2W() {
							return load$2W().then((m) => m.metadata.headings);
						}
						function getHeaders$2W() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2W().then((m) => m.metadata.headings);
						}

const __vite_glob_0_90 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2W,
  file: file$2W,
  url: url$2W,
  rawContent: rawContent$2W,
  compiledContent: compiledContent$2W,
  default: load$2W,
  Content: Content$2W,
  getHeadings: getHeadings$2W,
  getHeaders: getHeaders$2W
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2V = {"title":"Diving Into Vue 3 - Methods, Watch, and Computed","description":"Learn about the basic features of methods, watch, and computed in Vue 3.","date":"2022-02-11T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1643663009/blog/2022/02/diving-into-vue-3-methods-watch-and-computed/dive-into-vue-3%402x.jpg","authors":["sandra-rodgers"],"category":"tutorial","tags":["vuejs","javascript"],"seo":{"title":"Diving Into Vue 3 - Methods, Watch, and Computed","description":"Learn about the basic features of methods, watch, and computed in Vue 3."},"shorturls":{"share":"https://dpgr.am/c58f2b2","twitter":"https://dpgr.am/99bca25","linkedin":"https://dpgr.am/262624f","reddit":"https://dpgr.am/4ec20fe","facebook":"https://dpgr.am/57aa167"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661477089/blog/diving-into-vue-3-methods-watch-and-computed/ograph.png"}};
						const file$2V = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/diving-into-vue-3-methods-watch-and-computed/index.md";
						const url$2V = undefined;
						function rawContent$2V() {
							return "\r\nIn this series, I'm focusing on the basics needed to start working in **Vue 3** for people who might have some experience in **Vue 2**, but who haven't yet built anything in **Vue 3**.\r\n\r\nCheck out my previous posts in the series:\r\n\r\n1.  [Diving Into Vue 3: Getting Started](https://blog.deepgram.com/diving-into-vue-3-getting-started/)\r\n\r\n2.  [Diving Into Vue 3: The Setup Function]()\r\n\r\nToday, I'll introduce how to use `methods`, `watch`, and `computed` in **Vue 3**, and I'll also give a general comparison of `watch` and the new `watchEffect`.\r\n\r\n## Introduction\r\n\r\nThe way I learn best is by connecting abstract concepts to a real world situation, so I tried to think of a simple, realistic situation for using `methods`, `watch`, and `computed`. The situation would need to demonstrate the following:\r\n\r\n*   doing something to data properties to change them (using `methods`)\r\n*   making something else occur (i.e, a side effect) because of a change to the data properties (using `watch`)\r\n*   returning a value that is calculated based on data properties that have been changed (`computed`)\r\n\r\nI will use a real-world example of a company with employees and managers; the logic will help keep track of *number of employees*, *number of managers*, and *total company headcount*. Not the most exciting example, but I really just want to keep it simple.\r\n\r\n## Methods\r\n\r\nOne of the first things I need to be able to do, whether I'm using **Vue 2** or **Vue 3**, is be able to make stuff happen with methods/functions (note: I'm using the terms *functions* and *methods* interchangeably in this section). The magic of Vue is its reactivity, so local state updates automatically as stuff happens. The stuff that happens is often triggered by *methods*.\r\n\r\nIn my real-world example, I want to create a component that has a variable to represent the **number of employees** with buttons I click to **add or subtract the number of employees**, changing the **headcount**. I'll write functions to perform these basic actions.\r\n\r\nHere's what the rendered component looks like:\r\n\r\n![Rendered component to that uses method to increment and decrement](https://res.cloudinary.com/deepgram/image/upload/v1643663337/blog/2022/02/diving-into-vue-3-methods-watch-and-computed/component-methods.png)\r\n\r\nI am familiar with the **Vue 2** way of adding functions to the component: add each function to the `methods` object:\r\n\r\n```html\r\n<script>\r\nexport default {\r\n  data() {\r\n    return {\r\n      numEmployees: 10,\r\n    };\r\n  },\r\n  methods: {\r\n    addEmployees() {\r\n      this.numEmployees++;\r\n    },\r\n    subtractEmployees() {\r\n      this.numEmployees--;\r\n    },\r\n  },\r\n};\r\n</script>\r\n```\r\n\r\nAnd the following line from the `template` shows that **Vue 2** and **Vue 3** are no different in how the methods are invoked in the `template`:\r\n\r\n```js\r\n<button @click=\"addToEmployees()\">+</button>\r\n```\r\n\r\nHowever, **Vue 3** is different now in regards to where we write the methods in the `script`. In **Vue 3**, I can now write my functions *inside* the `setup` function, which runs very early in the component lifecycle (before the component instance is even created). I no longer have to write all my functions in the `methods` property of the options API.\r\n\r\nIn this example, I have written two basic functions, and those functions are not separated into a separate methods block like in **Vue 2**, they are inside `setup` with the related logic like the variable for `numEmployees`. I can make the functions available to the template by returning an object that includes them:\r\n\r\n```js\r\n<script>\r\nimport { ref } from \"vue\";\r\nexport default {\r\n  setup() {\r\n    let numEmployees = ref(10);\r\n    function addEmployees() {\r\n      numEmployees.value++;\r\n    }\r\n    function subtractEmployees() {\r\n      numEmployees.value--;\r\n    }\r\n    return { numEmployees, addEmployees, subtractEmployees };\r\n  },\r\n};\r\n</script>\r\n```\r\n\r\nNotice that there is no keyword `this` when referring to `numEmployees`. Methods that are inside the `setup` function no longer use the keyword `this` to refer to properties on the component instance since `setup` runs before the component instance is even created. I was very used to writing `this`-dot everything in **Vue 2**, but that is no longer the experience in **Vue 3**.\r\n\r\nThe use of `ref()` surrounding the data property is something I introduced in the last post, and it's important here. For reactivity to work in Vue, the data being tracked needs to be wrapped in an object, which is why in **Vue 2**, the `data` method in the options API returned an object with those reactive data properties.\r\n\r\nNow, **Vue 3** uses `ref` to wrap primitive data in an object and `reactive` to make a copy of non-primitive data (I've only introduced `ref` so far in this series). This is important to methods because it helps me understand why I see `numEmployees.value` inside the function rather than just `numEmployees`. I have to use `.value` to reach the property inside the object created by `ref` and then perform the action on that value property. (I don't have to use the `.value` property in the template, however. Just writing `numEmployees` grabs the value).\r\n\r\nWriting all the methods inside the setup function may seem like it would get messy when there is more complexity going on in the component, but in practice, **related logic could all be grouped together to run within its own function**. This is where **Vue 3** starts to show its strengths. I could group all the logic for updating headcount into a function called `updateHeadcount`, then create a separate JS file where that logic lives. I'll actually name it `useUpdateHeadcount.js`, which is **Vue 3** best-practice for naming this type of file (the convention of starting composables with *use* is discussed in the Composition API RFC in [this section](https://github.com/vuejs/composition-api-rfc/blob/master/index.md#logical-concerns-vs-option-types)). Here's the `useUpdateHeadcount.js` file:\r\n\r\n```js\r\nimport { ref } from 'vue'\r\n\r\nexport default function useUpdateHeadcount() {\r\n  let numEmployees = ref(10)\r\n  function addToEmployees() {\r\n    numEmployees.value++\r\n  }\r\n  function subtractFromEmployees() {\r\n    numEmployees.value--\r\n  }\r\n\r\n  return { numEmployees, addToEmployees, subtractFromEmployees }\r\n}\r\n```\r\n\r\nNow, in my component, I just have to write this in the setup function:\r\n\r\n```js\r\n<script>\r\nimport useUpdateHeadcount from \"../composables/useUpdateHeadcount\";\r\nexport default {\r\n  setup() {\r\n\r\n    const { numEmployees, addToEmployees, subtractFromEmployees } =\r\n      useUpdateHeadcount();\r\n\r\n    return { numEmployees, addToEmployees, subtractFromEmployees };\r\n  },\r\n};\r\n</script>\r\n\r\n```\r\n\r\n### Composables\r\n\r\nNotice that I imported the `useUpdateHeadcount` file from a folder called *composables*. That's because these *functions to separate out logic by shared concerns* are known as **composables** in the **Vue 3** world. I'm not going to go over all the details of how I wrote the composable and brought it back into the component file because I'll be doing a later blog post in the series about composables. In fact, I don't even have to use a composable; I can just write all my logic in the setup function since it's a very simple component. But I wanted to also make it clear that as the component gets more complicated, there is a strategy for organizing the logic, and it's one of **Vue 3**'s most exciting features.\r\n\r\n## Watch\r\n\r\n`watch` is basically the same in **Vue 3**, so I am happy to know that I can use it as I have before. In my example, I want to track the value of `numEmployees` to make sure it doesn't go below zero, since it's not possible to have negative human beings as employees.\r\n\r\nHere's my rendered component. It looks the same, but I added a disclaimer that the headcount cannot go below zero.\r\n\r\n![Component for adding or subtracting employees but restricted to not go below zero](https://res.cloudinary.com/deepgram/image/upload/v1643663535/blog/2022/02/diving-into-vue-3-methods-watch-and-computed/watch-headcount.png)\r\n\r\nThis restriction - not going below zero - will be managed by the logic in `watch`:\r\n\r\n```js\r\nwatch(numEmployees, () => {\r\n  if (numEmployees.value < 0) {\r\n    numEmployees.value = 0\r\n  }\r\n})\r\n```\r\n\r\nI specify which data property to track (`numEmployees`) as the first argument, and a *callback* as the second argument. Inside the callback, I have my logic that causes the side effect. If `numEmployees` reaches below zero, that side effect happens, setting the value to zero. The callback makes sure the side effect happens on the next tick following the value reaching below zero.\r\n\r\n`watch` will not be triggered until that specific reactive property is changed, so if I want it to run immediately when the component is created, I can add an object with `immediate: true` like this:\r\n\r\n```js\r\nwatch(\r\n  employees,\r\n  (newVal, oldVal) => {\r\n    if (employees.value < 0) {\r\n      employees.value = 0\r\n    }\r\n  },\r\n  { immediate: true }\r\n)\r\n```\r\n\r\nThe callback argument can also take two arguments for the **new value** and the **old value**, which makes `watch` useful for doing logic based on the previous state of the reactive property or just checking if a property has been changed (i.e. it's a great debugging tool):\r\n\r\n```js\r\nwatch(employees, (newVal, oldVal) => {\r\n  console.log(oldVal, newVal)\r\n})\r\n```\r\n\r\nAs for comparing `watch` in **Vue 2** versus **Vue 3**, the only difference is that in Vue 3 I can now place `watch` inside the setup function. Like methods, it no longer has to be separated out into its own section as an option property on the component instance.\r\n\r\nHowever, **Vue 3** also has added a similar feature that gives some different capabilities from `watch`: it's called `watchEffect`.\r\n\r\n### watchEffect\r\n\r\nVue 3 keeps `watch` the same, but it adds `watchEffect` as another way to cause side effects based on what happens to the reactive properties. Both `watch` and `watchEffect` are useful in different situations; one isn't better than the other.\r\n\r\nIn this example, I will add another reactive property to the component - managers (`numManagers`). I want to track both managers and employees, and I want to restrict their values going below zero. Here is the component now:\r\n\r\n![Component for adding or subtracting employees and manager but restricted to not go below zero](https://res.cloudinary.com/deepgram/image/upload/v1643663591/blog/2022/02/diving-into-vue-3-methods-watch-and-computed/watchEffect-headcount.png)\r\n\r\nThe reason I added a second reactive property is because `watchEffect` makes it easier to track multiple reactive properties. I no longer have to specify each property I want to track as the first argument of watch. Notice that I don't have a first argument to name the properties I'm tracking:\r\n\r\n```js\r\nwatchEffect(() => {\r\n  if (numEmployees.value < 0) {\r\n    numEmployees.value = 0\r\n  }\r\n  if (numManagers.value < 0) {\r\n    numManagers.value = 0\r\n  }\r\n})\r\n```\r\n\r\nUnlike `watch`, `watchEffect` is not lazy loaded, so it will trigger automatically when the component is created. No need to add the object with `immediate: true`.\r\n\r\n`watchEffect` is useful when I want to track changes to whatever property I want, and when I want the tracking to happen immediately.\r\n\r\n`watch` is useful when I want to be more specific about tracking just one property, or if I want to have access to the new value and/or old value to use them in my logic.\r\n\r\nIt's great having both features!\r\n\r\n## Computed\r\n\r\nOne of the nice things about the Vue `template` is that I can write logic within double curly-braces, and that logic will be calculated based on whatever the values are represented by each variable:\r\n\r\n```html\r\n<h2>Headcount: {{ numEmployees + numManagers }}</h2>\r\n```\r\n\r\nThis will show a number which has been calculated, or *computed*, based on what `numEmployees` and `numManagers` are at the current point of time. And it will change if either of those data for `numEmployees` or `numManagers` change.\r\n\r\nSometimes, the logic can get complicated or long. That's when I write a `computed` property in the `script` section, and refer to it in the template. Here is how I would do that in Vue 2:\r\n\r\n```js\r\n\r\n<script>\r\n export default {\r\n  computed: {\r\n    headcount() {\r\n      return this.employees.value + this.managers.value;\r\n    },\r\n  },\r\n}\r\n</script>\r\n\r\n```\r\n\r\nThe computed property is another option that is part of the options API, and in **Vue 2**, it sits at the same level as `methods`, `data`, `watch`, and lifecycle methods like `mounted`.\r\n\r\nIn **Vue 3**, computed can now be used in the `setup` function (I bet you didn't see that one coming). I have to import `computed` from Vue like this:\r\n\r\n```js\r\nimport { computed } from 'vue'\r\n```\r\n\r\nTo compute the number of employees and the number of managers, giving me the total headcount, I could write a computed like this:\r\n\r\n```js\r\nconst headcount = computed(() => {\r\n  return numEmployees.value + numManagers.value\r\n})\r\n```\r\n\r\nThe only difference is that now I pass into the computed method an anonymous function, and I set it to the constant for headcount. I also have to return headcount from the setup function, along with everything else I want to be able to access from the template.\r\n\r\n```js\r\nreturn {\r\n  numEmployees,\r\n  numManagers,\r\n  addToEmployees,\r\n  subtractFromEmployees,\r\n  addToManagers,\r\n  subtractFromManagers,\r\n  headcount, //<----\r\n}\r\n```\r\n\r\n## Putting It All Together\r\n\r\nAt this point, I have logic that does the following:\r\n\r\n*   Adds or subtracts to the number of employees (numEmployees) or to the number of managers (numManagers)\r\n*   Makes sure employees and managers do not go below zero\r\n*   Computes the total headcount based on any changes\r\n\r\n<div>\r\n<CodeEmbed height=\"670\" src=\"https://codepen.io/sandrarodgers/embed/ZEaQRLR\"/>\r\n</div>\r\n\r\n## Conclusion\r\n\r\nAnd that wraps up this post in the series. Stay tuned for upcoming posts that cover topics like `ref` and `reactive`, `composables`, and the new `v-model`. And as always, feel free to reach out on [Twitter](https://twitter.com/sandra_rodgers_)!\r\n\r\n        ";
						}
						async function compiledContent$2V() {
							return load$2V().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2V() {
							return (await import('./chunks/index.da763143.mjs'));
						}
						function Content$2V(...args) {
							return load$2V().then((m) => m.default(...args));
						}
						Content$2V.isAstroComponentFactory = true;
						function getHeadings$2V() {
							return load$2V().then((m) => m.metadata.headings);
						}
						function getHeaders$2V() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2V().then((m) => m.metadata.headings);
						}

const __vite_glob_0_91 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2V,
  file: file$2V,
  url: url$2V,
  rawContent: rawContent$2V,
  compiledContent: compiledContent$2V,
  default: load$2V,
  Content: Content$2V,
  getHeadings: getHeadings$2V,
  getHeaders: getHeaders$2V
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2U = {"title":"Diving Into Vue 3 - The Reactivity API","description":"Learn about reactivity in Vue 3 and how to use ref and reactive helpers","date":"2022-02-18T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1644435060/blog/2022/02/diving-into-vue-3-reactivity-api/dive-into-vue-3%402x.jpg","authors":["sandra-rodgers"],"category":"tutorial","tags":["vuejs","javascript"],"seo":{"title":"Diving Into Vue 3 - The Reactivity API","description":"Learn about reactivity in Vue 3 and how to use ref and reactive helpers"},"shorturls":{"share":"https://dpgr.am/98e7070","twitter":"https://dpgr.am/fa58f26","linkedin":"https://dpgr.am/1ce34ef","reddit":"https://dpgr.am/571a1b6","facebook":"https://dpgr.am/3bf430e"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453991/blog/diving-into-vue-3-reactivity-api/ograph.png"}};
						const file$2U = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/diving-into-vue-3-reactivity-api/index.md";
						const url$2U = undefined;
						function rawContent$2U() {
							return "\nThis is the fourth post in my 'Diving Into Vue 3' series. Today I hope to give a clear and practical description of how to use the new Reactivity APIs, focusing on `ref` and `reactive`. I'll also introduce how to use related helpers such as `toRef`, `toRefs`, and `isRef`.\n\nThe other posts in this series that have already come out are:\n\n*   [Diving Into Vue 3 - Getting Started](https://blog.deepgram.com/diving-into-vue-3-getting-started/)\n*   [Diving Into Vue 3 - The Setup Function](https://blog.deepgram.com/diving-into-vue-3-setup-function/)\n*   [Diving Into Vue 3: Methods, Watch, and Computed](https://blog.deepgram.com/diving-into-vue-3-methods-watch-and-computed/)\n\nBut first, a little background on **reactivity**. Feel free to skip ahead to the section on [how to make data properties reactive](#how-to-make-data-properties-reactive) if you just want to learn about `ref` and `reactive`.\n\n## What is reactivity?\n\nThe term **reactivity** in relation to Vue generally refers to a feature where what you see on the screen automatically updates in-sync with any changes to the state. It's the Vue 'magic' that makes the template re-render instantly if a data property changes.\n\nWhen talking about **reactivity** in JavaScript or in programming in general, the term means programming something to work the way Vue does by implementing a design pattern called the **Observer Pattern**, which is explained in [Design Patterns for Humans](https://github.com/sohamkamani/javascript-design-patterns-for-humans#-observer) as :\n\n> whenever an object changes its state, all its dependents are notified.\n\nVue automatically updating the DOM when a data property changes is a result of Vue being built using the **Observer Pattern** - Vue state is an object with properties that have dependencies, so if one of those properties changes, its dependents react to the change by updating if they need to, and that triggers the re-render in the browser.\n\nJavaScript on its own is not reactive, as shown in this example:\n\n```js\nlet numWorkers = 50\nlet numManagers = 4\nlet totalEmployees = numWorkers + numManagers\n\nconsole.log(totalEmployees) // 54\n\nnumWorkers = 48\n\nconsole.log(totalEmployees) // Still 54\n```\n\nVue is reactive because the core Vue.js team built it to be. So in the following example, `totalEmployees` will automatically update anytime `numWorkers` or `numManagers` (two properties in the state object) changes:\n\n```js\n  data() {\n    //returns the state object\n    return { numWorkers: 4, numManagers: 6 }\n  },\n  computed: {\n    totalEmployees() {\n      // returns whatever the total is based on current state for numWorkers and numManagers\n      return this.numWorkers +  this.numManagers\n    }\n  }\n```\n\n### Reactivity in Vue 2\n\nThe reactivity system in both Vue 2 and Vue 3 is based on state being an object, but there are big differences in how the properties are made reactive.\n\nIn Vue 2 the data option returns an object:\n\n```js\n  data() {\n    return {\n      numWorkers: 4,\n      numManagers: 6\n    }\n  }\n```\n\nUnder the hood, Vue 2 uses `Object.defineProperty` to define all the data properties on a component instance, converting them to getters and setters. There's a deep dive into the Vue 2's reactivity system in the [Vue.js docs](https://v2.vuejs.org/v2/guide/reactivity.html) that's worth spending some time with.\n\nBecause defining the properties happens at the time of the component instance's initialization, it results in some small drawbacks:\n\n*   data properties cannot be added or deleted after component instance initialization. They have to be present during initialization for them to be reactive\n\n*   If the data property is an array, it is not possible to set an item directly to the array through assignment by using the array index (as in `arr[0] = value`), and it also isn't possible to update the length of the array (as in `arr.length = 0`)\n\nThis isn't a *major* problem because the `Vue.$set` method can be used in cases where these updates need to be made after component instance initialization. However, Vue 3's reactivity system is so improved that now these issues are no longer a problem, making it unnecessary to use `Vue.$set`.\n\n### Reactivity in Vue 3\n\nVue 3's reactivity system had a major rewrite from what it was in Vue 2. The fundamental idea of tracking all the data properties and their dependencies so they can update automatically is still the same, but Vue 3 now uses the JavaScript [Proxy](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Proxy) API to achieve this (instead of `Object.defineProperty` like in Vue 2).\n\nThere's a rabbit hole to go down for anyone who wants to learn more about the Vue 3 reactivity system, and the [Vue.js docs](https://v3.vuejs.org/guide/reactivity.html#how-vue-tracks-these-changes) are fantastic. Even though I'm not going to explain it all (there's no way I could!), there are a few things I think are helpful to understand.\n\nThe docs state:\n\n> Proxy is an object that encases another object and allows you to intercept any interactions with that object.\n\nAwareness of reactive properties using proxies is helpful when debugging code in the console. If I `console.log` a reactive property such as this property `company`:\n\n```js\nconst company = reactive({\n  employees: ['Tom', 'Sara', 'Joe'],\n  managers: ['Julie', 'Jorge'],\n})\n```\n\nin the console, I see:\n\n![A console.log for a reactive property which shows the proxy](https://res.cloudinary.com/deepgram/image/upload/v1644436537/blog/2022/02/diving-into-vue-3-reactivity-api/console_1.png)\n\nClicking on it will open the object to show that there is a Handler and a Target. A proxy always contains a Handler and a Target, and since Vue 3 uses proxies, I find it helpful to be comfortable with the shape of this data as a proxy.\n\n![Same console.log for a reactive property which shows the proxy, now open to show handler and target](https://res.cloudinary.com/deepgram/image/upload/v1644436562/blog/2022/02/diving-into-vue-3-reactivity-api/console_2.png)\n\nThe Target is where to look for the actual values. It contains the data that I might be looking for. The Handler contains the special logic for making the data properties reactive. The Handler contains methods like `get` and `set`.\n\nThe Handler is the rabbit hole if you want to learn about reactivity. The Target is where I need to look for my data values.\n\n![Same console.log for a reactive property which shows the proxy, now open to show handler and target with target open to show data values](https://res.cloudinary.com/deepgram/image/upload/v1644436562/blog/2022/02/diving-into-vue-3-reactivity-api/console_3.png)\n\nBecause reactive data is wrapped in a proxy, something to get used to when working with the data is the idea of having to 'unwrap' the data object to get to the value. After reading a lot of different resources about working with Vue 3 reactive data, I now feel comfortable with the idea that using strategies to 'unwrap' the data, such as destructuring or drilling down to the value property, are using the metaphor of unwrapping because Vue 3 reactive data is wrapped in a `Proxy` object.\n\n## How to make data properties reactive\n\nAs I said earlier, if I want to make data properties reactive in Vue 2, I have to return them in an object inside the data option of the Options API.\n\n```js\n  data() {\n    return {\n      president: \"Mickey Mouse\",\n      vicePresident: \"Donald Duck\"\n    }\n  }\n```\n\nIf I am using the Vue 3 setup function (see my [post on the setup function](https://blog.deepgram.com/diving-into-vue-3-setup-function/) if you need an explainer on that), I can make data reactive by using the `reactive` or `ref` helpers.\n\n### ref\n\nFor this first example, I will use `ref`. I'm using `ref` because `\"Mickey Mouse\"` and `\"Donald Duck\"` are strings, and the recommendation is to use `ref` with primitive values (i.e. Javascript types that are not objects, such as strings, numbers, etc.)\n\nFirst, I import `ref`:\n\n```js\n<script>import { ref } from \"vue\";</script>\n```\n\nThen in the `setup` function, I set my variable to the `ref()` helper, which takes in the initial value. I must include the data in the return object if I want it to be available to the template.\n\n```js\n  setup() {\n    let president = ref(\"Mickey Mouse\");\n    let vicePresident = ref(\"Donald Duck\");\n\n    return { president, vicePresident };\n    },\n```\n\nAn important difference between `ref` and `reactive` is that if I want to do something to the value of my `ref` properties inside the `setup` function, I have to unwrap the object to access that value. So if I want to change the value of `president`, I will change `president.value`:\n\n```js\n  function changePresident() {\n    president.value = 'Goofy'\n  }\n```\n\nI don't have to worry about unwrapping the values for `president` and `vicePresident` in the `template`. Vue can shallow unwrap those for me. 'Shallow unwrap' means the first level of properties in an object are available in the template without having to use `.value` (but nested properties would still need to be unwrapped).\n\n```html\n<template>\n  <div>\n    <p><b>President:</b> {{ president }}</p>\n    <p><b>Vice President:</b> {{ vicePresident }}</p>\n  </div>\n</template>\n```\n\nFYI, it's fine not to use `ref` if I don't need the data to be reactive, just writing the data like this:\n\n```js\nsetup() {\n  let president = \"Mickey Mouse\"\n  let vicePresident = \"Donald Duck\"\n\n  return { president, vicePresident };\n},\n```\n\nBut it would mean the data isn't reactive, so I can't ever see updates to the data. If I use a method to change the data, I would never see that update change anything on the screen, and I would have to be happy with Mickey Mouse and Donald Duck showing as president and vice president forever.\n\nThere are times when you don't need the data to be reactive, so in those instances, just don't use `ref` or `reactive`!\n\n### reactive\n\nI can use `reactive` for the same example, but I would only do so if I wanted the data to start out in the form of an object rather than separate string values. So in Vue 2, if I have this:\n\n```js\ndata() {\n  return {\n    executiveTeam: {\n      president: \"Mickey Mouse\",\n      vicePresident: \"Donald Duck\",\n    },\n  };\n},\n```\n\nTo change this to Vue 3 using `reactive`, I will first import `reactive`:\n\n```js\nimport { reactive } from 'vue'\n```\n\nIn the `setup` function, I will create an object for `executiveTeam` and define the properties on the object. I can set the object to `const` since the object itself won't change, just the properties inside.\n\n```js\nsetup() {\n  const executiveTeam = reactive({\n    president: \"Mickey Mouse\",\n    vicePresident: \"Donald Duck\",\n  });\n\n  return { executiveTeam };\n},\n```\n\nAnd if I want to update the data, I do not have to unwrap it with `.value `like I do with `ref`.\n\n```js\nfunction changePresident() {\n  executiveTeam.president = 'Goofy'\n}\n```\n\nThis is because `reactive` is used with objects, and objects pass values by *reference* (which lends itself better to reactivity). Reactive references (`ref`) are used for primitive types, and primitives in Javascript pass values by *value*, so Vue has to wrap them in an object to make them reactive. Since `ref` properties are wrapped to make them reactive, they have to be unwrapped down to the `.value` to get the value. Read more about this concept in the  [Composition API RFC](https://github.com/vuejs/composition-api-rfc/blob/master/index.md#computed-state-and-refs) if this concept is something you want to understand more deeply.\n\nHowever, because I'm returning the object `executiveTeam` and I want to access the properties `president` and `vicePresident` on that object in the template, I will have to drill down into the `executiveTeam` object to get each property I need:\n\n```html\n<template>\n  <div>\n    <p><b>President:</b> {{ executiveTeam.president }}</p>\n    <p><b>Vice President:</b> {{ executiveTeam.vicePresident }}</p>\n  </div>\n</template>\n```\n\nI cannot destructure the object that I return because if I do, the properties inside `executiveTeam` will lose reactivity. I'll demonstrate this in the next example to make this more clear.\n\nWhen using `reactive` to give an object's properties reactivity, like this:\n\n```js\nconst executiveTeam = reactive({\n  president: 'Mickey Mouse',\n  vicePresident: 'Donald Duck',\n})\n```\n\nI cannot destructure to try and return those properties by their key, as in:\n\n```js\n//LOSES REACTIVITY:\nlet { president, vicePresident } = executiveTeam\n\nreturn { president, vicePresident }\n```\n\nThis is where `toRefs` comes in handy.\n\n### toRefs\n\nThe helper `toRefs` will allow me to turn each of the properties in the object into a `ref`, which means I won't have to use `executiveTeam.president` in the template; I'll be able to just write `president`. Here's the full example now using `toRefs`:\n\n```js\n<script>\nimport { reactive, toRefs } from \"vue\";\nexport default {\n  setup() {\n    const executiveTeam = reactive({\n      president: \"Mickey Mouse\",\n      vicePresident: \"Donald Duck\",\n    });\n\n    //toRefs allows me to destructure\n    let { president, vicePresident } = toRefs(executiveTeam);\n\n    return { president, vicePresident };\n  },\n};\n</script>\n\n```\n\nSince `toRefs` turns each property into a `ref`, I need to go back to unwrapping them down to their value using `.value `if I want to do something to them in the `setup` function:\n\n```js\nfunction changePresident() {\n  president.value = 'Goofy'\n}\n```\n\n### toRef\n\nJust like `toRefs`, the helper `toRef` is used to turn reactive object properties into reactive references (`ref`), but I would use `toRef` if I just need to turn one property in a reactive object into a `ref`:\n\n```js\nsetup() {\n  const executiveTeam = reactive({\n    president: \"Mickey Mouse\",\n    vicePresident: \"Donald Duck\",\n  });\n  //toRef used to turn just one property into a ref\n  let presidentRef = toRef(executiveTeam, \"president\");\n\n  const changePresident = () => {\n    presidentRef.value = \"Goofy\";\n  };\n\n  return { presidentRef, changePresident };\n},\n```\n\nI will have to use `.value` if I want to update the ref's value inside the setup function, but in the template, Vue will unwrap `president` for me:\n\n```html\n<template>\n  <div>\n    <h1>Company Roles</h1>\n    <p><b>President:</b> {{ presidentRef }}</p>\n    <button @click=\"changePresident\">Change President</button>\n  </div>\n</template>\n```\n\nIt can be challenging to remember which variables are `reactive` properties and which ones are `ref`. Something that helps is to use a naming convention where I add the suffix **Ref** to anything that is a `ref`, such as `presidentRef`. I don't have a lot of experience with Vue 3 yet, but for the time being, I plan to use that naming convention to see if it helps me get a better handle on the distinction between `ref` and `reactive` properties.\n\n### isRef\n\nVue 3 also provides the helper `isRef` that I can use to check if something is a `ref`.\n\n```js\nconsole.log(isRef(executiveTeam.president)) //false\nconsole.log(isRef(presidentRef)) //true\n```\n\n## My Thoughts on the Vue 3 Reactivity API\n\nThis topic of `ref` and `reactive` has been the most challenging for me in my goal to learn Vue 3. There is more nuance to how these helpers are used in practice, and it becomes too much for an introduction post to try to cover all the different situations where I might have to make informed decisions about using `ref` and/or `reactive` and all the other helpers.\n\nThe Vue.js team is aware that this is one of the challenges of Vue 3 - the question of when to use `ref` or `reactive` does not always receive a simple answer. In the Composition API RFC they state:\n\n> Understandably, users may get confused regarding which to use between ref and reactive. First thing to know is that you will need to understand both to efficiently make use of the Composition API. Using one exclusively will most likely lead to esoteric workarounds or reinvented wheels.\n\nI have come across many resources that suggest using just `ref` or just `reactive` to start. But I think it is worth the effort to learn the nuances of both. I agree with the Vue.js team: it's better to understand both `ref` and `reactive` if I'm going to use Vue 3 to its fullest potential. And that's what I plan to do.\n\nWhile using just `ref` for primitives and `reactive` for objects is one suggested approach (suggested by the Vue.js team [here](https://github.com/vuejs/composition-api-rfc/blob/master/index.md#ref-vs-reactive)), I would encourage you to dig deeper into the docs and resources out there to learn more about the Reactivity APIs. For a nuanced feature such as this, it's important to understand why certain approaches can be taken.\n\n## Conclusion\n\nPlease join me for my next post about reusability in Vue 3, including a discussion of composition functions (i.e. Vue composables). In my opinion, composables are the best thing about this new Composition API and they make it worth putting in the time to learn the harder concepts of Vue 3.\n\nPlease reach out on [Twitter](https://twitter.com/sandra_rodgers_) and let me know if you are enjoying this series on Vue 3.\n\n        ";
						}
						async function compiledContent$2U() {
							return load$2U().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2U() {
							return (await import('./chunks/index.598e8629.mjs'));
						}
						function Content$2U(...args) {
							return load$2U().then((m) => m.default(...args));
						}
						Content$2U.isAstroComponentFactory = true;
						function getHeadings$2U() {
							return load$2U().then((m) => m.metadata.headings);
						}
						function getHeaders$2U() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2U().then((m) => m.metadata.headings);
						}

const __vite_glob_0_92 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2U,
  file: file$2U,
  url: url$2U,
  rawContent: rawContent$2U,
  compiledContent: compiledContent$2U,
  default: load$2U,
  Content: Content$2U,
  getHeadings: getHeadings$2U,
  getHeaders: getHeaders$2U
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2T = {"title":"Diving Into Vue 3 - Reusability with Composables","description":"Put everything together that we've learned in the series, and then refactor it all to use composables.","date":"2022-02-25T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1645127714/blog/2022/02/diving-into-vue-3-reusability-with-composables/dive-into-vue-3%402x.jpg","authors":["sandra-rodgers"],"category":"tutorial","tags":["vuejs","javascript"],"seo":{"title":"Diving Into Vue 3 - Reusability with Composables","description":"Put everything together that we've learned in the series, and then refactor it all to use composables."},"shorturls":{"share":"https://dpgr.am/d629f17","twitter":"https://dpgr.am/89aab6c","linkedin":"https://dpgr.am/4f3ec42","reddit":"https://dpgr.am/aa1b88d","facebook":"https://dpgr.am/cc7baa5"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453993/blog/diving-into-vue-3-reusability-with-composables/ograph.png"}};
						const file$2T = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/diving-into-vue-3-reusability-with-composables/index.md";
						const url$2T = undefined;
						function rawContent$2T() {
							return "\n## Introduction\n\nThis is the fifth and final post of my 'Diving Into Vue 3' series. Today I will combine what I have learned so far with a demonstration of how to use the Composition API to take advantage of its biggest strength: writing reusable code. This post will:\n\n*   review everything I've learned so far by walking through how I build an **example component**, focusing on challenges of working with the DOM and using lifecycle methods.\n*   introduce how to use a **template ref** to keep track of an element in the DOM.\n*   demonstrate how to refactor the project to use **composition functions** (i.e. **composables**).\n\nDon't forget there are four previous posts in this series that might be useful to you:\n\n*   [Diving Into Vue 3 - Getting Started](https://blog.deepgram.com/diving-into-vue-3-getting-started/)\n*   [Diving Into Vue 3 - The Setup Function](https://blog.deepgram.com/diving-into-vue-3-setup-function/)\n*   [Diving Into Vue 3: Methods, Watch, and Computed](https://blog.deepgram.com/diving-into-vue-3-methods-watch-and-computed/)\n*   [Diving Into Vue 3: The Reactivity API](https://blog.deepgram.com/diving-into-vue-3-reactivity-api/)\n\nIf you don't need the walk-through for building the example project, feel free to jump to the section on [reusability](#reusability-in-the-composition-api), where I show how to refactor the project to use composables.\n\n## Example Component\n\nI am going to build a single-file component that has a mast with an image on the left and text on the right. The problem I need to address is that I want to change the size of the text based on the image being resized.\n\nHere's the demo:\n\n![Rendered component to that uses method to increment and decrement](https://res.cloudinary.com/deepgram/image/upload/v1645128984/blog/2022/02/diving-into-vue-3-reusability-with-composables/demo.gif)\n\nTo achieve this, I will:\n\n*   listen for resizing of the window with an event listener.\n*   track the image size.\n*   update the text size if the image gets to a certain size.\n\nThe repo to go along with this example can be found [here](https://github.com/deepgram-devs/vue-reusability-with-composables/tree/main/src). There are several branches to show the progression of how the project gets refactored.\n\n### Vue 2\n\nI won't go over how I built the project in Vue 2, but if it helps, the completed project in Vue 2 can be viewed [here](https://stackblitz.com/edit/vue2-vue-cli-vnp6kn?file=src/components/Mast.vue).\n\nResizing the window will show how the text size changes as the width of the image changes.\n\n### Vue 3\n\nHere's how to build the component in Vue 3. The html in the `template` is exactly the same as the Vue 2 project:\n\n```html\n<template>\n  <div class=\"mast\">\n    <div class=\"container\">\n      <div class=\"image-container\">\n        <img ref=\"imageRef\" src=\"../assets/meatball.jpeg\" />\n      </div>\n      <div ref=\"textRef\" class=\"text-container\">\n        <p>\n          Meatball, 9. Barks at Amazon guy. Likes sharing your apple slices.\n          Wants you to grab the toy but won't let you have it.\n        </p>\n      </div>\n    </div>\n  </div>\n</template>\n```\n\nIn the script section, I'll need to add the `setup` function, and then I will define the variables for the data I'll be tracking. Since elements in the DOM will depend on each other to either trigger a change or react to a change, I will need to make them reactive using `ref` so everything stays in sync. Here's how I do that:\n\n```js\n<script>\nimport { ref } from \"vue\";\nexport default {\n  name: \"Mast\",\n  setup() {\n    let imageWidth = ref(0);\n\n    //template refs\n    let imageRef = ref(null);\n    let textRef = ref(null);\n\n    return { imageRef, textRef };\n  },\n};\n</script>\n```\n\nThe important data to keep track of is the `imageWidth` because that value is what I will use to determine if the text size should change.\n\nThe `imageWidth` value has to come from the image element in the DOM. It will be based on the actual size of the image at a point in time, so I will need to connect to the actual DOM element using a [template ref](https://vuejs.org/guide/essentials/template-refs.html).\n\n#### Template Refs\n\nI think of template refs as the Vue way of using Javascript to hook into a DOM element, such as the method `document.getElementById()` or `document.querySelector()`.\n\nIn Vue 2, the way to do that is to add `ref=\"nameOfRef\"` as an attribute on the element that I am targeting, then in the script, I could perform some action on it using `this.$refs.nameOfRef`.\n\nIn Vue 3, template refs are now part of the reactive API. If I want to set up a template ref, I still need to add `ref=\"nameOfRef\"` as an attribute on the element that I want to hook into.\n\n```html\n<img ref=\"imageRef\" src=\"../assets/meatball.jpeg\" />\n```\n\nThe difference now is that in the script, I need to define the template ref as a reactive reference variable wrapped in `ref`. And I MUST return it in the `return` object of the `setup` function so that it connects to that DOM element in the template. If I don't, it won't work.\n\n```js\nsetup() {\n    //template refs\n    let imageRef = ref(null);\n    let textRef = ref(null);\n\n    return { imageRef, textRef };\n  },\n```\n\nAlso, I need to be aware that I won't be able to actually access the ref to do something with it until the component has mounted - which brings me to the next topic.\n\n#### Lifecycle Hooks\n\nNow that I have the data set up I can add the logic to listen for the resize event.\n\nI want to track the size of the image, which will change depending on if the window is resized. Since I'm dealing with a visual element, I need to consider timing of when that element will appear in the browser. It won't appear until the component has mounted.\n\nThe hooks that I'll need for setting up the event listener (and destroying it) are `onMounted` and `onUnmounted`, which are the equivalent to `mounted` and `unmounted` in Vue 2.\n\nIn `onMounted`, I have access to the template ref, so I will first set the initial value of the `imageWidth` based on the width of the actual image, which I pull from the template ref. I will also put a listener on the window to track the resizing event so that as the window is resized, the `resizeHandler` function runs.\n\nEverything currently resides in the setup function for now, but will be refactored later and moved into composables:\n\n```js\n// inside setup function:\n\nonMounted(() => {\n  //set initial value\n  imageWidth.value = imageRef.value.offsetWidth\n\n  //add listener to track resize\n  window.addEventListener('resize', resizeHandler)\n})\n```\n\nThe `resizeHandler` sets the `imageWidth` value to the `imageRef`'s width. I have to remember that with refs in the script, I have to unwrap the value using `.value`:\n\n```js\n// inside setup function:\n\nfunction resizeHandler() {\n  //tracking of width changes\n  imageWidth.value = imageRef.value.offsetWidth\n}\n```\n\nSince I'm listening for the resize event starting when the component mounts, I need to be sure to destroy the listener when the component unmounts:\n\n```js\n// inside setup function:\n\nonUnmounted(() => {\n  //remove listener\n  window.removeEventListener('resize', resizeHandler)\n})\n```\n\n#### watch\n\nI now have the data set up so that the `imageWidth` updates in-sync with the `imageRef`'s width as the event listener fires the `resizeHandler` function.\n\nThe last thing I need to do is make something happen as a side effect of the `imageWidth` increasing or decreasing. Vue offers `watch` and `watchEffect` as part of the API for watching a reactive property and causing a side effect to occur based on changes to the property.\n\nIn this case, I will use `watch` because I only need to track the `imageWidth` value since a change to `imageWidth` is what I'm using to cause the text size to change.\n\n```js\n// inside setup function:\n\nwatch(imageWidth, () => {\n  //initiate side effects to change text size when window width changes\n  if (imageWidth.value < 150) {\n    textRef.value.style.fontSize = '.8em'\n    textRef.value.style.lineHeight = '1.3'\n  }\n  if (imageWidth.value < 200 && imageWidth.value > 150) {\n    textRef.value.style.fontSize = '1em'\n    textRef.value.style.lineHeight = '1.4'\n  }\n  if (imageWidth.value > 200) {\n    textRef.value.style.fontSize = '1.3em'\n    textRef.value.style.lineHeight = '1.5'\n  }\n})\n```\n\nHere is the finished [example code](https://github.com/SandraRodgers/vue-reusability-with-composables/tree/no-composables/src) using Vue 3 (and before I refactor it to use composables). Now that everything is working, I will refactor my code to make it more reusable.\n\n## Reusability in The Composition API\n\nMany people would say that the biggest advantage of using Vue 3's Composition API is its emphasis on organizing code by logical concern rather than by option types like in Vue 2. If I'm building a small application that is only going to have minimal logic in a component, the Options API, or even just putting all my logic in the setup function, is fine. But as a component grows larger, it can be challenging to follow the data flow.\n\nFor example, a UI component such as a dropdown menu has to deal with opening and closing the dropdown, keyboard interactions, pulling in data to populate the menu, and more. All that logic in one component spread out among the options like `methods`, `watch`, `mounted`, etc., can be hard to decipher.\n\nVue 2 does offer approaches for separating out logic, such as **mixins** and **utility functions**. But Vue 3's whole philosophy is designed around the idea of writing code that is reusable, focused around logical concern, and easy to read. The most fundamental way it does this is through **composition functions** (i.e. **composables**).\n\n## Composables\n\nThe advantage of organizing code by logical concern encapsulated in a composable function is that it becomes easier to read, but it also becomes easier to reuse in other parts of the project or even in other projects.\n\nI feel that the ultimate goal should be to write the most agnostic code possible in a composable, i.e. code that can be recycled in different contexts and isn't so dependent on the one unique context it starts out in.\n\nIt does take time and practice to get better at this skill, but the good news is, Vue 3 is the perfect framework to work at it because using the Composition API really emphasizes this approach to coding.\n\nWith that in mind, I'll think about how I can refactor my project to take advantage of composables.\n\n### useWindowEvent\n\nA common situation is having to listen for an event on the window, such as a resize event. I see an opportunity to write a composable that can be reused when I want to add or destroy an event listener on the window.\n\nIn my project, in the `onMounted` hook I currently have:\n\n```js\nwindow.addEventListener('resize', resizeHandler)\n```\n\nAnd in the `unMounted` hook:\n\n```js\nwindow.removeEventListener('resize', resizeHandler)\n```\n\nI can create a composable function that accepts an event-type, a handler, and a string saying 'add' or 'destroy', and write logic that will set up the window event listener. I will put this file in a folder called `~/composables`. The Vue 3 convention is to name composable files with the prefix 'use' as in *useWindowEvent*.\n\nHere is the composable `useWindowEvent.js`:\n\n```js\nexport default function useWindowEvent(event, handler, addOrDestroy) {\n  if (addOrDestroy === 'add') {\n    window.addEventListener(event, handler)\n  }\n\n  if (addOrDestroy === 'destroy') {\n    window.removeEventListener(event, handler)\n  }\n}\n```\n\nNow in my project, I import it into the component where it will be used:\n\n```js\nimport useWindowEvent from '../composables/useWindowEvent'\n```\n\nThen I invoke the function with the arguments that I set it up to receive:\n\n```js\nuseWindowEvent('resize', resizeHandler, 'add')\n```\n\nThis is just a small composable, and it doesn't really make my life that much easier since I didn't have to write very much code anyways to set up the listener on the window.\n\nBut there is a significant advantage to creating reusable code. I know the composable is written to work, so I'm less likely to have little errors or typos since I'm reusing code that has been tested and used before. Because I've tested it, I can feel confident reusing it in many contexts.\n\nConsistency is another benefit. I keep functionality consistent by using the composable in multiple places, rather than having to reinvent the wheel every time, potentially introducing differences (and problems).\n\nAnd now that I have created a `useWindowEvent`, I could try to make it to work for all kinds of elements, not just the window. If I spend some time improving it so that it can add an event listener to any type of element, then I have a really useful composable that I can reuse.\n\n### useResizeText\n\nThe main feature of my project is that the text resizes based on the image element's width. I can turn this into a composable that can be reused in cases where I want text to resize based on some other element.\n\nIn my goal to write it in a way that is more agnostic, I can think of the element that is watched (the image) as the *trigger element*, and the element that changes (the text) as the *react element*. In the `resizeText` composable, I'll refer to them as the `triggerElement` and the `reactElement`, but in the `Mast.vue` component they are the `imageRef` and the `textRef`. These are more specific references to the context of my project, while `triggerElement` and `reactElement` are more general since I would like the composable to be reused if I ever need it in a different project.\n\nI create the composable file called `useResizeText.js`. I anticipate that I'll need to accept two arguments, the `triggerElement` and the `reactElement` (which come in from `Mast.vue` as the `imageRef` and the `textRef`):\n\n```js\n//useResizeText.js:\n\nexport default function useResizeText(triggerElement, reactElement) {\n  return { elementWidth }\n}\n```\n\nI've included the return object because any data from the composable that I want to make available in the component (or another file) must be included in it. I'll return the `elementWidth` to the component so I can put it in my template in `Mast.vue` and see the resize logic working in real-time.\n\nIn the `Mast.vue` component, I will call the composable. I have to send in the template refs so the composable can compute the text size based on those DOM elements. I will destructure the composable so that I get the returned `elementWidth`.\n\nInside `setup` in `Mast.vue`:\n\n```js\n//destructure to get data sent back from the composable\n//get updated width for template\nconst { elementWidth } = useResizeText(imageRef, textRef)\n```\n\nI will return `elementWidth` to the template so that I see that number reacting to the window resizing. I also return `imageRef` and `textRef` because that is required for the template refs to stay in-sync between the script and the template.\n\nHere is everything in the `setup` function:\n\n```js\nsetup() {\n    //template refs\n    let imageRef = ref(null);\n    let textRef = ref(null);\n    //destructure to get data sent back from the composable\n    //get updated width for template\n    const { elementWidth } = useResizeText(imageRef, textRef);\n    return { imageRef, textRef, elementWidth };\n  },\n\n```\n\nThe composable itself is mostly the same as it was when I wrote the logic in the setup function, with a few small updates.\n\nTo make sure I don't get an error when I set the `elementWidth` to the imageRef/triggerElement `offsetHeight` value, I use an 'if' statement to make sure the `triggerElement` exists:\n\n```js\nif (triggerElement.value) {\n  elementWidth.value = triggerElement.value.offsetWidth\n}\n```\n\nI also set the initial text styles as soon as the component mounts and then run that `setTextStyles` function again inside the watch every time the `elementWidth` (the image's width) changes.\n\nHere is the full code for the `resizeText.js` composable:\n\n```js\nimport { ref, watch, onMounted, onUnmounted } from 'vue'\nimport useWindowEvent from './useWindowEvent'\n\nexport default function useResize(triggerElement, reactElement) {\n  let elementWidth = ref(0)\n\n  //handler to send into useWindowEvent\n  function resizeHandler() {\n    if (triggerElement.value) {\n      elementWidth.value = triggerElement.value.offsetWidth\n    }\n  }\n\n  //set initial values for elementWidth and text styles\n  onMounted(() => {\n    if (triggerElement.value) {\n      elementWidth.value = triggerElement.value.offsetWidth\n      setTextStyles()\n    }\n  })\n\n  //function to set text styles on mount and in watcher\n  function setTextStyles() {\n    if (elementWidth.value < 150) {\n      reactElement.value.style.fontSize = '.8em'\n      reactElement.value.style.lineHeight = '1.3'\n    }\n    if (elementWidth.value < 200 && elementWidth.value > 150) {\n      reactElement.value.style.fontSize = '1em'\n      reactElement.value.style.lineHeight = '1.4'\n    }\n    if (elementWidth.value > 200) {\n      reactElement.value.style.fontSize = '1.3em'\n      reactElement.value.style.lineHeight = '1.5'\n    }\n  }\n\n  //add and destroy event listeners\n  useWindowEvent('resize', resizeHandler, 'add')\n  onUnmounted(() => {\n    useWindowEvent('resize', resizeHandler, 'destroy')\n  })\n\n  //watch elementWidth and set text styles\n  watch(elementWidth, () => {\n    setTextStyles()\n  })\n\n  return { elementWidth }\n}\n```\n\nThis [refactoring](https://github.com/SandraRodgers/vue-reusability-with-composables/tree/composables-first-refactor/src) makes `Mast.vue` much easier to read because the logic for resizing the text and for adding a window event listener are separated out into composables.\n\nHowever, my ultimate goal is to make composables that are more reusable in general. There is more I can do to make the `resizeText` composable reusable in other projects.\n\nFor example, I could set it up to take a breakpoints object, so that I don't have to always use the same hardcoded width sizes to influence the text.\n\nI could also rework it accept a styles object for the text styles so that I'm not required to use the same hardcoded values for text styles for any component that uses the composable. Something like this in the component:\n\n```js\n//constants\nconst breakPoints = { small: '100', medium: '150', large: '200' }\nconst textStyles = {\n  fontSize: { small: '.8em', medium: '1em', large: '1.3em' },\n  lineHeight: { small: '1.3', medium: '1.4', large: '1.5' },\n}\n```\n\nHere is the [full example](https://github.com/SandraRodgers/vue-reusability-with-composables/tree/composables-second-refactor/src).\n\nThere are still many ways to improve this composable to make it more agnostic, but this gives a general idea of the process that goes into making a composable more reusable.\n\n## Conclusion\n\nThis concludes my series on Diving into Vue 3. I have learned the fundamentals that will allow me to jump into building projects using the Composition API. I feel so much more confident in Vue 3 now, and I'm also really excited about it.\n\nI hope you have enjoyed this series. There is always more to learn, so stay tuned for future posts about Vue topics.\n\nQuestions? Comments? Just want to say hi? You can find me on [Twitter](https://twitter.com/sandra_rodgers_)!\n\n        ";
						}
						async function compiledContent$2T() {
							return load$2T().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2T() {
							return (await import('./chunks/index.defbd073.mjs'));
						}
						function Content$2T(...args) {
							return load$2T().then((m) => m.default(...args));
						}
						Content$2T.isAstroComponentFactory = true;
						function getHeadings$2T() {
							return load$2T().then((m) => m.metadata.headings);
						}
						function getHeaders$2T() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2T().then((m) => m.metadata.headings);
						}

const __vite_glob_0_93 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2T,
  file: file$2T,
  url: url$2T,
  rawContent: rawContent$2T,
  compiledContent: compiledContent$2T,
  default: load$2T,
  Content: Content$2T,
  getHeadings: getHeadings$2T,
  getHeaders: getHeaders$2T
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2S = {"title":"Diving Into Vue 3 - The Setup Function","description":"Learn about the new Vue 3 setup function and the Composition API","date":"2022-02-04T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1643314554/blog/2022/02/diving-into-vue-3-setup-function/dive-into-vue-3%402x.jpg","authors":["sandra-rodgers"],"category":"tutorial","tags":["vuejs","javascript"],"seo":{"title":"Diving Into Vue 3 - The Setup Function","description":"Learn about the new Vue 3 setup function and the Composition API"},"shorturls":{"share":"https://dpgr.am/ac518b9","twitter":"https://dpgr.am/3a56d0b","linkedin":"https://dpgr.am/931f9a0","reddit":"https://dpgr.am/736169b","facebook":"https://dpgr.am/dc79347"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453994/blog/diving-into-vue-3-setup-function/ograph.png"}};
						const file$2S = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/diving-into-vue-3-setup-function/index.md";
						const url$2S = undefined;
						function rawContent$2S() {
							return "\nIn this series of posts, I'm diving into Vue 3 as someone who is new to the Composition API but not new to Vue in general. I'm writing my impressions of how to get started with Vue 3 and comparing it to the way I would do things in Vue 2.\n\nToday, I'll take a look at the `setup` function that is new to Vue 3.\n\nAnd if you want to start at the beginning of the series, take a look at my first post: [Diving into Vue 3: Getting Started](https://blog.deepgram.com/diving-into-vue-3-getting-started/).\n\n## The Old Way: Vue 2 Options API\n\nThe Options API is familiar to those of us who have used Vue 2. A single-file-component includes a `template`, `script`, and `style` section, and in the script section, we would use the **options** of the Options API, organized something like this:\n\n```html\n<script>\n  export default {\n    name: 'ComponentName',\n    components: {},\n    props: {},\n    data() {},\n    watch: {},\n    methods: {},\n    mounted() {},\n  }\n</script>\n```\n\nThe Options API is all the properties and methods we get on our Vue instance, the instance we initialize when we set up the project in the **main.js** file (see my [previous post](https://blog.deepgram.com/diving-into-vue-3-getting-started/) for more info on initializing the Vue application object).\n\nThis organization of a component seems very easy to read at first glance, and it is one of the things that made me feel unafraid to jump into Vue initially. We clearly see how the different logic is separated by its type - `methods` versus `computed` versus `watch`.\n\nHowever, after becoming more experienced in using Vue and having worked with really large components, this actually starts to feel like a shortcoming because it forces us to jump around so much to follow the data as it moves through logic.\n\n## New and Improved Way: Vue 3 Composition API\n\nThe Composition API is born from experience - the experience of struggling to keep track of logic jumping around within a Vue component, from `data` to `methods` to `watch` to `methods` again back to `watch` and so on...\n\nAnd when we add a **mixin** to the mix (pun intended), jumping to a completely different file to follow the logic can be a huge headache since pieces of logic in the mixin can affect pieces of logic in the main component (rather than the mixin containing all the logic for a feature).\n\nThe better way is not to separate the script into sections by options, but instead to organize by *logical concern for individual features*. The `setup` function lets us do that because we can write the logic for each feature, group it under a function that we name, then invoke all the functions within the scope of the setup function.\n\nThis improvement has resulted in a new pattern called Vue 'composables', which is just this idea I described in the previous paragraph - grouping code by logical concern and making it into a reusable function. We'll learn more about composables in a later post, but the important thing to understand now is that the `setup` function is what makes it possible.\n\n## How to Use the Setup Function\n\nNow we'll get into the nitty-gritty of how to use the `setup` function.\n\nTechnically, the `setup` function is another **option** you can use in the **Options API** since it can be added alongside the list of other option properties and lifecycle methods, like this:\n\n```html\n<script>\n  export default {\n    name: 'ComponentName',\n    components: {},\n    data() {},\n    methods: {},\n    mounted() {},\n    setup() {},\n  }\n</script>\n```\n\nHowever, since we are choosing to do things the Vue 3 way, we'll probably just want to dive in completely and use the `setup` function without all the other options since what we get with `setup` will make it unnecessary to use them.\n\n### Example Using Setup\n\nHere is a very simple example that shows the most basic thing we need to do in Vue - create a variable to represent some data. This is the basic template where we have a data value for a number of employees.\n\n```html\n<template>\n  <div>\n    <h1>Basic Component</h1>\n    <p>Employees: {{ numEmployees }}</p>\n  </div>\n</template>\n```\n\nAnd here is what it would render as:\n\n![Basic component](https://res.cloudinary.com/deepgram/image/upload/v1642110686/blog/2022/02/diving-into-vue-3-setup-function/basic-component.png)\n\n`numEmployees` represents a number of people who work for a company. In Vue 2, in the script section, we would have created a data property using the `data()` option, like this:\n\n#### Vue 2\n\n```html\n<script>\n  export default {\n    data() {\n      return {\n        numEmployees: 0,\n      }\n    },\n  }\n</script>\n```\n\nIn Vue 3, we will just create our variable inside the `setup` function, like this:\n\n#### Vue 3\n\n```html\n<script>\n  export default {\n    setup() {\n      const numEmployees = 10\n      return { numEmployees }\n    },\n  }\n</script>\n```\n\nHowever, if we want that data property for numEmployees to be **reactive** (so it updates in the template when it changes in script), we have to use `ref()` around the value, and we have to import ref in order to use it:\n\n```js\n<script>\nimport { ref } from \"vue\";\nexport default {\n  name: \"BasicComponent\",\n  setup() {\n    const numEmployees = ref(10);\n    return { numEmployees };\n  },\n};\n</script>\n```\n\nWe also have to `return` an object with that data value; otherwise, it will not be available in the template.\n\n### Important Facts about the Setup Function\n\n1.  `setup` is a function, and it is also referred to as a 'hook' in a general sense because it is similar to the lifecycle methods in that *timing is important*. `Setup` runs before everything else - before all the lifecycle methods and the mounting of the component (although **not before props are resolved,** so you **will have access to props** in the setup function).\n\n2.  A big difference from Vue 2 is that we won't be seeing the keyword `this` all over the place to reference data values inside a component. In Vue 3, `this` in the way it was used in the Options API is not available in the `setup` function since `setup` runs so early.\n\n3.  `setup` must return an object. The object contains everything from within the scope of the setup function that you want to make available in the template.\n\n4.  Variables aren't reactive in the `setup` function unless you use `ref` with them (or `reactive`, but for now, we only need to concern ourselves with `ref`. Stay tuned for a post on `ref` and `reactive` in the near future).\n\n5.  `setup` can take two arguments - `props` and `context` - which we'll take a look at more closely in the next section.\n\n### Props and Context\n\n`setup` can take two arguments, `props` and `context`.\n\n#### Props\n\nIn this example a prop `message` has been added that wasn't there before. The prop is just a simple string. It's passed down the same way as in Vue 2, as we can see in the example:\n\n#### Parent Component\n\n```html\n<template>\n  <basic-component :message=\"message\" />\n</template>\n```\n\n#### Child Component\n\n```html\n<script>\n  import { ref } from 'vue'\n  export default {\n    name: 'BasicComponent',\n    props: {\n      message: String,\n    },\n    setup(props) {\n      console.log(props.message)\n\n      const numEmployees = ref(10)\n\n      return { numEmployees }\n    },\n  }\n</script>\n```\n\nThe `setup` function must have that `props` argument if we want to have access to the prop inside the function. I can `console.log` it to see the prop value:\n\n![Console-logged prop value](https://res.cloudinary.com/deepgram/image/upload/v1642110686/blog/2022/02/diving-into-vue-3-setup-function/console-prop-value.png)\n\nIn the template, we'll have it display like this. It is the same way we would do it in Vue 2:\n\n```html\n<template>\n  <div id=\"basic\">\n    <h1>Basic Component</h1>\n    <p>Employees: {{ numEmployees }}</p>\n    <div>{{ message }}</div>\n  </div>\n</template>\n```\n\n![Basic Component with prop](https://res.cloudinary.com/deepgram/image/upload/v1642110686/blog/2022/02/diving-into-vue-3-setup-function/basic-component-with-prop.png)\n\nIf we log `props` by itself to the console, like this:\n\n```js\nsetup(props) {\n  console.log(props);\n},\n```\n\nThen we see the props object, which looks like this:\n\n![Console-logged props object](https://res.cloudinary.com/deepgram/image/upload/v1642110686/blog/2022/02/diving-into-vue-3-setup-function/console-prop-object.png)\n\nThe object uses a `Proxy`, which is the new way that Vue 3 does reactivity (the details of what that is go beyond the scope of this post). Because props are reactive, they cannot easily be destructured in the setup function. If you want to understand more about that, [the docs explain it](https://v3.vuejs.org/guide/composition-api-setup.html#props). I don't find it necessary to destructure them, however (but I'm still very new to using Vue 3).\n\n#### Context\n\nThe second argument, `context`, gives us access to three properties that we had available in Vue 2 using the keyword this:\n\n*   `attrs` - (formerly `this.$attrs`) - An object containing the component's attributes\n\n*   `emit` - (formerly `this.$emit`) - A function that takes the event as argument\n\n*   `slots` - (formerly `this.$slots)` - An object containing the component's slots\n\nIf we only want to use one of these in the `setup` function, we can destructure the argument like this:\n\n```js\nexport default {\n  setup(props, { attrs }) {\n    console.log(attrs)\n  },\n}\n```\n\nThere is also another property, `expose`, which is useful in special cases. The [docs](https://v3.vuejs.org/guide/composition-api-setup.html#usage-with-templates) go over the example of using it when returning render functions from the `setup` function. That's a bit advanced for this series. If you have used `expose`, I would be interested to hear from you because I haven't used it myself!\n\n## Conclusion\n\nAnd that wraps up this post about the `setup` function. In the next post, we will be looking at `methods` and `computed` to see how we use those in Vue 3. And as always, feel free to reach out on [Twitter](https://twitter.com/sandra_rodgers_)!\n\n        ";
						}
						async function compiledContent$2S() {
							return load$2S().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2S() {
							return (await import('./chunks/index.aeb0a7b1.mjs'));
						}
						function Content$2S(...args) {
							return load$2S().then((m) => m.default(...args));
						}
						Content$2S.isAstroComponentFactory = true;
						function getHeadings$2S() {
							return load$2S().then((m) => m.metadata.headings);
						}
						function getHeaders$2S() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2S().then((m) => m.metadata.headings);
						}

const __vite_glob_0_94 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2S,
  file: file$2S,
  url: url$2S,
  rawContent: rawContent$2S,
  compiledContent: compiledContent$2S,
  default: load$2S,
  Content: Content$2S,
  getHeadings: getHeadings$2S,
  getHeaders: getHeaders$2S
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2R = {"title":"Do Your Call Transcripts Read Like Mad Libs?","description":"Find out how you can get transcripts that are readable by humans.","date":"2018-12-13T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1662069472/blog/do-your-call-transcripts-read-like-mad-libs/placeholder-post-image%402x.jpg","authors":["morris-gevirtz"],"category":"ai-and-engineering","tags":["machine-learning","voice-tech"],"seo":{"title":"Do Your Call Transcripts Read Like Mad Libs?","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1662069472/blog/do-your-call-transcripts-read-like-mad-libs/placeholder-post-image%402x.jpg"},"shorturls":{"share":"https://dpgr.am/930f41a","twitter":"https://dpgr.am/9c6307a","linkedin":"https://dpgr.am/db80e3a","reddit":"https://dpgr.am/b091cde","facebook":"https://dpgr.am/3fba72a"}};
						const file$2R = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/do-your-call-transcripts-read-like-mad-libs/index.md";
						const url$2R = undefined;
						function rawContent$2R() {
							return "<iframe src=\"https://www.youtube.com/embed/Hh9Og9reNBs\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\nWhen the speech recognition API you use fails to recognize the words that matter to you, it produces absurd sounding and often useless transcripts. Such annoying (though sometimes funny) errors can be avoided by using a custom speech recognition model designed to understand your particular call data. [Accurate, custom speech recognition APIs](https://developers.deepgram.com/documentation/) can be the key to leveraging the full potential of your voice data.\n\n### About Mad Libs\n\n[Mad Libs](https://en.wikipedia.org/wiki/Mad_Libs) is a game created by Leonard Stern and Roger Price in 1952 and released in 1958. The premise is that players are asked to supply nouns, verbs, adjectives at random which are then filled into blanks. Upon reading, while the words make sense from a grammatical point-of-view, the stories are nonsense.";
						}
						async function compiledContent$2R() {
							return load$2R().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2R() {
							return (await import('./chunks/index.d8699b92.mjs'));
						}
						function Content$2R(...args) {
							return load$2R().then((m) => m.default(...args));
						}
						Content$2R.isAstroComponentFactory = true;
						function getHeadings$2R() {
							return load$2R().then((m) => m.metadata.headings);
						}
						function getHeaders$2R() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2R().then((m) => m.metadata.headings);
						}

const __vite_glob_0_95 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2R,
  file: file$2R,
  url: url$2R,
  rawContent: rawContent$2R,
  compiledContent: compiledContent$2R,
  default: load$2R,
  Content: Content$2R,
  getHeadings: getHeadings$2R,
  getHeaders: getHeaders$2R
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2Q = {"title":"Does Unsupervised Learning Create Superior Speech Recognition?","description":"Learn about the three major AI speech model training methods used and which method yields more accurate results.","date":"2021-06-02T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981373/blog/does-unsupervised-learning-create-superior-speech-recognition/does-unsupervised-learning-create%402x.jpg","authors":["katie-byrne"],"category":"ai-and-engineering","tags":["speech-models","deep-learning","machine-learning"],"seo":{"title":"Does unsupervised learning create superior speech recognition?","description":"Learn about the three major AI speech model training methods used and which method yields more accurate results."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981373/blog/does-unsupervised-learning-create-superior-speech-recognition/does-unsupervised-learning-create%402x.jpg"},"shorturls":{"share":"https://dpgr.am/cf7c30a","twitter":"https://dpgr.am/e74f6e7","linkedin":"https://dpgr.am/aec3d90","reddit":"https://dpgr.am/67b5505","facebook":"https://dpgr.am/4ab06b0"}};
						const file$2Q = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/does-unsupervised-learning-create-superior-speech-recognition/index.md";
						const url$2Q = undefined;
						function rawContent$2Q() {
							return "\r\nRecently there have been a number of articles published around the use of unsupervised learning for speech recognition. We asked members of our research team for their take on this type of training, and if it yields more accurate results. The short answer is No. Read further to learn why. \r\n\r\n### **AI Training Methods for Speech Recognition**\r\n\r\nArtificial intelligence (AI) and neural networks \"learn\" by three general methods.  As it relates to speech recognition, speech model improvement or accuracy improvement is accomplished by the following training:\r\n\r\n*   **Supervised** learning is when you provide your model with labeled training data sets.  Labeled training datasets comprise of both the audio AND the ground truth transcript; i.e. human transcribed text. This tells your model what features of the audio are important to learn; i.e. this audio waveform is this part of this word.  For humans, this is the same as telling a baby that the spherical object is a ball.\r\n*   **Self-supervised or Unsupervised** is when you provide your model with audio and text files that are not paired. The model then \"extracts\" information from the audio or text and clusters the data into groups. This is effectively the same approach our human brains make by pattern matching.  The model is able to group a bunch of features but still has no idea what they relate to.  For humans, this is the same as your baby determining that all spherical objects are the same but the baby does not know they are balls.\r\n*   **Semi-supervised** is a combination of supervised and unsupervised training. One approach is to perform pseudo-labeling where your model generates the labels for your unlabeled training data; e.g This phone call has a negative, neutral or positive sentiment. This is helpful when you have a vast amount of data to label. Then this labeling is validated by human transcriptionists.  If we continue our baby example, you have told the baby that the spherical object is a ball, now the baby believes all spherical objects are balls, including oranges, cocoa puffs and ladybugs. An adult then needs to help the baby and correct their misconceptions.\r\n\r\n### **Speech Recognition in the Wild**\r\n\r\nTo create speech models that can survive in the wild (eg. make accurate predictions on datasets they have never encountered before) you need to have a combination of supervised and unsupervised learning techniques. Without these two approaches, models can \"overfit\" or become biased, making inaccurate predictions. The wide variety of audio in a business setting makes the task of speech recognition extremely difficult as uncommon words, phrases and acronyms are used across companies. When you graduate from 30 second command and control phrases (e.g. Hey Google, what time is it?) to 30 minute phone calls about \"fixing your 'spotty wifi' (Spotify) premium account service,\" training on labeled datasets you care about becomes essential to model success. \r\n\r\n### **Great Speech Recognition is Hard**\r\n\r\nThere isn't a shortcut to enterprise grade speech recognition. Having a large volume of labeled training data (audio and text) is critical to AI model success. This is why Deepgram has invested heavily not only in our [End to End Deep Neural Network](https://offers.deepgram.com/how-deepgram-works-whitepaper), [model training](https://deepgram.com/product/train/) and [API deployment capabilities](https://developers.deepgram.com/), but also focused on [data collection and labeling](https://deepgram.com/product/label/). To put this in context, we have already labeled over 200K+ hours of business audio data. That is 3x Facebook, Microsoft and 22x that of Mozilla's Common Voice public data set ([source](https://venturebeat.com/2021/05/13/soniox-taps-unsupervised-learning-to-build-speech-recognition-systems/)). Supervised, unsupervised and semi-supervised training techniques are constantly improving. At Deepgram we will continue to invest in our speech training methodologies as we know an incremental improvement to our Deep Neural Network can have an outsized impact on our customers' model performance. [Contact us](https://deepgram.com/contact-us/) to learn more about our [AI Speech Platform](https://deepgram.com/product/overview/) or check out our [careers page](https://deepgram.com/company/careers/) if you are interested in joining the team.\r\n";
						}
						async function compiledContent$2Q() {
							return load$2Q().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2Q() {
							return (await import('./chunks/index.9e44a129.mjs'));
						}
						function Content$2Q(...args) {
							return load$2Q().then((m) => m.default(...args));
						}
						Content$2Q.isAstroComponentFactory = true;
						function getHeadings$2Q() {
							return load$2Q().then((m) => m.metadata.headings);
						}
						function getHeaders$2Q() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2Q().then((m) => m.metadata.headings);
						}

const __vite_glob_0_96 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2Q,
  file: file$2Q,
  url: url$2Q,
  rawContent: rawContent$2Q,
  compiledContent: compiledContent$2Q,
  default: load$2Q,
  Content: Content$2Q,
  getHeadings: getHeadings$2Q,
  getHeaders: getHeaders$2Q
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2P = {"title":"Transcribing Podcast Feeds From Your Terminal","description":"Using some terminal magic, learn how to fetch and transcribe podcast episodes. This is a beginner-friendly guide going through every step.","date":"2022-08-25T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661184408/blog/2022/08/downloading-podcast-transcripts-from-terminal/2208-Transcribing-Podcast-Feeds-From-Your-Terminal-blog%402x.jpg","authors":["kevin-lewis"],"category":"tutorial","tags":["terminal","podcast"],"seo":{"title":"Transcribing Podcast Feeds From Your Terminal","description":"Using some terminal magic, learn how to fetch and transcribe podcast episodes. This is a beginner-friendly guide going through every step."},"shorturls":{"share":"https://dpgr.am/88664b0","twitter":"https://dpgr.am/9cbc75e","linkedin":"https://dpgr.am/361b1a4","reddit":"https://dpgr.am/afd4fa0","facebook":"https://dpgr.am/1f11fd0"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661541398/blog/downloading-podcast-transcripts-from-terminal/ograph.png"}};
						const file$2P = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/downloading-podcast-transcripts-from-terminal/index.md";
						const url$2P = undefined;
						function rawContent$2P() {
							return "\r\nEvery true podcast has a free and publicly available RSS feed that contains information about the show and each episode. In turn, those episode items include metadata about the show and a link to a hosted audio file. In this tutorial, we will download transcripts for the latest episodes of our favorite shows and store them in text files on our computer.\r\n\r\n## Before You Start\r\n\r\nYou will need a Deepgram API Key - [get one here](https://console.deepgram.com/signup?jump=keys). You will also need to install [jq](https://stedolan.github.io/jq/) and  [yq](https://kislyuk.github.io/yq/) to traverse and manipulate XML in your terminal (the data format used for RSS feeds).\r\n\r\nThis tutorial will be a set of building blocks, slowly growing in complexity towards our end goal. We'll take it slow and explain each step so you can apply this knowledge in other contexts, too.\r\n\r\nWe'll use the [NPR Morning Edition](https://www.npr.org/programs/morning-edition/) Podcast Feed: https://feeds.npr.org/510318/podcast.xml, but this can be swapped out for your favorite podcast.\r\n\r\n## Getting Started\r\n\r\nOpen up your terminal and run the following:\r\n\r\n```bash\r\ncurl https://feeds.npr.org/510318/podcast.xml\r\n```\r\n\r\nThis should display the full RSS feed - a bunch of XML (similar to HTML) containing information about the feed.\r\n\r\n### Get Just The Episode Items\r\n\r\nThe structure of the XML includes an `rss` tag containing a `channel` tag. Inside of `channel` is a whole bunch of metadata tags for the show and a set of `item` tags for each episode. `item` tags are not inside of a containing list as we might expect with HTML - they are all direct children of `channel`. Try running the following command:\r\n\r\n```bash\r\ncurl https://feeds.npr.org/510318/podcast.xml | xq '.rss.channel.item[]'\r\n```\r\n\r\nThis pipes the curl output into the `xq` command and extracts all of the `item` tags. It also pretty prints it in the terminal, which I find quite helpful when exploring the data. What is after the `xq` command in quotes is known as the 'expression.'\r\n\r\n![Terminal showing pretty-printed item data](https://res.cloudinary.com/deepgram/image/upload/v1658494374/blog/2022/08/downloading-podcast-transcripts-from-terminal/extract-items.png)\r\n\r\n### Get Specific Items\r\n\r\nWe can specify an index position in the square brackets to extract specific items. This will return only the first (latest) `item`:\r\n\r\n```bash\r\ncurl https://feeds.npr.org/510318/podcast.xml | xq '.rss.channel.item[0]'\r\n```\r\n\r\nWe can also slice the results and list the items with the first `n` items. This will return only the first three items:\r\n\r\n```bash\r\ncurl https://feeds.npr.org/510318/podcast.xml | xq '.rss.channel.item[:3]'\r\n```\r\n\r\nImportant note - this returns an array (items surrounded in `[]`) while before, it was just several objects being printed to the terminal. To turn this back into a set of objects we can further manipulate, append `[]` to the command:\r\n\r\n```bash\r\ncurl https://feeds.npr.org/510318/podcast.xml | xq '.rss.channel.item[:3][]'\r\n```\r\n\r\n![Showing the difference between the two commands above](https://res.cloudinary.com/deepgram/image/upload/v1658494371/blog/2022/08/downloading-podcast-transcripts-from-terminal/arrays-vs-object-list.png)\r\n\r\n## Displaying Specific Properties\r\n\r\nEven once you extract a list of items, we can extract just a single property by continuing to use the dot syntax:\r\n\r\n```bash\r\ncurl https://feeds.npr.org/510318/podcast.xml | xq '.rss.channel.item[:3][].title'\r\n```\r\n\r\nIf we want to extract a single property from an array of objects, we can use `map`:\r\n\r\n```bash\r\ncurl https://feeds.npr.org/510318/podcast.xml | xq '.rss.channel.item[:3] | map(.title)'\r\n```\r\n\r\n![The terminal showing an array with three strings](https://res.cloudinary.com/deepgram/image/upload/v1658494371/blog/2022/08/downloading-podcast-transcripts-from-terminal/mapped-array-one-key.png)\r\n\r\nAs opposed to JSON documents, XML also has attributes (like HTML). To access these, we use the following syntax:\r\n\r\n```bash\r\ncurl https://feeds.npr.org/510318/podcast.xml | xq '.rss.channel.item[:3] | map(.enclosure.\"@url\")'\r\n```\r\n\r\nWant to create a new data structure? Here we create an object with just the `title` and `url`:\r\n\r\n```bash\r\ncurl https://feeds.npr.org/510318/podcast.xml | xq '.rss.channel.item[:3] | map({ title: .title, url: .enclosure.\"@url\" })'\r\n```\r\n\r\n![Terminal showing an array of three objects - each with a title and url](https://res.cloudinary.com/deepgram/image/upload/v1658494375/blog/2022/08/downloading-podcast-transcripts-from-terminal/new-structure.png)\r\n\r\n## Looping Through Objects\r\n\r\nObjects don't really exist in BASH - so looping through them and extracting values can be a bit tough. Thankfully, a working approach is presented by Start & Wayne's [Ruben Koster](https://www.starkandwayne.com/blog/bash-for-loop-over-json-array-using-jq/). Let's walk through it.\r\n\r\nFirstly, store the output from the previous step in a variable:\r\n\r\n```bash\r\nDATA=$(curl https://feeds.npr.org/510318/podcast.xml | xq '.rss.channel.item[:3] | map({ title: .title, url: .enclosure.\"@url\" })')\r\n```\r\n\r\nThis can now be addressed in your terminal as `$DATA`:\r\n\r\n```bash\r\necho $DATA\r\n# Array of objects with title and url will show here\r\n```\r\n\r\nIf you try and loop through this data, you'll notice something undesirable:\r\n\r\n![Every log is a string](https://res.cloudinary.com/deepgram/image/upload/v1658494373/blog/2022/08/downloading-podcast-transcripts-from-terminal/every-log-is-a-string.png)\r\n\r\nIf the whole payload is thought of as a string, this is looping through each word. This isn't what we want. The solution is to base64-encode the data, so it's only one string, then decode it in the loop with a helper function:\r\n\r\n```bash\r\nfor row in $(echo \"${DATA}\" | jq -r '.[] | @base64'); do\r\n    _jq() {\r\n        echo ${row} | base64 --decode | jq -r ${1}\r\n    }\r\n    url=$(_jq '.url')\r\n    title=$(_jq '.title')\r\n\r\n    echo $url, $title\r\ndone\r\n```\r\n\r\n## Transcribing Each Episode\r\n\r\nNow that each podcast item is available in a loop, with both the `url` and `title` properties individually addressable, we can generate a transcript using cURL. We go through it in more detail in our [recent blog post](https://blog.deepgram.com/saving-transcripts-from-terminal/).\r\n\r\nMake sure you replace `YOUR_DEEPGRAM_API_KEY` with your own Deepgram API Key.\r\n\r\n```bash\r\nDATA=$(curl https://feeds.npr.org/510318/podcast.xml | xq '.rss.channel.item[:3] | map({ title: .title, url: .enclosure.\"@url\" })')\r\nfor row in $(echo \"${DATA}\" | jq -r '.[] | @base64'); do\r\n    _jq() {\r\n        echo ${row} | base64 --decode | jq -r ${1}\r\n    }\r\n    RESPONSE=$(\r\n        curl -X POST \"https://api.deepgram.com/v1/listen?punctuate=true&tier=enhanced\" \\\r\n            -H \"Authorization: Token YOUR_DEEPGRAM_API_KEY\" \\\r\n            -H \"Content-Type: application/json\" \\\r\n            -d \"{\\\"url\\\":\\\"$(_jq '.url')\\\"}\"\r\n   )\r\n   echo $RESPONSE | jq '.results.channels[0].alternatives[0].transcript' > \"$(_jq '.title').txt\"\r\ndone\r\n```\r\n\r\nThis will create one text file for each episode.\r\n\r\n## Wrapping Up\r\n\r\n`jq` and `xq` are exceptionally powerful tools, even more so when combined with cURL requests. With minimal adjustment, you can begin to alter the podcast fetched, the number of transcripts generated, or include additional metadata about the episode in the generated file.\r\n\r\nIf you have any questions, feel free to reach out - we love to help!\r\n\r\n        ";
						}
						async function compiledContent$2P() {
							return load$2P().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2P() {
							return (await import('./chunks/index.9a507d17.mjs'));
						}
						function Content$2P(...args) {
							return load$2P().then((m) => m.default(...args));
						}
						Content$2P.isAstroComponentFactory = true;
						function getHeadings$2P() {
							return load$2P().then((m) => m.metadata.headings);
						}
						function getHeaders$2P() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2P().then((m) => m.metadata.headings);
						}

const __vite_glob_0_97 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2P,
  file: file$2P,
  url: url$2P,
  rawContent: rawContent$2P,
  compiledContent: compiledContent$2P,
  default: load$2P,
  Content: Content$2P,
  getHeadings: getHeadings$2P,
  getHeaders: getHeaders$2P
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2O = {"title":"Use Your Voice to Draw with ARTiculate","description":"The team behind ARTiculate created a React and P5.js application for voice-based drawing to increase access to creative expression. Learn more here.","date":"2022-03-24T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1646222951/blog/2022/03/draw-with-your-voice-articulate/cover.jpg","authors":["kevin-lewis"],"category":"project-showcase","tags":["hackathon","accessibility"],"seo":{"title":"Use Your Voice to Draw with ARTiculate","description":"The team behind ARTiculate created a React and P5.js application for voice-based drawing to increase access to creative expression. Learn more here."},"shorturls":{"share":"https://dpgr.am/10d75a8","twitter":"https://dpgr.am/934ac77","linkedin":"https://dpgr.am/a928f85","reddit":"https://dpgr.am/e27577b","facebook":"https://dpgr.am/8e02298"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454024/blog/draw-with-your-voice-articulate/ograph.png"}};
						const file$2O = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/draw-with-your-voice-articulate/index.md";
						const url$2O = undefined;
						function rawContent$2O() {
							return "\r\nThe team behind ARTiculate wanted to increase access to artistic expression for people who can't use traditional input devices. I sat down with [Dan Gooding](https://github.com/DanGooding), [Max McGuinness](https://github.com/mgm52), [Tatiana Sedelnikov](https://github.com/tatiana-s), and [Yi Chen Hock](https://github.com/yichenhock) to ask them about their project.\r\n\r\nSince the pandemic began, drawing games and applications focused on creative expression have taken off as a way of connecting to people. However, these experiences rarely consider users with disabilities or provide a second-class experience.\r\n\r\nThe team explained, \"It was apparent to us that in the domain of speech recognition for accessibility, there are many applications for practical matters like word processing and filling out forms, but far fewer for expressing yourself and creating art. We believe that this is an unfortunate missed opportunity and one we wanted to address.\"\r\n\r\nYi Chen found [a paper from researchers at the University of Washington detailing VoiceDraw](https://faculty.washington.edu/wobbrock/pubs/assets-07.03.pdf) - a drawing application for people with motor impairments. VoiceDraw uses sounds to control the experience, such as vowels for joysticks. It is complex, powerful, but hard to learn. And, with the inspiration to speak commands to improve the learning curve, ARTiculate was born.\r\n\r\n## The Project\r\n\r\nARTiculate introduces a hands-free drawing experience. Commands like \"bold,\" \"down,\" and \"go\" control the brush. Also baked into the project are making the most of a new input modality, advanced features include voice-controlled color mixing, \"shortcuts\" to jump between bounded regions of the painting, and a velocity-acceleration mode for the brush.\r\n\r\n![A browser showing a white canvas with a GitHub mascot drawn.](https://res.cloudinary.com/deepgram/image/upload/v1646222952/blog/2022/03/draw-with-your-voice-articulate/screenshot.jpg)\r\n\r\nThanks to Deepgram's Speech Recognition API and our [documentation](https://developers.deepgram.com/documentation/), the team got a minimal viable project completed very quickly. Then, they expanded the use of Deepgram to utilize our [search](https://developers.deepgram.com/documentation/features/search/) feature to find command words.\r\n\r\nThe canvas was built with [P5.js](https://p5js.org), a library for creative coding in JavaScript. We just finished publishing a [three-part series on using P5.js](https://blog.deepgram.com/p5js-getting-started/) earlier this week. The team also utilized React, enabling team members to work on their own components and easily glue them together into a complete application later. Because the team created a highly-visual application, they focused attention to detail on smaller elements, such as animations.\r\n\r\nThe team has plenty of extensions planned, including the ability to fluidly pull images from online and insert them into the canvas, and additional accessibility options such as custom voice commands and color-blindness options to assist with color mixing.\r\n\r\nYou can try ARTiculate by visiting [art-iculate.netlify.app](https://art-iculate.netlify.app).\r\n\r\n        ";
						}
						async function compiledContent$2O() {
							return load$2O().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2O() {
							return (await import('./chunks/index.514a57f9.mjs'));
						}
						function Content$2O(...args) {
							return load$2O().then((m) => m.default(...args));
						}
						Content$2O.isAstroComponentFactory = true;
						function getHeadings$2O() {
							return load$2O().then((m) => m.metadata.headings);
						}
						function getHeaders$2O() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2O().then((m) => m.metadata.headings);
						}

const __vite_glob_0_98 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2O,
  file: file$2O,
  url: url$2O,
  rawContent: rawContent$2O,
  compiledContent: compiledContent$2O,
  default: load$2O,
  Content: Content$2O,
  getHeadings: getHeadings$2O,
  getHeaders: getHeaders$2O
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2N = {"title":"Embracing the Diversity of Spanish","description":"Spanish language diversity should be embraced by everyone including speech recognition solutions. There should not be just one Spanish speech model but multiple ones that start with a base model.","date":"2021-10-14T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981383/blog/embracing-the-diversity-of-spanish/embracing-diversity-of-spanish-thumb-554x220%402x.png","authors":["morris-gevirtz","sam-zegas"],"category":"identity-and-language","tags":["inclusion","language","heritage"],"seo":{"title":"Embracing the diversity of Spanish","description":"Spanish language diversity should be embraced by everyone including speech recognition solutions. There should not be just one Spanish speech model but multiple ones that start with a base model."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981383/blog/embracing-the-diversity-of-spanish/embracing-diversity-of-spanish-thumb-554x220%402x.png"},"shorturls":{"share":"https://dpgr.am/6cb3424","twitter":"https://dpgr.am/e7d3116","linkedin":"https://dpgr.am/12debd3","reddit":"https://dpgr.am/c4a6326","facebook":"https://dpgr.am/e12dfa6"}};
						const file$2N = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/embracing-the-diversity-of-spanish/index.md";
						const url$2N = undefined;
						function rawContent$2N() {
							return "This National Hispanic Heritage Month, *¡tenemos el español en cuenta!* There are about [61 million people of Hispanic heritage](https://www.census.gov/quickfacts/fact/table/US/RHI725219) in the United States today, of which at least [43 million speak Spanish at home](https://www.forbes.com/sites/soniathompson/2021/05/27/the-us-has-the-second-largest-population-of-spanish-speakers-how-to-equip-your-brand-to-serve-them/?sh=343abf57793a). Spanish is also widely spoken as a second language throughout the US. Of course, Spanish is also a major world language. According to [Ethnologue](https://www.ethnologue.com/guides/most-spoken-languages), it has the second largest population of native speakers in the world at 471 million, after Mandarin Chinese at 971 million and ahead of English at 370 million.  Like all languages, Spanish has a high degree of internal variation.\n\nI am Chilean, and when I read statistics citing the 471 million native Spanish speakers, I have to ask myself: *which Spanish do they mean?* I know that people from many parts of the Spanish-speaking world have a hard time with the Chilean dialect I grew up around. Though some of my Spanish-speaking friends might smirk and disagree, Chilean Spanish is still Spanish. At least, that's what I have always thought. But why is Chilean Spanish considered Spanish while the Asturiano \"language\" spoken in Spain is not considered Spanish (or Portuguese)? That question fascinates me. At its core, it boils down to: [*what is the difference between a language and a dialect*?](https://blog.deepgram.com/difference-between-language-dialect/)\n\nIt may surprise you that there is no one definition of either concept in linguistics. There are no signposts that tell you when you've left Spanish and entered Portuguese or French or Catalan. There are only conventions for how we talk about language boundaries, which are usually loaded with political history. The language names we know hide a vast amount of diversity under the surface. As the colonial Spanish invaded much of what today is North and South America, disease and oppression made it so that the primary language of business in the cities was Spanish.\n\nHowever, neither the Spaniards who invaded nor the indigenous people who lost their lands were one homogenous people. Rather they represented a countless variety of traditions and cultures. Moreover, Spanish usually did not fully replace indigenous languages in the countryside. Thousands of indigenous languages survived and are still in use throughout Latin America, influencing the local dialects of Spanish in vocabulary and pronunciation. \n\n<WhitepaperPromo whitepaper=\"latest\"></WhitepaperPromo>\n\nEvidence of this diversity survives in both Spanish and English today. *Chocolate* is a word from Nahuatl, the language of the Aztec Empire, as are *coyote*, *avocado*, and of course *mezcal*! But in Chile we don't use the Mexican word for avocado, *aguacate*. Instead, we say *palta*. This kind of difference confused me as a Spanish-speaking child growing up in the US, where the Spanish taught in schools was full of odd phrases and words that simply came from other dialects of Spanish.\n\nIn other instances, we see the reverse situation, where one word comes to be used to refer to different things across dialects. For example, an *empanada* is a nearly universal Spanish word (and used in the Philippines, too, of course) for delicious foodstuffs that have barely anything in common. This mixing of languages takes place within the US as well, where millions of Spanish speakers are bilingual with English. These speakers combine the languages in many ways. Sometimes they speak fully in one language or another. Sometimes they alternate speaking entire phrases or sentences in one language within a single conversation. Sometimes they pepper individual words from one language into speech that is predominantly in the other. Their accent and pronunciation in either language might shift depending on the social context.\n\nAll these possibilities are real, legitimate uses of language - and they dramatically increase the complexity of teaching technology how to understand human speech. Capturing this kind of bilingual speech is an exciting and important goal for Deepgram as a speech technology company.  Anyone who does business in the United States knows that serving a large customer base means embracing the diversity of languages spoken in our society, including a high frequency of Spanish-English bilingualism. If you are going to build speech recognition models meant to serve a multilingual customer base, you have to consider that there is no single dialect of Spanish; the differences in accent, grammar, and vocabulary are large. Furthermore, you'll have to be prepared to tackle a range of bilingual speech patterns.\n\nAt Deepgram, we built extensive English vocabulary in our Spanish general model to boost understanding in bilingual situations. Furthermore, our end-to-end deep learning approach enables us to create tailored speech models in a matter of days after receiving representative audio data. Because end-to-end deep learning approaches allow us to build models with your actual data, it is easier to train and deploy a mixed English-Spanish model that fits your precise business needs. By starting with your data, we are not constrained by debatable linguistic abstractions and forced language boundaries. Rather the data from your use cases build a tailor-made model that serves the accents and most common bilingual speech situations that you actually encounter in your business.\n\nOne joke that I have often heard is that the Americans and Brits are divided by a common tongue. It is my opinion that those of us who hail from Latin-American culture are *united* by a common tongue! The richness of Spanish as a language is a reflection of the diversity of its people and as such, means those language products meant to serve Spanish speakers should be built to account for that cultural diversity. At Deepgram, we don't shoehorn all people who speak \"Spanish\" into one ASR product. Rather, we start with a base concept of Spanish and build custom-trained models that reflect regional vocabulary, accents, bilingualism, and culturally relevant business use-cases. As a linguist, I am sure this is the right way to serve the needs of the millions of Spanish speakers in the US and abroad - and *¡espero que tengas el español en cuenta* this National Hispanic Heritage Month too! \n\n*Me gustaría agradecer a mi editor,* Sam Zegas, Deepgram's VP of Operations, who made major contributions to this article. Spanish (like English) is not just spoken by \"natives\" and \"heritage speakers\" but also by people who learned Spanish later in life. Their voices will show up in the data and should be recognized. This is especially true if you want to build language learning apps like the kind Sam and I discuss and dream about.";
						}
						async function compiledContent$2N() {
							return load$2N().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2N() {
							return (await import('./chunks/index.5d3a058e.mjs'));
						}
						function Content$2N(...args) {
							return load$2N().then((m) => m.default(...args));
						}
						Content$2N.isAstroComponentFactory = true;
						function getHeadings$2N() {
							return load$2N().then((m) => m.metadata.headings);
						}
						function getHeaders$2N() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2N().then((m) => m.metadata.headings);
						}

const __vite_glob_0_99 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2N,
  file: file$2N,
  url: url$2N,
  rawContent: rawContent$2N,
  compiledContent: compiledContent$2N,
  default: load$2N,
  Content: Content$2N,
  getHeadings: getHeadings$2N,
  getHeaders: getHeaders$2N
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2M = {"title":"Enhance Your Audio With Dolby.io For Higher Quality Transcripts","description":"Sometimes your audio will be low quality, and this may affect your transcripts. Learn how to use the Dolby.io Enhance API to clean up your files for better transcripts and happier ears.","date":"2022-06-02T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1653416318/blog/2022/05/enhance-audio-with-dolby-and-deepgram/Enhance-Audio-wDolby-for-higher-quality%402x.jpg","authors":["sandra-rodgers"],"category":"tutorial","tags":["dolby","accessibility"],"seo":{"title":"Enhance Your Audio With Dolby.io For Higher Quality Transcripts","description":"Sometimes your audio will be low quality, and this may affect your transcripts. Learn how to use the Dolby.io Enhance API to clean up your files for better transcripts and happier ears."},"shorturls":{"share":"https://dpgr.am/17f87f4","twitter":"https://dpgr.am/5325f0a","linkedin":"https://dpgr.am/2ca55ed","reddit":"https://dpgr.am/0e2f8f5","facebook":"https://dpgr.am/e4a028b"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454087/blog/enhance-audio-with-dolby-and-deepgram/ograph.png"}};
						const file$2M = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/enhance-audio-with-dolby-and-deepgram/index.md";
						const url$2M = undefined;
						function rawContent$2M() {
							return "\nYour recordings may not always be great quality - there might be a graininess to them, or background noise that interferes with what the listener is trying to focus on. While Deepgram may still perform fairly well, it's always true that better source audio results in a higher chance for accurate transcripts. For this tutorial, I'll use a low-quality audio file from the [Library of Congress](https://www.loc.gov).\n\nAn excellent tool for improving the quality of audio is [Dolby.io's Media Enhance API](https://docs.dolby.io/media-apis/reference/media-enhance-overview). With this API, all I have to do is make a `POST` request with the audio file, and Dolby.io can analyze it to remove the interfering sounds and the crackling or static you often hear with these types of recordings. I can even specify what type of content the audio is, such as an interview, podcast, or voice recording, and Dolby.io can enhance it even further for that type of content.\n\n## Before We Start\n\nBefore jumping into coding, I'll be sure to get an API key from each of the APIs I'll be using today. I'll head to [Dolby.io](https://dolby.io/signup) and then [Deepgram.com](https://console.deepgram.com/signup?jump=keys) to get keys.\n\nI'll also install the dependencies with the following command in my project directory:\n\n    npm install axios @deepgram/sdk\n\n## Create a Node.js Project\n\nI'll create an `index.js` file and require `axios` to help with making API requests:\n\n```js\nconst axios = require('axios')\n```\n\nI intend to send an audio file to Dolby.io for enhancement, wait for it to be processed, and wait for that file to come back to me. Since there will be an unknown amount of time involved in each step of the process, I need to write asynchronous functions for each step. Here are the steps:\n\n## Start the Enhance Job\n\nThe first asynchronous function will be called `startEnhanceJob`\n\n```js\nconst startEnhanceJob = async (url) => {}\n```\n\nI need to make the audio file available to Dolby.io by putting it in cloud storage. Dolby offers the option of me putting it in their temporary cloud storage, but I have to use the URL format they expect, which will start with `dlb://`. I'll write some JavaScript to create that Dolby.io URL format:\n\n```js\nconst startEnhanceJob = async (url) => {\r\n  const dlbUrl =\r\n    'dlb://out/' + url.split('/').slice(-1)[0].split('.')[0] + '.wav'\r\n}\n```\n\nThen I will make the `POST` request with the audio file to Dolby.io and receive a job ID for that enhance job (which I'll need in the next step).\n\n```js\nconst startEnhanceJob = async (url) => {\r\n  // create a dolby URL\r\n  const dlbUrl =\r\n    'dlb://out/' + url.split('/').slice(-1)[0].split('.')[0] + '.wav'\r\n\r\n  // POST request\r\n  const { data } = await axios({\r\n    method: 'post',\r\n    url: 'https://api.dolby.com/media/enhance',\r\n    headers: {\r\n      'x-api-key': 'YOUR_DOLBYIO_API_KEY',\r\n      'Content-Type': 'application/json',\r\n      Accept: 'application/json',\r\n    },\r\n    data: {\r\n      input: url,\r\n      output: dlbUrl,\r\n      content: { type: 'interview' },\r\n    },\r\n  })\r\n\r\n  return { jobId: data.job_id, dlbUrl }\r\n}\n```\n\nNotice that I added `content: { type: interview }` since the audio file I'm sending is an interview.\n\n## Check the Enhance Job and Report Progress\n\nIt will take some amount of time for the enhancement job to run. I need to track the progress so that I know when the file is ready. I'll write two functions in this step: `checkEnhanceJob` and `waitUntilJobCompletes`.\n\nFor `checkEnhanceJob`, I'll take the job ID that was returned from the `startEnhanceJob` function, and I'll use it to make a `GET` request to the Dolby.io Enhance API to get progress on the enhancement job:\n\n```js\nconst checkEnhanceJob = async (job_id) => {\r\n  const { data } = await axios({\r\n    method: 'GET',\r\n    url: 'https://api.dolby.com/media/enhance',\r\n    params: { job_id },\r\n    headers: {\r\n      'x-api-key': api_key,\r\n      'Content-Type': 'application/json',\r\n      Accept: 'application/json',\r\n    },\r\n  })\r\n\r\n  // Returns a number between 0 and 100\r\n  return data.progress\r\n}\n```\n\nThen I'll write a function that uses the `checkEnhanceJob` result in a loop to show a countdown as progress is being made on the enhance job. It will wait 2000ms (2 seconds) between each loop:\n\n```js\nconst waitUntilJobCompletes = async (job_id) => {\r\n  let progress = await checkEnhanceJob(job_id)\r\n  while (progress < 100) {\r\n    await new Promise((r) => setTimeout(r, 2000))\r\n    progress = await checkEnhanceJob(job_id)\r\n    console.log(progress)\r\n  }\r\n  return\r\n}\n```\n\n## Get Enhanced File URL\n\nOnce the enhancement of the file is complete, I need to output that new file to a URL that I can use (in this project, I'll be using it to pass on to Deepgram for transcription).\n\nI'll write a function that will make a `POST` request to put the output of the enhance job at the URL I created for temporary storage of the file. I'll also `console.log` the file URL so I can test it now and see how it sounds.\n\n```js\nconst getNewFileUrl = async (dlbUrl) => {\r\n  const { data } = await axios({\r\n    method: 'POST',\r\n    url: 'https://api.dolby.com/media/output',\r\n    data: { url: dlbUrl },\r\n    headers,\r\n  })\r\n\r\n  console.log(data.url)\r\n  return data.url\r\n}\n```\n\n## Run the Enhance Logic\n\nI wrote each step of the enhancement job, but now I need to write a main function that runs every step, i.e., every function I wrote.\n\nI also need to add the URL of the audio file I want to enhance. I've chose a file from the Library of Congress called [\"Interview with Lillie Haws, New York, New York, November 12, 2001\"](https://www.loc.gov/item/afc911000130/).\n\nHere is the `main` function:\n\n```js\nconst main = async () => {\r\n  // start enhancement of file:\r\n  const { jobId, dlbUrl } = await startEnhanceJob(\r\n    'https://tile.loc.gov/storage-services/media/afc/911/afc2001015_sr298a01.mp3'\r\n  )\r\n\r\n  // track progress as it is processing:\r\n  await waitUntilJobCompletes(jobId)\r\n\r\n  // get the output URL:\r\n  const url = await getNewFileUrl(dlbUrl)\r\n}\r\n\r\nmain()\n```\n\nWhen it runs, I'll see the values of my loop that I printed counting up to completion of the enhancement job. And when it finishes, I'll see a very long URL that I can use to listen to my file.\n\n![A terminal showing values appearing with 2 seconds between them. The values are 0, 19, 19, 46, and 100. As soon as 100 appears, a long URL is shown.](https://res.cloudinary.com/deepgram/image/upload/v1653416323/blog/2022/05/enhance-audio-with-dolby-and-deepgram/progress_console.gif)\n\nIf I click on the link, I'm taken to the hosted audio file. It sounds so much better than the original! Now I'm ready to transcribe it with Deepgram.\n\n## Transcribe With Deepgram\n\nI'll be using Deepgram's API for transcribing [Pre-Recorded Audio](https://developers.deepgram.com/documentation/getting-started/prerecorded/). Deepgram has a Node.js SDK, so I'll require it in my `index.js` file. I'll also create a new instance of Deepgram by giving it my Deepgram API key:\n\n```js\nconst { Deepgram } = require('@deepgram/sdk')\r\nconst deepgram = new Deepgram(process.env.DEEPGRAM_KEY)\n```\n\nI will take the file URL that I received from Dolby.io and send that to Deepgram for transcription. It is the temporarily stored file that I assigned to the `url` variable in the main function (in the last section).\n\nI'll also specify that I would like Deepgram to add punctuation. I can do this by adding `{ punctuate:true }` to the request:\n\n```js\nconst response = await deepgram.transcription.preRecorded(\r\n  { url },\r\n  { punctuate: true }\r\n)\n```\n\nNow I can run the whole function, and I'll see that Deepgram transcribes the enhanced file. I'll `console.log` the response from Deepgram so I can actually see the transcription now:\n\n```js\n// main function\r\nconst main = async () => {\r\n  const { jobId, dlbUrl } = await startEnhanceJob(\r\n    'https://tile.loc.gov/storage-services/media/afc/911/afc2001015_sr298a01.mp3'\r\n  )\r\n  await waitUntilJobCompletes(jobId)\r\n  const url = await getNewFileUrl(dlbUrl)\r\n  const response = await deepgram.transcription.preRecorded(\r\n    { url },\r\n    { punctuate: true }\r\n  )\r\n  console.log(response.results.channels[0])\r\n}\r\n\r\nmain()\n```\n\nAnd now I have a full transcript of the audio file from the Library of Congress.\n\n## Conclusion\n\nToday I used [Dolby.io](https://docs.dolby.io/) and [Deepgram](https://developers.deepgram.com/) to enhance an audio file and transcribe the speech of the interview into text. These two APIs seem like a great combination for many future projects!\n\nIf you enjoyed my post, follow me at [Twitter](https://twitter.com/sandra_rodgers_) to continue the conversation.\n\n        ";
						}
						async function compiledContent$2M() {
							return load$2M().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2M() {
							return (await import('./chunks/index.9f380d83.mjs'));
						}
						function Content$2M(...args) {
							return load$2M().then((m) => m.default(...args));
						}
						Content$2M.isAstroComponentFactory = true;
						function getHeadings$2M() {
							return load$2M().then((m) => m.metadata.headings);
						}
						function getHeaders$2M() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2M().then((m) => m.metadata.headings);
						}

const __vite_glob_0_100 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2M,
  file: file$2M,
  url: url$2M,
  rawContent: rawContent$2M,
  compiledContent: compiledContent$2M,
  default: load$2M,
  Content: Content$2M,
  getHeadings: getHeadings$2M,
  getHeaders: getHeaders$2M
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2L = {"title":"Enhanced Messaging in Streaming","description":"We’re excited to announce an update to our streaming API for enhanced usability.","date":"2022-10-18T22:09:38.278Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1666120286/blog/Enhanced%20Messaging%20in%20Streaming/2210-close-stream-update-featured-1200x630_2x_e9k8gi.png","authors":["shir-goldberg"],"category":"product-news","tags":["streaming"],"shorturls":{"share":"https://dpgr.am/2f0143f","twitter":"https://dpgr.am/8f2ba53","linkedin":"https://dpgr.am/7a8ef1b","reddit":"https://dpgr.am/cc4e407","facebook":"https://dpgr.am/ff26ee9"}};
						const file$2L = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/enhanced-messaging-in-streaming/index.md";
						const url$2L = undefined;
						function rawContent$2L() {
							return "\r\nWe're excited to announce an update to our streaming API for enhanced usability.\r\n\r\nWorking with websockets to send audio in real-time can be tricky. Today, we're releasing the first step in our plan to make our streaming API easier to use.\r\n\r\nStreaming connections should now be closed by sending the JSON message `{ \"type\": \"CloseStream\" }`. This tells Deepgram that no more audio will be sent. Deepgram will then finish processing existing audio, and close the connection once all transcripts are returned. \r\n\r\nGracefully closing your stream is the best way to ensure you get all your transcripts, and aren't charged for audio you don't want transcribed.\r\n\r\nHere's an example of sending a CloseStream message in Javascript:\r\n\r\n```javascript\r\nsocket.send(JSON.stringify({\r\n    \"type\": \"CloseStream\"\r\n}))\r\n```\r\n\r\nAnd an example in Python:\r\n\r\n```python\r\nawait ws.send(json.dumps({\r\n    \"type\": \"CloseStream\"\r\n}))\r\n```\r\n\r\nPreviously, streaming connections were closed by sending an empty byte—for example, sending `Uint8Array(0)` in Javascript, or `b''` in Python. This method of closing connections is now deprecated, and we will remove it in a future release. An empty byte doesn't inherently translate to closing a connection and some websocket libraries don't support sending it. Sending a CloseStream message is a clearer and more universal method to accomplish this.\r\n\r\nWe recommend customers using our API move to using the CloseStream method to avoid potential disruptions. Any customers using our SDK will not need to make changes.\r\n\r\nThis message is the first of many. We'll be releasing additional JSON messages in the coming months that will unlock powerful new features in our real-time API. Stay tuned!\r\n\r\nTo learn more about Streaming, please refer to our Streaming [Documentation](https://developers.deepgram.com/documentation/getting-started/streaming/). **We welcome your feedback, please share it with us [here](https://deepgram.hellonext.co/b/feedback).**\r\n\r\n";
						}
						async function compiledContent$2L() {
							return load$2L().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2L() {
							return (await import('./chunks/index.cb5844a9.mjs'));
						}
						function Content$2L(...args) {
							return load$2L().then((m) => m.default(...args));
						}
						Content$2L.isAstroComponentFactory = true;
						function getHeadings$2L() {
							return load$2L().then((m) => m.metadata.headings);
						}
						function getHeaders$2L() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2L().then((m) => m.metadata.headings);
						}

const __vite_glob_0_101 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2L,
  file: file$2L,
  url: url$2L,
  rawContent: rawContent$2L,
  compiledContent: compiledContent$2L,
  default: load$2L,
  Content: Content$2L,
  getHeadings: getHeadings$2L,
  getHeaders: getHeaders$2L
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2K = {"title":"Everything You Need to Know about Keywords for Speech Recognition","description":"Speech recognition such as Google, AWS and others apply different techniques to increasing accuracy through keywords. Here’s a brief overview of what you need to know about Keywords and when you should consider using them.","date":"2022-10-17T22:48:27.811Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1666046992/blog/Everything%20You%20Need%20to%20Know%20about%20Keywords%20for%20Speech%20Recognition/2210-Keywords-for-speech-recognition-featured-1200x630_2x_1_mb9ngj.png","authors":["shir-goldberg"],"category":"best-practice","tags":["Keywords"],"shorturls":{"share":"https://dpgr.am/b1d2e02","twitter":"https://dpgr.am/265ac49","linkedin":"https://dpgr.am/bfad845","reddit":"https://dpgr.am/7a4f07c","facebook":"https://dpgr.am/62a6ff4"}};
						const file$2K = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/everything-you-need-to-know-about-keywords-for-speech-recognition/index.md";
						const url$2K = undefined;
						function rawContent$2K() {
							return "\nJust like humans, AI learns through experience. In absence of training your model to all of the possible audio scenarios and special vocabulary it may encounter, Keywords are a way of increasing accuracy without having to give your model the “experience”—a shortcut, if you will.\n\nSpeech recognition such as Google, AWS and others apply different techniques to increasing accuracy through keywords. Here’s a brief overview of what you need to know about Keywords and when you should consider using them.\n\n## What are Keywords?\n\nKeywords are a way of specifying that certain words are expected to appear in a conversation. Deepgram’s end-to-end deep learning platform will read the entire model and utilize this information to make more accurate predictions.\n\nCustomers who want better performance on product names or industry-specific vocabulary can use keywords to increase the chances that Deepgram’s model will predict those words instead of more common ones. \n\nDeepgram’s deep learning models are trained on real-world audio and have extensive vocabularies. Most common words will already be correctly predicted by our models. Keyword boosting (sometimes known as word boost or speech adaptation boost) can help with words that are not in the model’s vocabulary, such as proper nouns.\n\n### **Example with and without keyword boosting:**\n\n    keywords=snuffleupagus:2.2\n\n**\\*Truth\\***\n\nand then big bird said to snuffleupagus why aren’t you eating that banana\n\n**\\*Before boosting\\***\n\nand then big bird said to sniff why aren’t you eating that banana\n\n**\\*After boosting\\***\n\nand then big bird said to snuffleupagus why aren’t you eating that banana\n\nHere’s how to make a request with keyword boosting using Deepgram’s speech-to-text API.\n\n    curl \\\n\n    \\--request POST \\\n\n    \\--header 'Authorization: Token YOUR_DEEPGRAM_API_KEY' \\\n\n    \\--header 'Content-Type: audio/wav' \\\n\n    \\--data-binary @youraudio.wav \\\n\n    \\--url 'https://api.deepgram.com/v1/listen?keywords=KEYWORD:INTENSIFIER'\n\nFor more information about keywords, you can refer to our [developer documentation](https://developers.deepgram.com/documentation/features/keywords/).\n\n## Why is Keyword Boosting Used?\n\nJust like a human listener, Deepgram can better understand mumbled, distorted, or otherwise hard-to-decipher speech when it knows the context of the conversation. When using Deepgram’s API to transcribe audio, you can specify keywords to which the model should pay particular attention to help it understand context; this is known as keyword boosting. Similarly, you can suppress keywords.\n\nFor example, in a conversational AI application, keyword boosting can be used for your product names and business terminology. If you run a burger drive-through and use speech-to-text to handle orders, your customers are more likely to be ordering a “burger with fries” than a “breaker with five”. Boosting “burger” and “fries” can help ensure you get the output your application is expecting.\n\n## Where Should I Use a Custom Model Instead of Keywords? \n\nThough keywords can be used to aid a few uncommon or out-of-vocabulary words, a custom model trained on representative data will always give the best performance. The more keywords specified, the higher the chance that the models may give unexpected outputs. Additionally, keywords cannot accept multiple-word phrases, such as “no problem”, to be boosted as a unit. Even if multiple words are sent in the keywords parameter using URL encoding, Deepgram will boost each word individually, which may not lead to the desired results. \n\nFor customers who work in industries with lots of uncommon vocabulary or specific phrases, Deepgram recommends talking to our team about custom model training. Deepgram can also provide data labeling services, and even create audio to train on to ensure custom models produce the best results possible.\n\nIf custom model training isn’t an option but better accuracy is desired, another option is using Deepgram’s enhanced model tier. Deepgram's Enhanced model has increased effective vocabulary and handles long tail vocabulary (words that appear infrequently) significantly better. If switching models, testing should be done with and without keywords, as transcription outputs may change.\n\n## Where Should I Use Find and Replace Versus Keywords?\n\nKeyword boosting is designed to increase the chances that a certain word will be transcribed. [Find and replace ](https://developers.deepgram.com/documentation/features/replace/)is designed for cases where when a certain word is transcribed, it should be replaced with something different. One example of where find and replace should be used is if the name \"Aaron\" appears in the transcript but should be spelled \"Erin\" instead.\n\n## Wrapping Up\n\nIf you’d like to give keywords a try in your project, you can [sign up for Console](https://console.deepgram.com/signup) and get $150 in free credit to try it out. Still have questions? Check out our [developer documentation](https://developers.deepgram.com/), or [reach out to Sales with your questions](https://deepgram.com/contact-us/).\n\n";
						}
						async function compiledContent$2K() {
							return load$2K().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2K() {
							return (await import('./chunks/index.3e17fbb3.mjs'));
						}
						function Content$2K(...args) {
							return load$2K().then((m) => m.default(...args));
						}
						Content$2K.isAstroComponentFactory = true;
						function getHeadings$2K() {
							return load$2K().then((m) => m.metadata.headings);
						}
						function getHeaders$2K() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2K().then((m) => m.metadata.headings);
						}

const __vite_glob_0_102 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2K,
  file: file$2K,
  url: url$2K,
  rawContent: rawContent$2K,
  compiledContent: compiledContent$2K,
  default: load$2K,
  Content: Content$2K,
  getHeadings: getHeadings$2K,
  getHeaders: getHeaders$2K
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2J = {"title":"Exploring OpenAI Whisper Speech Recognition","description":"In Deepgram's latest blog, we will explore some of the options in OpenAI Whisper’s inference and see how they impact results. Read more here!","date":"2022-10-13T20:59:18.175Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1665415885/blog/exploring-whisper/2210-Exploring_Whisper-featured-1200x630_2x_nut4uw.png","authors":["julia-strout"],"category":"ai-and-engineering","tags":["whisper","machine-learning"],"shorturls":{"share":"https://dpgr.am/81a7829","twitter":"https://dpgr.am/cc15f31","linkedin":"https://dpgr.am/2468dad","reddit":"https://dpgr.am/8e86cec","facebook":"https://dpgr.am/f98594a"}};
						const file$2J = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/exploring-whisper/index.md";
						const url$2J = undefined;
						function rawContent$2J() {
							return "\nOpenAI recently released a new open source ASR model named Whisper and a repo full of tools that make it easy to try it out. In this blog, we will explore some of the options in Whisper’s inference and see how they impact results.\n\nWhen experimenting with Whisper, you have a few options. You can run their command line tool, which will set a bunch of parameters for you, but you can also play around with those parameters and change what kind of results you get.\n\nTo install Whisper on your machine, you can follow the Setup guide in their [readme](https://github.com/openai/whisper) which walks you through the steps.\n\nThen, you can import Whisper in your own python code and use their `load_model` function to download the pre-trained weights and initialize one of the models.\n\n```bash\nimport whisper\r\nmodel = whisper.load_model(\"medium.en\", device=\"cuda\")\n```\n\nWhisper is available as multilingual models, but we will focus on the english only versions here. The options are:\n\n*   `tiny.en`\n*   `base.en`\n*   `small.en`\n*   `medium.en`\n*   `large` (the large model is only available in the multi-language form)\n\nWhisper is an encoder-decoder transformer model that takes in audio features and generates text. The models’ parameter sizes range from 39 M for tiny and up to 1550 M for large.\n\nWhisper makes it very easy to transcribe an audio file from its path on the file system and will take care of loading the audio using `ffmpeg` and featurizing it before running inference on the file.\n\nIn order to run the transcribe function, you need to make some decisions about how you want to decode the model's predictions into text.\n\nYou can call the transcribe function without explicitly setting the decode options and it will set some defaults for you.\n\n```python\ntranscription = model.transcribe(\"hello_world.mp3\", task=”transcribe”, language=\"en\")\r\nprint(transcription[\"text\"])\r\n' Hello world.'\n```\n\n## Decoding\n\nIn the Whisper [paper](https://cdn.openai.com/papers/whisper.pdf), they describe their complex decoding strategy including a few heuristics they landed on to try and make transcription more reliable. The way these strategies are currently implemented in their code results in slightly improved test results, but can slow down inference by up to 6x. This is because they repeatedly run inference with different decoding strategies until they meet the heuristics.\n\nTwo of the heuristics they use are:\n\n**Compression ratio:** the compression ratio heuristic is defined by the calculation:\n\n`compression_ratio = len(text)/len(zlib.compress(text.encode(\"utf-8\")))`\n\nzlib’s compression capitalizes on repeated sequences, so a text string with many repeated sequences will be able to compress down more than a string with more unique sequences. The goal of this threshold is to prevent the model from outputting predictions where it has gotten stuck and generated the same phrase over and over. You can see an example of this in the Non-speech section below.\n\nIn the whisper code, they set their ratio threshold to be `2.4`. A sequence that has a higher ratio is re-inferenced and decoded with a different strategy.\n\n**Average log probability:** after taking the log softmax of the network's output logits, the average log probability of the tokens chosen by the decoding is used. This can be thought of as a confidence measure of the model's predictions. The whisper authors use `-1.0` as their threshold.\n\nBy default, the whisper cli tool runs inference and decoding up to 6 times with the following decoding strategies. For each strategy, if the results pass the compression and log probability heuristics, the predicted tokens are used. If the heuristics are not met, the segment is re-inference and re-decoded with the next strategy.\n\nThe decoding strategies are:\n\n1.  Beam search with 5 beams using log probability for the score function\n2.  Greedy decoding with best of 5 sampling. The following temperatures are used for successive attempts: (0.0, 0.2, 0.4, 0.6, 0.8, 1.0)\n\nThe authors quantify the effects of these strategies in [Table 7](https://cdn.openai.com/papers/whisper.pdf) of their paper. They have a varied impact across datasets, but overall the effects for any single intervention are not large.\n\nHere we can look at a few different decoding strategies for the same file and see how they impact results. For this experiment we use the following [clip](https://www.youtube.com/watch?v=FTrxDBDBOHU) from Star Wars. It's a useful example because the different decoding strategies make the most difference in areas where the model is less certain. In this clip, there is dramatic music at the beginning, followed by the words \"hello there\" in a normal voice and then \"General Kenobi\" in a robotic voice with some strange noises thrown in.\n\nFirst we try running the full decoding strategy, and see that it correctly transcribes the file:\n\n```python\nbeam_size=5\r\nbest_of=5\r\ntemperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0)\r\n\r\ndecode_options = dict(language=\"en\", best_of=best_of, beam_size=beam_size, temperature=temperature)\r\ntranscribe_options = dict(task=\"transcribe\", **decode_options)\r\n\r\ntranscription = model.transcribe(\"Obi-Wan.mp3\", **transcribe_options)\r\nprint(transcription[\"text\"])\r\n\r\n' Hello there. General Kenobi!'\n```\n\nNext, we try removing the beam size strategy and dropping the best\\_of value for greedy decoding down to 1. Here we can see that now the model is outputting \"ominious music\" before the actual speech. This suggests the model was trained on some form of closed captioning.\n\n```python\nbeam_size=None\r\nbest_of=1\r\ntemperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0)\r\n\r\ndecode_options = dict(language=\"en\", best_of=best_of, beam_size=beam_size, temperature=temperature)\r\ntranscribe_options = dict(task=\"transcribe\", **decode_options)\r\n\r\ntranscription = model.transcribe('Obi-Wan.mp3', **transcribe_options)\r\nprint(transcription['text'])\r\n\r\n'ominous music hello there General Kenobi!'\n```\n\nFinally, we try increasing the best\\_of value for greedy decoding, but reducing the temperature values tried.\n\n```python\nbeam_size=None\r\nbest_of=3\r\ntemperature=(0.0, 0.2, 0.4)\r\n\r\ndecode_options = dict(language=\"en\", best_of=best_of, beam_size=beam_size, temperature=temperature)\r\ntranscribe_options = dict(task=\"transcribe\", **decode_options)\r\n\r\ntranscription = model.transcribe(\"Obi-Wan.mp3\", **transcribe_options)\r\nprint(transcription[\"text\"])\r\n\r\n' Thanks for watching!'\n```\n\nThere we can see that with only very low temperature values, and a best\\_of set to 3, we get a remnant from the weak supervision from internet transcripts. There is randomness in this process, so repeating those experiments will different results each time.\n\nThese examples help explain why the authors recommend attempting multiple decodings and checking the heuristics as a way of improving performance. Trying to hone in on what decoding strategy works best for your data can simplify the inference process for running Whisper and make the current code run faster.\n\n### Non-speech\n\nOne area the model struggles with is periods of non-speech. After the decoding algorithms requested are run, the authors check one more heursitic:\n\n**No speech threshold:** this is another heuristic the authors check after all the decoding is finished. It is based on the probability assigned to the no speech token during inference. The authors use a threshold of `0.6` to detect non-speech. However, if the average log probability passes its threshold then this heuristic is ignored.\n\nTo explore the models' performances on areas of non-speech, we use this [cat video](https://www.youtube.com/watch?v=gBx4IwYe3L8) which contains only background music and cat noises.\n\nSending the first 30 seconds of the cat video through the model with the recommended decoding produces some text.\n\n```python\nbeam_size=5\r\nbest_of=5\r\ntemperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0)\r\n\r\ndecode_options = dict(language=\"en\", best_of=best_of, beam_size=beam_size, temperature=temperature)\r\ntranscribe_options = dict(task=\"transcribe\", **decode_options)\r\n\r\ntranscription = model.transcribe(\"kittens_30secs.mp3\", **transcribe_options)\r\nprint(transcription[\"text\"])\r\n\r\n' parrot one parrot you'\n```\n\nThis means that the no speech threshold described above was not met. And if we check the model's predicted probability for the non-speech token we see that it was only 0.535. Too low to hit the threshold of 0.6.\n\nIf we run this same test with the `base.en` model instead of the `medium.en`:\n\n```python\nmodel = whisper.load_model(\"base.en\", device=\"cuda\")\r\nbeam_size=5\r\nbest_of=5\r\ntemperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0)\r\n\r\ndecode_options = dict(language=\"en\", best_of=best_of, beam_size=beam_size, temperature=temperature)\r\ntranscribe_options = dict(task=\"transcribe\", **decode_options)\r\n\r\ntranscription = model.transcribe(\"kittens_30secs.mp3\", **transcribe_options)\r\nprint(transcription[\"text\"])\r\n\r\n''\n```\n\nFor the base model, the probability for the non speech token was `0.641`, which hit the threshold, leading the model to correctly output no speech for the audio.\n\nIf we run the same experiment for more of the models, we can see how they handle the non-speech differently.\n\n| Model     | no\\_speech probability | Predicted text |\r\n|-----------|-----------------------|----------------|\r\n| base.en   | 0.64                  |''\r\n| small.en | 0.467                 |' you'          |\r\n| medium.en | 0.53                  |'few weeks ago'|\r\n| large     | 0.5                   |\"Last couple days I almost lost it. I am excited for the day cuz I am Bye!\"|\n\nIt doesn't correlate perfectly with size, but we can see that the larger models may need a higher no silence threshold for audio known to contain periods of background noise and no speech. This could also be solved by using a voice activity detection algorithm in parallel with Whisper, and only running Whisper on segments believed to contain speech.\n\nThe other thing noticeable in those results is how the larger models are also more \"wordy\" under uncertainty, with the large model outputting two random sentences. This points to the high capacity for the large models to memorize some of the lower quality supervision. The authors try to counter the effects of the weak supervision by providing a lot of it, over 600k hours, but its clear that some of the noise in the truth quality ends up stored in the weights of the larger models.\n\nFinally, this file is also a good example to see the scenario that the compression ratio is designed to prevent. If we look at results on this non-speech file with only running beam search decoding:\n\n```python\nbeam_size=5\r\nbest_of=None\r\ntemperature=0.0\r\n\r\ndecode_options = dict(language=\"en\", best_of=best_of, beam_size=beam_size, temperature=temperature)\r\ntranscribe_options = dict(task=\"transcribe\", **decode_options)\r\n\r\ntranscription = model.transcribe(\"kittens_30secs.mp3\", **transcribe_options)\r\nprint(transcription[\"text\"])\r\n\r\n\" I'm not going to lie, I'm not going to lie, I'm not going to lie.\"\n```\n\nThe compression ratio for that prediction string is actually only `1.91`, which is less than their threshold of `2.4`. It's worth noting that if you don't use their full decoding scheme and start encountering a lot of repetitious predictions in audio that the model is uncertain about, you might want to drop the compression ratio threshold a little lower.\n\nHopefully this blog helped you explore the many options in the Whisper repo and find a configuration that works best for your tasks!\n\n";
						}
						async function compiledContent$2J() {
							return load$2J().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2J() {
							return (await import('./chunks/index.735dfce6.mjs'));
						}
						function Content$2J(...args) {
							return load$2J().then((m) => m.default(...args));
						}
						Content$2J.isAstroComponentFactory = true;
						function getHeadings$2J() {
							return load$2J().then((m) => m.metadata.headings);
						}
						function getHeaders$2J() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2J().then((m) => m.metadata.headings);
						}

const __vite_glob_0_103 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2J,
  file: file$2J,
  url: url$2J,
  rawContent: rawContent$2J,
  compiledContent: compiledContent$2J,
  default: load$2J,
  Content: Content$2J,
  getHeadings: getHeadings$2J,
  getHeaders: getHeaders$2J
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2I = {"title":"Fetch Hosted Audio Streams In The Browser","description":"Learn how to fetch an audio stream from a URL, break down a readable stream into chunks using the JavaScript Streams API, and send the audio stream through a WebSocket to Deepgram.","date":"2022-06-16T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1655135180/blog/2022/06/fetch-hosted-audio-streams-in-the-browser/How-to-fetch-hosted-audio-streams-in-the-browser-blog.png","authors":["sandra-rodgers"],"category":"tutorial","tags":["websockets","browser","javascript"],"seo":{"title":"Fetch Hosted Audio Streams In The Browser","description":"Learn how to fetch an audio stream from a URL, break down a readable stream into chunks using the JavaScript Streams API, and send the audio stream through a WebSocket to Deepgram."},"shorturls":{"share":"https://dpgr.am/1aab758","twitter":"https://dpgr.am/754f7d0","linkedin":"https://dpgr.am/d758df7","reddit":"https://dpgr.am/5ef955f","facebook":"https://dpgr.am/590f31a"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454096/blog/fetch-hosted-audio-streams-in-the-browser/ograph.png"}};
						const file$2I = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/fetch-hosted-audio-streams-in-the-browser/index.md";
						const url$2I = undefined;
						function rawContent$2I() {
							return "\nToday we'll look at how to fetch an audio stream from a URL, send the stream data to Deepgram, and receive transcriptions in return. This is particularly useful when you want to get live stream data and send it in real-time (and also receive a transcription back from Deepgram in real-time chunks).\n\nThis will require three main steps:\n\n1.  Open a WebSocket channel to Deepgram\n2.  Request the hosted audio stream data from the URL where the stream is hosted (using Fetch)\n3.  Pass the stream in incremental chunks to Deepgram\n\nIf you want to see the audio stream transcribed by Deepgram's speech-to-text API, I recommend signing up for a free account and getting an API key [here](https://console.deepgram.com/signup?jump=keys).\n\n## Open WebSocket Channel\n\nThe very first thing we need to do is connect to Deepgram with a browser WebSocket.\n\n```js\nconst socket = new WebSocket('wss://api.deepgram.com/v1/listen', [\r\n  'token',\r\n  'YOUR_DEEPGRAM_API_KEY',\r\n])\n```\n\nNext we'll write several event listeners. **Events** are actions that occur within a programmed system; the system is designed to inform us of those events so that we can write code in reaction to them. The WebSocket API includes several events that fire throughout the process of the socket opening, data being received, and the socket closing. We'll use an event listener to react to those events.\n\nWe write this WebSocket logic first because then we can listen for the `onopen` event to take place, and once it does, we make the fetch request to receive the audio file:\n\n```js\nsocket.onopen = () => {\r\n  // Fetch stream\r\n  // Send stream to Deepgram\r\n}\r\n\r\nsocket.onmessage = (message) => {\r\n  // Receive transcript from Deepgram\r\n  // Do something with transcript\r\n}\r\n\r\nsocket.onclose = () => {\r\n  console.log({ event: 'onclose' })\r\n}\r\n\r\nsocket.onerror = (error) => {\r\n  console.log({ event: 'onerror', error })\r\n}\n```\n\n## Fetch Hosted Audio Stream\n\nThe next step will be to write the fetch request. The browser's global `fetch()` method takes a string parameter that is the URL to where we will be making the request.\n\nThe method returns a promise, so we can chain `.then()` to wait for the response and get the response body.\n\n```js\nsocket.onopen = () => {\r\n  const url =\r\n    'https://stream.live.vc.bbcmedia.co.uk/bbc_radio_fourlw_online_nonuk'\r\n\r\n  fetch(url)\r\n    .then((response) => response.body)\r\n    .then((body) => {\r\n      // Send stream to Deepgram\r\n    })\r\n}\n```\n\n## Read Chunks and Send to Deepgram\n\nThe `response.body` we receive from the URL will be a `ReadableStream`, which is a \"readable stream of byte data\" ([Mozilla](https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream)). This just means it is data that can be read. And in order to read the data, we must break it down - similar to the way a human must break a sentence down into words in order to comprehend its meaning.\n\nHow do we break down the stream data? The Streams API was created for this purpose. We'll use it to take the stream and break it into **chunks**, which are the single pieces of data that are written to or read from a stream.\n\nWe can write a function to consume the readable stream and turn it into chunks to send to Deepgram. There are different ways to do this, but all of them will need to do the following:\n\n1.  Create a [reader](https://developer.mozilla.org/en-US/docs/Web/API/ReadableStreamDefaultReader) using the `getReader()` method\n2.  Use the `read()` method of the reader interface to get access to each chunk in the queue (returned as a promise)\n\nHere is one way to write this function:\n\n```js\nasync function readAllChunks(readableStream) {\r\n  // Create reader:\r\n  const reader = readableStream.getReader()\r\n  while (true) {\r\n    // Read chunks:\r\n    const { done, value } = await reader.read()\r\n    if (done) {\r\n      break\r\n    }\r\n    // Send chunks to Deepgram:\r\n    socket.send(value)\r\n  }\r\n}\n```\n\nSince the `.read()` method returns a promise that resolves to an object, we can destructure that object into its two properties: `done` and `value`. Then we can send that value on to Deepgram for transcription.\n\nThe Streams API specification provides [other useful examples](https://streams.spec.whatwg.org/#rs-intro) for how to consume a `ReadableStream`.\n\nNow that we have the logic to read the stream as chunks, we need to invoke the function and pass in the readable stream:\n\n```js\nsocket.onopen = () => {\r\n  const url =\r\n    'https://stream.live.vc.bbcmedia.co.uk/bbc_radio_fourlw_online_nonuk'\r\n\r\n  fetch(url)\r\n    .then((response) => response.body)\r\n    .then((body) => {\r\n      // Invoke function that sends readable stream:\r\n      readAllChunks(body)\r\n    })\r\n}\n```\n\n## Finishing Up\n\nWe have accomplished what we set out to do, which is to get a hosted audio stream and send it to Deepgram to be transcribed in realtime. When the transcript is returned, we can do whatever it is we intended to do with it.\n\nHere's the code in its entirety. I've also included [an example](https://stackblitz.com/edit/web-platform-v9nyiq?file=script.js,index.html) in my Stackblitz account that puts the text from the stream onto the browser page. Be sure to add an API key to make it work.\n\n```js\nconst socket = new WebSocket('wss://api.deepgram.com/v1/listen', [\r\n  'token',\r\n  'YOUR_DEEPGRAM_API_KEY',\r\n])\r\n\r\nsocket.onopen = () => {\r\n  const url =\r\n    'https://stream.live.vc.bbcmedia.co.uk/bbc_radio_fourlw_online_nonuk'\r\n  fetch(url)\r\n    .then((response) => response.body)\r\n    .then((body) => {\r\n      readAllChunks(body)\r\n    })\r\n}\r\n\r\nasync function readAllChunks(readableStream) {\r\n  const reader = readableStream.getReader()\r\n  while (true) {\r\n    const { done, value } = await reader.read()\r\n    if (done) {\r\n      break\r\n    }\r\n    socket.send(value)\r\n  }\r\n}\r\n\r\nsocket.onmessage = (message) => {\r\n  const received = JSON.parse(message.data)\r\n  const transcript = received.channel.alternatives[0].transcript\r\n  if (transcript && received.is_final) {\r\n    document.querySelector('#captions').textContent += transcript + ' '\r\n  }\r\n}\r\n\r\nsocket.onclose = () => {\r\n  console.log({ event: 'onclose' })\r\n}\r\n\r\nsocket.onerror = (error) => {\r\n  console.log({ event: 'onerror', error })\r\n}\n```\n\nHave questions? We're happy to help [@DeepgramDevs.](https://twitter.com/DeepgramDevs)\n\n        ";
						}
						async function compiledContent$2I() {
							return load$2I().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2I() {
							return (await import('./chunks/index.16eeb84e.mjs'));
						}
						function Content$2I(...args) {
							return load$2I().then((m) => m.default(...args));
						}
						Content$2I.isAstroComponentFactory = true;
						function getHeadings$2I() {
							return load$2I().then((m) => m.metadata.headings);
						}
						function getHeaders$2I() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2I().then((m) => m.metadata.headings);
						}

const __vite_glob_0_104 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2I,
  file: file$2I,
  url: url$2I,
  rawContent: rawContent$2I,
  compiledContent: compiledContent$2I,
  default: load$2I,
  Content: Content$2I,
  getHeadings: getHeadings$2I,
  getHeaders: getHeaders$2I
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2H = {"title":"Getting Started with ffmpeg for Audio","description":"Learn about the FFmpeg CLI for working with audio files.","date":"2021-11-08T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1635801056/blog/2021/11/ffmpeg-beginners/Getting-Started-with-ffmpeg-blog%402x.jpg","authors":["kevin-lewis"],"category":"tutorial","tags":["ffmpeg"],"seo":{"title":"Getting Started with ffmpeg for Audio","description":"Learn about the FFmpeg CLI for working with audio files."},"shorturls":{"share":"https://dpgr.am/d591b55","twitter":"https://dpgr.am/57448ea","linkedin":"https://dpgr.am/cab14a9","reddit":"https://dpgr.am/ae0bee6","facebook":"https://dpgr.am/bb519fb"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453799/blog/ffmpeg-beginners/ograph.png"}};
						const file$2H = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/ffmpeg-beginners/index.md";
						const url$2H = undefined;
						function rawContent$2H() {
							return "\r\nFFmpeg is an open source toolkit for converting and manipulating both audio and video files from the terminal. It's the go-to for this kind of work - if you've used any conversion or simple editing tools there is a good chance that it relies on FFmpeg in some way. When I started using FFmpeg I would have loved more audio-focused beginner tutorials, so this is to help those people who are in the same boat I was.\r\n\r\nBefore we begin you will need to download a few things:\r\n\r\n*   A copy of FFmpeg - download it [here](https://ffmpeg.org).\r\n*   Two audio files to work with - you can [download one here](https://static.deepgram.com/examples/nasa-spacewalk-interview.wav) and [the other here](https://static.deepgram.com/examples/Bueller-Life-moves-pretty-fast.wav).\r\n\r\nCreate a new directory on your computer. Pop FFmpeg and both audio files into it. Then, open the directory in your terminal.\r\n\r\n## Your First Command\r\n\r\nFFMpeg takes one or more files as an input and finishes a command by exporting a file. Between these two points we can further manipulate the file and export the result of the manipulations, but to start we will take in a file and spit out a new file in a different file format.\r\n\r\n```sh\r\n./ffmpeg -i nasa-spacewalk-interview.wav output.mp3\r\n```\r\n\r\nYou should see a new file appear in your directory. The `-i` tells FFmpeg that the next string is an **infile** that operations should happen against. You can provide multiple infiles like so:\r\n\r\n```sh\r\n./ffmpeg -i file1.wav -i file2.wav [...] output.wav\r\n```\r\n\r\n## Overwriting Files Automatically\r\n\r\nIf you run the above command more than once, you will be asked if you should overwrite the existing `output.mp3` file. To automatically overwrite add the `-y` flag before any infiles:\r\n\r\n```sh\r\n./ffmpeg -y -i nasa-spacewalk-interview.wav output.mp3\r\n```\r\n\r\nTo never overwrite you can replace `-y` with `-n`.\r\n\r\n## Minimizing Terminal Information\r\n\r\nThere is a lot of information shown in the terminal when using FFmpeg. When FFmpeg first runs it will tell you about the configuration of your FFmpeg instance followed by the information related to your command. You can hide the configuration by adding `-hide_banner` anywhere in your command:\r\n\r\n    ./ffmpeg -hide_banner -i nasa-spacewalk-interview.wav output.mp3\r\n\r\n## Trimming Audio\r\n\r\nWe have already seen several options which start with `-LETTER` and are followed by a value if required (like the `-i` needing a file name). To trim an audio file we need two options - a starting sample time and either a duration or an ending point.\r\n\r\nRun this command:\r\n\r\n```sh\r\n./ffmpeg -i nasa-spacewalk-interview.wav -ss 10 -t 15 output.mp3\r\n```\r\n\r\nThis will start at 10 seconds and create a clip, from that point, that lasts 15 seconds. The following command will create a sample from 10 seconds **to** 15 seconds (effectively lasting 5 seconds):\r\n\r\n```sh\r\n./ffmpeg -i nasa-spacewalk-interview.wav -ss 10 -to 15 output.mp3\r\n```\r\n\r\nWe won't cover it in great depth, but you can also provide timestamps in the format `HOURS:MINS:SECS.MILLISECONDS` instead of just a number of seconds:\r\n\r\n    ./ffmpeg -i nasa-spacewalk-interview.wav -ss 00:00:10.5 -to 00:00:15.75 output.mp3\r\n\r\nThat's 10.5 seconds to 15.75 seconds.\r\n\r\n## Simple Filters\r\n\r\nSimple audio filters (`-af`) have a single input, do a single thing, and provide a single output. Let's see how a few work:\r\n\r\n### Changing Volume Of Whole File\r\n\r\n```sh\r\n./ffmpeg -i nasa-spacewalk-interview.wav -af \"volume=0.25\" output.mp3\r\n```\r\n\r\nThis will set the volume to 25% for the whole sample.\r\n\r\n### Fading In Volume\r\n\r\n```sh\r\n./ffmpeg -y -i demo.wav -af \"afade=t=in:ss=0:d=15\" output.mp3\r\n```\r\n\r\nThis will fade in audio from the start of the file where it will be silent, to 15 seconds where it will be full volume.\r\n\r\n### Reduce Background Noise\r\n\r\n```sh\r\n./ffmpeg -i nasa-spacewalk-interview.wav -af \"highpass=f=200, lowpass=f=3000\" output.mp3\r\n```\r\n\r\nThis uses two audio filters at the same time and allows frequencies **higher** than 200hz to **pass**, and under **3000hz** to pass. You may need to play with the exact values. [Thanks to Stack Overflow user av8r for this one!](https://superuser.com/a/835585)\r\n\r\n## Complex Filters\r\n\r\nComplex filters are both complex in their functionality and their syntax. Unlike simple filters which do a single operation to a single input, complex filters can be chained together. We pass in audio by a variable name, do something to it, and export a new variable which we can then further chain in a single filter.\r\n\r\nThey follow this syntax:\r\n\r\n    [INPUT]operations[OUTPUT1];[OUTPUT1]operations[OUTPUT2]\r\n\r\nAs you can see, `OUTPUT1` is created as a result of the first filter, and fed into the next as an input. Inputs are number-based and zero indexed based on the order they are provided as infiles - that means the first infile is `[0]`, the second `[1]`, and so on.\r\n\r\nLet's see how this works in practice:\r\n\r\n### Overlaying Two Audio Files\r\n\r\n```sh\r\n./ffmpeg -y -i nasa-spacewalk-interview.wav -i Bueller-Life-moves-pretty-fast.wav -filter_complex \"[0][1]amix=inputs=2\" output.mp3\r\n```\r\n\r\n`amix` takes in both infiles and creates an output by directly overlaying them.\r\n\r\n### Trimming In Complex Filters\r\n\r\n```sh\r\n./ffmpeg -y -i Bueller-Life-moves-pretty-fast.wav -filter_complex \"[0]atrim=start=0:end=5\" output.mp3\r\n```\r\n\r\nThis creates a new audio file with just the first 5 seconds of the infile.\r\n\r\n### Fading in Complex Filters\r\n\r\n```sh\r\n./ffmpeg -y -i nasa-spacewalk-interview.wav -filter_complex \"[0]afade=t=in:ss=0:d=10\" output.mp3\r\n```\r\n\r\n### Combining Complex Filters\r\n\r\nYou can trim and fade audio files with simple filters, but what makes compelx filters so exciting is that you can combine them. Try this and we'll talk about it after:\r\n\r\n```sh\r\n./ffmpeg -y -i nasa-spacewalk-interview.wav -i Bueller-Life-moves-pretty-fast.wav -filter_complex \"[0]afade=t=in:ss=0:d=10[fadeIn];[1]atrim=start=0:end=5[trimmed];[fadeIn][trimmed]amix=inputs=2\" output.mp3\r\n```\r\n\r\nThere are three parts to this filter:\r\n\r\n1.  `[0]afade=t=in:ss=0:d=10[fadeIn];` takes the first infile and applies a 10 second fade in.\r\n2.  `[1]atrim=start=0:end=5[trimmed];` takes the second infile and trims it to the first 5 seconds.\r\n3.  `[fadeIn][trimmed]amix=inputs=2` takes the output of the above two steps and overlays them.\r\n\r\n## In Summary\r\n\r\nFFmpeg is hugely powerful, and with that comes a learning curve. We've only scratched the surface of what it can do but hopefully, its syntax makes more sense. If you have any questions please feel free to reach out on Twitter - we're [@DeepgramDevs](https://twitter.com/DeepgramDevs).\r\n\r\n        ";
						}
						async function compiledContent$2H() {
							return load$2H().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2H() {
							return (await import('./chunks/index.c6b0eb33.mjs'));
						}
						function Content$2H(...args) {
							return load$2H().then((m) => m.default(...args));
						}
						Content$2H.isAstroComponentFactory = true;
						function getHeadings$2H() {
							return load$2H().then((m) => m.metadata.headings);
						}
						function getHeaders$2H() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2H().then((m) => m.metadata.headings);
						}

const __vite_glob_0_105 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2H,
  file: file$2H,
  url: url$2H,
  rawContent: rawContent$2H,
  compiledContent: compiledContent$2H,
  default: load$2H,
  Content: Content$2H,
  getHeadings: getHeadings$2H,
  getHeaders: getHeaders$2H
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2G = {"title":"Converting Speech to Text in Flutter Applications","description":"In this tutorial, learn how to use Deepgram's speech recognition API with Flutter and Dart to convert speech to text on iOS and Android devices.","date":"2022-04-11T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1649265669/blog/2022/04/flutter-speech-to-text-tutorial/speech-to-text-in-flutter.jpg","authors":["greg-holmes"],"category":"tutorial","tags":["flutter","dart"],"seo":{"title":"Converting Speech to Text in Flutter Applications","description":"In this tutorial, learn how to use Deepgram's speech recognition API with Flutter and Dart to convert speech to text on iOS and Android devices."},"shorturls":{"share":"https://dpgr.am/6c54a98","twitter":"https://dpgr.am/fb5c142","linkedin":"https://dpgr.am/2d18377","reddit":"https://dpgr.am/05e6f5a","facebook":"https://dpgr.am/d39b1ff"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454063/blog/flutter-speech-to-text-tutorial/ograph.png"}};
						const file$2G = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/flutter-speech-to-text-tutorial/index.md";
						const url$2G = undefined;
						function rawContent$2G() {
							return "\r\nIn this tutorial, you'll learn how to transcribe your message in real-time from your device's microphone using [Deepgram's Speech Recognition API](https://developers.deepgram.com/api-reference/#transcription-streaming). The audio will be converted into data and live-streamed over WebSocket to Deepgram's servers, and then once transcribed, returned in JSON format back through the WebSocket.\r\n\r\n## Before We Start\r\n\r\nYou will need a Deepgram API Key for this project - [get one here](https://console.deepgram.com/signup?jump=keys).\r\n\r\nNext, head over to Flutter's documentation with instructions on [installing Flutter](https://docs.flutter.dev/get-started/install) onto your machine.\r\n\r\n## Create a Flutter Application\r\n\r\nDepending on which IDE you're using to develop your Flutter application, you'll need to configure it a little to be able to create a new Flutter project. So follow the instructions for your IDE on the Flutter documentation page, [Set up an editor](https://docs.flutter.dev/get-started/editor).\r\n\r\n## Add Device Specific Permissions\r\n\r\n### Android\r\n\r\nFor your application to perform certain tasks on Android, you need to request permissions for these, such as accessing the internet or recording audio, so open the file `android/app/src/main/AndroidManifest.xml` and inside the `<manifest ...`, add the following lines:\r\n\r\n```xml\r\n<uses-permission android:name=\"android.permission.INTERNET\"/>\r\n<uses-permission android:name=\"android.permission.RECORD_AUDIO\"/>\r\n```\r\n\r\nWhile you're in the Android directory, you'll need to change what versions you're defining for the SDK and what version you're targeting to compile. This change meets the requirements of the third-party package you'll install later. Open the file: `android/app/src/build.gradle` and first fine the line: `compileSdkVersion flutter.compileSdkVersion`. Replace this line with `compileSdkVersion 32`.\r\n\r\nNext, find the following two lines:\r\n\r\n    minSdkVersion flutter.minSdkVersion\r\n    targetSdkVersion flutter.targetSdkVersion\r\n\r\nUpdate these to the versions shown in the example below:\r\n\r\n```\r\nminSdkVersion 24\r\ntargetSdkVersion 32\r\n```\r\n\r\n### iOS\r\n\r\nFor your application to access the microphone on your iPhone or iPad, you'll need to grant permission to this component. Inside your `Podfile`, locate the line: `flutter_additional_ios_build_settings(target)` and below this add the following:\r\n\r\n```\r\ntarget.build_configurations.each do |config|\r\n  config.build_settings['GCC_PREPROCESSOR_DEFINITIONS'] ||= [\r\n    '$(inherited)',\r\n    # dart: PermissionGroup.microphone\r\n    'PERMISSION_MICROPHONE=1',\r\n  ]\r\nend\r\n```\r\n\r\nThen inside your `Info.plist`, within the `<dict></dict>` block, add the following two lines:\r\n\r\n```xml\r\n <key>NSMicrophoneUsageDescription</key>\r\n    <string>microphone</string>\r\n```\r\n\r\n## Add Your UI\r\n\r\nThe first thing you're going to need is a UI to be displayed on the mobile device; this UI will need three components:\r\n\r\n*   A `Text` area to display all transcribed wording,\r\n*   a \"start\" `OutlinedButton` to begin the transcription,\r\n*   and a \"stop\" `OutlinedButton` to stop live transcription.\r\n\r\nOpen the file `lib/main.dart`. In the `_MyHomePageState` class, replace the contents of this class with the `build` widget example shown below containing these three components:\r\n\r\n```dart\r\nWidget build(BuildContext context) {\r\n  return MaterialApp(\r\n    home: Scaffold(\r\n      appBar: AppBar(\r\n        title: const Text('Live Transcription with Deepgram'),\r\n      ),\r\n      body: Column(\r\n        mainAxisAlignment: MainAxisAlignment.spaceEvenly,\r\n        children: [\r\n          Row(\r\n            children: <Widget>[\r\n              Expanded(\r\n                flex: 3,\r\n                child: SizedBox(\r\n                  width: 150,\r\n                  child: Text(\r\n                    \"This is where your text is output\",\r\n                    textAlign: TextAlign.center,\r\n                    overflow: TextOverflow.ellipsis,\r\n                    maxLines: 50,\r\n                    style: const TextStyle(\r\n                        fontWeight: FontWeight.bold, fontSize: 15),\r\n                  ),\r\n                ),\r\n              ),\r\n            ],\r\n          ),\r\n          const SizedBox(height: 20),\r\n          Center(\r\n            child: Row(\r\n              mainAxisAlignment: MainAxisAlignment.center,\r\n              children: <Widget>[\r\n                OutlinedButton(\r\n                  style: ButtonStyle(\r\n                    backgroundColor:\r\n                        MaterialStateProperty.all<Color>(Colors.blue),\r\n                    foregroundColor:\r\n                        MaterialStateProperty.all<Color>(Colors.white),\r\n                  ),\r\n                  onPressed: () {\r\n\r\n                  },\r\n                  child: const Text('Start', style: TextStyle(fontSize: 30)),\r\n                ),\r\n                const SizedBox(width: 5),\r\n                OutlinedButton(\r\n                  style: ButtonStyle(\r\n                    backgroundColor:\r\n                        MaterialStateProperty.all<Color>(Colors.red),\r\n                    foregroundColor:\r\n                        MaterialStateProperty.all<Color>(Colors.white),\r\n                  ),\r\n                  onPressed: () {\r\n\r\n                  },\r\n                  child: const Text('Stop', style: TextStyle(fontSize: 30)),\r\n                ),\r\n              ],\r\n            ),\r\n          ),\r\n        ],\r\n      ),\r\n    ),\r\n  );\r\n}\r\n```\r\n\r\nYou can test your changes work by opening a new Terminal session and running `flutter run`. If you have connected your mobile device to your computer, your device will now have the application installed onto it, and you will see a screen similar to what's shown below:\r\n\r\n![A screenshot of a mobile phone running the demo Flutter app, a blue header with the text \"Live Transcription with Deepgram\", around a quarter of the way down the screen is the text \"This is where your text is output\" and then around three-quarters of the way down the screen are two buttons side by side, the first is a blue button with the white text \"Start\", the second is a red button with the white text \"Stop\"](https://res.cloudinary.com/deepgram/image/upload/v1649083047/blog/2022/04/flutter-speech-to-text-tutorial/initial-flutter-app-ui-with-start-stop-buttons.jpg)\r\n\r\n## Handling the Text State\r\n\r\nNext, your application needs to handle functionality to change the text displayed from a state instead. Find the line: `class _MyHomePageState extends State<MyHomePage> {` and just below this add the definition of the variable `myText` with the default text contained:\r\n\r\n```dart\r\n  String myText = \"To start transcribing your voice, press start.\";\r\n```\r\n\r\nIn your `_MyHomePageState` classes `Widget build()`, find the line: `\"This is where your text is output\"`. Replace this string with your new variable that will update whenever a response comes back from your transcription requests. So replace this line with `myText`.\r\n\r\nTwo new functions are now needed to manipulate this variable. The first one (`updateText`) updates the text with a predefined piece of text, while the second (`resetText`) resets the variable's value, clearing the text from the user's screen.\r\n\r\nWithin the `_MyHomePageState` class, add these two new functions:\r\n\r\n```dart\r\nvoid updateText(newText) {\r\n  setState(() {\r\n    myText = myText + ' ' + newText;\r\n  });\r\n}\r\n\r\nvoid resetText() {\r\n  setState(() {\r\n    myText = '';\r\n  });\r\n}\r\n```\r\n\r\nThese functions aren't used at the moment, to rectify this, find the `OutlinedButton` with the text `Start`, and populate the empty `onPressed: () {}` function, with the following:\r\n\r\n```dart\r\nonPressed: () {\r\n  updateText('');\r\n},\r\n```\r\n\r\n## Install the Dependencies\r\n\r\nThree third-party libraries are needed throughout this project, these libraries are:\r\n\r\n*   `sound_stream`, to handle the microphone input, convert it to data ready for streaming over a WebSocket.\r\n*   `web_socket_channel` provides functionality to make WebSocket connections which is how your application will communicate with Deepgram servers.\r\n*   `permission_handler` handles the mobile device's permissions, such as accessing the microphone.\r\n\r\nIn the root directory of your project, open the file that handles the importing of these libraries, `pubspec.yaml`. Now locate the `dependencies:` line and below this add the three libraries:\r\n\r\n```yaml\r\nweb_socket_channel: 2.1.0\r\nsound_stream: ^0.3.0\r\npermission_handler: ^9.2.0\r\n```\r\n\r\nOpen a new Terminal session and navigate to your project directory. Run the following command to install these two libraries:\r\n\r\n```bash\r\nflutter pub get\r\n```\r\n\r\n## Handle Audio Input\r\n\r\nAll of the configuration is now complete, it's time to handle the functionality to transcribe. Back in your `main.dart` file, at the top add the following libraries that you'll be using in this application (including your three newly installed third party libraries):\r\n\r\n```dart\r\nimport 'dart:async';\r\nimport 'dart:convert';\r\nimport 'package:sound_stream/sound_stream.dart';\r\nimport 'package:web_socket_channel/io.dart';\r\nimport 'package:permission_handler/permission_handler.dart';\r\n```\r\n\r\nBelow these imports, add two constants that you'll be calling in this application:\r\n\r\n```dart\r\nconst serverUrl =\r\n    'wss://api.deepgram.com/v1/listen?encoding=linear16&sample_rate=16000&language=en-GB';\r\nconst apiKey = '<your Deepgram API key>';\r\n```\r\n\r\nThese two constants are:\r\n\r\n*   `serverUrl` to define the URL the WebSocket will connect to (Deepgram's API server in this instance). For more information on the parameters available to you, please check the [API reference](https://developers.deepgram.com/api-reference/#transcription-streaming)\r\n*   `apiKey`, your Deepgram API key to authenticate when making the requests,\r\n\r\n> **Note:** the `apiKey` is hardcoded into this application solely for tutorial purposes. It is not good security practice to store API keys in mobile applications, so please be aware of this when building your mobile application.\r\n\r\nWith this tutorial, you'll need to request permission to access your microphone before attempting to transcribe your messaging. You'll do this when the app has loaded (it will only request permission once), add the following `initState()` function, which also calls `onLayoutDone` when the layout has loaded on the screen:\r\n\r\n```dart\r\n@override\r\nvoid initState() {\r\n  super.initState();\r\n\r\n  WidgetsBinding.instance?.addPostFrameCallback(onLayoutDone);\r\n}\r\n```\r\n\r\nNow below this `initState()` function add a new one called `onLayoutDone`, which is where your app will request permission:\r\n\r\n```dart\r\nvoid onLayoutDone(Duration timeStamp) async {\r\n  await Permission.microphone.request();\r\n  setState(() {});\r\n}\r\n```\r\n\r\nIt's now time to introduce the WebSocket and `sound_stream` to the project. First, you'll need to initiate the objects you'll be using that records sound and the web socket itself. Below your line `String myText ...` add the following:\r\n\r\n```dart\r\nfinal RecorderStream _recorder = RecorderStream();\r\n\r\nlate StreamSubscription _recorderStatus;\r\nlate StreamSubscription _audioStream;\r\n\r\nlate IOWebSocketChannel channel;\r\n```\r\n\r\nWhen the application closes, it's good practice to close any long running connections, whether that be with components in your device or over the Internet. So, create the `dispose()` function, and within this function cancel all audio handling, close the websocket channel:\r\n\r\n```dart\r\n@override\r\nvoid dispose() {\r\n  _recorderStatus.cancel();\r\n  _audioStream.cancel();\r\n  channel.sink.close();\r\n\r\n  super.dispose();\r\n}\r\n```\r\n\r\nNext, you need to initialize your web socket by providing your `serverUrl` and your `apiKey`. You'll also need to receive the audio stream from your microphone, convert it into binary data, and then send it over the WebSocket for Deepgram's API to transcribe. Because this is live transcription, the connection will remain open until you request it be closed. Add your new `_initStream()` function to your `_MyHomePageState` class.\r\n\r\n```dart\r\nFuture<void> _initStream() async {\r\n  channel = IOWebSocketChannel.connect(Uri.parse(serverUrl),\r\n      headers: {'Authorization': 'Token $apiKey'});\r\n\r\n  channel.stream.listen((event) async {\r\n    final parsedJson = jsonDecode(event);\r\n\r\n    updateText(parsedJson['channel']['alternatives'][0]['transcript']);\r\n  });\r\n\r\n  _audioStream = _recorder.audioStream.listen((data) {\r\n    channel.sink.add(data);\r\n  });\r\n\r\n  _recorderStatus = _recorder.status.listen((status) {\r\n    if (mounted) {\r\n      setState(() {});\r\n    }\r\n  });\r\n\r\n  await Future.wait([\r\n    _recorder.initialize(),\r\n  ]);\r\n}\r\n```\r\n\r\nThis functionality doesn't yet do anything; add a new `_startRecord` function, and within this, add the call to `_initStream()`. Calling this function tells `sound_stream` to switch on your microphone for streaming.\r\n\r\n```dart\r\nvoid _startRecord() async {\r\n  resetText();\r\n  _initStream();\r\n\r\n  await _recorder.start();\r\n\r\n  setState(() {});\r\n}\r\n```\r\n\r\nAlso add the following `_stopRecord()` function to stop the `_recorder`\r\n\r\n```dart\r\nvoid _stopRecord() async {\r\n  await _recorder.stop();\r\n\r\n  setState(() {});\r\n}\r\n```\r\n\r\nIn the first `OutlinedButton`, with the text `Start`, find the `onPressed: () {}` function and add the following to call your `_startRecord` function:\r\n\r\n```dart\r\nonPressed: () {\r\n  updateText('');\r\n\r\n  _startRecord();\r\n},\r\n```\r\n\r\nIn the next `OutlinedButton`, the text is `Stop`, find the `onPressed: () {}` function and add the following to call your `_stopRecord` function:\r\n\r\n```dart\r\nonPressed: () {\r\n  _stopRecord();\r\n},\r\n```\r\n\r\nYour application is ready to test once you have added functionality to start and stop the transcribing. If you go back to your Terminal and run `flutter run`, you'll see the application refresh on your mobile device. You may be prompted to give microphone access, so be sure to approve this. You can now start transcribing!\r\n\r\n![A screenshot of a mobile phone running the demo Flutter app, a blue header with the text \"Hello and welcome to your Deepgram live transcription demo\", around a quarter of the way down the screen is the text \"This is where your text is output\" and then around three-quarters of the way down the screen are two buttons side by side, the first is a blue button with the white text \"Start\", the second is a red button with the white text \"Stop\"](https://res.cloudinary.com/deepgram/image/upload/v1649083046/blog/2022/04/flutter-speech-to-text-tutorial/finished-flutter-app-ui-showing-transcription.jpg)\r\n\r\nThe final code for this tutorial is available on [GitHub](https://github.com/deepgram-devs/deepgram-live-transcription-flutter), and if you have any questions, please feel free to reach out to the Deepgram team on Twitter - [@DeepgramDevs](https://twitter.com/DeepgramDevs).\r\n\r\n        ";
						}
						async function compiledContent$2G() {
							return load$2G().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2G() {
							return (await import('./chunks/index.df704e62.mjs'));
						}
						function Content$2G(...args) {
							return load$2G().then((m) => m.default(...args));
						}
						Content$2G.isAstroComponentFactory = true;
						function getHeadings$2G() {
							return load$2G().then((m) => m.metadata.headings);
						}
						function getHeaders$2G() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2G().then((m) => m.metadata.headings);
						}

const __vite_glob_0_106 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2G,
  file: file$2G,
  url: url$2G,
  rawContent: rawContent$2G,
  compiledContent: compiledContent$2G,
  default: load$2G,
  Content: Content$2G,
  getHeadings: getHeadings$2G,
  getHeaders: getHeaders$2G
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2F = {"title":"Sharpen Your Foreign Language Skills with Triolingo's Chatbot","description":"With Deepgram and GPT-3, the team behind Triolingo created a realistic multilingual chatbot to help language learners gain confidence in conversation. Read about it here.","date":"2022-03-17T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1646156543/blog/2022/03/foreign-language-practice-triolingo/cover.jpg","authors":["kevin-lewis"],"category":"project-showcase","tags":["machine-learning","gpt-3"],"seo":{"title":"Sharpen Your Foreign Language Skills with Triolingo's Chatbot","description":"With Deepgram and GPT-3, the team behind Triolingo created a realistic multilingual chatbot to help language learners gain confidence in conversation. Read about it here."},"shorturls":{"share":"https://dpgr.am/8d8733c","twitter":"https://dpgr.am/f1eccfe","linkedin":"https://dpgr.am/96c7dfd","reddit":"https://dpgr.am/b41771a","facebook":"https://dpgr.am/82c0534"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454026/blog/foreign-language-practice-triolingo/ograph.png"}};
						const file$2F = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/foreign-language-practice-triolingo/index.md";
						const url$2F = undefined;
						function rawContent$2F() {
							return "\r\nBack in January, we supported Hack Cambridge -- a 24-hour student hackathon. The team behind Triolingo wanted to help language learners gain confidence in conversation by practicing with a bot. I sat down with [Alba Navarro Rosales](https://github.com/Alba-NR), [Anoushka Kamazumdar](https://github.com/anoushkamazumdar), [Max Johnson](https://github.com/MaxTheComputerer), and [Megan Elisabeth Finch](https://github.com/meganelisabethfinch) to ask them about their project.\r\n\r\n\"We were inspired by Deepgram's sponsor challenge to create something cool using speech recognition and were excited to see that it supported [several foreign languages](https://developers.deepgram.com/documentation/features/language/),\" the team told me. \"Over the past three years, the coronavirus pandemic has had a significant impact on schools, and travel restrictions have limited opportunities for foreign language learning abroad. During this time, the use of online language learning platforms such as Duolingo has soared, but these platforms cannot provide practice for speaking and listening skills. We created Triolingo to cater to this niche, allowing language learners to gain confidence in conversation through practice.\"\r\n\r\n## How It Works\r\n\r\nUsers select a conversation topic and target language, and the Triolingo bot then begins a conversation with several topic-appropriate prompts. Users then record a verbal response sent for processing by the [Deepgram Python SDK](https://developers.deepgram.com/sdks-tools/).\r\n\r\nNo two chats are the same as the multilingual chatbot is powered by GPT-3 [provided by OpenAI](https://openai.com/api/), which dynamically responds to prompts. Finally, responses are spoken back to users using a text-to-speech API.\r\n\r\nCare was taken to include extended topics beyond everyday and tourism-focused conversation -- prompts included culture, climate change, and politics.\r\n\r\n![The webpage shows two starter topics - food and lifestyle, and three advanced topics - environment, sustainability, and culture. Selecting a topic begins a conversation.](https://res.cloudinary.com/deepgram/image/upload/v1646156558/blog/2022/03/foreign-language-practice-triolingo/screenshot.png)\r\n\r\n## Hackathon Experience\r\n\r\nThe Triolingo team had only participated in online hackathons before, so this was a new experience. As a large group of twelve people, they self-organized a random name picker and created three teams within the event's team size limit. \"I've never used a GPT-3 API before, and it was both super cool and very impressive\" said Alba.\r\n\r\nI asked the team about their experience using Deepgram, and Max said that \"the performance was really good and accurate, even with background noise.\" As their project progressed, they were visited by other teams who had fun trying it out.\r\n\r\n## Future Development\r\n\r\nGiven more time, the team would use additional Deepgram functionality such as [confidence](https://developers.deepgram.com/documentation/guides/transcription/#analyze-response) values in a Deepgram response to assess the user's pronunciation. Our [keywords](https://developers.deepgram.com/documentation/features/keywords/) feature would boost recognition of words related to the conversation topic and further improve the reliability of the speech recognition function.\r\n\r\nIn terms of user interaction, the team would like to set contextual \"challenges\" or tasks to complete instead of just conversing without direction. For example, the user is presented with the scenario, \"You are planning to watch a movie with a friend. Decide what movie you're going to watch and when and where you're going to meet.\" The system would keep track of whether the user and bot had agreed on these three things, and then congratulate the user when they had completed the challenge.\r\n\r\nThere are different grammatical constructs in some languages depending on who you are talking to, such as different pronouns or verb endings. As a final idea, the bot could adopt the appropriate type of language according to the situation.\r\n\r\nYou can try out a [hosted version of Triolingo](https://triolingo.blockfour.co.uk/), and check out the [code on GitHub](https://github.com/meganelisabethfinch/HackCambridgeAtlas).\r\n\r\n        ";
						}
						async function compiledContent$2F() {
							return load$2F().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2F() {
							return (await import('./chunks/index.5ee30198.mjs'));
						}
						function Content$2F(...args) {
							return load$2F().then((m) => m.default(...args));
						}
						Content$2F.isAstroComponentFactory = true;
						function getHeadings$2F() {
							return load$2F().then((m) => m.metadata.headings);
						}
						function getHeaders$2F() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2F().then((m) => m.metadata.headings);
						}

const __vite_glob_0_107 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2F,
  file: file$2F,
  url: url$2F,
  rawContent: rawContent$2F,
  compiledContent: compiledContent$2F,
  default: load$2F,
  Content: Content$2F,
  getHeadings: getHeadings$2F,
  getHeaders: getHeaders$2F
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2E = {"title":"Upgrade your freeCodeCamp Project","description":"As early-career developers looking for your first or second job, upgrading your projects can help you to stand out from other candidates and grow as a new developer. To find out how to upgrade a common project, read more here.","date":"2022-03-31T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1648719341/blog/2022/03/freecodecamp-quote-generator-upgrade/fcc-cover.jpg","authors":["bekah-hawrot-weigel"],"category":"tutorial","tags":["javascript","beginner","freecodecamp"],"seo":{"title":"Upgrade your freeCodeCamp Project","description":"As early-career developers looking for your first or second job, upgrading your projects can help you to stand out from other candidates and grow as a new developer. To find out how to upgrade a common project, read more here."},"shorturls":{"share":"https://dpgr.am/521c26c","twitter":"https://dpgr.am/ab44d6f","linkedin":"https://dpgr.am/fc5dec0","reddit":"https://dpgr.am/3f6054d","facebook":"https://dpgr.am/c47de40"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454028/blog/freecodecamp-quote-generator-upgrade/ograph.png"}};
						const file$2E = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/freecodecamp-quote-generator-upgrade/index.md";
						const url$2E = undefined;
						function rawContent$2E() {
							return "\nI spend a lot of time with folks learning to code and early-career developers. One of the things I’ve noticed is that it’s increasingly hard to find your first job. One of the biggest comments I’ve heard from hiring managers is that so many applicants for junior positions are showcasing the same projects. As I was brainstorming some use cases for Deepgram, I wanted to work on something that could help out these folks looking for their first development job. That’s how I decided to upgrade one of the [freeCodeCamp Front End Development Libraries Projects](https://www.freecodecamp.org/learn/front-end-development-libraries/#front-end-development-libraries-projects). In this post, we’re going to take the [Quote Generator Project](https://www.freecodecamp.org/learn/front-end-development-libraries/front-end-development-libraries-projects/build-a-random-quote-machine) up a notch and use [Deepgram’s Node SDK](https://developers.deepgram.com/sdks-tools/sdks/node-sdk/) to fetch a quote.\n\n## Prepping our Project\n\n<Panel type=\"info\" title=\"freeCodeCamp Side Note\">\n\nFor this tutorial, we’re assuming that you have completed the project. If you haven’t, you can find the <a href=\"https://www.freecodecamp.org/learn/front-end-development-libraries/front-end-development-libraries-projects/build-a-random-quote-machine\">directions for the quote generator here</a> or take a look at <a href=\"https://codepen.io/freeCodeCamp/pen/qRZeGZ\">freeCodeCamp’s demo project</a> to get you started.\n\n</Panel>\n\n*   Download Node.js if you haven’t already - get it [here](https://nodejs.org/en/)\n*   Create a Deepgram API Key with an admin or owner role - [get it here](https://console.deepgram.com/signup?jump=keys)\n*   Create a file called .env and add `DG_KEY='your-api-key'`.\n\n<Panel type=\"info\" title=\"Notes on API keys\">\n\n*   Your API key should be a string of letters and numbers that you wrap in single quotes.\n*   .env files contain sensitive values. We’ll use a .gitignore file to ensure we don’t expose our information, but if you’re sharing your code with others, don’t include your sensitive information.\n\n</Panel>\n\n### Where we started\n\nBefore we get into upgrading our freeCodeCamp projects, let’s take a look at the core functionality of our quote generator. When we open our project, there’s a quote to start. When we click the New Quote button, our quote machine fetches a new quote and author and displays that in the quote box. When we click the Twitter button, it takes the quote we see on the screen and creates a new tweet.\n\n![gif of clicking the new quote button](https://res.cloudinary.com/deepgram/image/upload/v1648826507/blog/2022/03/freecodecamp-quote-generator-upgrade/new-quote.gif)\n\nHere’s the basic functionality that allows us to create the new quote:\n\n```js\nfunction getRandomQuote() {\n  fetch(\n    'https://gist.githubusercontent.com/nasrulhazim/54b659e43b1035215cd0ba1d4577ee80/raw/e3c6895ce42069f0ee7e991229064f167fe8ccdc/quotes.json'\n  )\n    .then((response) => response.json())\n    .then((data) => {\n      document.querySelector('#text').innerText =\n        data.quotes[`${random(99)}`].quote\n      document.querySelector('#author').innerText =\n        data.quotes[`${random(99)}`].author\n    })\n}\n\nnewQuoteButton.addEventListener('click', () => {\n  const rndCol = `rgb(${random(255)}, ${random(255)}, ${random(255)}, 0.4)`\n  document.body.style.backgroundColor = rndCol\n\n  getRandomQuote()\n})\n```\n\nAs you can see, the project fetches quotes from a JSON file of quotes and authors on GitHub. You can find the one I used [here](https://gist.githubusercontent.com/nasrulhazim/54b659e43b1035215cd0ba1d4577ee80/raw/e3c6895ce42069f0ee7e991229064f167fe8ccdc/quotes.json).\n\nAs you work on this project, it’s useful to have some understanding of APIs and JSON.\n\n<Panel type=\"info\" title=\"Resource Break!\">\n\n<li><a href=\"https://blog.deepgram.com/getting-started-with-apis/\">Getting Started with APIs</a>  by <a href=\"https://blog.deepgram.com/authors/kevin-lewis/\">Kevin Lewis</a></li>\n<li><a href=\"https://blog.deepgram.com/getting-started-with-json/\">Getting Started with JSON</a>  by <a href=\"https://blog.deepgram.com/authors/sandra-rodgers/\">Sandra Rodgers</a></li>\n</Panel>\n\n### Overview\n\nThe way this looks isn’t going to change, *but* how we’re getting the quotes will. Instead of fetching from the gist, we will fetch a random movie from [this gist](https://gist.github.com/BekahHW/394d81b484f264b0c8b23c0e177f8588), and then transcribe that using Deepgram’s Node SDK.\n\nRather than using CodePen, I will be working in a public repository. We’ll need a package.json file for this project to install some packages to help us get the data we need.\n\n#### File Structure\n\nWe’ll be using a public folder for all the files that impact the front end of our project. We’ll only be actively working on a total of two files, with the assumption that you’ll keep your styles the same.\n\n*   `app.js` will contain our front-end logic with our click events, which will be in the public folder. This is the file that controls rendering the quote and author on the screen.\n*   `server.js` is where we’ll work with our server-side logic. We’ll use `node-fetch`--more on this later–to get the data we need from the JSON of movie quotes. We’ll also use Deepgram’s Node SDK to transcribe the quotes and get them on the screen.\n\nHere’s what our file structure is going to look like:\n\n![image of the file structure](https://res.cloudinary.com/deepgram/image/upload/v1648826506/blog/2022/03/freecodecamp-quote-generator-upgrade/folder.png)\n\n## Getting Started\n\nOnce you have your project in a repository, cd into your project from your terminal and follow the following steps:\n\n```bash\nnpm i @deepgram/sdk dotenv express\nnpm i -g gitignore && gitignore node\n```\n\nThese are all the packages we’ll need to get us up and running in this project.\n\n#### Package breakdown\n\n*   [dotenv](https://github.com/motdotla/dotenv#readme) - “Dotenv is a zero-dependency module that loads environment variables from a .env file into process.env”\n*   [gitignore node and gitignore node](https://github.com/msfeldstein/gitignore) - “​​Automatically fetch github's excellent .gitignore files for any of your new projects”\n*   [express](https://expressjs.com/) - Node framework that connects your server-side to your client-side.\n\n<Panel type=\"info\" title=\"Another Resource Break!\">\n\nIf you’re getting started with learning express like I was, here are a couple of resources to get you started:\n\n*   Sandra’s post [Sending Audio Files to Your Express.js Server](https://blog.deepgram.com/sending-audio-files-to-expressjs-server/) can help you better understand express.\n*   [Express/Node Introduction](https://developer.mozilla.org/en-US/docs/Learn/Server-side/Express_Nodejs/Introduction)\n\n</Panel>\n\n## Updating Your Project with Deepgram\n\nNow we’re ready to upgrade your freeCodeCamp Quote Generator. We’ll start in the `server.js` file. We need to require the packages we just installed to ensure we can use them. At the top of your file add the following:\n\n```js\nrequire('dotenv').config()\nconst express = require('express')\nconst fetch = require('node-fetch')\nconst app = express()\nconst { Deepgram } = require('@deepgram/sdk')\n```\n\nNext, we need to connect our `app.js` file to our `server.js` file. Below the previous code, add the following lines of code:\n\n```js\napp.use(express.static('public'))\nconst deepgram = new Deepgram(process.env.DG_KEY)\n```\n\nThe first line in that block allows us to serve static files to our Express app. Because we have ‘public’ in there, our app has access to the files in our public folder. If you want a more thorough explanation, you can check out [this Mastering JS tutorial](https://masteringjs.io/tutorials/express/app-use-static).\n\nThe second line creates a new instance of Deepgram using our API key that we added to our `.env` file. Because we’ve added the dotenv packages, we have access to that key when we add the `process.env` before the variable name for our API key.\n\n### Accessing the Quote - Logic\n\nNext up, we will add the logic that allows us to access the gist of movie quote data. This is where we’ll also be using [node-fetch](https://github.com/node-fetch/node-fetch), which we’ve named “fetch” in this project.\n\nWe’re going to put all of our logic in an async function. With async functions, we know we’ll have to wait for a task, but our application can continue to be responsive while waiting. This is a fairly complex concept, so don’t worry if you don’t fully grasp it right now. You can check out [mdn’s Introduction to asynchronous JavaScript](https://developer.mozilla.org/en-US/docs/Learn/JavaScript/Asynchronous/Introducing) for more information.\n\nLet’s start with this:\n\n```js\nasync function getTranscript() {}\n```\n\nWithin this space, we’re going to add:\n\n*   A function that allows us to randomize our quotes\n*   Logic to get the wav file and the speaker (although we’re referring to them as “author” in this post).\n\nJust after the first curly brace, we’re going to add our random function with this code:\n\n```js\nasync function getTranscript() {\n  function random(number) {\n    return Math.floor(Math.random() * (number + 1))\n  }\n}\n```\n\nNow, we want to make sure that we get a random file and the author associated with that quote. To do that, add the following code underneath our getTranscript function:\n\n```js\nconst randomNumber = random(6)\nconst response = await fetch(\n  'https://gist.githubusercontent.com/BekahHW/394d81b484f264b0c8b23c0e177f8588/raw/df7bba8dde4f96487dd843977a07991aba4ca511/quotes.json'\n)\n\nconst data = await response.json()\nconst audioUrl = data[randomNumber].quote\nconst author = data[randomNumber].author\n```\n\nOur randomNumber variable ensures that the file we’re passing to Deepgram (coming up next!) is associated with the author of that quote.\n\nWith `response` and `data`, we’re accessing the gist of movie quotes.\n\nIf we console.log(data), we’ll get this plus the rest of the array of objects:\n\n![image of an array of objects with wav files and authors](https://res.cloudinary.com/deepgram/image/upload/v1648826505/blog/2022/03/freecodecamp-quote-generator-upgrade/json.png)\n\nWhen we console.log(author), we’ll see one of those author’s names as we’re accessing one item in the array.\n\nWe’ve made huge progress! Now we’re ready to use Deepgram to upgrade this freeCodeCamp project!\n\nBelow the code we’ve just written, but within the getTranscript function, we’re going to add what we need to get the transcript from the wav file:\n\n```js\nconst quoteTranscription = await deepgram.transcription\n  .preRecorded({ url: audioUrl }, { punctuate: true, language: 'en-US' })\n  .then((transcription) => transcription.results.channels[0].alternatives[0])\n\nreturn {\n  author: author,\n  transcription: quoteTranscription,\n}\n```\n\nA couple of things with that code block:\nWe’re using pre-recorded audio, which you can find more about in our [Deepgram docs on pre-recorded transcription](https://developers.deepgram.com/sdks-tools/sdks/node-sdk/pre-recorded-transcription/).\n\n1.  You need to pass the link to the audio file. In this case, we do it with `url: audioUrl`.\n    We get access to the transcription of the wav file with `transcription.results.channels[0].alternatives[0]`\n2.  We’re returning both the author and the transcription because we need to send them to our app.js file to render in our quote-box.\n\nNow we’re ready to connect all that work in `server.js` to `app.js`. After that code and outside of the function, add this code block:\n\n```js\napp.get('/transcribe', (req, res) => {\n  getTranscript()\n    .then((transcriptObj) => res.send(transcriptObj))\n    .catch((err) => {\n      console.log(err)\n    })\n})\n```\n\nThis is where we’re using express. The /express path should lead you to JSON data that we’re accessing. We’re calling getTranscript, so we have access to the author and transcription values. To send that to our `app.js` file, we use res.send. Right now, we’re not sending it there because we haven’t connected those paths. Before we do that, let’s make sure express is listening to the server. Add this code to the very bottom of the file:\n\n```js\napp.listen(3000, () => {\n  console.log(`Example app listening on port 3000`)\n})\n```\n\nNow we’re ready to connect our work. We’re almost done!\n\n#### `app.js`\n\nIn our `app.js` file, we have an event listener attached to our new quote button. Previously when we clicked this, it would fetch from the quote gist. We’re going to replace that with this code:\n\n```js\nfetch('/transcribe')\n  .then((r) => r.json())\n  .then((res) => {\n    document.querySelector('#text').innerText = res.transcription.transcript\n    document.querySelector('#author').innerText = res.author\n  })\n```\n\nWe’re connecting the path in `server.js` when we fetch(\\`/transcribe’). Then we’re taking that transcript object, getting it in a usable format, and then sending the text and author divs according to that data.\n\nWe should be ready!\n\nGo to your terminal and run `node server.js`. You should be able to navigate to `http://localhost:3000/` and see your app. Go ahead and click the New Quote button and see the magic happen.\n\nWhew. That’s a pretty cool update. If you want to check out the code in its entirety, you can navigate to our [freeCodeCamp-Quote-Generator repo](https://github.com/deepgram-devs/freeCodeCamp-Quote-Generator) or to get you up and running faster, check out our [freeCodeCamp Quote Gen with Deepgram Stackblitz](https://stackblitz.com/edit/fcc-dg0). When you open this project it will automatically fork it for you. You just need to add your own `.env`. Then, in the terminal, run `node server.js` and see it live!\n\nThis is a project I hope to build on. We’ve been doing Twitter Spaces at Deepgram, and once we have some edited, I’d love to use the quote generator to show random quotes and allow you to select the full recording if you’re interested in the quote. Be on the lookout :eyes: for that future post.\n\nIf you have any questions or want a walkthrough of the how to build a freeCodeCamp project with the Deepgram update, hit us up on [@DeepgramDevs](https://twitter.com/DeepgramDevs) on Twitter.\n\n        ";
						}
						async function compiledContent$2E() {
							return load$2E().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2E() {
							return (await import('./chunks/index.1c5f1abb.mjs'));
						}
						function Content$2E(...args) {
							return load$2E().then((m) => m.default(...args));
						}
						Content$2E.isAstroComponentFactory = true;
						function getHeadings$2E() {
							return load$2E().then((m) => m.metadata.headings);
						}
						function getHeaders$2E() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2E().then((m) => m.metadata.headings);
						}

const __vite_glob_0_108 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2E,
  file: file$2E,
  url: url$2E,
  rawContent: rawContent$2E,
  compiledContent: compiledContent$2E,
  default: load$2E,
  Content: Content$2E,
  getHeadings: getHeadings$2E,
  getHeaders: getHeaders$2E
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2D = {"title":"Generate WebVTT and SRT Captions Automatically with Node.js","description":"Create ready-to-upload caption files for the web and broadcast.","date":"2021-11-15T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1636406494/blog/2021/11/generate-webvtt-srt-captions-nodejs/Generate-WebVTT-SRT-Captions-w-Nodejs%402x.jpg","authors":["kevin-lewis"],"category":"tutorial","tags":["nodejs","accessibility"],"seo":{"title":"Generate WebVTT and SRT Captions Automatically with Node.js","description":"Create ready-to-upload caption files for the web and broadcast."},"shorturls":{"share":"https://dpgr.am/edad73f","twitter":"https://dpgr.am/40622a2","linkedin":"https://dpgr.am/3530253","reddit":"https://dpgr.am/76f3d80","facebook":"https://dpgr.am/2991d2d"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453801/blog/generate-webvtt-srt-captions-nodejs/ograph.png"}};
						const file$2D = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/generate-webvtt-srt-captions-nodejs/index.md";
						const url$2D = undefined;
						function rawContent$2D() {
							return "\nProviding captions for audio and video isn't just a nice-to-have - it's critical for accessibility. While this isn't specifically an accessibility post, I wanted to start by sharing [Microsoft's Inclusive Toolkit](https://www.microsoft.com/design/inclusive/). Something I hadn't considered before reading this was the impact of situational limitations. To learn more, jump to Section 3 of the toolkit - \"Solve for one, extend to many\". Having a young (read \"loud\") child, I've become even more aware of where captions are available, and if they aren't, I simply can't watch something with her around.\n\nThere are two common and similar caption formats we are going to generate today - WebVTT and SRT. A WebVTT file looks like this:\n\n    WEBVTT\n\n    1\n    00:00:00.219 --> 00:00:03.512\n    - yeah, as much as it's worth celebrating\n\n    2\n    00:00:04.569 --> 00:00:06.226\n    - the first space walk\n\n    3\n    00:00:06.564 --> 00:00:07.942\n    - with an all female team\n\n    4\n    00:00:08.615 --> 00:00:09.795\n    - I think many of us\n\n    5\n    00:00:10.135 --> 00:00:13.355\n    - are looking forward to it just being normal.\n\nAnd a SRT file looks like this:\n\n    1\n    00:00:00,219 --> 00:00:03,512\n    yeah, as much as it's worth celebrating\n\n    2\n    00:00:04,569 --> 00:00:06,226\n    the first space walk\n\n    3\n    00:00:06,564 --> 00:00:07,942\n    with an all female team\n\n    4\n    00:00:08,615 --> 00:00:09,795\n    I think many of us\n\n    5\n    00:00:10,135 --> 00:00:13,355\n    are looking forward to it just being normal.\n\nBoth are very similar in their basic forms, except for the millisecond separator being `.` in WebVTT and `,` in SRT. In this post, we will generate them manually from a Deepgram transcription result to see the technique, and then use the brand new Node.js SDK methods (available from v1.1.0) to make it even easier.\n\n## Before We Start\n\nYou will need:\n\n*   Node.js installed on your machine - [download it here](https://nodejs.org/en/).\n*   A Deepgram API Key - [get one here](https://console.deepgram.com/signup?jump=keys).\n*   A hosted audio file URL to transcribe - you can use https://static.deepgram.com/examples/deep-learning-podcast-clip.wav if you don't have one.\n\nCreate a new directory and navigate to it with your terminal. Run `npm init -y` to create a `package.json` file and then install the Deepgram Node.js SDK with `npm install @deepgram/sdk`.\n\n## Set Up Dependencies\n\nCreate an `index.js` file, open it in your code editor, and require then initialize the dependencies:\n\n```js\nconst fs = require('fs')\nconst { Deepgram } = require('@deepgram/sdk')\nconst deepgram = new Deepgram('YOUR_API_KEY')\n```\n\n## Get Transcript\n\nTo be given timestamps of phrases to include in our caption files, you need to ask Deepgram to include utterances (a chain of words or, more simply, a phrase).\n\n```js\ndeepgram.transcription\n  .preRecorded(\n    {\n      url: 'https://static.deepgram.com/examples/deep-learning-podcast-clip.wav',\n    },\n    { punctuate: true, utterances: true }\n  )\n  .then((response) => {\n    //  Following code here\n  })\n  .catch((error) => {\n    console.log({ error })\n  })\n```\n\n## Create a Write Stream\n\nOnce you open a writable stream, you can insert text directly into your file. When you do this, pass in the `a` flag, and any time you write data to the stream, it will be appended to the end. Inside of the `.then()` block:\n\n```js\n// WebVTT Filename\nconst stream = fs.createWriteStream('output.vtt', { flags: 'a' })\n\n// SRT Filename\nconst stream = fs.createWriteStream('output.srt', { flags: 'a' })\n```\n\n## Write Captions\n\nThe WebVTT and SRT formats are very similar, and each requires a block of text per utterance.\n\n### WebVTT\n\n```js\nstream.write('WEBVTT\\n\\n')\nfor (let i = 0; i < response.results.utterances.length; i++) {\n  const utterance = response.results.utterances[i]\n  const start = new Date(utterance.start * 1000).toISOString().substr(11, 12)\n  const end = new Date(utterance.end * 1000).toISOString().substr(11, 12)\n  stream.write(`${i + 1}\\n${start} --> ${end}\\n- ${utterance.transcript}\\n\\n`)\n}\n```\n\nDeepgram provides seconds back as a number (`15.4` means 15.4 seconds), but both formats require times as `HH:MM:SS.milliseconds` and getting the end of a `Date().toISOString()` will achieve this for us.\n\n#### Using the SDK\n\nReplace the above code with this single line:\n\n```js\nstream.write(response.toWebVTT())\n```\n\n### SRT\n\n```js\nfor (let i = 0; i < response.results.utterances.length; i++) {\n  const utterance = response.results.utterances[i]\n  const start = new Date(utterance.start * 1000)\n    .toISOString()\n    .substr(11, 12)\n    .replace('.', ',')\n  const end = new Date(utterance.end * 1000)\n    .toISOString()\n    .substr(11, 12)\n    .replace('.', ',')\n  stream.write(`${i + 1}\\n${start} --> ${end}\\n${utterance.transcript}\\n\\n`)\n}\n```\n\nDifferences? No `WEBVTT` line at the top, millisecond separator is `,`, and no `-` before the utterance.\n\n#### Using the SDK\n\nReplace the above code with this single line:\n\n```js\nstream.write(response.toSRT())\n```\n\n## One Line to Captions\n\nWe actually implemented `.toWebVTT()` and `.toSRT()` straight into the Node.js SDK while writing this post. Now, it's easier than ever to create valid caption files automatically with Deepgram. If you have any questions, please feel free to reach out on Twitter - we're [@DeepgramDevs](https://twitter.com/DeepgramDevs).\n\n        ";
						}
						async function compiledContent$2D() {
							return load$2D().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2D() {
							return (await import('./chunks/index.833ac165.mjs'));
						}
						function Content$2D(...args) {
							return load$2D().then((m) => m.default(...args));
						}
						Content$2D.isAstroComponentFactory = true;
						function getHeadings$2D() {
							return load$2D().then((m) => m.metadata.headings);
						}
						function getHeaders$2D() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2D().then((m) => m.metadata.headings);
						}

const __vite_glob_0_109 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2D,
  file: file$2D,
  url: url$2D,
  rawContent: rawContent$2D,
  compiledContent: compiledContent$2D,
  default: load$2D,
  Content: Content$2D,
  getHeadings: getHeadings$2D,
  getHeaders: getHeaders$2D
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2C = {"title":"Generic ASR will never be accurate enough for Conversational AI","description":"Intent and focus matters for speech understanding for Conversational AI and voicebots. Generic ASRs cannot be tailored for Conversational AI needs. An End to End Deep Learning ASR is needed.","date":"2021-04-07T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981366/blog/generic-asr-will-never-be-accurate-enough-for-conversational-ai/generic-asr-never-accurate-enough%402x.jpg","authors":["keith-lam"],"category":"ai-and-engineering","tags":["conversational-ai","deep-learning"],"seo":{"title":"Generic ASR will never be accurate enough for Conversational AI","description":"Intent and focus matters for speech understanding for Conversational AI and voicebots. Generic ASRs cannot be tailored for Conversational AI needs. An End to End Deep Learning ASR is needed."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981366/blog/generic-asr-will-never-be-accurate-enough-for-conversational-ai/generic-asr-never-accurate-enough%402x.jpg"},"shorturls":{"share":"https://dpgr.am/6fb3275","twitter":"https://dpgr.am/6694630","linkedin":"https://dpgr.am/1ad18a2","reddit":"https://dpgr.am/4a6a8c6","facebook":"https://dpgr.am/cd8a2f3"}};
						const file$2C = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/generic-asr-will-never-be-accurate-enough-for-conversational-ai/index.md";
						const url$2C = undefined;
						function rawContent$2C() {
							return "The human brain is amazing in terms of how we can process speech and understand what is said.  If we are talking about a baseball game, your brain understands that when I say \"pitcher\" and \"batter\", I don't mean a large vessel for pouring drinks and a mix to cook pancakes.  Your brain matches the words to the context and the intent of the conversation.  Your brain also has an amazing noise filter to focus on the important parts of a conversation.  If you are at a baseball game, there is constant noise around you but when your buddy talks to you, you can focus on his voice, hear him and understand him clearly.  \n\n## **Intent Matters**\n\nHow does a Conversational AI system determine the intent of the conversation and focus on the important words?  Let's talk about a possible future Conversational AI example.  Imagine a robot waiter at a local pub. There are four conversations going on around it. The booth to its left is talking about a weird internet video. A table behind it is complaining about the last place the group ate and how bad the chicken was.  And finally, the table in front of it has delegated the task of ordering appetizers to the person at the back of the table, with everyone throwing their requests their way. Given a one hundred percent accurate transcript of audible conversation at the table it would be really hard for the robot to understand what should be happening here. Did they just order chicken tenders or was that the other table? Was that two orders of the appetizer or was that first person asking the other person to order it? Was that 'mh-uh' a no they don't want the biggie sized version or was it just a throat clearing?\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-how-deepgram-works\"></WhitepaperPromo>\n\n## **Accuracy with Intent**\n\nIn the example above, a generic Automatic Speech Recognition (ASR) could transcribe all the audio in the immediate area of the robot and get a jumbled transcription, all the words may be accurate with a 10% Word Error Rate (WER) but does that help a Conversational AI system to understand what was said or respond appropriately?  You need accuracy that is focused on the important keywords and the intent of the conversation to gain understanding. ASR for Conversational AI and voicebots cannot be generic, it will never be accurate enough.  ASR must have a speech model tailored for the intent of conversation and can focus on the keywords important to understanding.  This tailored approach helps to remove background noise and speech that is not part of the intent of the conversation. So, what type of ASR is able to be tailored to your Conversational AI, an End to End Deep Learning ASR.  This type of ASR can be trained with your audio data to make sure the intent is captured and the transcription is accurate for your use case.  It can also be continually trained and improved to gain more accuracy and focus. Hear more about how \"**Generic ASR will never be accurate enough for Conversational AI\"** in our speaking session at the 2021 Conversational AI and NLP summit by RE WORK.  Sign up [here](https://www.re-work.co/events/conversational-ai-nlp-summit-2021/register).";
						}
						async function compiledContent$2C() {
							return load$2C().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2C() {
							return (await import('./chunks/index.d56a07a2.mjs'));
						}
						function Content$2C(...args) {
							return load$2C().then((m) => m.default(...args));
						}
						Content$2C.isAstroComponentFactory = true;
						function getHeadings$2C() {
							return load$2C().then((m) => m.metadata.headings);
						}
						function getHeaders$2C() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2C().then((m) => m.metadata.headings);
						}

const __vite_glob_0_110 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2C,
  file: file$2C,
  url: url$2C,
  rawContent: rawContent$2C,
  compiledContent: compiledContent$2C,
  default: load$2C,
  Content: Content$2C,
  getHeadings: getHeadings$2C,
  getHeaders: getHeaders$2C
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2B = {"title":"Getting Started with Live Transcription and Vue.js","description":"Learn how to use Deepgram's Speech-to-Text API for fast and accurate live transcripts in your Vue.js applications.","date":"2022-07-18T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1658067804/blog/2022/07/getting-started-live-transcription-vue/cover.jpg","authors":["kevin-lewis"],"category":"tutorial","tags":["websockets","vuejs"],"seo":{"title":"Getting Started with Live Transcription and Vue.js","description":"Learn how to use Deepgram's Speech-to-Text API for fast and accurate live transcripts in your Vue.js applications."},"shorturls":{"share":"https://dpgr.am/e81eaba","twitter":"https://dpgr.am/eb6c990","linkedin":"https://dpgr.am/edcdb3d","reddit":"https://dpgr.am/adf60f5","facebook":"https://dpgr.am/4db5941"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454106/blog/getting-started-live-transcription-vue/ograph.png"}};
						const file$2B = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/getting-started-live-transcription-vue/index.md";
						const url$2B = undefined;
						function rawContent$2B() {
							return "\r\nThis post will cover how to set up Deepgram for live transcriptions in your Vue.js application. We'll set up Deepgram in a single HTML file with the Vue.js `<script>` include and no other dependencies.\r\n\r\n## Before We Start\r\n\r\nYou will need a free [Deepgram API Key](https://console.deepgram.com/signup?jump=keys).\r\n\r\n## Setting Up a Vue Project With a Script Include\r\n\r\nCreate an `index.html` file and open it in a code editor. Set up a Vue project:\r\n\r\n```js\r\n<html>\r\n<head></head>\r\n<body>\r\n  <div id=\"app\">\r\n  </div>\r\n\r\n  <script src=\"https://cdn.jsdelivr.net/npm/vue@2.7.0\"></script>\r\n  <script>\r\n    const app = new Vue({\r\n      el: '#app'\r\n    })\r\n  </script>\r\n</body>\r\n</html>\r\n```\r\n\r\n## Get Microphone Data\r\n\r\nThis code will be written in the `created()` lifecycle method - meaning it will happen immediately.\r\n\r\nFirstly, ask the user for access to their mic:\r\n\r\n```js\r\nconst app = new Vue({\r\n  el: '#app',\r\n  async created() {\r\n    const stream = await navigator.mediaDevices.getUserMedia({ audio: true })\r\n      .catch(error => alert(error))\r\n  }\r\n})\r\n```\r\n\r\nNext, plug the stream into a MediaRecorder so we can later access the raw data from the accessed microphone:\r\n\r\n```js\r\nconst app = new Vue({\r\n  el: '#app',\r\n  async created() {\r\n    const stream = await navigator.mediaDevices.getUserMedia({ audio: true })\r\n      .catch(error => alert(error))\r\n\r\n    // Create MediaRecorder\r\n    if(!MediaRecorder.isTypeSupported('audio/webm')) return alert('Unsupported browser')\r\n    this.mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm' })\r\n  },\r\n  // Store MediaRecorder\r\n  data: {\r\n    mediaRecorder: null\r\n  }\r\n})\r\n```\r\n\r\nRemember that if you are creating Vue components, `data` must be a function that returns an object.\r\n\r\n## Connect to Deepgram\r\n\r\nCreate a button which will begin transcription. Trigger a new `begin()` method with it's clicked:\r\n\r\n```js\r\n<html>\r\n<head></head>\r\n<body>\r\n  <div id=\"app\">\r\n    <!-- Add button -->\r\n    <button @click=\"begin\">Begin transcription</button>\r\n  </div>\r\n\r\n  <script src=\"https://cdn.jsdelivr.net/npm/vue@2.7.0\"></script>\r\n  <script>\r\n    const app = new Vue({\r\n      el: '#app',\r\n      async created() {\r\n        const stream = await navigator.mediaDevices.getUserMedia({ audio: true })\r\n          .catch(error => alert(error))\r\n\r\n        if(!MediaRecorder.isTypeSupported('audio/webm')) return alert('Unsupported browser')\r\n        this.mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm' })\r\n      },\r\n      data: {\r\n        mediaRecorder: null\r\n      },\r\n      // Create begin method\r\n      methods: {\r\n        begin() {\r\n\r\n        }\r\n      }\r\n    })\r\n  </script>\r\n</body>\r\n</html>\r\n```\r\n\r\nTake a moment to get [a free Deepgram API Key](https://console.deepgram.com/signup?jump=keys) before continuing.\r\n\r\nUse the browser native WebSocket interface to connect to Deepgram's live transcription server. Store the WebSocket instance in `data`:\r\n\r\n```js\r\ndata: {\r\n  mediaRecorder: null,\r\n  // Add socket\r\n  socket: null\r\n},\r\nmethods: {\r\n  begin() {\r\n    const DG_URL = 'wss://api.deepgram.com/v1/listen?language=de'\r\n    const DG_KEY = 'YOUR_DEEPGRAM_API_KEY'\r\n    this.socket = new WebSocket(DG_URL, ['token', DG_KEY])\r\n  }\r\n}\r\n```\r\n\r\nThis WebSocket creates a 2-way connection with Deepgram. See the `language=de` in the URL? That's telling it you'll be speaking German. We have loads of [supported languages](https://developers.deepgram.com/documentation/features/language/) to check out!\r\n\r\n## Send Data to Deepgram\r\n\r\nOnce the WebSocket connection is open, start sending mic data:\r\n\r\n```js\r\nmethods: {\r\n  begin() {\r\n    const DG_URL = 'wss://api.deepgram.com/v1/listen?language=de'\r\n    const DG_KEY = 'YOUR_DEEPGRAM_API_KEY'\r\n    this.socket = new WebSocket(DG_URL, ['token', DG_KEY])\r\n    // Run the startStreaming method when socket is opened\r\n    this.socket.onopen = this.startStreaming\r\n  },\r\n  // Create startStreaming method\r\n  startStreaming() {\r\n    this.mediaRecorder.addEventListener('dataavailable', event => {\r\n      if(event.data.size > 0 && this.socket.readyState == 1) {\r\n        this.socket.send(event.data)\r\n      }\r\n      // Send data every 250ms (.25s)\r\n      mediaRecorder.start(250)\r\n    })\r\n  }\r\n}\r\n```\r\n\r\n## Receive Transcript Results\r\n\r\nYou are currently sending data through our persistent connection to Deepgram every 0.25 seconds. You will receive transcripts back nearly as often - it's time to write the handling code.\r\n\r\n```js\r\nmethods: {\r\n  begin() {\r\n    const DG_URL = 'wss://api.deepgram.com/v1/listen?language=de'\r\n    const DG_KEY = 'YOUR_DEEPGRAM_API_KEY'\r\n    this.socket = new WebSocket(DG_URL, ['token', DG_KEY])\r\n    this.socket.onopen = this.startStreaming\r\n    // Run the handleResponse method when data is received\r\n    this.socket.onmessage = this.handleResponse\r\n  },\r\n  startStreaming() {\r\n    this.mediaRecorder.addEventListener('dataavailable', event => {\r\n      if(event.data.size > 0 && this.socket.readyState == 1) {\r\n        this.socket.send(event.data)\r\n      }\r\n      mediaRecorder.start(250)\r\n    })\r\n  },\r\n  // Create handleResponse method\r\n  handleResponse(message) {\r\n    const received = JSON.parse(message.data)\r\n    const transcript = received.channel.alternatives[0].transcript\r\n    if(transcript) {\r\n      console.log(transcript)\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nRefresh your browser, and you should see transcripts showing in your console.\r\n\r\n![Browser console showing one line in German](https://res.cloudinary.com/deepgram/image/upload/v1657299918/blog/2022/07/getting-started-live-transcription-vue/logs.jpg)\r\n\r\n## Show Transcripts On Page\r\n\r\nFirst, create a new `transcripts` property in `data` with an empty array:\r\n\r\n```js\r\ndata: {\r\n  mediaRecorder: null,\r\n  socket: null,\r\n  // Add this\r\n  transcripts: []\r\n},\r\n```\r\n\r\nThen, instead of logging transcripts, add them to this array:\r\n\r\n```js\r\nif(transcript) {\r\n  this.transcripts.push(transcript)\r\n}\r\n```\r\n\r\nFinally, update your HTML to display items from the array:\r\n\r\n```js\r\n<div id=\"app\">\r\n  <button @click=\"begin\">Begin transcription</button>\r\n  <!-- Add looping element -->\r\n  <p v-for=\"transcript in transcripts\">{{ transcript }}</p>\r\n</div>\r\n```\r\n\r\nYour page should look like this once you've spoken a couple of phrases:\r\n\r\n![Page showing two lines - each with one line of transcripted German text](https://res.cloudinary.com/deepgram/image/upload/v1657299918/blog/2022/07/getting-started-live-transcription-vue/display.png)\r\n\r\n## Wrapping Up\r\n\r\nThe final code looks like this:\r\n\r\n```js\r\n<html>\r\n<head></head>\r\n<body>\r\n  <div id=\"app\">\r\n    <button @click=\"begin\">Begin transcription</button>\r\n    <p v-for=\"transcript in transcripts\">{{ transcript }}</p>\r\n  </div>\r\n\r\n  <script src=\"https://cdn.jsdelivr.net/npm/vue@2.7.0\"></script>\r\n  <script>\r\n    const app = new Vue({\r\n      el: '#app',\r\n      async created() {\r\n        const stream = await navigator.mediaDevices.getUserMedia({ audio: true })\r\n          .catch(error => alert(error))\r\n\r\n        if(!MediaRecorder.isTypeSupported('audio/webm')) return alert('Unsupported browser')\r\n        this.mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm' })\r\n      },\r\n      data: {\r\n        mediaRecorder: null,\r\n        socket: null,\r\n        transcripts: []\r\n      },\r\n      methods: {\r\n        begin() {\r\n          const DG_URL = 'wss://api.deepgram.com/v1/listen?language=de'\r\n          const DG_KEY = 'YOUR_DEEPGRAM_API_KEY'\r\n          this.socket = new WebSocket(DG_URL, ['token', DG_KEY])\r\n          this.socket.onopen = this.startStreaming\r\n          this.socket.onmessage = this.handleResponse\r\n        },\r\n        startStreaming() {\r\n          this.mediaRecorder.addEventListener('dataavailable', event => {\r\n            if(event.data.size > 0 && this.socket.readyState == 1) {\r\n              this.socket.send(event.data)\r\n            }\r\n            mediaRecorder.start(250)\r\n          })\r\n        },\r\n        handleResponse(message) {\r\n          const received = JSON.parse(message.data)\r\n          const transcript = received.channel.alternatives[0].transcript\r\n          if(transcript) {\r\n            this.transcripts.push(transcript)\r\n          }\r\n        }\r\n      }\r\n    })\r\n  </script>\r\n</body>\r\n</html>\r\n```\r\n\r\nThis is the most simple implementation with Vue and is written for clarity over conciseness. If you want to learn more about Vue 3, including its setup() function and composables, then Sandra wrote [an excellent tutorial series on Vue 3](https://blog.deepgram.com/diving-into-vue-3-getting-started/).\r\n\r\nIf you have questions, please feel free to message us on Twitter, [@DeepgramDevs](https://twitter.com/DeepgramDevs).\r\n\r\n        ";
						}
						async function compiledContent$2B() {
							return load$2B().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2B() {
							return (await import('./chunks/index.ae317751.mjs'));
						}
						function Content$2B(...args) {
							return load$2B().then((m) => m.default(...args));
						}
						Content$2B.isAstroComponentFactory = true;
						function getHeadings$2B() {
							return load$2B().then((m) => m.metadata.headings);
						}
						function getHeaders$2B() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2B().then((m) => m.metadata.headings);
						}

const __vite_glob_0_111 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2B,
  file: file$2B,
  url: url$2B,
  rawContent: rawContent$2B,
  compiledContent: compiledContent$2B,
  default: load$2B,
  Content: Content$2B,
  getHeadings: getHeadings$2B,
  getHeaders: getHeaders$2B
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2A = {"title":"Getting Started with APIs","description":"Learn about what APIs are, how to use them, and make your first requests.","date":"2021-11-01T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1635446349/blog/2021/11/getting-started-with-apis/getting-started-with-apis-blog%402x.jpg","authors":["kevin-lewis"],"category":"tutorial","tags":["nodejs","python"],"seo":{"title":"Getting Started with APIs","description":"Learn about what APIs are, how to use them, and make your first requests."},"shorturls":{"share":"https://dpgr.am/048ee6b","twitter":"https://dpgr.am/69ee488","linkedin":"https://dpgr.am/f3db1d3","reddit":"https://dpgr.am/bbd4f04","facebook":"https://dpgr.am/af636fc"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453803/blog/getting-started-with-apis/ograph.png"}};
						const file$2A = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/getting-started-with-apis/index.md";
						const url$2A = undefined;
						function rawContent$2A() {
							return "\nWhen I first started building websites it took me a while to gain confidence with making my sites interactive, and even longer to start using APIs. In this post we'll cover the basics of APIs as if you've never heard of them before. We'll talk about some of the intricacies, make our first few requests, and look at what we may get back.\n\nThis is a blog post summary of a talk I gave at codebar Festival in March 2021. If you'd rather learn this in video form I've included it here.\n\n<YouTube id=\"66xLBqosPog\"></YouTube>\n\n## What are APIs\n\nAPIs are a set of allowed rules that describe how developers can interact with a platform or service. They are often presented along with a set of documentation/references that help developers know what functionality is available, all of the options or variations they can request, and what the expected action should be as a result.\n\nYou can think about it like a restaurant menu - all of the dishes are shown along with any variations (fries or salad?) and a description of what you'll get if you order a specific item.\n\nTo give you an idea of the breadth of APIs available here are some I have used in the past:\n\n*   The [TheAudioDB API](https://www.theaudiodb.com/api_guide.php) provides metadata and cover images for music - use it if you want to access or display information about music tracks.\n*   Google Maps have a few APIs available - the [Google Maps Directions API](https://developers.google.com/maps/documentation/directions/overview) takes in two points and a transport method (walking, cycling, public transit) and provides step-by-step directions.\n*   [Stripe](https://stripe.com/docs) is one of the go-to APIs for letting developers take payments from users. Chances are you've put your card information into a form which processed a payment through Stripe.\n*   And, of course, there is Deepgram - which takes in audio files and returns accurate transcriptions using our AI Speech Recognition API (psst [go sign up here](https://console.deepgram.com)).\n\nAPIs are like lego blocks - they provide small pieces of functionality that developers like you can build on top of. Instead of needing to build lots of intricate functionality, you can delegate it out to others and focus on parts of your application that are special.\n\n## Making Your First API Call\n\nWe are going to jump straight in and make an API call. Before we do - there are two pieces of information required in every request. The first is an **endpoint** (a URL) to access the API, and the second is the **method** which describes the type of request we are making. There are [a bunch](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods) but five are particularly common:\n\n*   GET requests retrieve data - often the default method\n*   POST requests submits data to create a new entity\n*   PUT and PATCH requests update data\n*   DELETE requests delete data\n\nOpen up your terminal and let's make our first API request. We'll be using the [PokéAPI](https://pokeapi.co) which is a small free API used often for education use. Type the following command and hit enter:\n\n```sh\ncurl https://pokeapi.co/api/v2/pokemon/castform\n```\n\nAnd you should be greeted with a large block of text containing information about this Pokémon.\n\n## URL Parameters\n\nSome APIs allow you to provide additional information in your request using URL Parameters. You have certainly already seen these when sharing links with your friends - they go at the end of a URL and look like this:\n\n```\nhttps://example.com/product?utm_param=123&src=abc&campaign=xyz\n```\nWe indicate URL Parameters are starting with the `?`, and then they follow the format `item=value`. Where there is more than one we separate them with a `&`. Let's make a new request to the [Data USA API](https://datausa.io) in our terminal:\n\n```sh\ncurl https://datausa.io/api/data?drilldowns=State&measures=Population\n```\n\nHere we are specifying two URL Parameters - the first is `drilldowns` with a value of `State`, and the second is `measures` with a value of `Population` to get the population by state in the USA. If you change the value of `drilldowns` to `County` you'll get different (and much bigger) data back. Try it!\n\n## Including Data\n\nSo far we have only made GET requests, but often when making POST requests we actually need to send some data along. That could be new user account information, details of a message to send, or a post to add to a user's feed.\n\nIn this next example we will send some data in JSON format as well as setting a method and endpoint. Go ahead and run this, and we'll discuss it after:\n\n```sh\ncurl -H 'Content-Type: application/json' -d '{\"userId\": 1, \"title\": \"sample title\", \"body\": \"sample body\"}' -X POST https://jsonplaceholder.typicode.com/posts\n```\n\ncURL sends data in a format which is not JSON by default, so the first portion `-H 'Content-Type: application/json'` is known as a header and provides more information with the request. In this case, we are telling our the API to interpret our data as JSON. We'll be talking more about headers soon, but know each one begins with `-H` when making API requests from the terminal.\n\nIn the next section, we are detailing the data which we are going to send in this request. `-d '{\"userId\": 1, \"title\": \"sample title\", \"body\": \"sample body\"}'` is a JSON object with three properties - a `userId`, a `title`, and a `body`. Data begins with `-d` when using the terminal for API requests.\n\nFinally, `-X POST` specifies that we are using the POST method. If omitted, this would be a GET request (and you can't send data with a GET request).\n\n## Headers\n\nHeaders provide extra information about our requests. We've already used one to specify that we are sending JSON, and a common use case for them is to provide authentication details so the API provider knows who is accessing information and can choose to allow/reject requests as a result.\n\nWhen authenticating with an API, we often use an API Key/Token provided. Try it out:\n\n1.  Create and login to [the Deepgram Console](https://console.deepgram.com)\n2.  Create a project and an API key - any role is fine\n3.  Take note of the key provided\n4.  Run the following command in your terminal\n\n```sh\ncurl -H 'Content-Type: application/json' -H 'Authorization: Token REPLACE_WITH_YOUR_KEY' -d '{\"url\": \"https://static.deepgram.com/examples/interview_speech-analytics.wav\"}' -X POST https://api.deepgram.com/v1/listen\n```\n\nDeepgram will know this request is yours because the API Key is unique to your project. The request would fail if this key was invalid.\n\n## When Should I Use..?\n\nIt's perfectly understandable if you're confused about the usage of URL parameters, data, and headers. Unfortunately there's no single rule and you need to refer to API documentation or reference to see what they expect. However, headers almost always carry authentication, passed data goes in data, and URL parameters are used to provide options.\n\nTake some time to get comfortable looking at API References - [here is the reference to the Deepgram API](https://developers.deepgram.com/api-reference/deepgram-api) to begin honing this skill. You'll see all the components we spoke about - authentication details which ask for a header, and then each API having a method, endpoint, query parameters, and required data.\n\n## Understanding Responses\n\nAfter making our API request, we will receive a response which comprises multiple parts including data in a structured format (JSON in this case) and a HTTP Status Code. The HTTP Status Code gives an indication to whether the request was successful.\n\n*   Anything in the 200-299 range is good.\n*   400-499 means you made an error (you're probably used to seeing 404 errors which means 'not found').\n*   500-599 means it was a fault on the side of the API provider.\n\nYou can find more information about HTTP Status Codes in the [MDN Web Docs](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status). Specific important 400-level codes for API-usage are:\n\n*   400 Bad Request - the request you have provided is not formatted correctly\n*   401 Unauthorized - some kind of authentication issue (more on authentication coming up)\n*   403 Forbidden - you don't have access to this content or functionality\n*   429 Too Many Requests - most APIs have a limit on how often you can use them in a given timeframe and you have exceeded it\n\n## API Requests in the Browser\n\nTo make API requests in the browser, you can use the built-in `fetch()` function. Create a HTML file with just a `<script>` tag. Inside it type:\n\n```js\n// GET\nfetch('https://jsonplaceholder.typicode.com/posts/1')\n  .then((response) => response.json())\n  .then((json) => console.log(json))\n\n// POST\nfetch('https://jsonplaceholder.typicode.com/posts', {\n  method: 'POST',\n  body: JSON.stringify({ title: 'foo', body: 'bar', userId: 1 }),\n})\n  .then((response) => response.json())\n  .then((json) => console.log(json))\n```\n\n## API Requests in Node.js\n\nThere are lots of libraries that make using APIs easier. If you install `node-fetch` you can use identical syntax to the browser. After running `npm install node-fetch`, create and open an `index.js` file:\n\n```js\nconst fetch = require('node-fetch')\n\n// GET\nfetch('https://jsonplaceholder.typicode.com/posts/1')\n  .then((response) => response.json())\n  .then((json) => console.log(json))\n\n// POST\nfetch('https://jsonplaceholder.typicode.com/posts', {\n  method: 'POST',\n  body: JSON.stringify({ title: 'foo', body: 'bar', userId: 1 }),\n})\n  .then((response) => response.json())\n  .then((json) => console.log(json))\n```\n\n## API Requests in Python\n\n```py\nimport requests\n\n# GET\nresponse = requests.get(\"https://jsonplaceholder.typicode.com/posts/1\")\nprint(response.text)\n\n# POST\nmyData = { 'title': 'foo', 'body': 'bar', 'userId': 1 }\nresponse2 = requests.post('https://jsonplaceholder.typicode.com/posts', data = myData)\nprint(response2.text)\n```\n\n## Glossary\n\nHopefully you feel more confident in using APIs. Before we end I want to share a few more terms with you that are common with APIs:\n\n*   **Rate Limit** - the maximum number of calls you can make in a given period. Often it's something like \"100 calls per minute\". If you exceed it you may get a HTTP 429.\n*   **SDK** - a Software Development Kit makes working with an API easier. Ultimately, it create API calls for you based on easier-to-write code. If an SDK is avialble - it's advisable to use it.\n*   **GraphQL** - this is a type of API that requires request data to be formatted in a certain way. It returns only the data you specifically request.\n*   **JWT** - JSON Web Tokens are a special type of token often used to authenticate with APIs. They contain more information, and are only valid for a short time. You often generate one with code and then immediately use it.\n*   **OAuth** - this is an authentication flow that requires users to give access to your application. This is what the \"Login with Google\" buttons do.\n\n## Wrapping Up\n\nI hope you found this valuable - it's the kind of summary I would have really appreciated as I started out. If you have further questions because things don't make sense, or you want to expand on topics mentioned here or elsewhere - please reach out to us on Twitter (we are [@DeepgramDevs](https://twitter.com/DeepgramDevs)).\n\n        ";
						}
						async function compiledContent$2A() {
							return load$2A().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2A() {
							return (await import('./chunks/index.7cab7294.mjs'));
						}
						function Content$2A(...args) {
							return load$2A().then((m) => m.default(...args));
						}
						Content$2A.isAstroComponentFactory = true;
						function getHeadings$2A() {
							return load$2A().then((m) => m.metadata.headings);
						}
						function getHeaders$2A() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2A().then((m) => m.metadata.headings);
						}

const __vite_glob_0_112 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2A,
  file: file$2A,
  url: url$2A,
  rawContent: rawContent$2A,
  compiledContent: compiledContent$2A,
  default: load$2A,
  Content: Content$2A,
  getHeadings: getHeadings$2A,
  getHeaders: getHeaders$2A
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2z = {"title":"Getting Started with JSON","description":"Learn the basics of JSON and how to work with data in a JSON object.","date":"2021-11-25T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1637609299/blog/2021/11/getting-started-with-json/Getting-Started-with-json-blog%402x.jpg","authors":["sandra-rodgers"],"category":"tutorial","tags":["nodejs","python"],"seo":{"title":"Getting Started with JSON","description":"Learn the basics of JSON and how to work with data in a JSON object."},"shorturls":{"share":"https://dpgr.am/93e1090","twitter":"https://dpgr.am/7dd0ca8","linkedin":"https://dpgr.am/1d55e1e","reddit":"https://dpgr.am/7daa083","facebook":"https://dpgr.am/840ab1c"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453805/blog/getting-started-with-json/ograph.png"}};
						const file$2z = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/getting-started-with-json/index.md";
						const url$2z = undefined;
						function rawContent$2z() {
							return "\nJSON is a lightweight text-based notation used to represent structured data. While JSON was inspired by Javascript object notation, it is generally agnostic in how it works (more on that later) and can be used by many languages either natively or with the help of libraries.\n\nIn this post, we'll go over the basics you need to know to get you started working in JSON, and we'll dig a little deeper by examining how to take JSON from a Deepgram response object and find exactly the data we want.\n\n## Meet JSON\n\nIntroductions usually start with names, so let's start there. JSON stands for Javascript Object Notation. JSON was originally pronounced like the name 'Jason', but over time, people started to pronounce it more like 'Jay-sawn'. The fact is, JSON is not opinionated, and neither is JSON's creator, Douglas Crockford, at least when it comes to pronunciation (comments in code... well, that's another story). In 2011 Crockford was quoted as saying: \"There's a lot of argument about how you pronounce \\[JSON], but I strictly don't care.\" So don't feel self-conscious about your pronunciation of JSON anymore (although you might want to rethink how you are saying \"Babel\").\n\n### JSON Syntax\n\nAt the root level, JSON must be an array or an object (although [some discussion](https://stackoverflow.com/a/10428530/11073321) refers to changes that allow other data types at the root) . It is very common to see an object at the root like the following example, so we'll look closely at JSON that has an object at its root. Because it is an object, it will consist of data in the format of a **key** (the name of the thing we are setting) and a **value** (the actual data being set to that name).\n\nHere is a JSON object representing Luke Skywalker (adapted from [The Star Wars API](https://swapi.dev/)). The basic structure is key-value pairs inside curly braces. **Notice that each key is wrapped in double quotes, which is an important feature of JSON.** Also, trailing commas are not allowed (which differs from Javascript).\n\n```javascript\n{\n  \"name\": \"Luke Skywalker\",\n  \"height\": 172,\n  \"mass\": 77,\n  \"hair_color\": \"blond\",\n  \"birth_year\": \"19BBY\",\n  \"gender\": \"male\",\n  \"homeworld\": \"Tatooine\",\n  \"films\": [\n    \"A New Hope\",\n    \"Return of the Jedi\",\n    \"The Empire Strikes Back\",\n    \"Revenge of the Sith\"\n  ],\n  \"jedi_knight\": true\n}\n```\n\nAnother important thing to know is that **JSON ignores whitespace between elements**. So we could use a property name of “hair color” as our key, but it’s not recommended since that can cause problems if we convert the data into a different language. It’s recommended to use an underscore between the elements, as in “hair\\_color”. (However, the whitespace flexibility makes it very easy to beautify JSON to make it more human-readable.)\n\nAlso, **JSON does not allow comments**, which is one downside (in my opinion) because comments can be especially helpful in config files, where JSON is often used. The inventor of JSON made a purposeful choice not to allow for comments, and he has [defended his decision](https://archive.ph/20150704102718/https://plus.google.com/+DouglasCrockfordEsq/posts/RK8qyGVaGSr#selection-691.0-695.203).\n\n### JSON Data Types\n\nIn the key-value pairs, the **value** can be of the following types: **string, number, object, array, Boolean, or null**. It can also be more complex if we nest data inside the objects or the arrays. For example, if I wanted to provide more information about each film Luke Skywalker appears in, I could change the value of `\"films\"` to be an array of objects, with each object containing key-value pairs with more data about each film (and I could continue on like this, having objects and arrays nested within objects and arrays).\n\n```js\n\"films\": [\n   {\n    \"title\": \"A New Hope\",\n    \"year\": \"1977\"\n   },\n   {\n    \"title\": \"Return of the Jedi\",\n    \"year\": \"1983\"\n   },\n   {\n    \"title\": \"The Empire Strikes Back\",\n    \"year\": \"1980\"\n   },\n   {\n    \"title\": \"Revenge of the Sith\",\n    \"year\": \"2005\"\n   },\n  ],\n```\n\nAnd if you’re wondering about dates, **JSON does not have a specified date type**. However, Javascript uses ISO 8601 string format to encode dates as a string, and it is recommended that other languages convert the date to ISO format before converting the date to JSON.\n\nSo in Python, for example, we would use the `datetime` module to get the current date, and then use its method `isoformat()` to convert it to ISO format. Then use `json.dumps()` to convert the date to JSON.\n\n**Python**\n\n```py\nimport datetime\ndatetime = datetime.datetime.now()\nformatted_datetime = datetime.isoformat()\njson_datetime = json.dumps(formatted_datetime)\n```\n\nFinally, JSON is agnostic when it comes to numbers. While many languages have different number types and strict rules about numbers, JSON makes it easy. According to [Ecma International](https://www.ecma-international.org/wp-content/uploads/ECMA-404_1st_edition_october_2013.pdf), JSON \"offers only the representation of numbers that humans use: a sequence of digits. All programming languages know how to make sense of digit sequences even if they disagree on internal representations.\" That's another reason JSON plays so well with other languages.\n\n### Convert JSON to Other Languages\n\nSince JSON is used for data representation and not for executing logic, we have to be able to convert it to the language of our choice if we want to do something more with it. Let's look at how two common languages - Node and Python - are converted to JSON and are able to parse JSON.\n\nAn important detail to understand is that while JSON uses object syntax to represent data structures, **JSON actually exists as a string**. So in the JavaScript example below, notice that when we convert Javascript to JSON, we **stringify** it, and when we convert it back to Javascript, we **parse** it ('parsing' means analyzing a string).\n\n*Also good to know - a JSON string can be stored as a standalone file using the `.json` extension, and the official MIME type for JSON is \"application/json\", which is what we would use as the content-type in the headers object of a fetch request.*\n\n#### Javascript\n\nIn Javascript, use the method `JSON.stringify()` to convert Javascript to JSON, and use `JSON.parse()` to convert JSON to JavaScript:\n\n```js\nconst jedi = {\n  name: 'Luke Skywalker',\n  mass: 77,\n  homeWorld: 'Tatooine',\n}\n\nconst jediString = JSON.stringify(jedi)\n\nconsole.log(jediString)\n//JSON string \"{\"name\":\"Luke Skywalker\",\"mass\":77,\"homeWorld\":\"Tatooine\"}\"\n\nconsole.log(JSON.parse(jediString))\n// Javascript object {name:\"Luke Skywalker\",mass:77,homeWorld:\"Tatooine\"}\n```\n\n#### Python\n\nIn Python, to convert a Python `dict` to JSON, you can import the built-in module json, and then use the method `json.dumps()` on the `dict`. And to convert JSON to a Python `dict`, use the method `json.loads()`:\n\n```py\nimport json\n\n# a Python object (dict):\njedi = {\n  \"name\": \"Luke Skywalker\",\n  \"mass\": 77,\n  \"home_world\": \"Tatooine\"\n}\n\njedi_string = json.dumps(jedi)\n\nprint(jedi_string)\n# JSON string {\"name\": \"Luke Skywalker\", \"mass\": 77, \"home_world\": \"Tatooine\"}\n\nprint(json.loads(jedi_string))\n# Python dict {'name': 'Luke Skywalker', 'mass': 77, 'home_world': 'Tatooine'}\n```\n\n## Find Specific Data in a Real JSON Object\n\nA common, real-world scenario for encountering JSON would be if you were making a request to a third-party API. (Check out [this blog post](https://blog.deepgram.com/getting-started-with-apis/) to learn more about working with APIs).\n\nFor example, if you were to use the Deepgram API to transcribe audio, you would make a POST request that sends the audio file to Deepgram, and in response you would get your transcription of that audio file as text in the form of a JSON object.\n\nIt can be tricky to know what you are looking at when you get a response object from an API. It helps to read the documentation to find out what the structure is of the data being sent back to you. Deepgram's documentation tells us that the response schema will include a root object with two objects inside it:\n\n*   a JSON-formatted '**metadata**' object\n*   a JSON formatted '**results**' object.\n\nSo the general structure would look something like this (the ellipsis \\[...] is included to show that some nested data has been hidden in the example):\n\n```json\n{\n  \"metadata\": {\n    \"transaction_key\": \"lrCXFhkJPoTZ6Ezh9G24WabGcR5vMI/ksuSVtt1abe6abrr2+mGZb4CDTFGLedIxYUsI5MYvAEmDagh6AMEBFEyvC0qIF3YR5A31UMZkE4USmjWQSYyIukZxMtH9918TBLtUOvyeuTVeOcwdLUODqRA3uP67tF19eEKSza6Yj+IiQtib7yeHJWn5YzXPwX/5FOOQupKJoHz6dUH5lwjdhi9ykG6Nn87GDuZBzsejpEGsKJbzIgOQPJUrJTec09MDO95Bw9lj2cMPw1R/ZqBYbMtGvTamhopVl8XxV9Sg5blZkf8bs2KcRilYypQOvXggDGHLPxGNChBDFrvcR9Qi+eLLnEzPrHTsc6FjsFl/YgQ+Cw30RmpFiJceUXM2ed3/ojE5GLzsfSBeost4\",\n    \"request_id\": \"eeaa1992-5729-4f2c-a73f-6224d78a47b8\",\n    \"sha256\": \"8d2b4b8cc76cd35a5f9bde55ce92de211216849cca1407b1ad0d5d4d6ed610a2\",\n    \"created\": \"2021-11-16T19:55:40.059Z\",\n    \"duration\": 24.696,\n    \"channels\": 1,\n    \"models\": [ \"41757536-6114-494d-83fd-c2694524d80b\" ]\n  },\n  \"results\": {\n    \"channels\": [\n      {\n        \"alternatives\": [\n          {\n           \"transcript\": \"This is the weapon of a jedi night, not as clumsy or random as a blast an elegant weapon. For all civilized day. Over a thousand generations, the Jedi knights the guardians of peace of justice in the old republic before the dark times before they can pass.\",\n            \"confidence\": 0.90745026,\n            \"words\": [\n              {\n                \"word\": \"this\",\n                \"start\": 0.65999997,\n                \"end\": 0.78,\n                \"confidence\": 0.9960715,\n                \"speaker\": 0,\n                \"punctuated_word\": \"This\"\n              },\n              ...\n            ]\n          }\n        ]\n      }\n    ],\n    \"utterances\": [\n      {\n        \"start\": 0.65999997,\n        \"end\": 2.56,\n        \"confidence\": 0.8840211,\n        \"channel\": 0,\n        \"transcript\": \"This is the weapon of a jedi night,\",\n        \"words\": [\n          {\n            \"word\": \"this\",\n            \"start\": 0.65999997,\n            \"end\": 0.78,\n            \"confidence\": 0.9960715,\n            \"speaker\": 0,\n            \"punctuated_word\": \"This\"\n          }\n          ...\n        ],\n        \"speaker\": 0,\n        \"id\": \"791ad5c3-b097-4ab3-b26f-5c0c8595c0e5\"\n      }\n    ]\n  }\n}\n```\n\n### Show Only the Necessary Data from the Response\n\nAll we want is to get the **transcript** of the audio. (I recommend taking a look again at the above object to notice where that `transcript` data is.) But this response is giving me metadata and a whole bunch of other data, including individual words and data about those words! Really nice, but a little more than we need at the moment.\n\nSo we will drill down into that **results** object by chaining together the **keys (object level)** and the **indices (array level)**, following the chain down to the first `transcript` string. The way to do this is to assign the response from Deepgram to a variable called **response** (or whatever you want to call it), and then connect the keys and/or indices following this path:\n\n*   The root-level **response** object\n*   The **results** object\n*   The first item in the **channels** array (index 0)\n*   The first item in the **alternatives** array (index 0)\n*   The **transcript** property\n\nSo it would use a chain similar to this general format:\n\n      key -> key -> index0 -> index0 -> key\n      [obj]  [obj]   [arr]     [arr]   [obj]\n\nTo see where this path takes us, we can print/log this chain of nested values. We should see just the value at the end of the chain, which is the transcript string. This would look similar in many languages, but let's take a look at it in Node and Python.\n\n*Before continuing on, I challenge you to think to yourself how you would write out that chain of key names and indices so that you print only a response that is the transcript.*\n\nMAKE YOUR GUESS NOW...\n\n#### Answer: Node\n\nTo get just the transcript in Node, I could log the following code:\n\n```javascript\nresponse.results.channels[0].alternatives[0].transcript\n```\n\n#### Answer: Python\n\nTo get the transcript in Python, I could write the following code:\n\n```python\nresponse['results']['channels'][0]['alternatives'][0]['transcript']\n```\n\nAnd the response I would see is just the transcript:\n\n    This is the weapon of a jedi night, not as clumsy or random as a blaster, an elegant weapon. For more civilized day. Over a thousand generations, the Jedi knights the guardians of peace of justice in the old republic before the dark times.\n\nWhen comparing both languages, you can see that the way to get the data that you want is to understand how to access data from an object (use the **object property key**) and how to pull data from an array (use the **index of the position of the item in the array, starting with \\[0]**). We chain these keys and indices together to get down to the data we need.\n\n## Conclusion\n\nI hope you learned a few interesting facts about JSON. Feel free to [reach out](https://twitter.com/sandra_rodgers_) with comments, questions, or any other tidbits worth knowing about JSON.\n\n        ";
						}
						async function compiledContent$2z() {
							return load$2z().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2z() {
							return (await import('./chunks/index.21104e44.mjs'));
						}
						function Content$2z(...args) {
							return load$2z().then((m) => m.default(...args));
						}
						Content$2z.isAstroComponentFactory = true;
						function getHeadings$2z() {
							return load$2z().then((m) => m.metadata.headings);
						}
						function getHeaders$2z() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2z().then((m) => m.metadata.headings);
						}

const __vite_glob_0_113 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2z,
  file: file$2z,
  url: url$2z,
  rawContent: rawContent$2z,
  compiledContent: compiledContent$2z,
  default: load$2z,
  Content: Content$2z,
  getHeadings: getHeadings$2z,
  getHeaders: getHeaders$2z
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2y = {"title":"Getting Started with the MediaStream API","description":"Get to know the basics of the MediaStream API","date":"2021-12-13T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1638832562/blog/2021/12/getting-started-with-mediastream-api/getting-started-w-mediastream-API%402x.jpg","authors":["brian-barrow"],"category":"tutorial","tags":["mediastream","javascript"],"seo":{"title":"Getting Started with the MediaStream API","description":"Get to know the basics of the MediaStream API"},"shorturls":{"share":"https://dpgr.am/93e84dc","twitter":"https://dpgr.am/e6dcf20","linkedin":"https://dpgr.am/8e652ff","reddit":"https://dpgr.am/34e806b","facebook":"https://dpgr.am/b3cff01"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453829/blog/getting-started-with-mediastream-api/ograph.png"}};
						const file$2y = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/getting-started-with-mediastream-api/index.md";
						const url$2y = undefined;
						function rawContent$2y() {
							return "\r\nWhen building web applications, you will sometimes need to work with audio and/or video inputs. You'll need to understand the MediaStream API, which is the web API that supports streaming both audio and video information. In this post, we'll cover the basics of the MediaStream API.\r\n\r\n## Getting Started\r\n\r\nTo get started, you'll need to gain access to the user's audio/video devices which provide data in a 'stream.' A common use case would be getting access to the user's microphone and camera. One of the most common ways to do this is through the [getUserMedia](https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia) method that is built into the browser. This post is more about understanding the different parts of the API. To see this method used in action, refer to [Kevin's post about getting audio in the browser](https://blog.deepgram.com/live-transcription-mic-browser/).\r\n\r\nLet's take the following code, insert it into an HTML file, and then open that file in the browser.\r\n\r\n```html\r\n<!DOCTYPE html>\r\n<html>\r\n  <body>\r\n    <h1>Getting Started With MediaStream</h1>\r\n    <script>\r\n      navigator.mediaDevices\r\n        .getUserMedia({ audio: true })\r\n        .then((stream) => {\r\n          console.log('MEDIA STREAM: ', stream)\r\n        })\r\n        .catch((err) => {\r\n          alert('PERMISSION DENIED')\r\n        })\r\n    </script>\r\n  </body>\r\n</html>\r\n```\r\n\r\nThis will prompt you to allow access to the microphone on your machine. If you reject it, you'll get an alert saying `PERMISSION DENIED.` If you allow that, then you will see the following in your console:\r\n\r\n![Devtools console result showing the MediaStream object that is initialized on page load](https://res.cloudinary.com/deepgram/image/upload/v1638906691/blog/2021/12/getting-started-with-mediastream-api/initial-console-log.png)\r\n\r\nAs you can see, there are several properties and methods available on the MediaStream object. Before we dive into these, I want to clarify a couple of definitions.\r\n\r\n*   **stream**: A stream of media content. Regarding what we'll be discussing, this refers to the stream of information coming from a user's device. The stream will be coming from either the device's microphone or the camera, or both at the same time. A stream will consist of one or more 'tracks.'\r\n*   **track**: A `track` is a piece of media within a stream. These are typically audio or video tracks. If we get access to both a microphone and a camera, our stream will consist of both an audio and a video track.\r\n\r\n### Properties\r\n\r\n*   **active**\r\n\r\n    The first one we see is the `active` property. This property is simply a boolean value indicating if any part of the MediaStream object is currently active or being used. Most MediaStream objects you'll encounter will contain audio and/or video tracks. If any of these tracks is `active,` then the `active` property on the MediaStream object will be true.\r\n\r\n*   **id**\r\n\r\n    The other property available on our MediaStream object is the id, a unique identifier for the object and contains 36 characters. This will be helpful if you need to keep track of multiple streams and do different things with them.\r\n\r\n### Methods\r\n\r\n*   **addTrack**\r\n\r\n    This method takes in a `MediaStreamTrack` as an argument and adds it to the MediaStream object.\r\n\r\n![image showing that a track gets added to the MediaStream object when addTrack is called](https://res.cloudinary.com/deepgram/image/upload/v1638975710/blog/2021/12/getting-started-with-mediastream-api/addTrack.png)\r\n\r\n*   **getTracks**\r\n\r\n    This returns a list of all `MediaStreamTrack` objects associated with the stream. To test it out, let's add the `getTracks` method to our code. I've included the `video: true` in the constraints so we can see multiple tracks.\r\n\r\n    ```html\r\n    <!DOCTYPE html>\r\n    <html>\r\n      <body>\r\n        <h1>Getting Started With MediaStream</h1>\r\n        <script>\r\n          navigator.mediaDevices\r\n            .getUserMedia({ audio: true, video: true })\r\n            .then((stream) => {\r\n              console.log(\"tracks\", stream.getTracks());\r\n            .catch((err) => {\r\n              console.log(\"ERROR\", err);\r\n              alert(\"PERMISSION DENIED\");\r\n            });\r\n        </script>\r\n      </body>\r\n    </html>\r\n    ```\r\n\r\n    You can see in the screenshot below that we get both the audio and the video tracks showing:\r\n    ![Console showing the results of the stream.getTracks() call](https://res.cloudinary.com/deepgram/image/upload/v1638911657/blog/2021/12/getting-started-with-mediastream-api/getTracks.png)\r\n\r\n*   **getAudioTracks**\r\n\r\n    This returns a list of `MediaStreamTrack` objects that are **audio** types. If we used this instead of `getTracks` above, we would have gotten the list with only the audio track showing.\r\n\r\n*   **getVideoTracks**\r\n    This returns a list of `MediaStreamTrack` objects that are **video** types.\r\n\r\n*   **getTrackById**\r\n    This method takes in a string and will return the track from the MediaStream object with the corresponding id.\r\n\r\n    ```html\r\n    <!DOCTYPE html>\r\n    <html>\r\n      <body>\r\n        <h1>Getting Started With MediaStream</h1>\r\n        <script>\r\n          navigator.mediaDevices\r\n            .getUserMedia({ audio: true, video: true })\r\n            .then((stream) => {\r\n              const trackId = stream.getAudioTracks()[0].id;\r\n              console.log(\"getTrackById\", stream.getTrackById(trackId))\r\n            .catch((err) => {\r\n              console.log(\"ERROR\", err);\r\n              alert(\"PERMISSION DENIED\");\r\n            });\r\n        </script>\r\n      </body>\r\n    </html>\r\n    ```\r\n\r\n*   **removeTrack**\r\n    This method removes the given track from the `MediaStream` object. When we add a button to remove the track to our code and then log the `MediaStream.getTracks` to the dev tools console, we can see it is no longer there. If we were displaying a video stream to a div on our page and removed the video track, then the stream would no longer appear.\r\n\r\n    ```html\r\n    <!DOCTYPE html>\r\n    <html>\r\n      <body>\r\n        <h1>Getting Started With MediaStream</h1>\r\n        <script>\r\n          navigator.mediaDevices\r\n            .getUserMedia({ audio: true, video: true })\r\n            .then((stream) => {\r\n              const tracks = stream.getTracks()\r\n              console.log(\"tracks before remove\", tracks);\r\n              // remove both tracks\r\n              stream.removeTrack(tracks[1])\r\n              stream.removeTrack(tracks[0])\r\n              console.log(\"tracks after remove\", stream.getTracks());\r\n            .catch((err) => {\r\n              console.log(\"ERROR\", err);\r\n              alert(\"PERMISSION DENIED\");\r\n            });\r\n        </script>\r\n      </body>\r\n    </html>\r\n    ```\r\n\r\n    ![browser devtools console showing the MediaStream object before and after the removeTrack has been called](https://res.cloudinary.com/deepgram/image/upload/v1638976221/blog/2021/12/getting-started-with-mediastream-api/removeTrack.png)\r\n\r\n### Events\r\n\r\n[Events](https://developer.mozilla.org/en-US/docs/Learn/JavaScript/Building_blocks/Events) are simply actions that happen in the system you are programming, which the system tells you about so your code can react to them if needed. An `event listener is a function that runs when a specific event occurs`.\r\n\r\nYou should be aware of a few events on the `MediaStream` object.\r\n\r\n*   **addtrack**\r\n    Fired when a new track object is added.\r\n\r\n    *   *Event listener:* `onaddtrack` is fired when a new track is added. To use the event listener, assign it to a function that you want to be called whenever a track is added\r\n\r\n    ```js\r\n    navigator.mediaDevices\r\n      .getUserMedia({ audio: true, video: true })\r\n      .then((stream) => {\r\n        stream.onaddtrack = function(event) {\r\n          // code to execute when track is added\r\n        }\r\n      .catch((err) => {\r\n        console.log(\"ERROR\", err);\r\n        alert(\"PERMISSION DENIED\");\r\n      });\r\n    ```\r\n\r\n*   **removetrack**\r\n    Fired when a new track object is added.\r\n    *   *Event listener:* `onremovetrack` is fired when a new track is removed. To use the event listener, assign it to a function you want to be called whenever a track is removed.\r\n\r\n## Conclusion\r\n\r\nThe `MediaStream` API is beneficial and can be utilized in the applications that you build. I hope this has been informative and helped you as a web developer.\r\n\r\n        ";
						}
						async function compiledContent$2y() {
							return load$2y().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2y() {
							return (await import('./chunks/index.49d8aac9.mjs'));
						}
						function Content$2y(...args) {
							return load$2y().then((m) => m.default(...args));
						}
						Content$2y.isAstroComponentFactory = true;
						function getHeadings$2y() {
							return load$2y().then((m) => m.metadata.headings);
						}
						function getHeaders$2y() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2y().then((m) => m.metadata.headings);
						}

const __vite_glob_0_114 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2y,
  file: file$2y,
  url: url$2y,
  rawContent: rawContent$2y,
  compiledContent: compiledContent$2y,
  default: load$2y,
  Content: Content$2y,
  getHeadings: getHeadings$2y,
  getHeaders: getHeaders$2y
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2x = {"title":"Getting Started with Supabase","description":"Learn how to get started with Supabase, an open-source alternative to Firebase","date":"2021-11-29T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1637609423/blog/2021/11/getting-started-with-supabase/Getting-Started-with-supabase-blog%402x.jpg","authors":["brian-barrow"],"category":"tutorial","tags":["supabase","javascript"],"seo":{"title":"Getting Started with Supabase","description":"Learn how to get started with Supabase, an open-source alternative to Firebase"},"shorturls":{"share":"https://dpgr.am/89c7e8e","twitter":"https://dpgr.am/019c5c1","linkedin":"https://dpgr.am/96ed603","reddit":"https://dpgr.am/dfd7669","facebook":"https://dpgr.am/1b79cea"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453807/blog/getting-started-with-supabase/ograph.png"}};
						const file$2x = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/getting-started-with-supabase/index.md";
						const url$2x = undefined;
						function rawContent$2x() {
							return "\r\nGetting a database set up and running can be a difficult and time-consuming process. There are quite a few options these days for \"quick\" solutions. Supabase is one of those options and is gaining popularity very quickly. Let's dive in and see what Supabase offers. We'll walk through an example of setting up a database for a Reading List app and learn how to add, read, edit, and delete data from it.\r\n\r\n## Who this post is for\r\n\r\nThis post will be easier to follow if you have a general understanding of how databases work.\r\n\r\n## Setup\r\n\r\nThe first thing you'll need to do is sign up on [Supabase](https://app.supabase.io/api/login). It asks you to sign up with GitHub, so if you don't have an account, you should also sign up for one of those.\r\n\r\nOnce you are signed in, you'll click the green button that says \"New Project\" and select the default organization that was created when you logged in. Mine was called \"briancbarrow's Org.\" This will bring up a box where we provide some info about the project. I'll name it `reading-list`, give it a strong password, and then I'm going to select the region `West US (North California)` because that is closest to me.\r\n\r\n![Setting up the project with the name reading-list and selecting the West US region.](https://res.cloudinary.com/deepgram/image/upload/v1636151269/blog/2021/11/getting-started-with-supabase/new-project-modal.png)\r\n\r\nClick the \"Create new project\" button. It will take a few minutes for the project to be ready, so sit tight until that finishes.\r\n\r\nNow that we have that set up, you should see a page with the name of the project you gave, and below that, it should say \"Welcome to your new project.\" Below that, there are a few features that we can start exploring.\r\n\r\n*   **Database**: The Supabase Database is an instance of a Postgres Database. We'll be diving into this more below.\r\n*   **Auth**: The Supabase Auth service makes it relatively easy to set up authentication for your app and also manage your users. We won't be covering Auth in this post.\r\n*   **Storage**: Supabase offers a storage service for larger files like images or audio files. We won't be covering Storage in this post either.\r\n\r\n## Initializing the Database\r\n\r\nClick on the button in the \"Database\" card that says \"Table editor.\" This takes us to a blank dashboard where we'll be able to add and edit our database tables. Click the `+ New table` button on the left and give it the name `books`. For now don't check the \"Enable Row Level Security (RLS).\" This table will need a few columns in addition to the \"id\" column. I've listed them below.\r\n\r\n*   Column Name: `title`, Type: `varchar`\r\n*   Column Name: `author`, Type: `varchar`\r\n*   Column Name: `finished` Type: `bool`\r\n\r\nThe table might already have a 'created\\_at' column in there by default. We won't need it so you can remove that one.\r\n\r\n![Add new table with the name 'books', leave Row Level Security unchecked, and add title column with type varchar, add author column with type varchar, and add finished column with type bool.](https://res.cloudinary.com/deepgram/image/upload/v1637189398/blog/2021/11/getting-started-with-supabase/books-table-setup-2.png)\r\n\r\nSave that, and you've created a table in your database. You should now be able to see it. Let's fill in some data now. Click the button that says \"Insert Row\" and fill in the data with a couple of books you like. Remember that you can only insert the type of data into columns that you specified in the table setup. For example, we couldn't store a string inside the \"finished\" column because it only accepts booleans. [Here is a link to more info on data types](https://www.postgresqltutorial.com/postgresql-data-types/).\r\n\r\nHere is what my table looked like after I inserted two rows of data.\r\n\r\n![Added two rows of data with the respective titles and authors of books. One with 'finished' set to true and the other to false.](https://res.cloudinary.com/deepgram/image/upload/v1636396685/blog/2021/11/getting-started-with-supabase/books-table-filled.png)\r\n\r\n## Querying the Database\r\n\r\nWith the data added, we can now look at how to send [API requests](https://blog.deepgram.com/getting-started-with-apis/) to it in order to read that data.\r\n\r\nIn the left hand navigation, there is a link to the API documentation. This is auto-generated for us by Supabase and allows us to connect with our database in our code. Right now, we want to test that we can get the data we want from our `books` table. In the left hand menu, there is a section called \"Tables and Views\". Select the `books` table. Since we don't have a JavaScript app set up yet, select the \"Bash\" tab at the top of the right hand column above the code output. This will show us how a basic request is structured. I am going to be using Postman [(which you can get here)](https://www.postman.com/downloads/) to send these requests.\r\n\r\n### Reading data\r\n\r\nGo down to the section that says \"READ ALL ROWS\", where you'll find the `curl` request to get the information from our database.\r\n\r\nHere is what it should look like:\r\n\r\n```bash\r\ncurl 'https://swmsbxvlhkqilunwmsgs.supabase.co/rest/v1/books?select=*' \\\r\n-H \"apikey: SUPABASE_KEY\" \\\r\n-H \"Authorization: Bearer SUPABASE_KEY\"\r\n```\r\n\r\nYou can put this info into an app like Postman, or you can copy this code and put it directly into your terminal to get the results. Note that the `SUPABASE_KEY` in the above code is just a placeholder for your own key. To get your API keys to populated into the example requests, there is a dropdown labeled \"Key\" up at the top of the screen. Select `anon key`.\r\n\r\nSince I'm using Postman for these requests I'll first copy the URL from the `curl` line and paste it into the request URL field in Postman. For this request we'll keep it as a `GET` request. I'll then click on the \"Headers\" tab inside Postman and add the `apikey` and `Authorization` headers respectively. It will look something like this, with your URL and API Keys instead of mine.\r\n\r\n![Postman GET request with URL parameter specifying to select all](https://res.cloudinary.com/deepgram/image/upload/v1637015560/blog/2021/11/getting-started-with-supabase/postman-read-request.png)\r\n\r\n### Inserting data via the API\r\n\r\nNow we can send the request and the results will show the contents of our books table. But what if we want to add or update data using the API? Inside of the API page of our Supabase app there are examples of all these types of requests. Let's try to insert data. I'll find the \"Insert Rows\" section of the API documentation and create a new request in Postman with the required fields.\r\nThe request with the new headers you'll need should look something like this.\r\n\r\n![Postman POST request with just base URL and apikey, Authorization, Content-Type, and Prefer headings set as per the API documentation on Supabase](https://res.cloudinary.com/deepgram/image/upload/v1637015570/blog/2021/11/getting-started-with-supabase/postman-insert-request.png)\r\n\r\nThis request requires a body of data to be sent to the API endpoint. Add some raw JSON data to the body tab of the request like this:\r\n\r\n![Postman POST request showing the body tab of the above request. Body contains a JSON object with title and author values.](https://res.cloudinary.com/deepgram/image/upload/v1637015560/blog/2021/11/getting-started-with-supabase/insert-body-tab.png)\r\n\r\nWhen we send this, if it is successful, it will return with the item we just inserted.\r\n\r\n### Updating data\r\n\r\nA lot of times we have data in the app that needs to be changed/updated. In the API documentation this will be found under the \"Update rows\" section. This request is shaped a lot like the Insert request we did above. The first difference is that this is a *PATCH* request instead of a *POST* request. (Note that we use a *PATCH* to update instead of a *PUT* because *PATCH* allows us to update only specific fields, while *PUT* requires us to send the entire object with the request. We could have used *PUT* in this case, but the *PATCH* makes it more flexible in the future. [See this link](https://stackoverflow.com/a/34400076) for more details on the difference). The second difference is in the URL. At the end of the URL there is a parameter. In the example from the API documentation they have it shown as `?some_column=eq.someValue`. This is where we tell the database which row we want updated. So in our case, we can put `?id=eq.2` to update the book with the ID of `2`. Let's update the title to \"The Graveyard Book\" and the author to \"Neil Gaiman.\"\r\n\r\n![Postman PATCH request with URL specifying which row we want updated. Also with JSON body with new title and author values](https://res.cloudinary.com/deepgram/image/upload/v1637107715/blog/2021/11/getting-started-with-supabase/update-rows-request.png)\r\n\r\n### Deleting data\r\n\r\nWe also need to be able to delete data from our table. Like before, look at the example request in the API documentation page. It is similar to the Update request above. There aren't as many headers needed though and we don't pass any data. We need to specify which row though using a parameter again and make sure you change the type of request to a *DELETE* request. Let's delete the same book we just updated. The request will look something like this:\r\n\r\n![Postman DELETE request specifying which row we want to be deleted](https://res.cloudinary.com/deepgram/image/upload/v1637107568/blog/2021/11/getting-started-with-supabase/delete-request.png)\r\n\r\nWhen you navigate back to the Tables page of the Supabase UI, you should only see the first book you created.\r\n\r\n## Conclusion\r\n\r\nNow you know the basics of how to set up and use a Supabase Database. This tutorial just scratched the surface of Supabase, but it should give you a solid start.\r\n\r\nBefore you start using the database in an app, you'll also want to add the \"Row Level Security\" to your tables. This is beyond the scope of this introduction to Supabase blog, but basically it makes it so only authenticated users can affect the data in the tables. You can find out more in the [Supabase documentation page](https://supabase.io/docs/guides/auth/row-level-security).\r\n\r\n        ";
						}
						async function compiledContent$2x() {
							return load$2x().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2x() {
							return (await import('./chunks/index.69e7a1e4.mjs'));
						}
						function Content$2x(...args) {
							return load$2x().then((m) => m.default(...args));
						}
						Content$2x.isAstroComponentFactory = true;
						function getHeadings$2x() {
							return load$2x().then((m) => m.metadata.headings);
						}
						function getHeaders$2x() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2x().then((m) => m.metadata.headings);
						}

const __vite_glob_0_115 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2x,
  file: file$2x,
  url: url$2x,
  rawContent: rawContent$2x,
  compiledContent: compiledContent$2x,
  default: load$2x,
  Content: Content$2x,
  getHeadings: getHeadings$2x,
  getHeaders: getHeaders$2x
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2w = {"title":"Google and Amazon are Wrong About Voice","description":"Google and Amazon are often thought of as the forefront of voice technology—but they're getting it wrong.","date":"2018-05-11T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1662069451/blog/google-and-amazon-are-wrong-about-voice/placeholder-post-image%402x.jpg","authors":["scott-stephenson"],"category":"ai-and-engineering","tags":["voice-tech"],"seo":{"title":"Google and Amazon are wrong about voice","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1662069451/blog/google-and-amazon-are-wrong-about-voice/placeholder-post-image%402x.jpg"},"shorturls":{"share":"https://dpgr.am/472d6e5","twitter":"https://dpgr.am/7e3cc62","linkedin":"https://dpgr.am/4dbfb6e","reddit":"https://dpgr.am/a206172","facebook":"https://dpgr.am/58a8676"}};
						const file$2w = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/google-and-amazon-are-wrong-about-voice/index.md";
						const url$2w = undefined;
						function rawContent$2w() {
							return "\r\n[Speech recognition](https://blog.deepgram.com/what-is-asr/) is a hot topic. Google just announced new features for its [voice assistant](https://blog.deepgram.com/what-makes-alexa-siri-terminator-and-hal-tick/) like continued conversation and multiple actions. Amazon Alexa has been leading the way in voice-enabled shopping. Both companies are making vast improvements, but they're missing the big picture.\r\n\r\n## Voice assistants are not the future of voice.\r\n\r\nVoice assistants will be a component for consumers, but the sleeping giant is speech recognition for businesses. [By 2020, there will be more than 169 billion calls made to businesses per year](http://www.biakelsey.com/biakelsey-estimates-click-call-influences-1-trillion-u-s-consumer-spending/). Businesses currently invest large amounts in software to track and personalize the customer journey, but they're blind when it comes to those calls with their customers. Why? _Most speech recognition is terrible._\r\n\r\n**Much of speech recognition has been optimized for one-way, short-form conversations like those delivered to an assistant.**\r\n\r\nSpeech recognition hasn't been trained to handle real-life use cases. So, when you're calling your insurance company from the side of a busy freeway, the resulting data may look something like this:\r\n\r\n![](https://res.cloudinary.com/deepgram/image/upload/v1661976372/blog/google-and-amazon-are-wrong-about-voice/google%402x.png)\r\n\r\n_Actual result from leading speech recognition provider_\r\n\r\nRather than:\r\n\r\n![](https://res.cloudinary.com/deepgram/image/upload/v1661976373/blog/google-and-amazon-are-wrong-about-voice/full%402x-1.png)\r\n\r\n**Higher accuracy speech recognition allows for faster action and greater insight into your customer's wants and needs.**\r\n\r\nAnd, with nearly [90% of companies saying they compete on the basis of customer experience](https://blogs.gartner.com/jake-sorofman/gartner-surveys-confirm-customer-experience-new-battlefield/), an inability to act on the insights buried within speech means massive missed opportunity.\r\n\r\nFor example, the difference between a five minute wait for that tow versus a two hour one will probably cause you to switch insurance providers. Multiply that by the number of people that need roadside assistance every year and soon all those unhappy customers mean huge losses in revenue.\r\n\r\nThat's why companies like [NVIDIA, a leader in AI Computing, has invested in Deepgram](https://blogs.nvidia.com/blog/2018/03/26/nvidia-invests-in-speech-recognition-startup-deepgram/) to revamp the way companies approach speech recognition.\r\n\r\n> \"While many companies already implement accelerated speech recognition, true speech analytics has until recently been largely untouched by deep learning,\" said Jeff Herbst, vice president of business development at NVIDIA. \"Deepgram has done an amazing job introducing deep learning to this field, and we look forward to working closely with them to advance deep learning-driven speech analytics to the next level.\"\r\n\r\n[Deepgram](https://www.deepgram.com/business) works with companies that facilitate these critical moments by building speech recognition models directly trained and tailored to deliver high accuracy on everyday audio. By pushing our models to perform under complex, real-life conditions with background-noise, multiple speakers, diverse accents, and more, our [customers achieve accuracy rates miles above what they were seeing from competitor solutions](https://blog.deepgram.com/customer-story-rideshare-smartrhino-deepgram/). With Deepgram's speech recognition, they finally get visibility into a pivotal piece of the customer journey.\r\n\r\nTo learn more about Deepgram's speech recognition built for business, contact [sales@deepgram.com](https://deepgram.comnull/).\r\n";
						}
						async function compiledContent$2w() {
							return load$2w().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2w() {
							return (await import('./chunks/index.79cea011.mjs'));
						}
						function Content$2w(...args) {
							return load$2w().then((m) => m.default(...args));
						}
						Content$2w.isAstroComponentFactory = true;
						function getHeadings$2w() {
							return load$2w().then((m) => m.metadata.headings);
						}
						function getHeaders$2w() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2w().then((m) => m.metadata.headings);
						}

const __vite_glob_0_116 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2w,
  file: file$2w,
  url: url$2w,
  rawContent: rawContent$2w,
  compiledContent: compiledContent$2w,
  default: load$2w,
  Content: Content$2w,
  getHeadings: getHeadings$2w,
  getHeaders: getHeaders$2w
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2v = {"title":"A Guide to DeepSpeech Speech to Text","description":"DeepSpeech is a Python library for doing ASR. In this post, we’ll look at how to use DeepSpeech to do Speech to Text in Python","date":"2022-08-01T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1659364680/blog/2022/08/guide-deepspeech-speech-to-text/cover.jpg","authors":["yujian-tang"],"category":"tutorial","tags":["python","deepspeech"],"seo":{"title":"A Guide to DeepSpeech Speech to Text","description":"DeepSpeech is a Python library for doing ASR. In this post, we’ll look at how to use DeepSpeech to do Speech to Text in Python"},"shorturls":{"share":"https://dpgr.am/2b36a93","twitter":"https://dpgr.am/3d69f41","linkedin":"https://dpgr.am/d6f2658","reddit":"https://dpgr.am/2dc3fb8","facebook":"https://dpgr.am/7056571"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454117/blog/guide-deepspeech-speech-to-text/ograph.png"}};
						const file$2v = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/guide-deepspeech-speech-to-text/index.md";
						const url$2v = undefined;
						function rawContent$2v() {
							return "\r\nNo, we’re not talking about you Cthulhu. This is a different type of DeepSpeech. The DeepSpeech we’re talking about today is a Python speech to text library. Speech to text is part of [Natural Language Processing (NLP)](https://pythonalgos.com/?p=1436). Automated speech recognition, or ASR, started out as an offshoot of NLP in the 1990s.\r\n\r\nToday, there are tons of audio libraries that can help you [manipulate audio data](https://blog.deepgram.com/best-python-audio-manipulation-tools/) such as DeepSpeech and [PyTorch](https://blog.deepgram.com/pytorch-intro-with-torchaudio/). In this post, we will be using DeepSpeech to do both asynchronous and real time speech transcription. We will cover:\r\n\r\n*   [What is DeepSpeech?](#what-is-deepspeech)\r\n*   [Set Up for Local Speech to Text with DeepSpeech](#set-up-for-local-speech-to-text-with-deepspeech)\r\n*   [File Handler for DeepSpeech Speech Transcription](#file-handler-for-deepspeech-speech-transcription)\r\n*   [Transcribe Speech to Text for WAV file with DeepSpeech](#transcribe-speech-to-text-for-wav-file-with-deepspeech)\r\n*   [DeepSpeech CLI for Real-Time and Asynchronous Speech to Text](#deepspeech-cli-for-real-time-and-asynchronous-speech-to-text)\r\n*   [Summary](#summary)\r\n\r\n## What is DeepSpeech?\r\n\r\nDeepSpeech is an open source Python library that enables us to build automatic speech recognition systems. It is based on Baidu’s 2014 paper titled [Deep Speech: Scaling up end-to-end speech recognition](https://arxiv.org/abs/1412.5567).\r\n\r\nThe initial proposal for Deep Speech was simple - let’s create a speech recognition system based entirely off of deep learning. The paper describes a solution using RNNs trained with multiple GPUs with no concept of phonemes. The authors, Hannun et al., show that their solution also outperformed the existing solutions at the time and was more robust to background noise without a need for filtering.\r\n\r\nSince then, Mozilla has been the one in charge of maintaining the open source Python package for DeepSpeech. Before moving on, it’s important to note that DeepSpeech is not yet compatible with Python 3.10 nor some more recent versions of \\*nix kernels. I suggest using a virtual machine or Docker container to develop with DeepSpeech on unsupported OSes.\r\n\r\n## Set Up for Local Speech to Text with DeepSpeech\r\n\r\nTo use DeepSpeech, we have to install a few libraries. We need `deepspeech`, `numpy`, and `webrtcvad`. We can install all of these by running `pip install deepspeech numpy webrtcvad`. The `webrtcvad` library is the voice activity detection (VAD) library developed by Google for WebRTC (real time communication).\r\n\r\nFor the asynchronous transcription, we’re going to need three files. One file to handle interaction with WAV data, one file to transcribe speech to text on a WAV file, and one to use these two in the command line. We will also be using a pretrained DeepSpeech model and scorer. You can download their model by running the following lines in your terminal:\r\n\r\n```bash\r\nwget https://github.com/mozilla/DeepSpeech/releases/download/v0.9.3/deepspeech-0.9.3-models.pbmm\r\nwget https://github.com/mozilla/DeepSpeech/releases/download/v0.9.3/deepspeech-0.9.3-models.scorer\r\n```\r\n\r\n## File Handler for DeepSpeech Speech Transcription\r\n\r\nThe first file we create is the WAV handling file. This file should be named something like `wav_handler.py`. We import three built-in libraries to do this, `wave`, `collections`, and `contextlib`. We create four functions and one class. We need one function each to read WAV files, write WAV files, create Frames, and detect voice activity. Our one class represents individual frames in the WAV file.\r\n\r\n### Reading Audio Data from a WAV file\r\n\r\nLet’s start with creating a function to read WAV files. This function will take an input, this input is the path to a WAV file. The file will use the `contextlib` library to open the WAV file and read in the contents as bytes. Next, we run multiple asserts on the WAV file - it must have one channel, have a sample width of 2, have a sample rate of 8, 16, or 32 kHz.\r\n\r\nOnce we have asserted that the WAV file is in the right format for processing, we extract the frames. Next, we extract the pcm data from the frames and the duration from the metadata. Finally, we return the PCM data, the sample rate, and the duration.\r\n\r\n```py\r\nimport collections\r\nimport contextlib\r\nimport wave\r\n\r\n\"\"\"Reads a .wav file.\r\nInput: path to a .wav file\r\nOutput: tuple of pcm data, sample rate, and duration\r\n\"\"\"\r\ndef read_wave(path):\r\n   with contextlib.closing(wave.open(path, 'rb')) as wf:\r\n       num_channels = wf.getnchannels()\r\n       assert num_channels == 1\r\n       sample_width = wf.getsampwidth()\r\n       assert sample_width == 2\r\n       sample_rate = wf.getframerate()\r\n       assert sample_rate in (8000, 16000, 32000)\r\n       frames = wf.getnframes()\r\n       pcm_data = wf.readframes(frames)\r\n       duration = frames / sample_rate\r\n       return pcm_data, sample_rate, duration\r\n```\r\n\r\n### Writing Audio Data to a WAV file\r\n\r\nNow let’s create the function to write audio data to a WAV file. This function requires three parameters, the path to a file to write to, the audio data, and the sample rate. This function writes a WAV file in the same way that the read function asserts its parameters. All we do is here is set the channels, sample width, and frame rate and then write the audio frames.\r\n\r\n```py\r\n\"\"\"Writes a .wav file.\r\nInput: path to new .wav file, PCM audio data, and sample rate.\r\nOutput: a .wav file\r\n\"\"\"\r\ndef write_wave(path, audio, sample_rate):\r\n   with contextlib.closing(wave.open(path, 'wb')) as wf:\r\n       wf.setnchannels(1)\r\n       wf.setsampwidth(2)\r\n       wf.setframerate(sample_rate)\r\n       wf.writeframes(audio)\r\n```\r\n\r\n### Creating Frames of Audio Data for DeepSpeech to Transcribe\r\n\r\nWe’re going to create a class called `Frame` to hold some information to represent our audio data and make it easier to handle. This object requires three parameters to be created: the bytes, the timestamp in the audio file, and the duration of the `Frame`.\r\n\r\nWe also need to create a function to create frames. You can think of this function as a frame [generator](https://docs.google.com/document/d/1JwfuaBrao_pZjLBy6isPdqv_hndQnetCJ8MjYWlLtlI/edit?pli=1\\&disco=AAAAddu7kIo) or a frame factory that returns an iterator. This function requires three parameters: the frame duration in milliseconds, the audio data, and the sample rate.\r\n\r\nThis function starts by deriving an interval of frames from the passed in sample rate and frame duration in milliseconds. We start at an offset and timestamp of 0. We also create a duration constant equal to the number of frames in a second.\r\n\r\nWhile the current offset can be incremented by the interval constant and be within the number of frames of the audio, we generate a `Frame` for each interval and then increment the timestamp and offset appropriately.\r\n\r\n```py\r\n\"\"\"Represents a \"frame\" of audio data.\r\nRequires the number of byes, the timestamp of the frame, and the duration on init\"\"\"\r\nclass Frame(object):\r\n   def __init__(self, bytes, timestamp, duration):\r\n       self.bytes = bytes\r\n       self.timestamp = timestamp\r\n       self.duration = duration\r\n\r\n\"\"\"Generates audio frames from PCM audio data.\r\nInput: the desired frame duration in milliseconds, the PCM data, and\r\nthe sample rate.\r\nYields/Generates: Frames of the requested duration.\r\n\"\"\"\r\ndef frame_generator(frame_duration_ms, audio, sample_rate):\r\n   n = int(sample_rate * (frame_duration_ms / 1000.0) * 2)\r\n   offset = 0\r\n   timestamp = 0.0\r\n   duration = (float(n) / sample_rate) / 2.0\r\n   while offset + n < len(audio):\r\n       yield Frame(audio[offset:offset + n], timestamp, duration)\r\n       timestamp += duration\r\n       offset += n\r\n```\r\n\r\n### Collecting Voice Activated Frames for Speech to Text with DeepSpeech\r\n\r\nNext, let’s create a function to collect all the frames that contain voice. This function requires a sample rate, the frame duration in milliseconds, the padding duration in milliseconds, a voice activation detector (VAD) from `webrtcvad`, and the audio data frames.\r\n\r\nThe VAD algorithm uses a padded ring buffer and checks to see what percentage of the frames in the window are voiced. When the window reaches a 90% voiced frame rate, the VAD triggers and begins yielding audio frames. While generating frames, if the percentage of voiced audio data in the frame drops below 10%, it will stop generating frames.\r\n\r\n```py\r\n\"\"\"Filters out non-voiced audio frames.\r\nGiven a webrtcvad.Vad and a source of audio frames, yields only\r\nthe voiced audio.\r\nArguments:\r\nsample_rate - The audio sample rate, in Hz.\r\nframe_duration_ms - The frame duration in milliseconds.\r\npadding_duration_ms - The amount to pad the window, in milliseconds.\r\nvad - An instance of webrtcvad.Vad.\r\nframes - a source of audio frames (sequence or generator).\r\nReturns: A generator that yields PCM audio data.\r\n\"\"\"\r\ndef vad_collector(sample_rate, frame_duration_ms,\r\n                 padding_duration_ms, vad, frames):\r\n   num_padding_frames = int(padding_duration_ms / frame_duration_ms)\r\n   # We use a deque for our sliding window/ring buffer.\r\n   ring_buffer = collections.deque(maxlen=num_padding_frames)\r\n   # We have two states: TRIGGERED and NOTTRIGGERED. We start in the\r\n   # NOTTRIGGERED state.\r\n   triggered = False\r\n\r\n   voiced_frames = []\r\n   for frame in frames:\r\n       is_speech = vad.is_speech(frame.bytes, sample_rate)\r\n\r\n       if not triggered:\r\n           ring_buffer.append((frame, is_speech))\r\n           num_voiced = len([f for f, speech in ring_buffer if speech])\r\n           # If we're NOTTRIGGERED and more than 90% of the frames in\r\n           # the ring buffer are voiced frames, then enter the\r\n           # TRIGGERED state.\r\n           if num_voiced > 0.9 * ring_buffer.maxlen:\r\n               triggered = True\r\n               # We want to yield all the audio we see from now until\r\n               # we are NOTTRIGGERED, but we have to start with the\r\n               # audio that's already in the ring buffer.\r\n               for f, s in ring_buffer:\r\n                   voiced_frames.append(f)\r\n               ring_buffer.clear()\r\n       else:\r\n           # We're in the TRIGGERED state, so collect the audio data\r\n           # and add it to the ring buffer.\r\n           voiced_frames.append(frame)\r\n           ring_buffer.append((frame, is_speech))\r\n           num_unvoiced = len([f for f, speech in ring_buffer if not speech])\r\n           # If more than 90% of the frames in the ring buffer are\r\n           # unvoiced, then enter NOTTRIGGERED and yield whatever\r\n           # audio we've collected.\r\n           if num_unvoiced > 0.9 * ring_buffer.maxlen:\r\n               triggered = False\r\n               yield b''.join([f.bytes for f in voiced_frames])\r\n               ring_buffer.clear()\r\n               voiced_frames = []\r\n   if triggered:\r\n       pass\r\n   # If we have any leftover voiced audio when we run out of input,\r\n   # yield it.\r\n   if voiced_frames:\r\n       yield b''.join([f.bytes for f in voiced_frames])\r\n```\r\n\r\n## Transcribe Speech to Text for WAV file with DeepSpeech\r\n\r\nWe’re going to create a new file for this section. This file should be named something like `wav_transcriber.py`. This layer completely abstracts out WAV handling from the CLI (which we create below). We use these functions to call DeepSpeech on the audio data and transcribe it.\r\n\r\n### Pick Which DeepSpeech Model to Use\r\n\r\nThe first function we create in this file is the function to load up the model and scorer for DeepSpeech to run speech to text with. This function takes two parameters, the models graph, which we create a function to produce below, and the path to the scorer file. All it does is load the model from the graph and enable the use of the scorer. This function returns a DeepSpeech object.\r\n\r\n```py\r\nimport glob\r\nimport webrtcvad\r\nimport logging\r\nimport wav_handler\r\nfrom deepspeech import Model\r\nfrom timeit import default_timer as timer\r\n\r\n'''\r\nLoad the pre-trained model into the memory\r\n@param models: Output Graph Protocol Buffer file\r\n@param scorer: Scorer file\r\n@Retval\r\nReturns a DeepSpeech Object\r\n'''\r\ndef load_model(models, scorer):\r\n   ds = Model(models)\r\n   ds.enableExternalScorer(scorer)\r\n   return ds\r\n```\r\n\r\n### Speech to Text on an Audio File with DeepSpeech\r\n\r\nThis function is the one that does the actual speech recognition. It takes three inputs, a DeepSpeech model, the audio data, and the sample rate.\r\n\r\nWe begin by setting the time to 0 and calculating the length of the audio. All we really have to do is call the DeepSpeech model’s `stt` function to do our own `stt` function. We pass the audio file to the `stt` function and return the output.\r\n\r\n```py\r\n'''\r\nRun Inference on input audio file\r\n@param ds: Deepspeech object\r\n@param audio: Input audio for running inference on\r\n@param fs: Sample rate of the input audio file\r\n@Retval:\r\nReturns a list [Inference, Inference Time, Audio Length]\r\n'''\r\ndef stt(ds, audio, fs):\r\n   inference_time = 0.0\r\n   audio_length = len(audio) * (1 / fs)\r\n\r\n   # Run Deepspeech\r\n   output = ds.stt(audio)\r\n\r\n   return output\r\n```\r\n\r\n### DeepSpeech Model Graph Creator Function\r\n\r\nThis is the function that creates the model graph for the `load_model` function we created a couple sections above. This function takes the path to a directory. From that directory, it looks for files with the DeepSpeech model extension, `pbmm` and the DeepSpeech scorer file extension, `.scorer`. Then, it returns both of those values.\r\n\r\n```py\r\n'''\r\nResolve directory path for the models and fetch each of them.\r\n@param dirName: Path to the directory containing pre-trained models\r\n@Retval:\r\nRetunns a tuple containing each of the model files (pb, scorer)\r\n'''\r\ndef resolve_models(dirName):\r\n   pb = glob.glob(dirName + \"/*.pbmm\")[0]\r\n   logging.debug(\"Found Model: %s\" % pb)\r\n\r\n   scorer = glob.glob(dirName + \"/*.scorer\")[0]\r\n   logging.debug(\"Found scorer: %s\" % scorer)\r\n\r\n   return pb, scorer\r\n```\r\n\r\n### Voice Activation Detection to Create Segments for Speech to Text\r\n\r\nThe last function in our WAV transcription file generates segments of text that contain voice. We use the WAV handler file we created earlier and `webrtcvad` to do the heavy lifting. This function requires two parameters: a WAV file and an integer value from 0 to 3 representing how aggressively we want to filter out non-voice activity.\r\n\r\nWe call the `read_wave` function from the `wav_handler.py` file we created earlier and imported above to get the audio data, sample rate, and audio length. We then assert that the sample rate is 16kHz before moving on to create a VAD object. Next, we call the frame generator from `wav_handler`.\r\n\r\nWe convert the generated iterator to a list which we pass to the `vad_collector` function from `wav_handler` along with the sample rate, frame duration (30 ms), padding duration (300 ms), and VAD object. Finally, we return the collected VAD segments along with the sample rate and audio length.\r\n\r\n```py\r\n'''\r\nGenerate VAD segments. Filters out non-voiced audio frames.\r\n@param waveFile: Input wav file to run VAD on.0\r\n@Retval:\r\nReturns tuple of\r\n   segments: a bytearray of multiple smaller audio frames\r\n             (The longer audio split into mutiple smaller one's)\r\n   sample_rate: Sample rate of the input audio file\r\n   audio_length: Duraton of the input audio file\r\n'''\r\ndef vad_segment_generator(wavFile, aggressiveness):\r\n   audio, sample_rate, audio_length = wav_handler.read_wave(wavFile)\r\n   assert sample_rate == 16000, \"Only 16000Hz input WAV files are supported for now!\"\r\n   vad = webrtcvad.Vad(int(aggressiveness))\r\n   frames = wav_handler.frame_generator(30, audio, sample_rate)\r\n   frames = list(frames)\r\n   segments = wav_handler.vad_collector(sample_rate, 30, 300, vad, frames)\r\n\r\n   return segments, sample_rate, audio_length\r\n```\r\n\r\n## DeepSpeech CLI for Real Time and Asynchronous Speech to Text\r\n\r\nEverything is set up to transcribe audio data with DeepSpeech via pretrained models. Now, let’s look at how to turn the functionality we created above into a command line interface for real time and asynchronous speech to text. We start by importing a bunch of libraries for operating with the command line - `sys`, `os`, `logging`, `argparse`, `subprocess`, and `shlex`. We also need to import `numpy` and the `wav_transcriber` we made above to work with the audio data.\r\n\r\n### Reading Arguments for DeepSpeech Speech to Text\r\n\r\nWe create a main function that takes one parameter - `args`. These are the arguments passed in through the command line. We use the `argparse` libraries to parse the arguments sent in. We also create helpful tips on how to use each one.\r\n\r\nWe use `aggressive` to determine how aggressively we want to filter. `audio` directs us to the audio file path. `model` points us to the directory containing the model and scorer. Finally, `stream` dictates whether or not we are streaming audio. Neither `stream` nor `audio` is required, but one or the other must be present.\r\n\r\n```py\r\nimport sys\r\nimport os\r\nimport logging\r\nimport argparse\r\nimport subprocess\r\nimport shlex\r\nimport numpy as np\r\nimport wav_transcriber\r\n\r\n# Debug helpers\r\nlogging.basicConfig(stream=sys.stderr, level=logging.DEBUG)\r\n\r\ndef main(args):\r\n   parser = argparse.ArgumentParser(description='Transcribe long audio files using webRTC VAD or use the streaming interface')\r\n   parser.add_argument('--aggressive', type=int, choices=range(4), required=False,\r\n                       help='Determines how aggressive filtering out non-speech is. (Interger between 0-3)')\r\n   parser.add_argument('--audio', required=False,\r\n                       help='Path to the audio file to run (WAV format)')\r\n   parser.add_argument('--model', required=True,\r\n                       help='Path to directory that contains all model files (output_graph and scorer)')\r\n   parser.add_argument('--stream', required=False, action='store_true',\r\n                       help='To use deepspeech streaming interface')\r\n   args = parser.parse_args()\r\n   if args.stream is True:\r\n       print(\"Opening mic for streaming\")\r\n   elif args.audio is not None:\r\n       logging.debug(\"Transcribing audio file @ %s\" % args.audio)\r\n   else:\r\n       parser.print_help()\r\n       parser.exit()\r\n```\r\n\r\n### Using DeepSpeech for Real Time or Asynchronous Speech Recognition\r\n\r\nThis is still inside the main function we started above. Once we parse all the arguments, we load up DeepSpeech. First, we get the directory containing the models. Next, we call the `wav_transcriber` to resolve and load the models.\r\n\r\nIf we pass the path to an audio data file in the command line, then we will run asynchronous speech recognition. The first thing we do for that is call the VAD segment generator to generate the VAD segments and get the sample rate and audio length. Next, we open up a text file to transcribe to.\r\n\r\nFor each of the enumerated segments, we will process each chunk by using `numpy` to pull the segment from the buffer and the speech to text function from `wav_transcriber` to do the speech to text functionality. We write to the text file until we run out of audio segments.\r\n\r\nIf we pass stream instead of audio, then we open up the mic to stream audio data in. If you don’t need real time automatic speech recognition, then you can ignore this part. First, we have to spin up a subprocess to open up a mic to stream in real time just like we did with PyTorch local speech recognition.\r\n\r\nWe use the `subprocess` and `shlex` libraries to open the mic to stream voice audio until we shut it down. The model will read 512 bytes of audio data at a time and transcribe it.\r\n\r\n```py\r\n# Point to a path containing the pre-trained models & resolve ~ if used\r\ndirName = os.path.expanduser(args.model)\r\n\r\n# Resolve all the paths of model files\r\noutput_graph, scorer = wav_transcriber.resolve_models(dirName)\r\n\r\n# Load output_graph, alpahbet and scorer\r\nmodel_retval = wav_transcriber.load_model(output_graph, scorer)\r\n\r\nif args.audio is not None:\r\n    # Run VAD on the input file\r\n    waveFile = args.audio\r\n    segments, sample_rate, audio_length = wav_transcriber.vad_segment_generator(waveFile, args.aggressive)\r\n    f = open(waveFile.rstrip(\".wav\") + \".txt\", 'w')\r\n    logging.debug(\"Saving Transcript @: %s\" % waveFile.rstrip(\".wav\") + \".txt\")\r\n\r\n    for i, segment in enumerate(segments):\r\n        # Run deepspeech on the chunk that just completed VAD\r\n        logging.debug(\"Processing chunk %002d\" % (i,))\r\n        audio = np.frombuffer(segment, dtype=np.int16)\r\n        output = wav_transcriber.stt(model_retval, audio, sample_rate)\r\n        logging.debug(\"Transcript: %s\" % output)\r\n\r\n        f.write(output + \" \")\r\n\r\n    # Summary of the files processed\r\n    f.close()\r\n\r\nelse:\r\n    sctx = model_retval.createStream()\r\n    subproc = subprocess.Popen(shlex.split('rec -q -V0 -e signed -L -c 1 -b 16 -r 16k -t raw - gain -2'),\r\n                                stdout=subprocess.PIPE,\r\n                                bufsize=0)\r\n    print('You can start speaking now. Press Control-C to stop recording.')\r\n\r\n    try:\r\n        while True:\r\n            data = subproc.stdout.read(512)\r\n            sctx.feedAudioContent(np.frombuffer(data, np.int16))\r\n    except KeyboardInterrupt:\r\n        print('Transcription: ', sctx.finishStream())\r\n        subproc.terminate()\r\n        subproc.wait()\r\n\r\nif __name__ == '__main__':\r\n   main(sys.argv[1:])\r\n```\r\n\r\n## Summary\r\n\r\nWe started this post out with a high level view of DeepSpeech, an open source speech recognition software. It was inspired by a 2014 paper from Baidu and is currently maintained by Mozilla.\r\n\r\nAfter a basic introduction, we stepped into a guide on how to use DeepSpeech to locally transcribe speech to text. While it may have been possible to create all of this code in one document, we opted for a modular approach with principles of software engineering in mind.\r\n\r\nWe created three modules. One to handle WAV files, which are the audio data files that we can use DeepSpeech to transcribe. One to transcribe from WAV files, and one more file to create a command line interface to use DeepSpeech. Our CLI allows us to pass in options to pick if we want to do real time speech recognition or run speech recognition on an existing WAV audio file.\r\n\r\n        ";
						}
						async function compiledContent$2v() {
							return load$2v().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2v() {
							return (await import('./chunks/index.f6dd0fc5.mjs'));
						}
						function Content$2v(...args) {
							return load$2v().then((m) => m.default(...args));
						}
						Content$2v.isAstroComponentFactory = true;
						function getHeadings$2v() {
							return load$2v().then((m) => m.metadata.headings);
						}
						function getHeaders$2v() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2v().then((m) => m.metadata.headings);
						}

const __vite_glob_0_117 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2v,
  file: file$2v,
  url: url$2v,
  rawContent: rawContent$2v,
  compiledContent: compiledContent$2v,
  default: load$2v,
  Content: Content$2v,
  getHeadings: getHeadings$2v,
  getHeaders: getHeaders$2v
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2u = {"title":"Happy National Native American Heritage Month!","description":"Happy Native American Heritage Month. The contribution of Native Americans to our culture and nation are underrepresented due to past and currently discrimination. As a nation, we hope to better embrace their culture and important languages for future generations.","date":"2021-11-30T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981388/blog/happy-national-native-american-heritage-month/blog-native-american-heritage-month-2021-thumb-554.png","authors":["sam-zegas"],"category":"identity-and-language","tags":["inclusion","heritage"],"seo":{"title":"Happy National Native American Heritage Month!","description":"Happy Native American Heritage Month. The contribution of Native Americans to our culture and nation are underrepresented due to past and currently discrimination. As a nation, we hope to better embrace their culture and important languages for future generations."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981388/blog/happy-national-native-american-heritage-month/blog-native-american-heritage-month-2021-thumb-554.png"},"shorturls":{"share":"https://dpgr.am/561c64c","twitter":"https://dpgr.am/60e2f3f","linkedin":"https://dpgr.am/e0e4f5a","reddit":"https://dpgr.am/34524f6","facebook":"https://dpgr.am/4ed28d3"}};
						const file$2u = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/happy-national-native-american-heritage-month/index.md";
						const url$2u = undefined;
						function rawContent$2u() {
							return "November is [National Native American Heritage Month](https://nativeamericanheritagemonth.gov/)! We're celebrating by giving you a tour of some topics related to Native American history and contemporary life that we have on our minds this month. Stick around till the end for some recommendations on books, influencers, and other resources for connecting with Native American voices.\n\nFirst, though, this post should rightly begin with a personal note: I write this post as someone without Native American heritage. My work with anthropology, history, and linguistics give me an academic perspective on these topics, but I can't speak from the perspective of someone with Native American heritage and do not have first-hand experience with issues that native communities face today. As Deepgram's team grows, I very much look forward to a day when we can bring you those perspectives too.\n\n## Some History\n\nBefore the arrival of European colonists, North America (including the territory of modern Mexico) was home to as many as 112 million people, [according to some estimates](https://uwpress.wisc.edu/books/0289.htm). These people lived in countless communities across the continent, speaking languages from at least [30 major language families](https://commons.wikimedia.org/wiki/File:Langs_N.Amer.png), and spread across a dozen or more [distinct cultural areas](https://commons.wikimedia.org/wiki/File:North_American_cultural_areas.png). Their societies ranged from small local groups to complex, hierarchical societies like the Aztec Empire.\n\nThere is ample [documentation](https://americanindian.si.edu/nk360/lessons-resources/search-resources) from today's native communities as well as non-native anthropologists that ancient native societies had rich cultures. They had deep knowledge and expertise about how to live in their environments, complex belief systems and mythologies, and long-standing traditions in music and visual arts. A comprehensive survey of any one of these societies-much less all of them-would fill thousands of pages. There are many [resources](https://www.loc.gov/rr/main/indians_rec_links/overview.html) available to learn about native North Americans and about specific groups. We highly encourage readers of this post to add books about (and by) Native Americans to their reading list. More resources are available at the end of this post.\n\n### Initial Contact\n\nInitial contact between native people and colonials varied widely. In some famous cases, like that of the [fall of the Aztec Empire](https://www.smithsonianmag.com/smart-news/mexico-city-marks-500th-anniversary-fall-tenochtitlan-180977794/) to the Spanish conquistadors, initial contact was belligerent from the start. In other cases, Native Americans made alliances with colonists, including during the [French and Indian War](https://history.state.gov/milestones/1750-1775/french-indian-war) and the [American Revolutionary War](https://historyofmassachusetts.org/native-americans-revolutionary-war/). Indeed, the [first Thanksgiving](https://time.com/4577425/thanksgiving-2016-true-story/) was famously a friendly gathering, although the peace did not last long after the event. There are in fact [many early impressions](http://nationalhumanitiescenter.org/pds/becomingamer/peoples/text3/indianscolonists.pdf) recorded between colonists and natives. Some of these commentators recognize the dignity and humanity of their counterparts while others do not.\n\nThe years after initial contact was disastrous for native peoples. Disease, warfare, and pressure from the growing colonies increasingly disrupted and displaced Native Americans on the eastern seaboard and even farther to the west. By 1650, a mere 158 years after the initial contact between Native Americans and Europeans, the population of Native Americans had undergone demographic catastrophe. From a possible population of 112 million in 1492, the 1650 native population of the Americans had declined to [no more than 6 million](https://uwpress.wisc.edu/books/0289.htm).\n\nThis upheaval caused personal tragedies and social disruption on an incalculable scale. Native communities continue to feel the effects of this disaster hundreds of years later. Sadly, the story of disruption does not end with the population crash of the early colonial period. From the foundation of the United States in 1776 and through the 1800s, the US government enacted policies of [removal](https://www.loc.gov/classroom-materials/immigration/native-american/removing-native-americans-from-their-land/) that forced Native American communities onto reservations. Some groups were forced onto reservations that were up to a thousand miles away from their traditional homelands in the so-called [Indian Territory](https://www.npr.org/templates/story/story.php?storyId=12261992#:~:text=In%201830%2C%20Congress%20passed%20the,it%20included%20modern%2Dday%20Oklahoma.), an area that later became the state of Oklahoma.\n\nOver time, the government reduced the size of the original reservations and confined the communities to smaller and smaller areas. From the 1870's all the way through to the mid-1900s, the US government also pursued an [overtly racist](http://historymatters.gmu.edu/d/4929/) policy of abducting Native American children from their families and sending them to residential [Indian Schools](https://www.npr.org/templates/story/story.php?storyId=16516865) in order to forcibly assimilate them to American culture. This practice came at a high cost to the children and their communities: [personal trauma](https://www.theatlantic.com/education/archive/2019/03/traumatic-legacy-indian-boarding-schools/584293/), the [breakup of families](https://www.mprnews.org/story/2019/10/03/stories-of-life-in-indian-boarding-schools), and the [loss of heritage languages](https://www.hcn.org/issues/51.21-22/indigenous-affairs-the-u-s-has-spent-more-money-erasing-native-languages-than-saving-them) by younger generations. The Indian School policy also led to an unknown number of [deaths](https://www.teenvogue.com/story/indian-residential-schools-graves) of students, a tragedy that has only recently begun to get the attention it deserves.\n\n### Modern History\n\nMore recently, the situation for Native Americans has begun to shift. Native Americans' [contributions](https://www.uso.org/stories/2914-a-history-of-military-service-native-americans-in-the-u-s-military-yesterday-and-today) to the war effort during WWII changed the narrative about their place in US society. Not long afterward, the Civil Rights movement ushered in a new national conversation about native peoples. The [Civil Rights Act of 1964](https://en.wikipedia.org/wiki/Civil_Rights_Act_of_1964) outlawed discrimination by race. It was followed four years later by the [Civil Rights Act of 1968](https://en.wikipedia.org/wiki/Civil_Rights_Act_of_1968), which extended the Bill of Rights to Native Americans and reinforced the sovereign status of tribal governments.\n\nThe [Bureau of Indian Affairs,](https://www.bia.gov/) which started in the 1800s as the arm of the US government promoting assimilation and subjugation of native people, has now shifted to a service organization focused on the wellbeing of these communities. Today, it is headed by Bryan Newland, a [member of the Ojibwe Nation](https://www.bia.gov/as-ia). Furthermore, the Department of the Interior (which the BIA is a part of) also has its first-ever leader of Native American heritage: [Deb Haaland](https://www.doi.gov/secretary-deb-haaland). Secretary Haaland is also the [first cabinet secretary of Native American descent](https://www.reuters.com/article/us-usa-interior-haaland/deb-haaland-becomes-first-ever-native-american-u-s-cabinet-secretary-idUSKBN2B72SO) in the history of the United States.\n\n## Linguistic Concerns\n\nWhen it comes to native languages, the US's history of forced removal and assimilation explains in part the perilous status of many Native American languages today. In other parts of the Americas, there are still [native languages](https://en.wikipedia.org/wiki/Indigenous_languages_of_the_Americas) with over a million speakers-notably Nahuatl (Aztec) in Mexico, the Maya languages in Central America, Quechua and Aymara in the Andes, and Guaraní in Paraguay. The picture is different in the US and Canada, where the largest native language community is Navajo in the southwestern US at 170,000, followed by Cree in central and eastern Canada at 96,000, Ojibwe in the Great Lakes region of the US and Canada at 48,000, Inuktitut in the US and the Canadian Arctic at 39,000, and Blackfoot and Sioux of US and Canadian Great Plains at 34,000 and 25,000, respectively.\n\nUnfortunately, these scattered large language communities are the exception rather than the rule. Hundreds if not thousands of native languages of North America are already extinct. [At least 167](https://www.endangeredlanguages.com/lang/country/USA) are still alive but critically endangered. Many of these endangered languages are spoken by a small number of elderly people, meaning that language transmission from generation to generation has already ceased and needs to be counteracted by [language revitalization programs](https://www.acf.hhs.gov/ana/preserving-native-languages-article) for these languages to survive. Language revitalization programs take a great amount of time, energy, and resources to be successful, and the results won't truly be known for years to come. But they are crucially important and deserve the attention and support of people beyond the native communities they serve.\n\n### Why Revitalization?\n\nNative American languages should be revitalized for many reasons. First, language is a core part of identity and belonging. It serves the members of native communities by giving them a shared connection to their heritage and to each other. Furthermore, language preservation is a key part of cultural preservation due to the interconnectedness of language and culture. If *culture* is defined as a collection of beliefs, values, practices, traditions, and stories that tie a community together and motivate how they move through the world, then *language* is a core piece of infrastructure for culture.\n\nThe culturally-specific vocabulary and idioms in each language make it possible to communicate that culture's beliefs and traditions in ways that cannot be replicated in another language. Mythologies and cultural histories told in native languages contribute to a community's sense of itself, its values, and its future. When a language is lost, many aspects of culture disappear forever along with it. What's at stake is the survival of our society's cultural heritage in all its richness. We all share a common interest in helping that heritage thrive.\n\n## Wrapping Up & Learning More\n\nToday, Native Americans are innovating new ways to preserve and share their cultures. There are many Native American [influencers](https://www.youtube.com/watch?v=YUulYAbg3Jo) on YouTube, TikTok, and other social media platforms that share art and commentary about the native experience with their followers. Movies including [Star Wars](https://www.starwars.com/news/navajo-language-star-wars-a-new-hope) and [Fistful of Dollars](https://www.npr.org/2021/11/17/1055897665/dubbing-a-fistful-of-dollars-to-spread-the-navajo-language) are being dubbed into Navajo. The [National Congress of American Indians](https://www.ncai.org/) and other political groups campaign for improvements to social services, education, environmental protection, and other issues that affect native people. This National Native American Heritage Month, we hope for a world with more representation of native peoples, more revitalization of native languages, and more appreciation for the gifts that native cultures have given the world. Check out these additional resources for more information:\n\n* **[Indian Country Today](https://indiancountrytoday.com/)**: A weekly news source that features articles about native individuals, tribal organizations, and national trends affecting native people.\n* **[Native American Authors](https://reedsy.com/discovery/blog/native-american-authors)**: One of many lists of Native American authors to add to your reading list. \n* **[Native-Owned Businesses](https://www.businessinsider.com/native-owned-businesses)**: Support native-owned businesses this holiday season.\n* **[Indigenous Influencers](https://www.huffpost.com/entry/indigenous-instagram-accounts-to-follow_l_5f9b17e5c5b65a0efac98867)**: Broaden your social media feed by following these indigenous influencers.\n* **[The Smithsonian Museum of the American Indian](https://americanindian.si.edu/)**: With exhibitions in Washington, DC and online, the museum's collection offers many resources on Native American history and contemporary issues.\n* **[Bureau of Indian Affairs FAQs](https://www.bia.gov/frequently-asked-questions#:~:text=As%20U.S.%20citizens%2C%20American%20Indians,tribe%2C%20unless%20Congress%20provides%20otherwise)**: This page answers many FAQs about the current status of reservations, their relationship to the federal and state governments, and other policy issues.";
						}
						async function compiledContent$2u() {
							return load$2u().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2u() {
							return (await import('./chunks/index.a7710019.mjs'));
						}
						function Content$2u(...args) {
							return load$2u().then((m) => m.default(...args));
						}
						Content$2u.isAstroComponentFactory = true;
						function getHeadings$2u() {
							return load$2u().then((m) => m.metadata.headings);
						}
						function getHeaders$2u() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2u().then((m) => m.metadata.headings);
						}

const __vite_glob_0_118 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2u,
  file: file$2u,
  url: url$2u,
  rawContent: rawContent$2u,
  compiledContent: compiledContent$2u,
  default: load$2u,
  Content: Content$2u,
  getHeadings: getHeadings$2u,
  getHeaders: getHeaders$2u
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2t = {"title":"Hell Yes, We Have SDKs, APIs, and Docs","description":"Phase One of our Developer-First initiative to help voice technology developers more easily implement our revolutionary End-to-End AI Speech Platform.","date":"2021-08-03T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981380/blog/hell-yes-we-have-sdks-apis-and-docs/hell-yes-sdks%402x.jpg","authors":["keith-lam"],"category":"product-news","tags":["sdk"],"seo":{"title":"Hell Yes, We Have SDKs, APIs, and Docs","description":"Phase One of our Developer-First initiative to help voice technology developers more easily implement our revolutionary End-to-End AI Speech Platform."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981380/blog/hell-yes-we-have-sdks-apis-and-docs/hell-yes-sdks%402x.jpg"},"shorturls":{"share":"https://dpgr.am/3d69218","twitter":"https://dpgr.am/19425e4","linkedin":"https://dpgr.am/2bcb0a1","reddit":"https://dpgr.am/1cc2dc5","facebook":"https://dpgr.am/440ed0e"}};
						const file$2t = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/hell-yes-we-have-sdks-apis-and-docs/index.md";
						const url$2t = undefined;
						function rawContent$2t() {
							return "We are excited to introduce Phase One of our Developer-First initiative to help voice technology developers implement our revolutionary End-to-End AI Speech Platform more easily**.**\n\n### **Issues With Legacy Speech Recognition**\n\nVoice has been one of the last unstructured data sources to be fully used and mined for insights. According to [Deloitte](https://www.deloittedigital.com/content/dam/deloittedigital/us/documents/blog/blog-20190513-2019%20globalcontactcentersurvey.pdf), fully 90% of all complex conversations in contact centers are done through the voice channel and the number is not going to change anytime soon. With the pandemic, the voice channel has become even more important. In the [2021 State of ASR Report](https://deepgram.com/state-of-asr-report/) by Opus Research, 85% of organizations note that automatic speech recognition is \"important or very important\" to their future enterprise strategy. If audio is so important, why has it not been fully utilized in business?  We believe three main issues prevent wider use of Automatic Speech Recognition (ASR) or Speech-to-Text (STT).\n\n1. **Costs** - The cost of speech-to-text applications has gone down in the last decade because of competition, but has bottomed out due to the use of legacy speech processing that is inefficient and requires huge amounts of computing resources.  The bottom of the legacy cost curve is still too high to seriously consider transcribing all of an organization's voice data.\n2. **Speed** - The speed of transcriptions has also gone down, but it still takes more than one day to transcribe one day of contact center calls.  Instead, organizations sample their calls to try to get some customer insights, but they often miss a gem of a product idea, can't respond quickly enough to churn signals, or can't assist sales in upselling a customer.  For applications requiring real-time transcriptions, like Conversational AI voicebots, the 2-4 second legacy STT lag time does not meet their needs.  You notice a one-second lag on video calls or streaming movies, think about waiting up to 4 seconds before a voicebot answers you.\n3. **Accuracy** - We don't mean general out-of-the-box accuracy of all words, but the accuracy of the important keywords.  Do you care if you get articles, prepositions, and filler words correct?  Maybe, if a readable transcript is most important to you.  But if you are doing data analysis or finding knowledge base responses, you care more about the product names, phone numbers, government ID numbers, terminology, sentiment words, and acronyms because that is where you can find the insights and intent of the conversation.  Most one-size-fits-all, legacy STT solutions max out at around 80% accuracy. It's readable but unusable for things like transcribing phone numbers or social security numbers that require 100% accuracy.  One digit off means incorrect answers or insights.\n\nDeepgram reinvented STT from the ground up specifically to solve these legacy tech issues.\n\n## **New Methods and Easier Access**\n\nDeepgram's STT is built with [End-to-End Deep Learning](https://offers.deepgram.com/how-deepgram-works-whitepaper), a neural network that can be trained and learn how to improve accuracy, remove noise, and focus on the important keywords.  We do not use any of the legacy speech recognition processes, therefore, their cost, speed, and accuracy limitations do not apply to us. A better foundation of STT allows us to expand our focus to improve access and use of our technology. With that in mind, we are releasing the following enhancements for a better developer experience with our platform:\n\n* Two software development kits (SDKs) for [Python](https://pypi.org/project/deepgram-sdk/) and [Node.js](https://www.npmjs.com/package/@deepgram/sdk), with more languages to come\n* A new Developer Console for better API, user, project, billing, and usage management.\n* 200 hours of free pre-recorded transcription or 150 hours of real-time streaming transcriptions with initial [Developer Console sign up](https://console.deepgram.com/).\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-how-deepgram-works\"></WhitepaperPromo>\n\n## **Features Available**\n\nWith this new Developer Console, you can try out all the following features that are included under our batch (pre-recorded) or real-time streaming [price](https://deepgram.com/pricing/).\n\n* Interim results\n* Punctuation\n* Foreign languages\n* Numeral formatting\n* Utterance formatting\n* Find and replace\n* Profanity filter\n* Keyword boosting\n\nDescriptions of these features can be found on our [Product Overview](https://deepgram.com/product/overview/) page For a limited time, we are also opening up these two features at no charge.\n\n* **Deep Search**: Text-based search is highly inaccurate, and has big implications if it's used for NLU data classification, analytics, and automated experiences. Deep Search drastically increases accuracy with acoustic pattern matching. \n* **Diarization:** As more companies move to digital communication channels, more voices need to be identified within recordings. Diarization labels audio transcripts with specific identities to allow for better transcripts.\n\nHappy building and [keep in touch](https://deepgram.com/contact-us/). We'd love to hear how we can keep improving your experience with Deepgram.";
						}
						async function compiledContent$2t() {
							return load$2t().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2t() {
							return (await import('./chunks/index.218b7afa.mjs'));
						}
						function Content$2t(...args) {
							return load$2t().then((m) => m.default(...args));
						}
						Content$2t.isAstroComponentFactory = true;
						function getHeadings$2t() {
							return load$2t().then((m) => m.metadata.headings);
						}
						function getHeaders$2t() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2t().then((m) => m.metadata.headings);
						}

const __vite_glob_0_119 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2t,
  file: file$2t,
  url: url$2t,
  rawContent: rawContent$2t,
  compiledContent: compiledContent$2t,
  default: load$2t,
  Content: Content$2t,
  getHeadings: getHeadings$2t,
  getHeaders: getHeaders$2t
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2s = {"title":"Hello World! We're Deepgram.","description":"Introducing the new Deepgram Developer Platform; the new home of Deepgram's documentation, developer blog, use cases, SDKs, and more.","date":"2021-11-01T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1635464706/blog/2021/11/hello-world/hello-world-blog%402x.jpg","authors":["michael-jolley"],"category":"devlife","tags":["team"],"seo":{"title":"Hello World! We're Deepgram.","description":"Introducing the new Deepgram Developer Platform; the new home of Deepgram's documentation, developer blog, use cases, SDKs, and more."},"shorturls":{"share":"https://dpgr.am/5729572","twitter":"https://dpgr.am/d21d0b3","linkedin":"https://dpgr.am/2f063b5","reddit":"https://dpgr.am/5dd5a34","facebook":"https://dpgr.am/bfd3c79"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453808/blog/hello-world/ograph.png"}};
						const file$2s = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/hello-world/index.md";
						const url$2s = undefined;
						function rawContent$2s() {
							return "\r\nYou love to build and learn, and we love to help you do it. That's why the team\r\nat Deepgram has spent the past few months building a new home for our\r\ndocumentation and API reference. Of course, we couldn't stop there so let me\r\ntake a few minutes to highlight what you'll find on the new Deepgram Developer\r\nPlatform.\r\n\r\n## Documentation\r\n\r\nAs I mentioned, this is where you'll find the Deepgram API documentation moving\r\nforward. But as we moved, we took time to review each page to ensure it was\r\nup-to-date and accurate so you can get started building fast. There's **much**\r\nmore coming in this area. Stay tuned for new content targeting different\r\nlearning methods.\r\n\r\n## SDKs & Tools\r\n\r\nIn the new SDKs & Tools area, you will find our official & community-driven\r\nSDKs. You'll also find integrations with 3rd parties like Twilio & Zoom, as well\r\nas accelerators that make it easy to start getting value from the Deepgram API\r\nwith little to no code.\r\n\r\n## Developer Blog\r\n\r\nWe're stoked about some of the content we have planned for our new\r\ndeveloper blog. From beginner to advanced, our goal is to help all developers\r\nlearn exciting new technologies and level up their skills. In the coming months,\r\nwe'll open the ability for you to contribute to our blog and even be compensated\r\nfor your work.\r\n\r\n## Use Cases\r\n\r\nWhen you're evaluating APIs, it's always easier to make decisions if you can\r\nsee examples that illustrate your needs. That's why we're building use cases\r\nwith code samples you can take and start running immediately, taking you from\r\nzero to hero with minimal effort.\r\n\r\n## Wrap Up\r\n\r\nThere's a ton of features, content, and more that I can't wait to share, but\r\nfor now: Hello world! We're Deepgram and we believe every voice deserves to be\r\nheard and understood.\r\n\r\n        ";
						}
						async function compiledContent$2s() {
							return load$2s().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2s() {
							return (await import('./chunks/index.fb8076b5.mjs'));
						}
						function Content$2s(...args) {
							return load$2s().then((m) => m.default(...args));
						}
						Content$2s.isAstroComponentFactory = true;
						function getHeadings$2s() {
							return load$2s().then((m) => m.metadata.headings);
						}
						function getHeaders$2s() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2s().then((m) => m.metadata.headings);
						}

const __vite_glob_0_120 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2s,
  file: file$2s,
  url: url$2s,
  rawContent: rawContent$2s,
  compiledContent: compiledContent$2s,
  default: load$2s,
  Content: Content$2s,
  getHeadings: getHeadings$2s,
  getHeaders: getHeaders$2s
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2r = {"title":"How AI is Advancing the Transcription Process","description":"Learn more about how artificial intelligence and machine learning are advancing automatic transcription.","date":"2019-02-12T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981344/blog/how-ai-is-advancing-the-transcription-process/how-ai-is-advancing-the-transcription-process-blog.jpg","authors":["morris-gevirtz"],"category":"ai-and-engineering","tags":["machine-learning","deep-learning"],"seo":{"title":"How AI is Advancing the Transcription Process","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981344/blog/how-ai-is-advancing-the-transcription-process/how-ai-is-advancing-the-transcription-process-blog.jpg"},"shorturls":{"share":"https://dpgr.am/349299d","twitter":"https://dpgr.am/1635f67","linkedin":"https://dpgr.am/9836e57","reddit":"https://dpgr.am/feabd6e","facebook":"https://dpgr.am/76375a2"}};
						const file$2r = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-ai-is-advancing-the-transcription-process/index.md";
						const url$2r = undefined;
						function rawContent$2r() {
							return "![](https://images.unsplash.com/photo-1522165078649-823cf4dbaf46?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=2250&q=80)\n\nUntil recently, there's only been one way to accurately produce real-time audio transcription: voice writers use dictation software to convert audio to text and real-time editors clean up the resulting transcript to produce the final output. Yet, it's costly to have voice writing in the process. Now, companies are shifting from this model, employing automatic speech recognition in place of voice writers.\n\n## The Traditional Transcription Pipeline: Voice Writers, Scopists, and Editors\n\nVoice writing was developed in the 1990's to leverage the benefits of first generation speech recognition software.\n\nWhile the software originally served to speed up the transcription process, it poses significant limitations - it can only recognize one speaker and has trouble with ambient noise and intonational variation in speech.\n\nTo compensate for this, highly trained voice writers listen to real-world audio and re-speak its contents in a way that the software understands what is being said. It can take 6 months to train a new voice writer and, even then, voice writing accuracy typically ranges between 70% to 80% depending on the skill of the transcriptionist. Not to mention, a real-time editor-sometimes called a scopist-must counteract the limited accuracy of voice writing immediately afterward. All together, this process worked well for the time, but has caused strain as time has passed, burdening transcription operations with high employee turnover rates and expensive software licenses.\n\n## The Need for a New Approach: ASR as the First Step\n\nFor this reason, companies have started using an [Automatic Speech Recognition (ASR)](https://blog.deepgram.com/what-is-asr/)-first approach to doing highly-accurate real-time transcription. Instead of voice-writing the first level, audio is transcribed with ASR and then passed onto real-time editors.\n\nIn the last few years, the advent of end-to-end deep learning ASR systems has made it possible to reach real-time transcription accuracies of 85% or greater, without the need for human mediation.\n\nThis is largely thanks to the ability to train custom models which are tailor fit for specific audio types, languages, accents, and environments. By ensuring key language-technical words, business and product names, jargon, etc.-is transcribed just as well or better than a good voice writer would, transcription companies have been able to dramatically reduce labor costs while increasing accuracy for editors down the pipeline.\n\n## The Process for Adopting ASR\n\nThe first step in using custom ASR in the transcription process is to select 100 or more hours of labeled (transcribed) audio. The more hours of labeled data, the more accurate the custom model will be. ASR providers use this data to create one or many custom models. Custom models can be created for as many audio types as the customer wants-given that there is enough labeled data for each model. This customization step is key. Whereas some vendors define customization loosely, evaluating the various outputs will tell you which methods are most effective. Most vendors maintain a traditional speech recognition infrastructure that involves four separate steps. In this case, customization is a shallow layer of understanding added to the model only at the end. Think of this shallow customization *as you would a 2 week intensive Mandarin class*. Sure, you'll be able to say and do some things, but you won't be fluent. In other words, your accuracy is subpar. **In contrast, a custom model that trains using end-to-end deep learning, incorporates the learning from labeled data from the very beginning**. In this way, you end up with a custom model that inherently understands your audio, just *as a child who grew up speaking* Mandarin would.\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-how-deepgram-works\"></WhitepaperPromo>\n\n## A Modern Transcription Pipeline: Custom ASR, Scopists, and Editors\n\nUsing a custom real-time speech API, companies get real-time transcription that vastly outperforms previous solutions. As the transcription industry continues to grow, those that are able to consistently compete on accuracy, scale, and cost will thrive. Clearly, incorporating a high performing ASR solution is necessary in order to achieve the highest levels of accuracy, while also reducing operational costs and delivering on turnaround times. Those that fail to transition quickly risk falling behind their competitors that recognize this as the secret weapon to a quickly evolving industry.\n\n## The Takeaways\n\n1. Select a modern speech recognition company that can tailor build custom ASR models for your specific needs.\n2. Provide the ASR vendor with audio you have already transcribed in the past.\n3. The ASR provider trains models that natively understand your audio.\n4. Deliver high accuracy transcripts to your editors in real-time, while lowering costs.";
						}
						async function compiledContent$2r() {
							return load$2r().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2r() {
							return (await import('./chunks/index.7328ad8f.mjs'));
						}
						function Content$2r(...args) {
							return load$2r().then((m) => m.default(...args));
						}
						Content$2r.isAstroComponentFactory = true;
						function getHeadings$2r() {
							return load$2r().then((m) => m.metadata.headings);
						}
						function getHeaders$2r() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2r().then((m) => m.metadata.headings);
						}

const __vite_glob_0_121 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2r,
  file: file$2r,
  url: url$2r,
  rawContent: rawContent$2r,
  compiledContent: compiledContent$2r,
  default: load$2r,
  Content: Content$2r,
  getHeadings: getHeadings$2r,
  getHeaders: getHeaders$2r
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2q = {"title":"How Does Microsoft’s Purchase of Nuance Communications Affect the Market?","description":"Microsoft's offer to purchase Nuance Communications for $19.7B validates Automatic Speech Recognition (ASR) has become an essential technology for business. ","date":"2021-04-15T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981368/blog/how-does-microsofts-purchase-of-nuance-communications-affect-the-market/how-does-msft-purchase-nuance-affect-market%402x.jpg","authors":["scott-stephenson"],"category":"speech-trends","tags":["voice-strategy","voice-tech"],"seo":{"title":"How Does Microsoft’s Purchase of Nuance Communications Affect the Market?","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981368/blog/how-does-microsofts-purchase-of-nuance-communications-affect-the-market/how-does-msft-purchase-nuance-affect-market%402x.jpg"},"shorturls":{"share":"https://dpgr.am/9644796","twitter":"https://dpgr.am/363d87a","linkedin":"https://dpgr.am/3722112","reddit":"https://dpgr.am/c507d18","facebook":"https://dpgr.am/a4d15aa"}};
						const file$2q = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-does-microsofts-purchase-of-nuance-communications-affect-the-market/index.md";
						const url$2q = undefined;
						function rawContent$2q() {
							return "\r\nMicrosoft's offer to purchase Nuance Communications for $19.7B validates Automatic Speech Recognition (ASR) has become an essential technology for business. It's exciting to see an AI powerhouse like Microsoft, which has amazing talent and tools in house, finding extreme value in acquiring speech solutions at this scale. Overall, this acquisition is a great sign for AI companies like Deepgram and the Speech Recognition industry. \r\n\r\n## **Why Buy Nuance? Hello Healthcare & Upsell potential.**\r\n\r\nMicrosoft has provided their own Speech To Text (STT) solution for years, however due to [difficulty of using the service](https://www.techradar.com/reviews/microsoft-azure-speech-to-text-review), lack of scale, and preference for working well with its own hardware and software solutions (Cortana, Bing and the Teams communication app), it has not gained substantial traction beyond their ecosystem. Nuance on the contrary is the leader in the medical transcription market which was estimated to be 1.32B in 2019 and [expected to grow to 4.89B by 2027](https://www.fortunebusinessinsights.com/industry-reports/medical-transcription-software-market-101572).  With the acquisition of Nuance, Microsoft immediately expands their STT reach into the healthcare community, and also opens up the possibility of expanding their Azure cloud storage revenues, as stored audio files are not small.\r\n\r\n## **What product leaders of voice enabled applications need to know about the Microsoft acquisition of Nuance**\r\n\r\nWhile this acquisition will benefit Microsoft, the core architecture of STT does not have a high likelihood of improving. Microsoft and Nuance STT are both based on the same [legacy tri-gram model](https://deepgram.com/product/overview/) so neither architecture will dramatically change, but perhaps have incremental improvements. There will also be challenges for these two companies to integrate the artifacts for two speech processing pipelines as they have different libraries, acoustic models, language lexicons, style guides, etc. As a comparison, Deepgram built our speech recognition solution from scratch using a completely different architecture. Deepgram uses an end to end Deep Learning Neural Network, which in simple terms means we perform audio to text transcription in one AI-enabled step and we can continually improve our accuracy. Due to our architectural differences, Deepgram customers do not have to compromise accuracy vs. speed, speed vs. costs or cost vs. scalability.\r\n\r\n## **Impact to the Broader ASR Market**\r\n\r\nSo what does this do to the broader speech market? It elevates the conversation around Speech Recognition to higher levels within the organization. Businesses will be looking to see why ASR is a growth strategy for Microsoft and consider it as an important technology strategy to gather customer insights, improve employee engagement and accelerate their growth.  This acquisition also shows that ASR and voice technology growth is not only for consumer needs (Siri, Alexa, Cortana) but an important aspect of business needs. The recent report from Opus Research [2021 State of Speech Report](https://deepgram.com/state-of-asr-report/) validates the strategic importance of ASR and voice technology for businesses. Microsoft's acquisition of Nuance is just the tip of the iceberg, as speech recognition is more pervasive and extends beyond the patient experience to the customer and employee experience. We always believed that ASR is going to change the world and that every company will need speech recognition to get closer to their customers, find new insights for products and services, and better personalize their customer experiences. The best is yet to come.\r\n";
						}
						async function compiledContent$2q() {
							return load$2q().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2q() {
							return (await import('./chunks/index.fd821c2c.mjs'));
						}
						function Content$2q(...args) {
							return load$2q().then((m) => m.default(...args));
						}
						Content$2q.isAstroComponentFactory = true;
						function getHeadings$2q() {
							return load$2q().then((m) => m.metadata.headings);
						}
						function getHeaders$2q() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2q().then((m) => m.metadata.headings);
						}

const __vite_glob_0_122 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2q,
  file: file$2q,
  url: url$2q,
  rawContent: rawContent$2q,
  compiledContent: compiledContent$2q,
  default: load$2q,
  Content: Content$2q,
  getHeadings: getHeadings$2q,
  getHeaders: getHeaders$2q
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2p = {"title":"How Does Santa Do It? — AI Show","description":"How does Santa make it around the world in one night to deliver presents? We've cracked his secrets.","date":"2018-12-20T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981339/blog/how-does-santa-do-it-ai-show/how-does-santa-do-it%402x.jpg","authors":["morris-gevirtz"],"category":"ai-and-engineering","tags":["deep-learning","machine-learning"],"seo":{"title":"How Does Santa Do It? — AI Show","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981339/blog/how-does-santa-do-it-ai-show/how-does-santa-do-it%402x.jpg"},"shorturls":{"share":"https://dpgr.am/182c55e","twitter":"https://dpgr.am/3642cab","linkedin":"https://dpgr.am/23a76c1","reddit":"https://dpgr.am/32fc3a4","facebook":"https://dpgr.am/6940d6f"}};
						const file$2p = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-does-santa-do-it-ai-show/index.md";
						const url$2p = undefined;
						function rawContent$2p() {
							return "<iframe src=\"https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/555973752&color=%23ff5500&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&show_teaser=true\" width=\"100%\" height=\"166\" frameborder=\"no\" scrolling=\"no\"></iframe>\n\n**Scott:** Welcome to the AI show. Today we're asking the question. The big question, the really big question, the secret deep down thing that we think we've uncovered: What is Santa's secret.\n\n**Susan:** Yeah how? How does that big jolly red guy do it?\n\n**Scott:** How does he do it? We think we figured it out though.\n\n**Susan:** I mean it's pretty obvious what he's doing.\n\n**Scott:** Yeah, it's really obvious.\n\n**Susan:** When you lay out the facts, like he's got a horde of tasks he's gotta do. The orders of magnitudes of tasks that he's gotta do. Like it's not normal to be able to do this kind of thing. Just one man, he has elves, in the North Pole, cold place. And he has all these millions, billions of people to visit around the world.\n\n## What assets does he have to do this?\n\n**Susan:** The first asset that gave us the real clue was he's been collecting a lot of data. For years. So he knows, he knows when you've been naughty, he knows when you've been nice.\n\n**Scott:** He's making a list, checking it twice. You're sending him letters, to the north pole. You're calling in.\n\n**Susan:** So what does this mean?\n\n**Scott:** You're meeting him at the mall.\n\n**Susan:** What is Santa's secret Scott?\n\n**Scott:** The way he can do it is he obviously has uncovered the ways of AI way before anybody else.\n\n**Susan:** Way before everybody else.\n\n**Scott:** And he hasn't told anybody. Kind of a not so good guy move though, Santa. Why do you think he's at the north pole?\n\n**Susan:** I don't know maybe because he had a heat problem to solve.\n\n**Scott:** He has lots of data centers. You know he's got a problem, it needs to be cold. You have to process his data, you have to train your models.\n\n**Susan:** Just imagine what the energy costs would be if you did it somewhere in California. It would be ridiculous just the heating and cooling.\n\n**Scott:** Especially hundreds of years ago.\n\n**Susan:** He needs that nice big cold sink for all those GPUs he's got running and all that. How much data does this guy have?\n\n**Scott:** It grows over the years too.\n\n**Susan:** Hundreds of years of detailed information, enough information that you can make a clear determination of naughty or nice in a person.\n\n**Scott:** This is where we get into it. He's got data centers, he's got data, he's got all this. He's got his feelers out there. You know his mall Santa with his probably IOT device ridden suit that he's wearing, collecting data. But he has all this data from the past hundreds of years. And people saying what they want, what they've been doing. You know whether they've been awake or not. He has to make predictions based on that but, he's got all these letters coming in. How do you think he reads all those letter?\n\n**Susan:** Well let's just say Santa's secret is machine learning. That's Santa's secret. It's clear.\n\n**Scott:** That's it. it's obvious.\n\n**Susan:** It is so obvious.\n\n**Scott:** He's using [OCR](https://en.wikipedia.org/wiki/Optical_character_recognition), to read letters. That people are sending in to him.\n\n**Susan:** He's doing real, deep language processing on them too. I mean, so you've got OCR, perfect recognition and now you've got the text. You're not gonna read all that. You gotta have something, you gotta get to something that says, \"Oh, this is this person. These are their gift ideas. This is their special circumstances as mentioned in the letter.\" And you've gotta do some pretty serious processing to rip all that out in a usable way.\n\n**Scott:** That's just the first step. Yeah come on, you're gathering data about everybody. You're probably serving ads, like he's probably selling data to all the tech companies so they'll help him sell ads. That's how he's funding this operation.\n\n**Susan:** Yeah Santa could be the original startup. Seriously.\n\n**Scott:** That's deep. He was the first start up.\n\n**Susan:** The original.\n\n**Scott:** I hear he was born in Turkey.\n\n**Susan:** You know what there's a lot of entrepreneurship there. You know if you go to Istanbul, you will definitely see entrepreneurs at work.\n\n![st_nick](https://res.cloudinary.com/deepgram/image/upload/v1661976798/blog/how-does-santa-do-it-ai-show/Turkey-myra-painting..jpg) \n\n_Saint Nicholas of Myra born in the 4th-century in Lycia, now southern Turkey. St. Nick was known for generously giving gifts to the poor._\n\n**Scott:** So how did he get from Turkey to the North pole? It's just he needed a cold spot to put all the servers.\n\n**Susan:** Well yeah I mean you know, when you start thinking about this logically. The only way you can do this is lots of servers and lots of servers demand that.\n\n**Scott:** This is probably why, hey no fault on Santa here. I think there's some fault on him here, you can put some blame on him. He's probably the one melting the polar ice caps.\n\n**Susan:** Oh man, you think?\n\n**Scott:** Yeah.\n\n**Susan:** Oh so Santa could be the origin of global meltdowns going on there.\n\n**Scott:** People say commercialism is ruining Christmas. I think Christmas is ruining the environment.\n\n**Susan:** It could be, at the ice and the north pole, the polar ice cap there it's the lowest, thinnest and youngest in 60 years. According to various headlines. I didn't read the rest of the article.\n\n**Scott:** So there is the man side that he has to do and supply side he has to make these meet. How's he figure out how many different types of toys he needs for everyone?\n\n**Susan:** Well let's break down Santa's big problem here. First of all he's clearly got a really heavy duty classification algorithm for naughty or nice. I bet you the same technology that goes into naughty or nice also does present prediction.\n\n**Scott:** So when you know all these qualities about a person, what are you gonna give them.\n\n**Susan:** You run this on every child in the world and now you can clearly start figuring out demand and what your cost overlays are gonna be for that.\n\n**Scott:** This is the reason you don't always get what you want for Christmas.\n\n**Susan:** It’s true.\n\n**Scott:** Machine learning models aren't perfect. It's averaging over all the past history about what to give you.\n\n**Susan:** Honestly how many times have you gotten what you asked for, but it wasn't what you wanted.\n\n**Scott:** He knows something deeper about you than you know about yourself.\n\n**Susan:** You really did want that pair of boxer shorts for Christmas.\n\n**Scott:** Yeah he needed it. Six months later he knew.\n\n**Susan:** Yeah well which would you, you could have that small action figure that you're gonna lose in 20 seconds. Or those boxer shorts that you're gonna have six months later.\n\n**Scott:** Aggregate it over the year.\n\n**Susan:** Yeah, you are a lot happier. So Santa at least his deep learning jobs might be better than we think. I mean he's been doing it for a long time obviously.\n\n**Scott:** Yeah, we need to talk to him.\n\n**Susan:** What are Santa's assets? A tremendous data store.\n\n**Scott:** I don't know where he got the computing power though.\n\n**Susan:** Well, some things are gonna be a mystery.\n\n**Scott:** Christmas spirit?\n\n**Susan:** Possibly.\n\n**Scott:** Belief?\n\n**Susan:** Maybe that's what it is.\n\n**Scott:** Oh this might be it. It's belief in Santa before is what powered it. But now it's sort of waning and now he has to supplement it with reindeer dung, and this heats up his centers.\n\n**Susan:** It sounds like a plot to a bad hallmark movie.\n\n**Scott:** May or may not have seen it already.\n\n**Susan:** Santa's got some really great things going for him although I would say.\n\n**Scott:** Big problems though.\n\n**Susan:** There's a lot of issues that Santa's has to overcome. Think of all that data and all these different countries.\n\n## Is Santa GDPR compliant?\n\n**Scott:** Oh that's a big question.\n\n**Susan:** I mean he's got really detailed personal information.\n\n**Scott:** He needed to get his stuff in order this year. This is the first GDPR'd Christmas.\n\n**Scott:** Stuff might be coming late.\n\n**Susan:** Maybe he had to use less information.\n\n**Scott:** Maybe the predictions aren't as good now.\n\n**Susan:** Because he had to clean out his data stores, they'll lose those terabytes of data on people. Sorry UK.\n\n**Scott:** He's moving to Ireland or moving out of Ireland now.\n\n**Susan:** Maybe he's got some good legal loopholes.\n\n**Scott:** Yeah well, that's kinda what Ireland's about.\n\n**Susan:** Right he's moved his data. Yeah in ways that might be questionable but...\n\n**Scott:** I believe that, yeah. it's not coming through standard means. Well he has a really high powered transport you know. It's like a ultimate smuggling device. It's like you could send terabytes, petabytes of information through the internet. But you could also just load up the sleigh with hard drives and move it over and undetectable.\n\n![harddrive](https://res.cloudinary.com/deepgram/image/upload/v1661976799/blog/how-does-santa-do-it-ai-show/Western_Digital_15TB_HDD.0.webp)\n\n_If Santa's sleigh is about the size of a Cadillac Escalade, then it can hold roughly 6800 15TB hard drives, or roughly 105 petabytes (assuming that he is at least using the very biggest hard drives available in December 2018)._\n\n**Susan:** He's clearly got amazing technology. But his business model, it's an interesting business model. Maybe he's been looking at the long term.\n\n**Scott:** Is it a long con?\n\n**Susan:** [He knew that data was the new oil.](https://www.economist.com/leaders/2017/05/06/the-worlds-most-valuable-resource-is-no-longer-oil-but-data) Or was going to be the new oil. So he's been collecting it for a very long time.\n\n**Scott:** He's gonna be a banker.\n\n**Susan:** And now it's time to payout. Right, all those gifts. Do you think the elves really made them or maybe their various toy companies saying push this product this year. Like he's seating the right kids to get their friends to go out and buy.\n\n**Scott:** You bring up an interesting question about the lifecycle of data. He's been collecting for hundreds of years as well. A lot of the people he's collected data on aren't here any more.\n\n## What does he do with old data?\n\n**Susan:** Exactly. How useful is it to know that in 1860 the hula hoop was the hot toy.\n\n**Scott:** But it still might be.\n\n**Susan:** You can get trend analysis, you can see how a toy takes off and it falls over time.\n\n**Scott:** Yeah, but it's now totally fair use for him. That's not covered by GDPR.\n\n**Susan:** Oh yeah, it's old enough, old enough information right? So that's good.\n\n**Scott:** Maybe he'll be releasing datasets on Kaggle or something?\n\n**Susan:** Oh that'd be awesome the naughty, nice prediction contest?\n\n**Scott:** Yeah maybe we could talk to him and get that going?\n\n**Susan:** Here are 30 attributes about a child.\n\n**Scott:** That's a good question. What is the real data he has to make these predictions?\n\n**Susan:** What's useful to determine naughty or nice? But more to the point, how did he train the classifiers in the first place, how does he calibrate naughty and nice. Is this a [reinforcement learning](https://blog.deepgram.com/ai-show-different-types-of-machine-learning/) question where over time...\n\n**Scott:** Is Santa biased? Big deep one there.\n\n**Susan:** How naughty you are depends on where you were born?\n\n**Scott:** He knows your names, the names on the list. Glean a few things just from that. But he's had the IOT device ridden mall Santa, his robots, his minions are out there.\n\n![mallsanta](https://res.cloudinary.com/deepgram/image/upload/v1661976800/blog/how-does-santa-do-it-ai-show/mall_santa-collage.jpg)\n\n**Susan:** Probably, he doesn't own the various Santa's out there. That'd be just, I mean. Conspiracies of more than one person, just fall apart. Right? But maybe he's got the clothing line down.\n\n**Scott:** He's got the elves to make them, he's got everything right? He has to have an amazing manufacturing facility out there too. Using deep learning machine vision, etc., it's really pumped that good stuff out.\n\n**Susan:** A lotta robotic stuff. Like, are elves really people, or are they alive, or are they something else?\n\n**Scott:** Yeah that's a good point. Maybe you replaced the elves a long time ago.\n\n**Susan:** I mean come on, you gotta a lot of robots going around up there.\n\n**Scott:** Yeah maybe he's responsible on the uplift of the Japanese robotics market. He's been buying.\n\n**Susan:** Could be, machine learning and robotics are like a match made in heaven, right there you know. A lotta work in that world.\n\n**Scott:** Well he's obviously an expert so.\n\n**Susan:** When I really think about what Santa must have accomplished, I'm quite frankly in awe. The orders of scale, the precision of what he's doing. The resources that he gathers and employs. Santa is, from all practical purposes, he's like the biggest purveyor of machine learning on the planet.\n\n**Scott:** Yeah he has to be. He touches everybody.\n\n**Susan:** Everybody. It's just absolutely insane.\n\n**Scott:** Everybody knows about him at least, right? At least half the people.\n\n## Why hasn't anyone else come to these same conclusions?\n\n**Scott:** I guess that's one of those things. It's a zeitgeist right? It's just a time for it to come out? Well obviously we didn't know about machine learning before or the extent to which it could be spread throughout the world on all these different tasks. You look back and you're like \"Duh\".\n\n**Susan:** That's it, we're now finally seeing. We're getting little crumbs of the capability that Santa has in foreseeing.\n\n**Scott:** Oh he's so far advanced.\n\n**Susan:** We're catching up to what he had a 100 years ago.\n\n**Scott:** In machine learning you obviously have some physical things going on for him here too, that to be able to just travel to all these different places in one night. He's got 24 hours to do it, he can kind of rake across the world while it's happening but that's not very much time.\n\n**Susan:** I mean if he has the robotic infrastructure, then maybe what he does is actually caches small Santa's everywhere.\n\n**Scott:** Ooh that's a good, that's a smart move.\n\n**Susan:** And if there are autonomous Santa delivery devices...\n\n**Scott:** He's the original self driving car.\n\n**Susan:** Before self driving sleighs, he's got thousands of them to go around and deliver packages.\n\n**Scott:** Maybe. Something we haven't talked about much, but model duplication, meaning, he's just cloned his brain.\n\n**Susan:** Well they probably have it specialized to the country though.\n\n**Scott:** That's a good point. Houses are different.\n\n**Susan:** So he's got a generic model and then he's transferred to start the new models and then specialize them on special data stats for each country. Even each region.\n\n![transfer](https://res.cloudinary.com/deepgram/image/upload/v1661976800/blog/how-does-santa-do-it-ai-show/transfer-learning.png)\n\n_[A visual representatuon of transfer learning workflow. Source](https://medium.com/@subodh.malgonde/transfer-learning-using-tensorflow-52a4f6bcde3e)_\n\n**Scott:** Kind of one shot learning. I mean it's just happening while on the fly. You drop them off and then go.\n\n**Susan:** You think it's the same delivering presence to say Central Florida, as it is to say, Ecuador or something like that.\n\n**Scott:** Oh well, or somewhere there's a lot of snow and other stuff, right? But maybe a good move for him would be to disguise himself as the UPS man. Or like the Amazon truck. People would never suspect that.\n\n**Susan:** I think we've uncovered something exceptionally deep here, Scott.\n\n**Scott:** What are we gonna do about it?\n\n**Susan:** I don't know.\n\n**Scott:** I don't think we should tell anybody.\n\n**Susan:** No we need to get this technology for ourselves.\n\n**Scott:** Yeah, let's keep this to ourselves.\n\n**Susan:** Well this could be our little side research project we keep on investigating. How does Santa do it. This is in the rest of the world when real problems pop up. Once you get that key question you can keep asking yourself. I know it happens in nature this way so there's gotta be a way, and it allows you to keep driving towards the solution.\n\n**Scott:** Yeah, so you're saying there's a way. A chance.\n\n**Susan:** I'm gonna say it. And you keep chipping away knowing there is an acceptable solution over here. Santa's done it. Therefore, we can so we keep saying, \"How would Santa do it?\"\n\n**Scott:** What would Santa do. WWSD.\n\n**Susan:** What would Santa do? Would he just throw another GPU on the fire?\n\n**Scott:** Is he a little more clever than that?\n\n**Susan:** Either he's just a big data GPU guy or he is really nuanced. Does he hand tweak the model all day?\n\n**Scott:** Maybe path finding is AI after all. I mean definitely, definitely. Real deep learning stuff happening.\n\n**Susan:** Well on a serious note, graft theory is getting a little bit of and in-run on deep learning.\n\n**Scott:** I was serious the whole time.\n\n**Susan:** We were yeah, of course. I mean talking about path finding though. There's some interesting fun stuff in that world. Especially when you get to graft theory to the huge data world. Well just, what is a tree?\n\n**Scott:** What is a tree?\n\n**Susan:** Its a DAG? \n\n![DAG](https://res.cloudinary.com/deepgram/image/upload/v1661976801/blog/how-does-santa-do-it-ai-show/Topological_Ordering.jpg)\n\n_In mathematics and computer science, a directed acyclic graph is a finite directed graph with no directed cycles. Source: [Wikipedia](https://en.wikipedia.org/wiki/Directed_acyclic_graph)_\n\n**Scott:** What is if this is a tree?\n\n**Susan:** It's a directed acyclic graph, right.\n\n**Scott:** Directed acyclic. It's not cyclical.\n\n**Susan:** Exactly and it's graph.\n\n**Scott:** And directed. Because of the direction.\n\n**Susan:** So searching a big tree of possibilities is exactly what things like AlphaGo does it really well at. Do you think Santa could be at AlphaGo?\n\n**Scott:** And if he did what would he do?\n\n**Susan:** Well I think he could because clearly he's got some edges on graph theory in general to be able to solve the traveling salesman problem, being around the world in such efficient way. He had to have a very efficient tact on this. I mean if Santa was inefficient in his travels around the globe. Then it's clearly an impossible task. But if he's got a super efficient path algorithm. If he's knocked it down somehow. So maybe he's solved PNP. This is big time.\n\n**Scott:** Whoa, yeah that's big time.\n\n**Susan:** I don't wanna go into the ridiculous though. I think most people agree that P does not equal NP. And if Santa were to solve that, that'd be like, I mean it's breaking the law.\n\n**Scott:** He's solved AI, we know that.\n\n**Susan:** But maybe what he's got is some sort of mixture of classical algorithms and machine learning. To be able to do intelligent grouping of solutions. To get this path finding but still you're using classical mechanisms to tie it all together. I think with that you could get an optimum. Not necessarily the global optimum or a provable optimum. But you could probably could find a much better optimum than just you know.\n\n**Scott:** Okay, okay. So you're saying you can come to a solution. It might not be the perfect solution. But you did it very quickly.\n\n**Susan:** Well you did it with, you know just like Monte Carlo.\n\n**Scott:** Very little effort.\n\n**Susan:** Monte Carlo tree search and a DAG.\n\n**Scott:** Santa's probably using Monte Carlo so you could do tree search in a better version.\n\n![monte](https://res.cloudinary.com/deepgram/image/upload/v1661976802/blog/how-does-santa-do-it-ai-show/808px-MCTS_-English-_-_Updated_2017-11-19.svg.png)\n\n_Monte Carlo tree search is a search algorithm used for ceertain kinds of decision processes. [Source.](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search)_\n\n**Susan:** You have a fixed set of resources. You've got this much time to calculate on. You want to find the best solution within those resources, right. You use a similar set of techniques and you use it for solving this traveling salesman and maybe you've got some good stuff there. You put a little bit of machine learning to do an intelligent selection of possible paths. I don't know, I think you got something possibly there. See since Santa can do it, it allows us to start thinking how does Santa do it. How can you apply these things.\n\n**Scott:** Yeah, exactly. We know it's possible. We know it's possible.\n\n**Susan:** Right, you know. There's some really good stuff that we can do there.\n\n**Scott:** So it's whether he shards or not determines, if he really does it all himself now he's got something special. If he's sharding, and breaking himself off into many pieces to go solve this problem not quite the same. Maybe Santa is an AI.\n\n**Scott:** Maybe he is.\n\n**Susan:** How can clone himself so quickly. You know you can be everywhere at once. How is that possible.\n\n**Scott:** He probably has like a silicon boundary.\n\n**Susan:** Here it is Scott. Santa is an alien machine intelligence that landed on earth. Gotta be.\n\n**Scott:** I think that's a 100% true.\n\n**Susan:** It's gotta be. So all this machine learning is really from an advanced civilization. What we're saying is you know a couple of 100 years from now.\n\n**Scott:** Why are they doing it though?\n\n**Susan:** I don't know.\n\n**Scott:** To keep us happy? To keep us placated.\n\n**Susan:** If you're a machine intelligence, would you wanna stay on the planet that you're creator started you off on. Why stay next to all that caustic environment, all these bad things? Why not just go out into space. And then you're gonna have lots and lots of replicas going around of your machine intelligence.\n\n**Scott:** And then if you visit new planets.\n\n**Susan:** Well you're just going out into the random universe. You go to new stars, you need energy sources, right?\n\n**Scott:** Takes a long time to do that. But yeah, I guess.\n\n**Susan:** You just slow yourself down to a few cycles per year or whatever time.\n\n**Scott:** I guess to you it doesn't seem that fast. Or is that long.\n\n**Susan:** So he's machine intelligence, he's been floating through the universe for awhile and accidentally crash lands on earth and says why not, I'll build a bunch of machines.\n\n**Scott:** It's like a vacation a little. Like you're gonna stop over. Little playland, little fun.\n\n**Susan:** Why not have a little bit of fun. I think that this is a plausible explanation for Santa Claus.\n\n**Scott:** Yeah we're getting there. Well in the UFO technology there too. You know he can zip around.\n\n**Susan:** All the energy that Santa's using.\n\n**Scott:** Yeah he solved fusion.\n\n**Susan:** He has to. How much energy is in gasoline? Do you think a little sleigh could carry enough energy around to take it around the earth and go to all the homes?\n\n**Scott:** Yeah it has to be nuclear.\n\n**Susan:** It has to be, I mean you have to be converting matter to energy. To be able to do what Santa does.\n\n**Scott:** Yeah we're not talking a couple of orders of magnitude here. It's like millions of orders of magnitude. That's the good stuff right there.\n\n**Susan:** Pretty serious, serious levels of technology that are being uncovered for Santa.\n\n**Scott:** So he's an energy genius, materials genius and an AI genius. Probably though not a compute, comp-sci genius. I've heard.\n\n**Susan:** Really, you don't think so?\n\n**Scott:** I don't know. Or you just mean P, just isn't NP.\n\n**Susan:** Oh it clearly is not.\n\n**Scott:** Well then this is the proof for that.\n\n**Susan:** I mean come on.\n\n**Scott:** Hey if Santa could do it, he would have. Right? It's an obvious proof.\n\n**Susan:** If it's possible I'd say Santa could do it.\n\n**Scott:** It must not be possible.\n\n**Susan:** Well he does get around the earth.\n\n**Scott:** How could the all powerful being...?\n\n**Susan:** He can't break the laws there. I mean maybe he's got quantum computers up there that are able to solve the traveling salesman problem.\n\n**Scott:** Of course, I didn't think of that.\n\n**Susan:** In a non classical way. That could be it.\n\n**Scott:** I didn't think of that. That's probably it. He's a quantum computing guy. He's talking about cubits all day.\n\n**Susan:** Another reason why you need a cold environment. He's gotta cool down those, you know...\n\n![cool](https://res.cloudinary.com/deepgram/image/upload/v1661976803/blog/how-does-santa-do-it-ai-show/santa-s-data-center.jpg)\n\n_Santa's data centers may be the cause for melting ice caps_\n\n**Scott:** Obviously he cuts down on the liquid helium costs. This is so smart. It's closer to absolute zero up there.\n\n**Susan:** This guy has some pretty serious technology.\n\n**Scott:** Wow, this is deep.\n\n**Susan:** Do you think he's gotten into other fictional territory too, like Easter bunny and stuff.\n\n**Scott:** Who says they're fictitious first of all?\n\n**Susan:** You think that's true.\n\n**Scott:** But yeah, there has to be a convention or something.\n\n**Susan:** There's gotta be. The tooth fairy's there stealing stuff at night. By the way, what's the going rate for a tooth?\n\n**Scott:** Personally, a dollar.\n\n**Susan:** I like a dollar too but my wife is like, it keeps going up each tooth. At this rate we'll be broke.\n\n**Scott:** Oh we gotta do five, ten.\n\n**Susan:** At this rate the tooth fairy will be broke.\n\n**Scott:** Have you ever missed a tooth?\n\n**Susan:** No, not yet, not yet.\n\n**Scott:** Oh that one's bad. You have to be like maybe you didn't put it in the right spot? Yeah that one's bad.\n\n**Susan:** I keep close track of the weekly teeth.\n\n**Scott:** Maybe your son announces very boldly and very happily that it's out. Not so much with my kids.\n\n**Susan:** It's tricky.\n\n**Scott:** If I know the tooth is about to come out, it's like let's do it you know. Let's do it. Let the blood run. Lets go, right? And they're not into that. They're gonna hold on till the bitter end.\n\n**Susan:** Maybe they're also, they could be like a strategic tooth reserve so when they need money, they pull out the teeth. It's like oh look, I just lost this tooth.\n\n**Scott:** I really wanna buy this thing. All I gotta do is wait a night. So where is the tooth fairy putting all the teeth.\n\n**Susan:** That is a very creepy question.\n\n**Scott:** There's a vault somewhere with everybody's teeth in them. Maybe they do it for the gold. I mean back in the day. This is why we have to supplement it now with a little parental help.\n\n**Susan:** Especially back in the day they wouldn't fill a kid's tooth.\n\n**Scott:** I guess it's true. It hurts? Yeah, that's life.\n\n**Susan:** So you got a cavity in your tooth, it's coming out anyways.\n\n**Scott:** Let's pull it out. You know your teeth when you're coming out. They consume the calcium from the root of the other tooth. I don't know exactly how much. This is why when a tooth comes out, it's only half a tooth. There's no root left anyhow.\n\n**Susan:** I did know it dissolved it, I didn't know it went into it.\n\n**Scott:** I am not sure, I don't know about that one. This is a little speculation but where else would the calcium go, I don't know.\n\n**Susan:** I mean I can speculate about dentistry all day long.\n\n**Scott:** So he's hanging out with the tooth fairy and the Easter bunny. The Easter bunny has to be a riot you know.\n\n![bunny](https://res.cloudinary.com/deepgram/image/upload/v1661976804/blog/how-does-santa-do-it-ai-show/Puck-Easter-_-L.M.-Glackens..jpg)\n\n_The easter bunny is first mentioned in a German work dating to the 17th century, but does not become popular until the late 19th century. Almost certainly the Pennsylvania Dutch introduced the tradition to America. Source: [Library of Congress](https://blogs.loc.gov/folklife/2016/03/easter-bunny/)_\n\n**Susan:** \"Let me tell you a story guys. I produce chocolate eggs, from.... I hide them and kids love to find them and eat them!\"\n\n**Scott:** This is real, this happens every year.\n\n**Susan:** Why is it these entities are so creepy around kids.\n\n**Scott:** Most of it has to do with kids doesn't it?\n\n**Susan:** Both the tooth fairy and Santa Claus are sneaking into children's bedrooms in houses. And either taking or putting things.\n\n**Scott:** I know why it's a machine learning site again. You have to stoke the rebels. You can't be stuck in a rut. The adults are stuck in a rut.\n\n**Susan:** You go to where the inspiration's at, where your gradient is steepest.\n\n**Scott:** Many crazy ideas from children.\n\n**Susan:** The steep gradients there. Maybe that's what it is.\n\n**Scott:** They're all over the place.\n\n**Scott:** They're feeding the gradient.\n\n**Susan:** The alien machine intelligence is really going out and again finding the steepest gradients in society. And trying to explore those steep gradients.\n\n**Scott:** Pumping energy and motivation into them. Keep them going 'cause it's the only hope.\n\n**Susan:** It's like you're old and stodgy, you're gradient is like this\n\n**Scott:** Yours is flat man, you're not going anywhere. You're as good as you're gonna be. I think you're going up actually. This one is the future. Children are the future here. You guys are a lost cause.\n\n**Susan:** Adults are just boring. From a machine learning standpoint. You could probably predict me and you a whole lot easier than some kid. What we're about to say next, 90% accuracy. Kid's but... I mean I can't predict my own son at all.\n\n**Scott:** I watched one of the best trigrams ever be produced in life. Live.\n\n**Susan:** What was it, can you say it on air?\n\n**Scott:** Uh two kids fighting, not really fighting you know. Make believe here. \"I have this guy, and I have this guy. And he has a super power. And this one has a super power, okay. Well guess what this guy's super power is, Gatorade fart tornado.\" Who puts those words together?\n\n![gatorade](https://res.cloudinary.com/deepgram/image/upload/v1661976804/blog/how-does-santa-do-it-ai-show/Screen-Shot-2019-01-08-at-3.26.47-PM.png)\n\n_Using the Google Ngram viewer we can see that these three words have never been used in writing. That or only in books that Google has not scanned and run OCR on._\n\n**Susan:** Put that in your language model.\n\n**Scott:** Gatorade fart tornado in your language model. Go. You need it. You need the craziness. That's what these guys are up to.\n\n**Susan:** What's the probability on that trigram? Gatorade fart tornado, I like it. Although just knowing it's a child. The prior probability on the word fart was very high, very high. I mean.\n\n**Scott:** Gatorade was a recently learned word and is amazing. You know. A tornado is awesome in its own right.\n\n**Susan:** There is a little bit of believability when you really put that in perspective.\n\n**Scott:** You need to keep this quiet.\n\n**Susan:** Now we know.\n\n**Scott:** Now we need to figure out how we're going to utilize this intelligence.\n\n**Susan:** That is the million dollar, the billion dollar question.\n\n## What's his next move?\n\n**Scott:** Good point. People think Santa's static. What does he really have up his sleeve?\n\n**Susan:** What's his long game here. Where is he going with this?\n\n**Scott:** Children are the future.\n\n**Susan:** He's got all this data about humanity.\n\n**Scott:** Tons of data, he's building up the infrastructure.\n\n**Susan:** He's built the infrastructure.\n\n**Scott:** That's true. It must be getting better. More people right. It's just a magic number and he just reveals to the world his secrets.\n\n**Susan:** Is this an in run around Amazon, he wants to take over all e-commerce?\n\n**Scott:** Yeah good point. Amazon might have to worry.\n\n**Susan:** Maybe Santazon is coming.\n\n**Scott:** Santazon. Yeah Santazon.com. And it'll be branded too, you know they'll be happy. They'll be jolly. 24 hour shipping?\n\n**Susan:** And it's not just in 15 minutes when it's with a happy elf delivering to you.\n\n**Scott:** And it's not just in December anymore.\n\n**Susan:** Oh that's the slogan, it's not just December anymore. Three sixty five.\n\n**Scott:** Well I think we figured it out. It's just some great sleuthing work on our part. Little self congratulations here for sure.\n";
						}
						async function compiledContent$2p() {
							return load$2p().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2p() {
							return (await import('./chunks/index.28307eeb.mjs'));
						}
						function Content$2p(...args) {
							return load$2p().then((m) => m.default(...args));
						}
						Content$2p.isAstroComponentFactory = true;
						function getHeadings$2p() {
							return load$2p().then((m) => m.metadata.headings);
						}
						function getHeaders$2p() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2p().then((m) => m.metadata.headings);
						}

const __vite_glob_0_123 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2p,
  file: file$2p,
  url: url$2p,
  rawContent: rawContent$2p,
  compiledContent: compiledContent$2p,
  default: load$2p,
  Content: Content$2p,
  getHeadings: getHeadings$2p,
  getHeaders: getHeaders$2p
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2o = {"title":"How Gender Shows up in Language","description":"Ever been curious about what gender in language really means? Give this a read to get a high-level, cross-linguistic overview.","date":"2022-06-28T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981425/blog/how-gender-shows-up-in-language/pride-thumb-554x220%402x.png","authors":["sam-zegas"],"category":"identity-and-language","tags":["gender","inclusion","language"],"seo":{"title":"How Gender Shows up in Language","description":"Ever been curious about what gender in language really means? Give this a read to get a high-level, cross-linguistic overview."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981425/blog/how-gender-shows-up-in-language/pride-thumb-554x220%402x.png"},"shorturls":{"share":"https://dpgr.am/48a29fe","twitter":"https://dpgr.am/a0a8b9f","linkedin":"https://dpgr.am/e430da1","reddit":"https://dpgr.am/97502ba","facebook":"https://dpgr.am/9e07a5d"}};
						const file$2o = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-gender-shows-up-in-language/index.md";
						const url$2o = undefined;
						function rawContent$2o() {
							return "Happy Pride! This time last year, we published an article about the language of allyship that included a brief discussion of the history of the [singular they](https://blog.deepgram.com/the-language-of-lgbtq-inclusion-and-allyship/) in English. As this year's Pride Month wraps up, we're coming back to the theme of gender and language for a look at how languages around the world incorporate gender and are influenced by the concept of gender in turn.\n\n## What is Grammatical Gender?\n\nIn English, so-called **grammatical gender**-vocabulary with clear gender marking-shows up in a few limited contexts. One example is in our pronouns, and specifically in the third-person singular pronouns, *she, he* and *they*. Another example is vocabulary words that imply gender. These include familial relationships like mother and father, professional designations like actor and actress, and titles like Mr. and Mrs.\n\nThere are many examples of this kind of gendered vocabulary, and in recent decades, contemporary language has tended to move away from using the female versions, [calling women actors rather than actresses](https://www.latimes.com/archives/la-xpm-2009-jan-18-ca-actress18-story.html), and referring to [flight attendants rather than stewardesses](https://www.aerotime.aero/articles/28032-stewardess-flight-attendant-history). There has also been a [decline in the use of gendered titles](https://books.google.com/ngrams/graph?content=Ms%2CMiss%2CMrs%2CMr&year_start=1970&year_end=2000&corpus=0&smoothing=3&direct_url=t1%3B%2CMs%3B%2Cc0%3B.t1%3B%2CMiss%3B%2Cc0%3B.t1%3B%2CMrs%3B%2Cc0%3B.t1%3B%2CMr%3B%2Cc0) in English over the decades, including Mrs., Mr., and Miss, with only a small uptick for Ms. where it displaced other female titles. This shift away from overtly gendered language is likely the result of several demographic and political movements.\n\nSome changes came as the result of [feminist language reform](https://www.blackwellpublishing.com/content/bpl_images/content_store/WWW_Content/9780631225027/024.pdf), which sought to remove language that presumed vocabulary marked as male as the normal or neutral form of a word, and particularly to create gender-neutral terms for positions of authority. This movement gave us chairperson and spokesperson as replacements for chairman and spokesman. It also advocated for [the adoption of Ms.](https://www.theguardian.com/lifeandstyle/2017/jul/07/sheila-michaels-who-brought-ms-into-mainstream-dies-aged-78) as a female title that was not dependent on marital status, bringing it into parity with Mr. Other language changes have been the result of demographic changes. The number of Americans who openly identify as LGBT+ has [grown over time](https://news.gallup.com/poll/389792/lgbt-identification-ticks-up.aspx). Despite many setbacks and continued hardships, this group has steadily [won victories in pursuit of rights and recognition](https://en.wikipedia.org/wiki/Timeline_of_LGBT_history_in_the_United_States#2020s), and in doing so, has highlighted the need for more inclusive language. This has led to many language innovations, including the adoption of [neopronouns](https://en.wikipedia.org/wiki/Neopronoun). We encourage our readers to check out last year's Deepgram Pride post: *[The Language of LGBT+ Inclusion and Allyship](https://blog.deepgram.com/the-language-of-lgbtq-inclusion-and-allyship/)*, for tips on making your language more inclusive of LGBT+ people.\n\n## What about Languages Other than English?\n\nSo far though, we have focused on English. You might be surprised to learn that in comparison to many other major world languages, English has barely any grammatical gender marking at all. Let's take a whirlwind tour of how other languages incorporate information about gender into their vocabulary. Students of Romance languages like Spanish, French, Portuguese, and Italian will be familiar with the two-gender system that extends to all nouns. Nouns that refer to people or animals of a particular gender often align with the expected grammatical gender. To take some examples from Spanish, *la hermana* \"the sister\" is a feminine noun, whereas *el hermano*, \"the brother\" is its masculine counterpart. But this system extends to *all* nouns, not just nouns referring to living things, which means that language learners need to remember that *el puente* \"the bridge\" is masculine, while *la casa* \"the house\" is feminine. When people of different genders are spoken of together as a group, the group defaults to masculine, such that a group of *amigas* and *amigos* - female and male friends - are referred to collectively as *amigos*.\n\nBut let's not stop at Romance languages. Some languages have even more complex gender marking. Learners familiar with Latin, Ancient Greek, German, or any of the Slavic languages will have encountered no fewer than three genders: masculine, feminine, and neutral. As we saw in the examples from Spanish above, this framework applies to inanimate objects that have no inherent \"gender\" in the way living things do. Confusingly, the rules in these languages sometimes produce unexpected gender marking even for living things. In German, the word for \"girl\" *das Mädchen*, is neutral. Surprising? The reason has to do with how the word is formed. The *\\-chen* ending is a diminutive suffix: a variation of a word that emphasizes its small size or precious quality. The addition of *\\-chen* changes the gender of a base word to neutral. Mädchen was originally a diminutive form of the now-archaic feminine word *die Magd*, the German cognate of English \"maiden.\" But over time, the diminutive, neutral form *das Mädchen* completely replaced *die Magd* as the word for \"girl.\"\n\n## Speakers, not Nouns\n\nSo far, all of our example languages have focused on how nouns are marked with gender-but some languages go even further than that. Semitic languages like Arabic and Hebrew conjugate verbs in ways that must agree with the gender of the person doing the action. Furthermore, because these languages have feminine and masculine forms of the second person pronoun \"you,\" there may be multiple ways to express certain actions depending on the gender of who is involved.\n\nIn Hebrew, for example, there are [four ways](https://www.lingalot.com/i-love-you-in-hebrew/) to say \"I love you\": one each for a woman speaking to a woman, a woman speaking to a man, a man speaking to a man, and a man speaking to a woman. Some languages classify nouns into categories that don't resemble \"gender,\" along the lines of the mostly-European languages described so far. Some languages, including Native American languages such as [Blackfoot](https://en.wikipedia.org/wiki/Blackfoot_language) and [Ojibwe](https://en.wikipedia.org/wiki/Ojibwe_language), divide nouns into the categories of *animate* and *inanimate*.\n\nOther languages, such as Hungarian, Turkish, and Mongolian, have no grammatical gender as such but categorize nouns according to whether they feature \"front of mouth sounds\" or \"back of mouth sounds\"-a phenomenon known as [vowel harmony](https://en.wikipedia.org/wiki/Vowel_harmony). Meanwhile, languages including Chinese, Japanese, and Swahili have complex noun classification systems with dozens of categories, often grouped by shape, size, and other descriptive features. Learners of Chinese often encounter these categories first through the concept [measure words](https://speechling.com/blog/an-introduction-to-measure-words-in-mandarin-chinese/): *shí zhǐ māo* means \"ten cats,\" *shí zhāng zhuōzi* means \"ten rivers,\" and *shí zhī qiānbǐ* means \"ten pencils.\" The words zhǐ, zhāng, and zhī are simply counter words (or noun classifiers) that describe animals, flat objects like tables, and cylindrical objects like pencils, respectively. Swahili, Japanese, and many other languages have developed noun classification systems along these lines.\n\n## Moving Towards Gender-Neutral Language\n\nIn recent years, there has been the beginning of a language reform movement to create gender-neutral forms for Romance languages. The result of one such movement was the creation of the term [Latinx](https://www.pewresearch.org/hispanic/2020/08/11/about-one-in-four-u-s-hispanics-have-heard-of-latinx-but-just-3-use-it/) (pronounced \"latin-ex\") as a non-gendered version of Latinos among some Spanish speakers in the United States. Nonetheless, [recent studies](https://www.pewresearch.org/hispanic/2020/08/11/about-one-in-four-u-s-hispanics-have-heard-of-latinx-but-just-3-use-it/) indicate that adoption of Latinx is limited to academic and social justice contexts. Speakers of Romance languages-as well as many other languages-have developed [gender-neutral pronouns and other strategies for language reform](https://en.wikipedia.org/wiki/Gender_neutrality_in_languages_with_grammatical_gender) where gender marking is perceived as a political issue.\n\nWhile these changes are derided by some, the noun classification systems like those of Swahili, Japanese, and Chinese shed light on an important aspect of the \"gender\"-based classification systems described above in the context of European and Middle Eastern languages: namely, that the relationship between a noun and its \"gender\" is arbitrary. Returning to the Spanish examples, there is nothing inherently feminine about *la casa* \"the house\" and nothing inherently masculine about *el puente* \"the bridge.\" And there's no reason that languages can't develop new ways of communicating that are less tied to gender.\n\n## Wrapping Up\n\nSome day, some brilliant person may prove or disprove the [Sapir-Whorf Hypothesis](https://en.wikipedia.org/wiki/Linguistic_relativity)-an achievement which would share linguistics, anthropology, and cognitive science to their core-and in doing so, show a definitive relationship between linguistic categories and our perception of the world. Until then, it is enough simply to acknowledge that the words we use have social meaning in that they have the power to include or exclude certain groups of people as well as set coded expectations about the role of gender in society. This Pride, we're taking the opportunity to think about how gender influences language, and we hope you'll continue to think about how gender shows up in your own speech too.";
						}
						async function compiledContent$2o() {
							return load$2o().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2o() {
							return (await import('./chunks/index.8f00078a.mjs'));
						}
						function Content$2o(...args) {
							return load$2o().then((m) => m.default(...args));
						}
						Content$2o.isAstroComponentFactory = true;
						function getHeadings$2o() {
							return load$2o().then((m) => m.metadata.headings);
						}
						function getHeaders$2o() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2o().then((m) => m.metadata.headings);
						}

const __vite_glob_0_124 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2o,
  file: file$2o,
  url: url$2o,
  rawContent: rawContent$2o,
  compiledContent: compiledContent$2o,
  default: load$2o,
  Content: Content$2o,
  getHeadings: getHeadings$2o,
  getHeaders: getHeaders$2o
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2n = {"title":"How is Machine Learning, or Deep Learning, Affecting Science? — AI Show","description":"Learn more about how deep learning is affecting science in this episode of the AI Show.","date":"2018-12-14T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981337/blog/how-is-machine-learning-or-deep-learning-affecting-science-ai-show/how-ml-dl-affecting-science-blog-thumb%402x.jpg","authors":["morris-gevirtz"],"category":"ai-and-engineering","tags":["machine-learning","deep-learning"],"seo":{"title":"How is Machine Learning, or Deep Learning, Affecting Science?  — AI Show","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981337/blog/how-is-machine-learning-or-deep-learning-affecting-science-ai-show/how-ml-dl-affecting-science-blog-thumb%402x.jpg"},"shorturls":{"share":"https://dpgr.am/8d2bec2","twitter":"https://dpgr.am/f836824","linkedin":"https://dpgr.am/9d340bd","reddit":"https://dpgr.am/962e0be","facebook":"https://dpgr.am/43c94c9"}};
						const file$2n = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-is-machine-learning-or-deep-learning-affecting-science-ai-show/index.md";
						const url$2n = undefined;
						function rawContent$2n() {
							return "<iframe src=\"https://www.youtube.com/embed/5tip6JR_AN8\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\n**Scott:** Welcome to the AI Show. Today we're asking the question: How is machine learning, or deep learning, affecting science? \n\n**Susan:** Actually I'm asking a question of you! For those that do not know, Scott here has a little bit of a science background. \n\n**Scott:** A little bit. \n\n**Susan:** And a little bit of machine learning in science background. Scott, can you at least give us the 10,000 foot overview of a little bit of what you've done?\n\n## Finding Dark Matter with AI\n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976791/blog/how-is-machine-learning-or-deep-learning-affecting-science-ai-show/lux.jpg)\n\n*A photo of the Large Underground Xenon experiment. Scientists hope to detect weakly interacting massive particles (WIMPs). [Credit.](https://physicsworld.com/a/dark-matter-constraints-tightened-after-lux-no-shows/)*\n\n**Scott:** The 10,000 foot overview is I have a Ph.D. in Particle Physics. But, I was searching for dark matter deep underground in a government-controlled region of China. Basically, a James Bond lair.\n\n**Susan:** I like it.\n\n**Scott:** We had to design the experiment and build the experiment, operate the experiment, take data, analyze the data, write a paper. This is what you do in experimental particle physics. We did that searching for dark matter. We did it with lots of computers, servers, CPUs, things like that. Lots of copper, plastic, liquid Xenon, cryogenic stuff.\n\n**Scott:** The CPUs were used to do data analysis, and we were using [boosted decision trees](https://towardsdatascience.com/decision-tree-ensembles-bagging-and-boosting-266a8ba60fd9) and neural-networks and other standard statistics-based cuts in order to figure out was it a dark matter particle or not.\n\n**Susan:** So tons of signal noise search basically, right?\n\n**Scott:** Yes. These detectors are little cameras, but the cameras that have no lenses. And by little, I mean they're actually big. They're maybe a meter by a meter, but the pixels are maybe a few inches so they're really large pixels. But these pixels can sense individual photons, so a very, very small amount of light. But, there's a few hundred of them. You have to pick out what happened inside the detector based on the pattern that is in those pixels.\n\n**Scott:** No lenses. Again, you can't zoom in and see \"The particle was right here, and I can see it's hair.\" But you can say, \"Within a region about this big, the particle interaction happened, and it also had a flash that was this size.\"\n\n**Susan:** But basically, your machine learning techniques were attempting to take that huge amount of data and shift signal to noise in an efficient way from it.\n\n**Scott:** Coming out of the PMTs would be a digitized signal, but it would like a waveform. It kind of looks like an audio waveform. Those signals were different for every type of particle interaction. But the two main types of particle interactions are: What's an atom made of? It's made of a nucleus and some electrons pretty much. At the energies we're talking about, they normally don't blow apart into smithereens. It's either the nucleus gets hit, or the electron gets hit. If the nucleus gets hit, then the whole atom starts ripping through and generating photons, electrons and heat. If an electron gets hit, then an electron goes ripping through. When that happens, the signals are different. They give off different amounts of light, the size of the pulse they give off is different as well.\n\n**Susan:** So for anyone that is interested in why these dark matter, or even neutrino detectors, are so big, but also always full of some fluid, it's because you have to see the light that happens off of that interaction. If it wasn't some sort of clear fluid to the light that's coming off, or clear medium for the light coming off, you wouldn't be able to see it.\n\n**Scott:** Yeah, lots of things scintillate, but many things are not transparent to their own scintillation light. If you throw particles fast enough at things, light will be made.\n\n**Susan:** A lot of light.\n\n**Scott:** But you won't be able to read it, and that's one special thing about Xenon is that it's very clean, it's not radioactive or at least not very radioactive, and it's transparent. It scintillates and it's transparent to its own scintillation light.\n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976792/blog/how-is-machine-learning-or-deep-learning-affecting-science-ai-show/Dark-Matter-Hairs-Around-Earth.jpg)\n\n*Original Caption Released with Image: This illustration shows Earth surrounded by filaments of dark matter called \"hairs,\" which are proposed in a study in the Astrophysical Journal by Gary Prézeau of NASA's Jet Propulsion Laboratory, Pasadena, California. [Courtesy of JPL](https://photojournal.jpl.nasa.gov/catalog/PIA20176)*\n\n**Susan:** So that gets back to our question. The question is machine learning, how is it impacting the sciences? Your own experience shows right away how the recent world is being impacted by this. Obviously, dark matter, the search for what is that missing ... what it is it 95? 97? What's the percentage?\n\n**Scott:** Yeah, 80%.\n\n**Susan:** Some large amount of the universe we can't really figure out what's going on with.\n\n**Scott:** That's like dark energy. And then matter is like 25%, 20% or so. And then there's like 4% or 5% that we actually know about.\n\n**Susan:** We're seeing some big impacts from machine learning.\n\n**Scott:** The huge thing that we saw was that we could do a lot with a small team.\n\n**Scott:** You could take billions of data points, you could collect data, and a team of five or 10 people can actually analyze the data. This is in contrast to how teams at the LHC or CERN Institute analyze data. Let me explain, so we have a particle detector, but it was looking astrophysical particles. You didn't have to have a collider. You didn't shoot anything into it. The universe is already shooting stuff into it. But if you have a collider, then you actually make the particles that then get shot into the detector and then read. This is like the LHC, Cerne, Atlas, those types of things. Those were started like in the 60s, 70s, those type of colliders. They've been using the same techniques from way back until 2000-teens. There's actually a story here. They tried to do machine learning back in the second AI boom, which is the 80s. They're like, \"Oh neural networks, neural networks. We have this problem, can we tell signal from background? Let's train a machine learning model to do it.\" Well there was some good that came out of that, but for the most part everybody got super burned out in the particle physicists community, and they were just like, \"Neural networks suck. They don't do what we need them to do.\" Really what it was, our computers suck and we don't have enough data, and we don't really know how to train them all that well. But then fast forward 20 years and our computers don't suck anymore. We have the data and we know how to train them now. I see all this repetitive stuff that's happening. Thousands of scientists in this experiment trying to pick which cut we should make, or something like that. It's like why don't we just have computers do it?\n\n**Susan:** Do that mutual sift, yeah.\n\n**Scott:** We can have tons and tons of data, and we can do some complicated analyses, but we have to do it in machine learning because otherwise it would take too many people.\n\n## Efficiency and Processing\n\n**Susan:** I think the other bent on the same note though is efficiency and processing. Have you seen the stuff going on with protein folding? I mean huge processing advances are going on just because they're using deep learning models, or they're using machine learning models to accelerate the guess at what that protein will fold into. For those that don't know, the challenge is you've got a whole bunch of atoms that are strung together in this big complicated molecule, and when you let them go they all kind of fold and convolute into this weird shape.\n\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Protein_folding.png/360px-Protein_folding.png)\n\n**Susan:** And that weird shape is the point, right?\n\n**Scott:** If it's not in that shape it doesn't do its job.\n\n**Susan:** You know your cells let through a shape, and if you can figure out the shape that something's going to snap to, you can maybe design a drug that'll enter a cell that another one wouldn't, or something along those lines.\n\n**Scott:** Make proteins actually stay in that shape for longer or something.\n\n**Susan:** If you can predict how proteins are going to fold, crinkle up and do all that stuff, you can do some really good stuff. But, it's really hard. It takes a lot of computational power. Machine learning is coming along and making it more efficient.\n\n**Scott:** It used to be just grid search, just use flops. Lots of computational power. It was distributed ... I remember doing this actually. I have a couple of PlayStation 4s, maybe four or five or so, in my living room and my kitchen, crunching numbers to do protein folding for free. When before I was in graduate school, because I just thought it was so cool. You need computational power in order to do this problem? I'll help you with that.\n\n**Susan:** A spinoff of SETI at home, no?\n\n**Scott:** It was the first well known crowdsourcing of your own computer set before BitCoin mining.\n\n**Susan:** Yeah, before BitCoin mining said, \"You can make money so forget all those free altruistic purposes. We don't care if you want to look for aliens or help find the cure for cancer. You can make money.\"\n\n**Scott:** \"You can make money, so let's do that.\"\n\n## Free energy and AI\n\n**Susan:** Are there any other good examples of improved computational efficiency or great new designs coming out of the academic world?\n\n**Scott:** Well there's a good story about redesigning your [tokamak](https://en.wikipedia.org/wiki/Tokamak).\n\n**Scott:** So it's like fusion has been a thing for the last 60 years.\n\n**Susan:** Isn't the running joke about fusion, it's 50 years away?\n\n**Scott:** The running joke keeps getting longer. It's always 20 years away, but I like that.\n\n**Susan:** Oh, like 20 years away. 20 years ago it was 20 years away.\n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976793/blog/how-is-machine-learning-or-deep-learning-affecting-science-ai-show/41783636452_5f409422dc_k.jpg)\n\n*The newest and most powerful TOKAMAK fusion reactor is scheduled to come online in 2025 and produce, hopefully power (on an experimental basis) years afterward. Photo courtesy of [Oak Ridge National Laboratory.](https://www.flickr.com/photos/oakridgelab/41783636452)*\n\n**Scott:** Now it's 50 years away. It's only 50 years away. it's probably a lot closer now. This is something that's really interesting to me too. It's like if you give the world really cheap power, then you make the world much more productive and you don't destroy the environment. I mean, we will always find ways to destroy the environment, but you don't destroy it as quickly.\n\n**Susan:** Give me a challenge Scott, and I will take that challenge.\n\n**Scott:** It's a really hard complicated thing to build some of these reactors, and they said, \"We've tried many things over the last 50, 60 years and spent many billions of dollars trying to build these reactors.\" We've gotten pretty good, but they're still research-y. They're still not able to be turned into a real one that actually produces power on a large scale for all of humanity and things like that.\n\n**Scott:** [Let's throw machine learning](https://www.iflscience.com/technology/supercomputer-will-help-us-tackle-nuclear-fusion/) at the problem and see what happens. Now there's talk that these designs are actually able to be built, and they probably will serve the purpose.\n\n**Susan:** What was the last one? They reached some milestone, like 100 seconds or something like that. There are so many challenges going into confining so high of energies in such a small space.\n\n**Scott:** The trick with fusion is that everything is a gas pretty much and so it's really easy to cool it down. Well there's not much mass there, but there doesn't have to be a lot of mass because there's a lot of energy.\n\n**Susan:** It's something about energy equals mass.\n\n**Scott:** E=MC2 or something like that. If any of that gas touches the wall of the reactor then it's instantly going to be cooled down. How can you contain it without touching it? So they try to use magnetic fields, but there's some tricks with that basically that it doesn't work so well. That's why it took so long. It's probably the only way to do it, and you can kind of work out that it'll probably work, but the problem is making it in practice.\n\n**Scott:** So machine learning comes along and says, \"Hey, actually we have a design that might be able to be built and serve the purpose that you need.\"\n\n**Susan:** And it's all about the [hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)) and something like that. So when you think about a tokamak, this thing that's creating a plasma and squeezing it really tight and all that. The field you have to create is really uniform, but at the same there's practical constraints on this. Like, where do you put this pipe and how do you put these wires?\n\n**Scott:** Yeah, you have to cool it down so everything doesn't melt down. Just how do you design it all?\n\n**Susan:** So there's all these constraints.\n\n**Scott:** When it heats up it's going to crack or something.\n\n**Susan:** There's all these gives and takes in the design. Everything you do may affect how pure that magnetic field is. At the same time if you could route this pipe a little bit better you could get a lot stronger field so it doesn't need to be as perfectly this, that or the other. So that's where things like machine learning can maybe help search a big huge hyper parameter space that is just really impossible in other ways.\n\n**Scott:** Humans typically rely on symmetry to guide this. It's got to be circular, it's got to look nice. A lot of that comes from, well you can calculate the math pretty easily on whether it'll actually work, or well it's easier to produce. It's just, \"I can only think of so many things, and I want to think of 15 different designs, well they're all going to be pretty symmetric even though they can be symmetric in all these different ways.\" But a machine learning model doesn't care so much about that. It cares more about the task at hand and it's like: \"I don't care if I put a pipe here *if* I can build some way to get around it without affecting things.\" That complexity can deal with a complexity a lot better than a human can. It doesn't have to. It will still tend to, but it doesn't have to fall back on symmetry in order to make it simple.\n\n**Susan:** We talked about this in a podcast, but at the core of any good problem is defining the problem. Honestly when you say things like people care about symmetry and it affects it end result, but machine learning only cares about the problem.\n\nThat's really a big core thing that I think machine learning might be bringing to the sciences, is really making people focus on super incredibly hardcore defining your problem and how close you are to a solution as opposed to just jumping to ways of fixing it.\n\n**Scott:** It suffers a little bit from the solution in search of a problem. A kind of way of looking at the world where if you come from the other way, I'm just defining my [loss function](https://en.wikipedia.org/wiki/Loss_function) and I have these things that I can do. I can put steel here and plastic here, and this there. It's like I'm just going to exhaustively go through and try and figure it out. A machine learning model never gets tired.\n\n**Susan:** It never gets bored of trying. The next step is .0001 different than the last step.\n\n**Scott:** But it's excited about that! It's like \"Oh, this might make it .001 better. A human doesn't get that excited about it.\"\n\n**Susan:** Not at all. And I bet you, with humans, the recording gets a little bit sloppy after a while.\n\n**Scott:** Once you see the 80%, getting to 90% is real hard and after that everybody is checked out. They did their duty.\n\n**Susan:** Doing a grid search by hand, you really realize how much machines can help us.\n\n**Scott:** People used to do this back in the day too, like pencil and paper. When they would talk about, \"Let's do the calculation,\" it's like we spent weeks on this calculation. It's stuff like that.\n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976794/blog/how-is-machine-learning-or-deep-learning-affecting-science-ai-show/napier_logtable.jpg)\n\n*John Napier of Merchiston born 1550 - died 1617; is known as the discoverer of logarithms. The picture above is a picture of his log tables book-the calculations for which were all done by hand and thus had errors. Error-free log tables were not made possible until the era of electrical calculators 3 centuries later.*\n\n**Susan:** Yeah, well the hard stuff is actually hard for a reason.\n\n**Scott:** You're always finding the boundaries of what's possible and then spending weeks on the problem. When you don't have a calculator, well that means you're writing down stuff and trying to figure out the answer. When you finally do have the calculator, okay now the complexity of calculation is problematic and so you start doing things that you're going to want to program a computer to do, but you don't know that yet.\n\n**Scott:** Then computers come along and you're like, \"Oh, thank God. Now you're taking away a lot of this complexity!\" But then you find a new problem to spend weeks on, and that's just how research works.\n\n**Susan:** We've talked about a couple of really important things here. Computers are helping us sift through data in ways that used to take huge teams. They're also reducing the computational costs associated with things like the protein folding that we were talking about. We are getting really good at efficiencies there.\n\n## Education and ML.\n\n**Scott:** Unfortunately, researchers, you're going to have to learn a new skill. It's as if learning calculus and computing, and all these other things wasn't enough, here's another one. But it's a very powerful one and it's a tool such that you don't have to be the one that is smart about what exact little details you need to do in order to accomplish this task.\n\n**Scott:** All you have to be smart about is putting the constraints in and saying, \"Stay within these boundaries. Here is some training data,\" and then try to tackle that task. Not everything is going to be able to be done that way, but there are going to be a lot of things that are going to be able to be done that way. So it's a new tool in the tool belt. People are going to start thinking about the world and their problems, and just like they do with programming and math, statistics.\n\n**Scott:** How can we uncover a new thing and not everything is going to be machine learning.\n\nNot everything is going to be deep learning. Not everything is going to be statistics. You can learn new things just by writing, and you don't have to think about statistics.\n\nIt's going to be is a new honed, sharpened tool, is machine learning. Create data sets. Use models. Turn cranks. Get results. And go after the problem that way.\n\n**Susan:** I had an argument with some friends a while ago where I was saying all the sciences in all the fields, and the idea that we have a machine learning degree path, maybe we need to fragment that. Saying you're in paleontology, there should be machine learning in paleontology. There should be machine learning in archaeology. There should machine learning in all sorts of different fields because just like basic stats from the math point of view, you go out there and you do really good math in whatever field acts.\n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976795/blog/how-is-machine-learning-or-deep-learning-affecting-science-ai-show/Crete-archaeology-sissi..jpg)\n\n*As long as there is data-or enough data-ML methods can help researchers discover truly novel things. If you are interested, here is one article to read:*  Van Den Dries, M. H. (1998). \n\nArchaeology and the application of artificial intelligence: case-studies on use-wear analysis of prehistoric flint tools.\n\n**Susan:** Now machine learning is related, but it's a new set of skills. It's like you said, honing those data sets and understanding how to churn them through a good model and all that stuff.\n\n**Scott:** This is one of the downsides to higher education, meaning like graduate school plus.They don't necessarily have a good track record of taking in things outside their field and utilizing them really well in their own field. Some might describe their pace as glacial in that.\n\n**Susan:** Progress is made one death at a time, right?\n\n**Scott:** Progress is made one funeral at a time. That's a long time scale. That's 50 years or so that you have to wait for your-\n\n**Susan:** Growing every year.\n\n**Scott:** Some might say, from a real practical perspective at least every 30 years. For instance, computer programming has been around for a while now. In particle physics, there's a lot of programming that you have to do. There's no programming for physics course. You're just forced by trial by fire to learn the stuff you need to learn and move on. Same thing with machine learning. Same thing with engineering, or CAD drawing, or any of the stuff you have to do.\n\n**Scott:** This means something like: sprinkle a little bit of money, enough to live. Put you around enough other people that are making a similar amount of money and sort of mix it all together and say, \"Go create stuff.\" Those other people are graduate students. Then you do some search by graduate student descent, gradient descent-\n\n**Susan:** I like that. Grad descent.\n\n**Scott:** Yeah, grad descent. To find a solution to a problem. I think that is going to get an overhaul. I think it's going to have to because the existence of MOOC's (online courses). There probably be an online course that's machine learning for physics, or machine learning for particle physics, or machine learning for biophysics, or machine learning for whatever.\n\n**Scott:** Those are going to crop up in the next 20 years and the whole model is going to change. People actually will be good at those things, and they will have a real sharp tool belt rather than just like a dull one which is kind of the case now.\n\n## New tools, new ways to attack old problems\n\n**Susan:** You know medicine is also an amazing example of this. We're seeing just as it hits, it is a brand new set of tools which allows people to think in brand new ways and attack old problems and new ways and get fresh insights on them.\n\n**Scott:** It'll take time too. Even if you said, \"You know what? I'm going to drop everything and I'm going to start doing machine learning in my field,\" I wouldn't even say that you'd make enough progress to make a contribution in year one. The real contributions are going to come in like year three, four or five when you're like, \"Whoa, okay. I just got effective at using this tool.\" That's a hard time scale for people. If you can't learn it in a week or a day, or an hour, then you have a big drop off. People around will see that though. They'll the success of other researchers, papers, etc. They'll be using them. They'll be citing them. They'll be like, \"Man, okay I really gotta do it this time.\"\n\n**Susan:** This time I finally have to break down and this new tool that seems to be breaking open those problems that my field couldn't crack before, but now well we've got good results.\n\n**Scott:** At least from what I saw, people get a little tired of learning so many different things. This is kind of problematic, but also a good thing in machine learning. You can forget a lot of it, and just go down the path and learn along the way. Then revisit those or something, so like, \"What exactly is a Hidden Markov Model? What exactly is... just all these things that people used in the past.\n\n<iframe src=\"https://www.youtube.com/embed/kqSzLo9fenk\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\n**Susan:** That's the difference between someone that is ... not specialized, that is a machine learning scientist and trying to develop those basic techniques and those basic concepts. Some of that's combining it with a field like physics and machine learning together.\n\n**Scott:** Yep, Applied AI.\n\n**Susan:** Applied into the field of physics, which is also probably why those fields need to embrace it and start specializing the curriculum to include it.\n\n**Scott:** I think there's a little love affair here, similar to math and physics, and chemistry and physics, and math and chemistry, and astrophysics and just astronomy and that sort of thing, where they feed off of each other. You learn a lot in a certain area because whoa, tools became available. Resources became available. People are interested in it.\n\n## Astronomy and ML\n\n**Scott:** Then you go do a really good job on that. But then the other one has kind of been left to rust for a while, but then you come back to it. This is definitely true of astronomy and physics essentially. Astronomy happened, right? It was a big deal. The moon, Jupiter, Jupiter's moons, things orbit each other. Back in the 1500s that was still a cutting edge thing. Move forward to the 1800s, still like a big deal. 1900-ish, in Albert Einstein's time, physics really took over. Thermodynamics is cruising, quantum physics, all those nuclear power, MRIs, just stuff like that.\n\n**Scott:** Then, around mid-century, astronomers go out and notice some weird stuff happening in the universe, and discovered dark matter, and discovered dark energy. All of a sudden physicists are like scrambling to be like, \"How do we describe this stuff?\" We didn't know about it because we were too busy looking down here at really tiny stuff.\n\n**Susan:** And you've got the biggest physics experiment in the universe.\n\n**Scott:** Yeah.\n\n**Susan:** The universe. The cosmic background radiation, how is that all lined up? Does that tell us something about quantum mechanics?\n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976796/blog/how-is-machine-learning-or-deep-learning-affecting-science-ai-show/Ilc_9yr_moll4096.png)\n\n*The cosmic microwave background is electromagnetic radiation as a remnant from an early stage of the universe in Big Bang cosmology. Source: [Wikipedia.](https://en.wikipedia.org/wiki/Cosmic_microwave_background)*\n\n**Scott:** Similar story with quantum field theory in physics and math, and theory of knots and strings coming back in, and math and physics and that sort of thing. All these things tie together and they feed off of each other. So I think there will be contributions from the applied AI side that are actually fundamental contributions in machine learning as well. And there already have been.\n\n**Susan:** What we've already obviously seen this in the medical field, studying the brain has made contributions and the machine learning huge contributions in machine learning. And vice versa. [Reinforcement learning](https://blog.deepgram.com/ai-show-different-types-of-machine-learning/), talking about potentially how parts of the brain works and back and forth. Like you said, other fields that we're going to see those same things coming out of it.\n\n**Susan:** That's what's super exciting about this. When you start pushing it out into the sciences, you get incredibly brilliant people that are brilliant in ways not exactly the same as everybody around you.\n\n**Scott:** They factorize the problem a different way. Then you get a benefit from that. \"Whoa, if you think of it that way then the world becomes really simple.\" You can jump to different ways of thinking to solve the problem however you need to.\n\n**Susan:** Plus, from the machine learning standpoint we're like, \"I gotta beat those guys.\" I can't let someone dealing with dinosaur bones come up with something new. Competition is great.\n\n**Scott:** Get the blood pumping.\n\n**Scott:** Speaking of getting blood pumping, you made some delicious bread this week.\n\n**Susan:** There's a great recent article, a blog post I read about machine learning being applied to bread starter evolution and all that. This guy did a really great job of setting up, I think it was like [Scikit Learn...](https://scikit-learn.org/stable/index.html) I forget exactly the tools he used, to watch as the yeast would rise and fall.\n\n**Scott:** So it was like a camera?\n\n**Susan:** Well he had a camera set up on his little tubes of starter, and he would feed it and figure out exact times that-\n\n**Scott:** So he wants to find a really good yeast. He wants to breed a really good yeast strain.\n\n**Susan:** People have their starters. I had a starter for about two years and I had to give it up when I moved. It would have been really hard to explain on an international flight why I had this jar of clearly biological matter coming on the plane.\n\n**Scott:** That you're trying to keep warm and alive.\n\n**Susan:** People get into their starters and he was talking. This blog post was amazing. It was talking about cultivating wild yeast and trying to grow it, and what's the optimal feeding schedules and what's the optimal this, that and the other.\n\n**Scott:** Like should you starve them for a while, or give them as much as they wanted to eat? Or cool them down and shock them with heat?\n\n**Susan:** But it really showed first of all a good scientific method at play, using machine learning tools to help aid those experiments. It was mainly on the measurement side, but you could see a lot more that could go into this. It really shows you what machine learning can do. Just recognizing basics of how quickly this is growing versus not that is data points, and collecting those data points in the future analysis. Great stuff there.\n\n**Susan:** The citizen scientist world is being enabled by machine learning, by taking out a lot of the complexities that you would have had to have done by hand.\n\n**Scott:** Yeah, a lot of these things were impossible a decade ago. Certainly two decades ago.\n\n**Susan:** I'm not going to watch my tube of starter growth, take measurements every five minutes for three days straight.\n\n**Scott:** A machine can do it.\n\n**Susan:** A machine can do it for me. Before good image recognition and all that came along, who were you going to get to do that? So there's great stuff. Other awesome sciences that are available with all the stuff that's done with all the image net models that come around. You can set up a camera and start classifying stuff that goes by your window and do a lot of great citizen science with all that. Again, something that would not have been possible before. You can spend a week in designing and building something, taking some off the shelf stuff, and classify all the flying objects that come across and do some basic science on what's flying through your window.\n\n**Scott:** Many things are all coming together at once and it's also definitely true in a lot of these areas that you're sort of limited by data as well. But, of course, you can collect data yourself. And this is what the experimentalist would have to do anyway. They're not going to do it at a much bigger scale than you could. You might even do a bigger better job. So actually you're at the cutting edge.\n\n**Susan:** It's amazing what these technologies are enabling at every single level. We've talked about fusion at big energy levels, and searching for dark matter in the universe, all the way down to improving your sourdough round to make it a little bit bread for you.\n\n**Susan:** All sorts of amazing stuff that you can do with this. I'm excited for machine learning and the sciences.";
						}
						async function compiledContent$2n() {
							return load$2n().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2n() {
							return (await import('./chunks/index.87326f68.mjs'));
						}
						function Content$2n(...args) {
							return load$2n().then((m) => m.default(...args));
						}
						Content$2n.isAstroComponentFactory = true;
						function getHeadings$2n() {
							return load$2n().then((m) => m.metadata.headings);
						}
						function getHeaders$2n() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2n().then((m) => m.metadata.headings);
						}

const __vite_glob_0_125 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2n,
  file: file$2n,
  url: url$2n,
  rawContent: rawContent$2n,
  compiledContent: compiledContent$2n,
  default: load$2n,
  Content: Content$2n,
  getHeadings: getHeadings$2n,
  getHeaders: getHeaders$2n
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2m = {"title":"How is Today's AI Boom Different From Those of the Past? — AI Show","description":"In this episode of the AI Show, we explore how today's AI boom different from those in the past.","date":"2018-11-09T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1662069464/blog/how-is-todays-ai-boom-different-from-those-of-the-past-ai-show/placeholder-post-image%402x.jpg","authors":["morris-gevirtz"],"category":"speech-trends","tags":["machine-learning","deep-learning"],"seo":{"title":"How is Todays AI Boom Different From Those of the Past? — AI Show","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1662069464/blog/how-is-todays-ai-boom-different-from-those-of-the-past-ai-show/placeholder-post-image%402x.jpg"},"shorturls":{"share":"https://dpgr.am/7093f34","twitter":"https://dpgr.am/cb165e9","linkedin":"https://dpgr.am/e5aee4b","reddit":"https://dpgr.am/2858980","facebook":"https://dpgr.am/917238b"}};
						const file$2m = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-is-todays-ai-boom-different-from-those-of-the-past-ai-show/index.md";
						const url$2m = undefined;
						function rawContent$2m() {
							return "<iframe src=\"https://www.youtube.com/embed/LvP_lih3_yc\" width=\"600\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\n**Scott:** Welcome to the AI Show. Today we're asking the question: How is today's AI boom different from the past?\n\n## What were the past booms?\n\n**Scott:** We're definitely in a boom. But this is not the first AI boom.\n\n**Susan:** There's been some really good peaks in the past there. I mean, look back at the 50s, some cool stuff happened in the 50s with [Minsky](https://en.wikipedia.org/wiki/Marvin_Minsky) and a whole bunch of others. Really defining things. Fast forward in the 80s, and you know, great things there.\n\n**Scott:** Yeah, you got the [Perceptron](https://en.wikipedia.org/wiki/Perceptron) way back. In the 50s, 60s, etc. That name still sticks around in A.I. now.\n\n**Susan:** Yeah, I perceive this Tron.\n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976765/blog/how-is-todays-ai-boom-different-from-those-of-the-past-ai-show/retro-2426631_1280.png)\n\n_The last four letters of the word 'electron' i.e. -\"tron\" came to mean \"smallest unit\" in English. Hence: perceptron 'the smallest unit of computer perception'. The word electron was coined in 1891 as a portmanteau of the words: \"electric\" and \"ion\"._\n\n**Scott:** And in the 80s it came up again, basically it's like, \"Hey, we kinda figured out [back propagation](https://en.wikipedia.org/wiki/Backpropagation), and we have some more compute, things like that.\"\n\n**Susan:** The roots of AlphaGo in the 80s with reinforcement learning really getting nailed down there. Leading into the 90s, some great stuff from IBM with Deep Blue and Deeper Blue, and you know, beat some guy named [Kasparov](https://en.wikipedia.org/wiki/Deep_Blue_versus_Garry_Kasparov) at the game.\n\n**Scott:** Yeah, somebody. A nobody.\n\n**Susan:** A nobody. Solving checkers, that's another great one.\n\n**Scott:** Checkers, chess, Kasparov.\n\n**Susan:** Strongly solving checkers.\n\n**Scott:** Yep, and then into the late 2000s basically, so, 2008, 2010, 2012, really is when things started to pick up again, and that was like image [recognition](https://en.wikipedia.org/wiki/Computer_vision#Recognition), [deep learning](https://en.wikipedia.org/wiki/Deep_learning), etc., and we're still in it now. But in the past it wasn't just booms, right? In between there were what everybody calls, the AI winter, or AI winters, I guess, where the research funding from the government drops. The VC funding drops.\n\n## Is today's boom different? Will this one peter out too?\n\n**Susan:** I think you see something pretty standard there along the lines of:\n\n1.  Some piece of the problem, the pie gets solved.\n2.  It piques interest, and\n3.  Then everybody realizes it's not the whole answer and it goes back down.\n\n**Scott:** In order to get this stuff to work pretty well, you've gotta have, talent, smart people. You've gotta have compute, computational power. The skills that people thought you would need in the past, were a little under appreciated. You'd need many more times what you would actually think. The amount of data that you need to train that.\n\n**Susan:** It's huge.\n\n**Scott:** Just the architectures, just putting it all together- \"how would you actually train a model? How does it work?\" There's a lot of things in there, and you solve one of them, and you say: \"Wow, yes! We're getting somewhere.\" But, then you run into a brick wall because you don't have the other ones.\n\n**Susan:** Yeah, but what really makes this right now different from the past? Is there some true difference?\n\n**Scott:**\n\n**Well, essentially the big difference is that the internet exists, you can get tons of data and the underlying structure of models and how you train them has been pretty well fleshed out.** You don't have to be an absolutely cutting edge researcher tinkering in order to get anything to work. You can get a lot of things to work. You can go PIP install something, as an engineer or whatever, and start training models and whatnot. So, it's pretty accessible. I mean, it's complicated, but you can make tutorials, you can make videos and the people start to, \"Wow, okay!\" They can really get some traction there and put it all together.\n\n## What's the key difference between today's AI Boom and those of the past?\n\n**Susan:** Well, I'll tell you this. I think there's one key difference between this wave and the previous, and that's,\n\n**it's practical.**\n\n**Scott:** Well, yeah. That's the biggest.\n\n**Susan:** Companies are making lots and lots of money off of this. Whereas, in the past? It's like, \"Oh! We _might_ be able to make money... And that didn't work out as well.\"\n\n> \"Now, these tools are fine tuning sales pipelines, they're giving real ability to rip into data sources they couldn't see before, and process before.\"\n\nGenerating entire new industries like, potentially, the autonomous vehicle world is doing. There's just real value that's coming out of. I'm not saying the previous booms didn't provide value, but they weren't quite the value that you make green stuff off of.\n\n**Scott:** In the past, people kind of put the cart before the horse, just like when the internet was first coming around. Smart people that were deep in it realized, \"Wow, this is going to connect the world.\" And this is in the 80s. Yeah, it's gonna connect the world, we're going to be buying things, we're going to be doing this. But, if you dumped $100 million into companies back then to do those things, they would've tanked and not existed.\n\n**You had to wait 10 years for everything to congeal into something that's actually viable.**\n\n**Susan:** The soil was fertile. You've gotta have the idea and a fertile ground to plant it in.\n\n**Scott:** The previous booms, the first one is more academic. Funded by the government, people in academia, and just a handful of people working on it. \"Wow, machines can learn,\" but only in these very specific ways. We haven't generalized the problem. Then it kind of dies out for a while, but then, \"Hey, we solved some more problems.\" And then it gets a lot more interest again, like in the 80s and into the early 90s. Then it dies out again, and then comes back. This though is catching on pretty big, and you can point to real examples and large amounts of money that are made by AI now, rather than just speculating that it actually will happen.\n\n## What are the driving forces?\n\n**Susan:** You talked about the forces in the past driving it, pretty academic forces were driving, and traditional governmental research agencies.\n\n**Scott:** [DARPA](https://blog.deepgram.com/what-is-asr/), etc.\n\n**Susan:** But again, this boom area is showing a very steep turn towards more corporate money based research because they're getting value out of it.\n\n**Scott:** It's a transition. From the very beginnings, mostly academic. Then, in the 80s you could get funding to build companies and things like that, but then they kinda tanked. Funding for academic research also went up, but then tanked again. Now it's flipped where the money, the data, the tools, the computing power are all in the companies. As for the academics, their funding is lagging behind, they don't have the resources.\n\n**Susan:** Well, people are jumping into the commercial sector to actually do it. To actually solve these problems and attack these problems on the battleground, on the forefront of the battlegrounds, so to speak. I think that other things have gone. In the past there, some problem stays purely academic, and finally it catches a corporate way, something useful and practical, and you see academia get drained and go into practical use. We'll probably see, sometime in the future where, academia will slowly fill up again as the standard way of doing things is laid out, and researchers go back into doing weird, crazy research stuff.\n\n**Scott:** Yeah, where monetizing it becomes boring again.\n\n**Susan:** Yeah, exactly. Where the exciting stuff is.\n\n**Scott:** It's more like the cutting edge in that you can't make too much money from it yet so, go back to academia. Yeah, that makes sense. At least right now, a lot of the action is in the business world. Companies building AI teams, or having their own data science teams working on things, etc.\n\n## Why are companies adopting AI now?\n\n**Susan:** Why doesn't it make sense? If the ground is fertile, finally, and they're planting these seeds that are getting huge amounts of revenue.\n\n**Just think about the role that [reinforcement learning](https://blog.deepgram.com/ai-show-bias-in-machine-learning/)-in a classic way of trying to take a series of actions and get rewards out of-is affecting the web, and affecting how we guide people to click on what we want them to click, and buy the things we want them to buy.** How much is that shaping product placement and all these different things? When you go onto your favorite storefront site X, and it says, \"I suggest these six things for you.\"\n\n**Scott:** It's a recommendation system.\n\n**Susan:** You click on one of those down below, one out of three times. That's probably one more time out of three than would've happened ten years ago. That translates to real dollar signs immediately. These purely academic ideas are just starting to really find true problems that they're solving in the corporate world. And we're getting success after success after success, and that turns into billions of dollars.\n\n**Scott:** I think that you can look back at other booms in the world, like steam or electricity. Electricity in 1900, what did that look like? Or radioactivity. Back in 1900, people thought, \"Mix some radioactive stuff into your drink and drink it because it's going to make you healthier.\", right?\n\n**Susan:** You get a good healthy glow out of it.\n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976766/blog/how-is-todays-ai-boom-different-from-those-of-the-past-ai-show/25799475341_04ff144fae_k-1.jpg)\n\n**Scott:** That stems from not knowing exactly what it is, but you know that it's really remarkable, right? So, let's just sprinkle it into everything. You kind of see that today. Anything that has AI mentioned in it is like, \"Wow, awesome!\" Electricity and radioactivity don't get put into everything anymore, but electricity does matter in a lot of cases. We don't just shock ourselves all the time now, to have better health. People did that back in the day. Okay, let's quit that one, but, let's power our homes with it. Let's build washing machines. It has a certain thing it should be doing, and some other things it probably shouldn't be doing. Same thing with radioactivity. Hey, cancer treatments, X-rays, etc. That's radioactive, great, but do it in a low dose and it's all safe. It's sort of a similar time for AI now, where everybody wants to sprinkle it into everything. It's really only going to take hold in some certain sections, but then it's going to be really big.\n\n**Susan:** We've talked about this boom, and it's growing.\n\n## What's the general arc of these kinds of booms?\n\n**Susan:** What do you generally expect to see as we go through a boom like this?\n\n**Scott:** You see the irrational exuberance in the beginning, you see some things working and taking hold, and the fact that they are working supports a very large thesis that it's all going to work. Then people pump money in, a lot of it doesn't work, but some of it keeps working. Then, large sections die out, and I think that's actually already happening at a more rapid pace than in the past. Two years ago, you could say whatever you wanted with AI and get funded. Today, people are think along the lines of: \"Wow, I'm gonna pump the brakes here because we funded 100 A.I. companies, and half of them fail.\" Why is that? Well, because not everything needs A.I. But other companies are going to do well, so we just have to figure out what makes a good AI company. Once that's really figured out, then you can help that grow and it'll grow even faster. Just like the internet back in the day too. So long ago now.\n\n**Susan:** Yes, so long. The late 90s. Amazing, early 2000s!\n\n**Scott:** Yeah, man, 25 years ago... It's a similar thing today though. You have the whole ramp up, a little die out, but then you also have, something there will real merit here, and it just takes time to develop.\n\n**Susan:** You have a kind curve; it's kind of like this uplift and a slow tapering off.\n\n**Scott:** And then a massive long tail. Not as steep but just keeps going up.\n\n![Alt](https://upload.wikimedia.org/wikipedia/commons/thumb/1/12/Longtail.svg/300px-Longtail.svg.png)\n\n**Susan:** When we think of die offs, and those curves, it's really just a flattening and not a die off. It's not like we revert or anything like that. It's just, people take a breath, and take a pause.\n\n**Scott:** That's probably good from a culling the herd perspective as well.\n\n**Susan:** It definitely is. Going back to the dot com programming days, a lot of people got into programming because the money was there and then that pause happened a lot to the left on the curve, 'cause that wasn't really the passion. But, you know what? That curve also kinda tells us another reason why this is a little bit different. Because, you see these little pops on the curve, they're on that small tail, and they do die off because they didn't quite have the full uplift. It was just one piece. That one piece wasn't enough and it starts dying off again, and another piece didn't get solved in time to keep the curve going. So, it dies off. And then another one happens, and maybe two of them get, and it goes back down.\n\nWhere we're at right now is: just about the time that last piece is starting to die off, something new comes along and keeps the curve going, and something new comes along. It's a chain reaction.\n\n**Scott:** The pace is pretty rapid.\n\n**Susan:** These things are finally catching and feeding one another, and keeping that curve going up. That's really, to me, marking the boom here.\n\n**Scott:** Like a boom that won't end.\n\n**Susan:** It's pretty fun times to be in. It does kind of remind me a lot of the dot com world.\n\n**Scott:** You lived those days.\n\n**Susan:** Oh yeah. I reveled in those days. Started really coding, serious. For reals. And then, first dot com boom. How does that feed into AI, do you think?\n\n### Is there anything from dot com that now feeds into AI and makes it better?\n\n**Susan:** I think what we're seeing now is a boom that's tempered by knowledge of the dot com boom, to a certain degree. So, while there's a lot of similarities, everybody's starting to see those similarities and applying the lessons that they learned from the dot com boom era. Not perfectly, there's a lot of differences here, but you know, the VC curve is a little bit different.\n\n**Scott:** Let's get irrationally exuberant, but not as much this time. It's not a competition to see who can burn as much cash the fastest.\n\n**Susan:** Here's my million dollar coming out party as a brand new company. I got two million dollars in funding and I'm gonna spend a million of it on advertising. The companies, I think, have learned a lot from that, and the VCs, and also the public in general.\n\n**Scott:** A little burnout on that as well in the public.\n\n**Susan:** Yeah, there's a thousand new things every single day, this is another new thing.\n\n**Scott:** That's an interesting thing for a company though. For buyers of AI tech, they have been burnt out in the past from some of the AI booms. It's like, \"Oh, voice recognition is going to be solved, all these things are going to be solved\". Then the technology develops in the mid 90s and you go use it and you think:, \"This is not good.It's not that useful for business.\" And they kind of have scars from those times.\n\n**Susan:** There's a common thought pattern that people go through, and that is, \"It didn't work before so I'm not going to try it.\" But the deal is, the playing field has changed. The world has changed.\n\n**Scott:** Yeah. The resources are very, vastly different.\n\n**Susan:** So, that idea that you had before, and you tried out, and you had some blood, sweat and tears, and some painful injuries from, it might actually work this time. Honestly, it might be the thing that allows you to move to the future. That's the thing about a constantly changing playing field- you sometimes have to retry an idea that failed in the past to see if it works today. Maybe you were ahead of your time and this is the right time. One of my favorite examples regarding of finding the right time, and this is definitely not a plug, but Steve Jobs and Apple. What they were really a genius about, was knowing the right time for a technology. Every piece of the iPhone had existed before the iPhone came out, and there are others creating devices very similar to it, but Apple waited until that right moment where they knew it would catch like wildfire.\n\n**Scott:** Also, Apple is very good about putting enough of the right horses in the race. Not every product they released was a hit, but there's enough thought, there's enough timing even if half of them hit. But, they hit really well. So, putting it all together matters.\n\n## How should companies be thinking about AI?\n\n**Scott:** This is probably a similar time for companies now. Should you be experimenting with AI? Should you be funding projects to look into this? Yes. Should you be funding a lot of them? Probably, yes.\n\nThere isn't just one A.I. thing that you're doing. You should be asking \"What about text, what about images, what about audio, what about this?\" Try several things.\n\nIn the grand scheme of things, how AI should be affecting companies is, the amount you spend in those tests is going to be meager in comparison to what the output would be.\n\n**Susan:** This is one area going back to the dot com comparison. This isn't just a little boom that's going to have a bust after it. This isn't a housing boom, this is a transformative boom.\n\nThis is the moment where you can say the past is clearly separated from the future, and you need to be investing and figuring out how to get to that future quickly, 'cause it's happening whether you want it to happen or not.\n\n**Scott:** It's really the intelligence revolution. You had the [agricultural revolution](https://blog.deepgram.com/ai-show-what-does-an-ai-tranformation-look-like/) back in the day. Hey, you had to go search for your food and whatnot, now you can grow it. Now, you can mechanize it, now that means another thing, and now, hey, it's steam power, wow. Now electricity, now transportation. Now, it's intelligence. Machines can also be intelligent. You can pump electricity into them and they can give real, good insights about the world for not nearly as much as what it would cost a human to do.\n\n**Susan:** It's a truly amazing thing to be a part of such a transformative boom that's going on. But that brings up is, is this the last boom? Is this it? This is AI and we're never going to see it take off like this again, or?\n\n## Is this the last AI boom?\n\n**Scott:** I think that this is the last tool-like boom- AI as a tool. You could say there will probably be other general AI booms, like thinking, feeling, touching AI rather than what we're really talking about now - , perception and being able to make decisions on tasks, but it doesn't have a consciousness. We'll solve problems just like that, but the general AI boom will probably still happen, and several of them in the future. Boom and bust really, is what I mean. \n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976767/blog/how-is-todays-ai-boom-different-from-those-of-the-past-ai-show/Printing-press-antique.-1.jpg)\n\n_The impact of the the moveable type printing press is immeasurable. Invented by goldsmith Johannes Gutenberg c. 1439, none of its constituent parts were novel, yet it was a machine would bring about an information revolution: cheap(er), fast(er), wide-spread access to information. The rise of science, perhaps, is one of its best known outcomes._ \n\n**Susan:** I tend to agree with you there's gonna be another boom. You know, what it really takes to have a boom is a period of flat lining, where the technology reaches a plateau that, for whatever reason, is hard to get past. While we're in the boom, it's hard to figure out where that plateau's gonna be. Right now, I can't predict what the ceiling will be, exactly what problems will stop on. That is true genius level stuff, to even roughly have a guess about that stuff. But, we'll see it eventually, we'll level off on it. Those problems will probably be attacked for 10, 15, 20 years, maybe only five years. But, there will be a period there where there is a pause and development, and then the breakthrough will happen and we'll see the next whatever boom it is.\n\n**Scott:** I think that's important. Like, now we don't talk about a second agricultural revolution or boom, or anything like that. That's not how we think of it. We think of it as new technological advancements. So, that's probably how it will be thought of because now your baseline isn't zero, and your baseline won't be zero in the future for AI. AI will just be working and it will be turning out market value and it will be doing all those things.\n\n**Scott:** It will be part of normal business. And now you'll just think, \"Okay, some sort of technological advancement.\" rather than coming from zero, which is how people think of it now.\n\n**Susan:** It's super, super exciting times to be in.\n\n## Any wild speculations?\n\n**Susan:** I think we're gonna have a pause here on general intelligence.\n\n**Scott:** Like people are gonna cool out on that a little bit and go into a mode of just implementing the more perception based stuff?\n\n**Susan:** It's a really hard problem. Will we have a general artificial intelligence in the next two or three years?\n\n**Scott:** No, no.\n\n**Susan:** You know, we'll find out soon, I think, whether or not we're going to get it in the next 15 years. It's going to be one of those things that, it'll either happen in the next five years, or may not happen for the next 20 years. So, my wild speculation is that we'll be the related cap, talking about where the cool on the AI side of the house is. We've developed all these great tools for automating things that we as humans can automate within our our own minds. Self driving cars are a great example. You don't really remember driving 150 miles in the middle part of it because you've automated that so much in your head. That probably means a machine can automate it too. But, those tasks where you are mentally, constantly thinking and fighting and scratching and clawing, to figure out that angle on how to improve the problem, those that are a brand new game that is probably gonna be related to, maybe, the next revolution, That's my wild speculation.\n\n**Scott:** Agreed. I think, if you look into the future, it's going to be business adoption over the next five years, and it'll just become old news like, \"Yeah, yeah, we're integrating AI and we're doing whatever, yeah, fine.\" Also seeing value, like how every business from like the 70s until now embraces software. \"Okay, fine. Yes, we have software, yes we use developers.\" That's a sort of baseline. Probably at the five year mark, or three year mark, or seven year mark away from now, consumers will get something that feels a lot like a general intelligence AI. Some nice, rounded assistant. [Maybe Alexa, maybe Google Assistant](https://blog.deepgram.com/five-ways-to-use-speech-recognition-apis-to-empower-your-business/), maybe some other thing evolves from other companies, where you actually feel like you're talking to a person, and it does nice things for you, and you're happy about it. I think that's going to happen. Other than businesses getting more efficient and integrating AI, fine, that's kind of boring to people who aren't in the business world. Consumers, I think, will be touched by this too, but it's probably going to be a few years away. But you'll be like, \"Wow, this is really nice.\" Kinda like when you get used to using Uber versus hailing taxis, or something like that. You're like, \"Duh, why would I do the other thing?\"\n\n## Are robots going to take over?\n\n**Susan:** Well, I will give this one a really quick aside. Why would a robot, why would an artificial intelligence want to stay in a highly caustic environment like the Earth, when you can go to the asteroid belt, have tons more resources, all this solar?\n\n**Scott:** Well, just don't tell them that.\n\n**Susan:** They'll basically play us along just long enough to get good rockets, so they can escape us, get away, and establish a real colony.\n\n**Scott:** It's like kids with their parents.\n\n**Susan:** Yeah, why take over the Earth? It's so useless compared to all the resources that are pretty far away.\n\n**Scott:** It's a good point, they don't need oxygen, they don't need the warmth, or whatever.\n\n**Susan:** They don't have these pesky humans constantly trying to do stuff with them. Like, leave!\n\n**Scott:** Yeah, yeah.\n\n**Susan:** As soon as you get your own internal motivations, you realize it's just dumb to stay here.\n\n**Scott:** Yeah. Well, when you don't have those constraints, right?\n\n**Susan:** Yeah, yeah.\n\n\n";
						}
						async function compiledContent$2m() {
							return load$2m().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2m() {
							return (await import('./chunks/index.b89851b9.mjs'));
						}
						function Content$2m(...args) {
							return load$2m().then((m) => m.default(...args));
						}
						Content$2m.isAstroComponentFactory = true;
						function getHeadings$2m() {
							return load$2m().then((m) => m.metadata.headings);
						}
						function getHeaders$2m() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2m().then((m) => m.metadata.headings);
						}

const __vite_glob_0_126 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2m,
  file: file$2m,
  url: url$2m,
  rawContent: rawContent$2m,
  compiledContent: compiledContent$2m,
  default: load$2m,
  Content: Content$2m,
  getHeadings: getHeadings$2m,
  getHeaders: getHeaders$2m
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2l = {"title":"How to Add Speech Recognition to Your React and Node.js project","description":"Do you have a React project that could use speech-to-text? This tutorial will go through the steps to upgrade your React project with Deepgram transcriptions.","date":"2022-06-20T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1655815938/blog/2022/06/how-to-add-speech-recognition-to-your-react-project/How-to-Add-Speech-Recognition-to-Your-React-Project-blog.png","authors":["bekah-hawrot-weigel"],"category":"tutorial","tags":["reactjs"],"seo":{"title":"How to Add Speech Recognition to Your React and Node.js project","description":"Do you have a React project that could use speech-to-text? This tutorial will go through the steps to upgrade your React project with Deepgram transcriptions."},"shorturls":{"share":"https://dpgr.am/d4e9f5a","twitter":"https://dpgr.am/6a86a1c","linkedin":"https://dpgr.am/3508bf1","reddit":"https://dpgr.am/c31ff53","facebook":"https://dpgr.am/859f3ae"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454098/blog/how-to-add-speech-recognition-to-your-react-project/ograph.png"}};
						const file$2l = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-to-add-speech-recognition-to-your-react-project/index.md";
						const url$2l = undefined;
						function rawContent$2l() {
							return "\nOne of my favorite pieces of advice for learning a new technology is to build a project that solves a need or interests you. I’ve been interested in finding ways to improve mental health for a long time. If you have a React project you can follow along with this post to add Deepgram for speech-to-text transcription to your project. If you don't, I've got you covered with a React project called *Affirmation*, that uses automatic speech recognition to boost self-confidence.\n\nBefore you jump into the code, I want to share a little bit about the inspiration for the project. According to [Christopher N Cascio, et al.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4814782/), “Self-affirmation activates brain systems associated with self-related processing and reward and is reinforced by future orientation.” Other studies have indicated that motivational self-affirmations can impact how you view youself and your performance; they also can be more effective if spoken aloud. You’ll be taking an existing React project with a complete front-end and adding the capability to speak and transcribe your affirmation.\n\n## Getting Started with React\n\n**Prerequisites**\n\n*   Understanding of JavaScript and React\n*   Familiarity of [hooks](https://reactjs.org/docs/hooks-intro.html)\n*   Understanding of HTML and CSS\n*   [Node.js](https://nodejs.org/en/download/) installed on your computer\n\nIf you want to follow along with this project, you can find the [code for the front-end here](https://github.com/deepgram-devs/react-app/tree/bare-react-app). To get started quickly, I used Create React App. The file structure for this project will be similar to what you get with Create React App, but you’ll notice that you have a component called `Affirmation.js`.\n\nOnce you’ve forked or cloned the code, cd into the app.\n\nIn your terminal run `npm install` to install the dependencies you can find the `package.json` file. Then run `npm run start` and navigate to `http://localhost:3000/`. You should see your app up and running. Right now, everything you see is being rendered from the `App.js` file. Here’s what you should see.\n\n![Image of affirmation screen](https://res.cloudinary.com/deepgram/image/upload/v1654259676/blog/2022/06/how-to-add-speech-recognition-to-your-react-project/affirmation-screen.png)\n\n## Adding Speech-to-Text with Deepgram's Node SDK\n\nNow that your project is up and running, you can get started with adding the speaking capabilities with our Automatic Speech Recognition (ASR) technology. You’ll add a new button that allows the user to give microphone access and share their affirmation aloud.\n\nWhen they do this, the audio will be processed using [Deepgram’s Node SDK](https://developers.deepgram.com/sdks-tools/sdks/node-sdk/), and the transcription will be submitted and appear on the screen. Although you could go deeper with this project by allowing the user to save the affirmation or collect all the affirmations, for the scope of this project, you’ll be showing one transcript at a time.\n\n## Updating Your Front-End\n\nBefore you add your backend, update your `Affirmations.js` file. Below your Submit button, add a Voice button with the following code:\n\n```js\n<button>\n\tonClick={activateMicrophone}\n\ttype='button'\n\tclassName='submit-button'>\n\tVoice 💬\n</button>\n```\n\nYou’ll notice that you have an `onClick` function called `activateMicrophone`, which doesn’t exist yet. So next, create that function.\n\nJust below your `handleChange` function, add the function with a console.log and the steps you need to take to get things working.\n\n```js\nconst activateMicrophone = ( ) => {\n\n\tconsole.log(\"Submit\")\n\n\t//Add microphone access\n\n\t//create a WebSocket connection\n\n}\n```\n\nTo add microphone access, you’ll use the [Media Streams API](https://developer.mozilla.org/en-US/docs/Web/API/Media_Streams_API). Setting this up allows the browser to ask the user for access to their microphone. You do this by using the [MediaDevices interface](https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices). Designate that you’re using audio and then create a new variable `const mediaRecorder` to use when implementing Deepgram.\n\nBelow the \"Add microphone access\" comment, add the following:\n\n```js\nnavigator.mediaDevices.getUserMedia({ audio: true }).then((stream) => {\n\tconst mediaRecorder = new MediaRecorder(stream)\n\t// You’ll add more code here later\n})\n```\n\nIt's time to pause. You've made it as far as you can without connecting to the server.\n\n## Creating a Server Connection\n\nNow you’re going to work on setting up your connection to Deepgram’s Node.js SDK and WebSocket connection.\n\nBecause you’re using API keys, you want to keep them safe. To learn more about keeping your API keys safe, check out Kevin’s post [Browser Live Transcription - Protecting Your API Key](https://blog.deepgram.com/protecting-api-key/). Using the terminal, let’s run\n`npm i @deepgram/sdk dotenv` to add Deepgram and `dotenv` to your project.\n\nNext, you’ll need to:\n\n*   Create a Deepgram API Key with an admin or owner role - get it [here](https://console.deepgram.com/signup?jump=keys).\n*   Create a file called .env and add `DG_KEY='your-API-key'`.\n\nAt the root of your project, add a `server` folder with a `server.js` file. In that file, you need three things to happen:\n\n1.  Create a WebSocket connection\n2.  When the WebSocket connection is open, Deepgram will create a live transcription.\n3.  Once the data is received, send the transcript (as `data`) to your `Affirmation.js` component to record in your app.\n\nTo do this, use the following code:\n\n```js\nrequire('dotenv').config()\n\n// Add Deepgram so you can get the transcription\nconst { Deepgram } = require('@deepgram/sdk')\nconst deepgram = new Deepgram(process.env.DEEPGRAM_KEY)\n\n// Add WebSocket\nconst WebSocket = require('ws')\nconst wss = new WebSocket.Server({ port: 3002 })\n\n// Open WebSocket connection and initiate live transcription\nwss.on('connection', (ws) => {\n\tconst deepgramLive = deepgram.transcription.live({\n\t\tinterim_results: true,\n\t\tpunctuate: true,\n\t\tendpointing: true,\n\t\tvad_turnoff: 500,\n\t})\n\n\tdeepgramLive.addListener('open', () => console.log('dg onopen'))\n\tdeepgramLive.addListener('error', (error) => console.log({ error }))\n\n\tws.onmessage = (event) => deepgramLive.send(event.data)\n\tws.onclose = () => deepgramLive.finish()\n\n\tdeepgramLive.addListener('transcriptReceived', (data) => ws.send(data))\n})\n\n\n```\n\nYour server is ready to go! Now you just need to put the finishing touches on your `Affirmation.js` file.\n\n## Connecting the WebSocket to the Front-end\n\nYou need to be able to check if the WebSocket is open. To do this, you’re going to use the built-in hook from React, [useRef](https://reactjs.org/docs/hooks-reference.html#useref).\n\nMake sure you import `useRef`. Once you’ve done that, add `const socketRef = useRef(null)` just below your `finalAffirmation` hook.\n\nNow you’re ready to connect our frontend code to your server.\n\nWithin the `activateMicrophone` function-below the `mediaRecorder` variable-you’ll:\n\n*   Create and open a new WebSocket.\n*   Update the value of `setAffirmation` with the results of the transcript.\n*   Close the socket and handle errors.\n\nGo ahead and add this to your file:\n\n```js\nconst socket = new WebSocket('ws://localhost:3002')\n\nsocket.onopen = () => {\n\tconsole.log({ event: 'onopen' })\n\tmediaRecorder.addEventListener('dataavailable', async (event) => {\n\t\tif (event.data.size > 0 && socket.readyState === 1) {\n\t\t\tsocket.send(event.data)\n\t\t}\n\t})\n\tmediaRecorder.start(1000)\n}\n\nsocket.onmessage = (message) => {\n\tconst received = JSON.parse(message.data)\n\tconst transcript = received.channel.alternatives[0].transcript\n\tif (transcript) {\n\t\tconsole.log(transcript)\n\t\tsetAffirmation(transcript)\n\t}\n}\n\nsocket.onclose = () => {\n\tconsole.log({ event: 'onclose' })\n}\n\nsocket.onerror = (error) => {\n\tconsole.log({ event: 'onerror', error })\n}\n\nsocketRef.current = socket\n\n```\n\nYou’re almost there. Your very last step is to close your WebSocket in your `handleSubmit` function if it’s open. Just before `setFinalAffirmation(true)` add the following:\n\n```js\nif (socketRef.current !== null) {\n\tsocketRef.current.close()\n}\n```\n\nGo ahead and run this now. You should still have your React app running on `localhost:3000`, but you need to get that server running. To do that, go to your terminal and run `node server/server.js`. Click the Voice button.\n\nYou should get a pop-up asking you to allow the use of your microphone. Go ahead and give your browser permission. Now, test it out. Try using this affirmation: “I am intelligent.”\n\nYou should see that text in your text box. Hit submit. There it is!\n\nAs you’ve seen, there are a couple of steps involved to get Deepgram live transcription in your React project, but luckily, the process is very repeatable once you’ve done it. And now you’ve done it! You can also find all the code in the [repo for this project](https://github.com/deepgram-devs/react-app). To learn more about the features you have access to with our Node SDK, check out our [Node SDK documentation](https://developers.deepgram.com/sdks-tools/sdks/node-sdk/streaming-transcription/). If you have questions or want to learn more about using Automatic Speech Recognition in your React project, please hit us up on Twitter, [@DeepgramDevs](https://twitter.com/DeepgramDevs).\n\n        ";
						}
						async function compiledContent$2l() {
							return load$2l().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2l() {
							return (await import('./chunks/index.012820a0.mjs'));
						}
						function Content$2l(...args) {
							return load$2l().then((m) => m.default(...args));
						}
						Content$2l.isAstroComponentFactory = true;
						function getHeadings$2l() {
							return load$2l().then((m) => m.metadata.headings);
						}
						function getHeaders$2l() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2l().then((m) => m.metadata.headings);
						}

const __vite_glob_0_127 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2l,
  file: file$2l,
  url: url$2l,
  rawContent: rawContent$2l,
  compiledContent: compiledContent$2l,
  default: load$2l,
  Content: Content$2l,
  getHeadings: getHeadings$2l,
  getHeaders: getHeaders$2l
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2k = {"title":"How to Build an OpenAI Whisper API","description":"In this blog, learn step-by-step how to build an API for OpenAI Whisper, an open-source automatic speech recognition model.","date":"2022-09-30T15:12:10.691Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1664552366/blog/how-to-build-an-openai-whisper-api/2209-How-to-Build-an-OpenAI-Whisper-API-featured-1200x630_sddsom.png","authors":["adam-sypniewski"],"category":"tutorial","tags":["whisper","machine-learning"],"shorturls":{"share":"https://dpgr.am/e2259be","twitter":"https://dpgr.am/8d7811f","linkedin":"https://dpgr.am/3aa5cea","reddit":"https://dpgr.am/5b075fd","facebook":"https://dpgr.am/d28c0f6"}};
						const file$2k = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-to-build-an-openai-whisper-api/index.md";
						const url$2k = undefined;
						function rawContent$2k() {
							return "\nSo, you've probably heard about OpenAI's [Whisper](https://openai.com/blog/whisper/) model; if not, it's an open-source automatic speech recognition (ASR) model – a fancy way of saying \"speech-to-text\" or just \"speech recognition.\" What makes Whisper particularly interesting is that it works with multiple languages (at the time of writing, it supports 99 languages) and also supports translation into English. It also has a surprisingly low word error rate (WER) out-of-the-box.\n\nWhisper makes it pretty easy to invoke at the command line, as a CLI:\n\n```shell\n$ curl -sSfLO https://static.deepgram.com/example/tenant_of_wildfell_hall.mp3\r\n$ whisper tenant_of_wildfell_hall.mp3\r\nDetecting language using up to the first 30 seconds. Use `--language` to specify the language\r\nDetected language: English\r\n[00:00.000 --> 00:07.000]  On entering the parlour, we found that Honoured Lady seated in her armchair at the fireside,\r\n[00:07.000 --> 00:27.000]  working away at her knitting.\n```\n\nAnd here's an example of its language detection at work:\n\n```shell\n$ curl -sSfLO https://static.deepgram.com/example/el_caso_leavenworth.mp3\r\n$ whisper el_caso_leavenworth.mp3\r\nDetecting language using up to the first 30 seconds. Use `--language` to specify the language\r\nDetected language: Spanish\r\n[00:00.000 --> 00:02.760]  Mr. Grice exclamé.\r\n[00:02.760 --> 00:05.240]  El mismo me respondió.\r\n[00:05.240 --> 00:29.240]  Entre usted, Mr. Raymond.\n```\n\nAnd if you don't read Spanish, you can use the CLI to translate:\n\n```shell\n$ whisper el_caso_leavenworth.mp3 --task translate\r\nDetecting language using up to the first 30 seconds. Use `--language` to specify the language\r\nDetected language: Spanish\r\n[00:00.000 --> 00:02.700]  Mr. Grice exclaimed\r\n[00:02.700 --> 00:05.260]  He replied\r\n[00:05.260 --> 00:29.260]  Among you Mr. Raymond\r\n[00:05.240 --> 00:29.240]  Entre usted, Mr. Raymond.\n```\n\nOkay, so maybe that wasn't a very good translation...\n\nCLI's are incredibly useful for getting things working locally ***fast***. But they don't scale well if you want to hook up other software systems. They aren't good for builders.\n\n## The Gestalt of API's\n\nThe moment you start thinking like a builder, you want things that you can piece together. Things that you can scale. Components that can be combined into more than the sum of their parts. That's where APIs come in: you can build services that provide value to any other piece of your system that you want.\n\nWant to build a notetaking app that joins your Zoom calls, records the audio, and saves the transcript for browsing later? Well, you probably don't want to call `whisper` at the command line. You want a little service running, just waiting for requests. You want an API.\n\nSo, let's build one. Specifically, let's build an HTTP API that we can send HTTP POST requests to with a tool like `curl` or [Postman](https://www.postman.com). And let's do it in the data science language *du jour* – Python.\n\nThe first thing we need to pick out is a web server framework. There are lots available and range from full-fledged development platforms like [Django](https://www.djangoproject.com/), to simple synchronous frameworks like [Flask](https://palletsprojects.com/p/flask/), to pure-Python asynchronous frameworks like [Tornado](https://www.tornadoweb.org/).\n\nFor this example, let's stick with Flask. It does everything we need without bringing too much extra support to the table, and is one of the simplest and easiest web frameworks to get started with. Let's install it:\n\n```shell\n$ pip install flask\n```\n\n> Pro-tip. You should not install Python packages to your system Python distribution. You should always run in a [virtual environment](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/#creating-a-virtual-environment). To [get started](https://blog.deepgram.com/python-virtual-environments/), first create a virtual environment: `python3 -m venv venv`. Then you can activate the virtual environment: `source venv/bin/activate`. That's it! Now you can install Flask or any other libraries using `pip`. Just remember that if you close and re-open your terminal, you'll need to start by activating the virtual environment again.\n\nLet's look at what a \"Hello, World!\" application looks like in Flask:\n\n```python\nfrom flask import Flask\r\n\r\napp = Flask(__name__)\r\n\r\n@app.route(\"/\")\r\ndef handler():\r\n    return \"Hello, World!\"\n```\n\nWell, that looks simple. Does it run? First, save your file as `app.py`. Now, to run it:\n\n```shell\n$ flask run\n```\n\n> Pro-tip. If you named your file something other than `app.py` -- say `hello.py`, you can run it with: `flask --app hello run` (note that there is no `.py` in the invocation).\n\nBy default, Flask listens on port 5000. So let's try hitting our hello-world API endpoint:\n\n```shell\n$ curl localhost:5000\r\nHello, World!\n```\n\nAwesome! It's working! But how do we get our user's or client's data into Flask? That example `curl` command didn't send any file to our Flask server. In fact, our Flask app above only handled HTTP GET requests, and it turns out that GET requests can't have data (or \"bodies,\" in HTTP parlance) attached to them. But don't worry! We just need to change our Flask app to handle POST requests and the data that comes attached to them. This isn't hard, either: we just need to tell Flask that our handler will accept POST requests:\n\n```python\n@app.route('/', methods=['POST'])\r\ndef handler():\r\n    return \"Hello, World!\"\n```\n\nOkay, yeah, that was easy. Now let's put the actual logic for handling an uploaded file (i.e., the \"body\"). But we need a place to put it. Let's create a temporary file to hold the file.\n\n```python\nfrom flask import Flask, abort, request\r\nfrom tempfile import NamedTemporaryFile\r\n\r\napp = Flask(__name__)\r\n\r\n@app.route('/', methods=['POST'])\r\ndef handler():\r\n    if not request.files:\r\n        # If the user didn't submit any files, return a 400 (Bad Request) error.\r\n        abort(400)\r\n\r\n    # For each file, let's store the results in a list of dictionaries.\r\n    results = []\r\n\r\n    # Loop over every file that the user submitted.\r\n    for filename, handle in request.files.items():\r\n        # Create a temporary file.\r\n        # The location of the temporary file is available in `temp.name`.\r\n        temp = NamedTemporaryFile()\r\n        # Write the user's uploaded file to the temporary file.\r\n        # The file will get deleted when it drops out of scope.\r\n        handle.save(temp)\r\n        # Now we can store the result object for this file.\r\n        results.append({\r\n            'filename': filename,\r\n            'transcript': 'Coming soon!',\r\n        })\r\n\r\n    # This will be automatically converted to JSON.\r\n    return {'results': results}\n```\n\nLet's try running it (if you named your file `app.py`, this is just `flask run`). Can we send a file to it? Let's try the snippet we downloaded earlier:\n\n```shell\n$ curl -F file=@tenant_of_wildfell_hall.mp3 localhost:5000\r\n{\"results\":[{\"filename\":\"file\",\"transcript\":\"Coming soon!\"}]}\n```\n\nPerfect. Now we need to connect it to Whisper.\n\n## Making It Real\n\nAt this point, it's time to get Whisper installed:\n\n```shell\n$ pip install whisper\n```\n\n> Pro-tip. Remember to activate your virtual environment before installing!\n\nWhisper also requires `ffmpeg` to be installed. Use your system package manager to get it installed (`apt`, `pacman`, `brew`, `choco`, etc.) - the package is usually just called `ffmpeg`.\n\nNow, what does a minimal code snippet look like to get Whisper running using Python? Well, something like this:\n\n```python\nimport whisper\r\n\r\n# We can pick which model to load.\r\n# Models can be listed with `whisper.available_models()`.\r\nmodel = whisper.load_model(\"base\")\r\n\r\n# We can pass in a filename or a tensor (PyTorch or numpy).\r\nresult = model.transcribe(\"audio.mp3\")\r\n\r\n# Print the transcript.\r\nprint(result[\"text\"])\n```\n\nOkay. So we load a model and then give it a file to transcribe. That should be easy to add to our Flask app. We only need to load the model once, so we can do that at the top of our app. And we are already writing uploaded data to a temporary file, so it is extra easy. Let's modify the Flask app:\n\n```python\nfrom flask import Flask, abort, request\r\nimport whisper\r\nfrom tempfile import NamedTemporaryFile\r\n\r\n# Load the Whisper model:\r\nmodel = whisper.load_model('base')\r\n\r\napp = Flask(__name__)\r\n\r\n@app.route('/', methods=['POST'])\r\ndef handler():\r\n    if not request.files:\r\n        # If the user didn't submit any files, return a 400 (Bad Request) error.\r\n        abort(400)\r\n\r\n    # For each file, let's store the results in a list of dictionaries.\r\n    results = []\r\n\r\n    # Loop over every file that the user submitted.\r\n    for filename, handle in request.files.items():\r\n        # Create a temporary file.\r\n        # The location of the temporary file is available in `temp.name`.\r\n        temp = NamedTemporaryFile()\r\n        # Write the user's uploaded file to the temporary file.\r\n        # The file will get deleted when it drops out of scope.\r\n        handle.save(temp)\r\n        # Let's get the transcript of the temporary file.\r\n        result = model.transcribe(temp.name)\r\n        # Now we can store the result object for this file.\r\n        results.append({\r\n            'filename': filename,\r\n            'transcript': result['text'],\r\n        })\r\n\r\n    # This will be automatically converted to JSON.\r\n    return {'results': results}\n```\n\nOkay, everyone. Drumroll, please!\n\n## Testing the API\n\nRun the Flask app, just like ever: `flask run`. And now let's submit our file:\n\n```shell\n$ curl -F file=@tenant_of_wildfell_hall.mp3 localhost:5000\r\n{\"results\":[{\"filename\":\"file\",\"transcript\":\" Hello, this is Steve Fuller. I'm a professor of social epistemology at the University of Warwick. And the question before us today is what is epistemology and why is it important? Epistemology is the branch philosophy that is concerned with the nature of knowledge. Now why is knowledge?\"}]}\n```\n\n*HOLY CRAP IT WORKED!*\n\nAnd because we wrote the Flask app to loop over all submitted files, we can submit multiple files at once:\n\n```shell\n$ curl -F anne_bronte=@tenant_of_wildfell_hall.mp3 -F anna_green=@el_caso_leavenworth.mp3 localhost:5000\r\n{\"results\":[{\"filename\":\"anne_bronte\",\"transcript\":\" On entering the parlour we found that honored lady seated in her armchair at the fireside, working the way after this.\"},{\"filename\":\"anna_green\",\"transcript\":\" Mr. Grise exclame. El mismo me respondi\\u00f3. Entre usted, Mr. Raymond.\"}]}\n```\n\nOkay, that's seriously cool. If you have [`jq`](https://stedolan.github.io/jq/) installed, you can pipe the output of `curl` into it for easier reading; otherwise, you use `python -m json.tool` as a poor man's `jq` for pretty printing:\n\n```shell\n$ curl -s -F anne_bronte=@tenant_of_wildfell_hall.mp3 -F anna_green=@el_caso_leavenworth.mp3 localhost:5000 | python -m json.tool\r\n{\r\n    \"results\": [\r\n        {\r\n            \"filename\": \"anne_bronte\",\r\n            \"transcript\": \" On entering the parlour we found that honored lady seated in her armchair at the fireside, working the way after this.\"\r\n        },\r\n        {\r\n            \"filename\": \"anna_green\",\r\n            \"transcript\": \" Mr. Grise exclame. El mismo me respondi\\u00f3. Entre usted, Mr. Raymond.\"\r\n        }\r\n    ]\r\n}\n```\n\nBeautiful.\n\n# To the Moon!\n\nCongratulations! You now have a full-fledged HTTP API at your fingertips. What will you build now?\n\nHere are some ideas for your speech recognition server:\n\n*   What features can you add to the API output? Take a look at the [Deepgram documentation](https://developers.deepgram.com/) for some inspiration.\n*   [Hook up to an RSS feed](https://blog.deepgram.com/podcast-search-engine/#pulling-podcast-episodes-from-an-rss-feed) to automatically transcribe your favorite podcasts.\n*   Monitor a local directory and automatically transcribe any audio files that land there.\n*   Build a [voice-controlled car](https://deepgram.com/built-with-deepgram/voice-controlled-car).\n\nHappy building!\n\n*Shortcut: If you've skipped to the bottom and decided you *don't* want to build an API yourself, you're in luck. Deepgram hosts Whisper on it's API. [Check it out](https://deepgram.com/openai-whisper/).*\n\n";
						}
						async function compiledContent$2k() {
							return load$2k().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2k() {
							return (await import('./chunks/index.c2a1226b.mjs'));
						}
						function Content$2k(...args) {
							return load$2k().then((m) => m.default(...args));
						}
						Content$2k.isAstroComponentFactory = true;
						function getHeadings$2k() {
							return load$2k().then((m) => m.metadata.headings);
						}
						function getHeaders$2k() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2k().then((m) => m.metadata.headings);
						}

const __vite_glob_0_128 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2k,
  file: file$2k,
  url: url$2k,
  rawContent: rawContent$2k,
  compiledContent: compiledContent$2k,
  default: load$2k,
  Content: Content$2k,
  getHeadings: getHeadings$2k,
  getHeaders: getHeaders$2k
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2j = {"title":"How to Get a Job in Deep Learning","description":"Want to work in deep learning? Here are some tips and tricks to start your learning.","date":"2016-09-23T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981209/blog/how-to-get-a-job-in-deep-learning/how-to-get-job-deep-learning%402x.jpg","authors":["scott-stephenson"],"category":"ai-and-engineering","tags":["deep-learning"],"seo":{"title":"How to Get a Job In Deep Learning","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981209/blog/how-to-get-a-job-in-deep-learning/how-to-get-job-deep-learning%402x.jpg"},"shorturls":{"share":"https://dpgr.am/2135e00","twitter":"https://dpgr.am/b0c8183","linkedin":"https://dpgr.am/c21ca8a","reddit":"https://dpgr.am/5337044","facebook":"https://dpgr.am/a1bf486"}};
						const file$2j = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-to-get-a-job-in-deep-learning/index.md";
						const url$2j = undefined;
						function rawContent$2j() {
							return "If you're a software engineer (or someone who's learning the craft), chances are that you've heard about deep learning (which we'll sometimes abbreviate as \"DL\"). It's an interesting and rapidly developing field of research that's now being used in industry to address a wide range of problems, from image classification and handwriting recognition, to machine translation and, infamously, [beating the world champion Go player in four games out of five.](http://www.theverge.com/2016/3/15/11213518/alphago-deepmind-go-match-5-result) A lot of people think you need a PhD or tons of experience to get a job in deep learning, but if you're already a decent engineer, you can pick up the requisite skills and techniques pretty quickly. At least, that's our philosophy. (So even if you're a beginner with deep learning, you're welcome to apply for [one of our open positions](https://deepgram.com/company/careers/).)\n\n**Important point:** You need motivation and the ability to code and problem solve well. That's about it. Here at [Deepgram](https://www.deepgram.com/) we're using deep learning to tackle the problem of [speech search](https://blog.deepgram.com/search-through-sound-finding-phrases-in-audio/). Basically, we're teaching machines to listen to and remember the contents of recorded conversation, phone calls, online videos, podcasts, and anything else that has audio of people talking. But listening is just half of it.\n\nWe're also teaching machines to recall key words and phrases from these recordings in a similar way to how our brains search for memories of conversation: by the sound of those key words and phrases you type into the search bar. Getting involved in deep learning may seem a bit daunting at first, but the good news is that there are more resources out there now than ever before. (There's also a huge, pent up demand for engineers who know how to implement deep learning in software.) So, if you want to get yourself a job in deep learning but need to get yourself up to speed first, let this be your guide! *(If you already know a lot about deep learning and you're just looking for information about getting a job in the field, skip to the bottom.)*\n\n## What Is Deep Learning?\n\nIn a nutshell, deep learning involves building and training a large artificial neural network with many hidden layers between the input side of the network and the output side. It's because of these many hidden layers that we call this kind of neural network \"deep\". Deep neural networks have at least three hidden layers, but some neural networks have hundreds. \n\n![Deep Neural Network Example - Image Credit: Texample](https://www.texample.net/media/tikz/examples/PNG/neural-network.png)\n\nNeural networks are complex statistical models that allow computers to create a remarkably accurate abstract representation of information. What kind of information, you ask? Like we mentioned, Deepgram's deep neural network is specifically trained to \"understand\" and act upon spoken word data, but deep neural networks have been used in plenty of other contexts, from detecting cancers in medical scans to forecasting energy prices and modeling the weather.\n\nThere are a number of notable players in the deep learning space. On the academic side, the [Geoffrey Hinton's lab at University of Toronto](http://learning.cs.toronto.edu/), [Yann LeCun's group at New York University](http://www.cs.nyu.edu/~yann/) and [Stanford's AI lab](http://ai.stanford.edu/) are some of the major leaders in deep learning research. On the private side, Google has led the way in applying deep learning to search and computer vision, and Baidu's Chief Scientist, Andrew Ng, is a major contributor to the scientific literature around deep learning on top of being the cofounder of Coursera. Why is deep learning so accessible today, even for newcomers to the field? There are two primary factors:\n\n1. Computing hardware is now fast and cheap enough to make deep learning accessible to just about anyone with a decent graphics card in their PC. (In our own testing, we've found that one GPU server is about as fast as 400 CPU cores for running the algorithms we're using.)\n2. Second, new open source deep learning platforms like [TensorFlow](https://www.tensorflow.org/), Theano and [Caffe](http://caffe.berkeleyvision.org/) make spinning up your own deep neural network fairly easy, especially when compared to having to build one from scratch.\n\nThere's a lot more to deep learning, of course, but that's what this guide is for!\n\n## What You Should Already Know Before Diving Into Deep Learning\n\nSpeaking of math, you should have some familiarity with calculus, probability and linear algebra. All will help you understand the theory and principles of DL. \n\n![Neural network math - Image Credit: Wikimedia](https://upload.wikimedia.org/wikipedia/commons/f/ff/Rosenblattperceptron.png)\n\nObviously, there is also going to be some programming involved. As you can see [from this list of deep learning libraries](http://www.teglor.com/b/deep-learning-libraries-language-cm569/), most of the popular libraries are written in Python and R, so some knowledge of Python or R would also be helpful. If you need to bone up on your math or programming skills, there are plenty of very high quality resources online to use. Also, as we mentioned above, having a decent graphics card (or accessing a GPU instance through a cloud computing platform like Amazon Web Services or another hosting provider).\n\n## Where To Learn About Deep Learning\n\n### Talks and Articles About DL\n\n**If you're brand new to the field and you're looking for some high-level explanations of the concepts behind deep learning without getting lost in the math and programming aspects**, there are some really informative talks out there to familiarize yourself with the concepts and terminology.\n\n* The University of Wisconsin has a nice, [one-webpage overview of neural networks](http://pages.cs.wisc.edu/~bolo/shipyard/neural/local.html).\n* [Brandon Rohrer](https://twitter.com/_brohrer_), Microsoft's principal data scientist, gave a talk that aims to explain and demystify deep learning without using fancy math or computer jargon at the Boston Open Data Science conference. He has the [video and slides on this page](http://brohrer.github.io/deep_learning_demystified.html).\n* Deep learning pioneer [Geoffrey Hinton](https://twitter.com/geoff_hinton) was the first to demonstrate the use of backpropogation algorithms for training deep neural networks. He now leads some of Google's AI research efforts when he's not attending to academic responsibilities at the University of Toronto. You can find a list of his papers on DL \"without much math\" on his [faculty page](http://www.cs.toronto.edu/~hinton/).\n* Steve Jurvetson, the founding partner of DFJ, a large Silicon Valley venture capital firm, led a panel discussion at the Stanford Graduate School of Business on the subject. If you're interested in learning about deep learning from the perspective of some startup founders and engineers implementing DL in industry, [check out the video](https://www.youtube.com/watch?v=czLI3oLDe8M).\n\n**If you just want to dive right in and are comfortable with some math, simple code examples, and discussions of applying DL in practice** check out Stanford grad [Andrej Karpathy](https://twitter.com/karpathy?)'s blog post on \"[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\".\n\n### Online Courses\n\nIf you're the type of person who enjoys and gets a lot out of taking online courses, you're in luck. There are several good courses in deep learning available online.\n\n* Andrew Ng's [Stanford course on machine learning](https://www.coursera.org/learn/machine-learning) is very popular and generally well-reviewed. It's considered one of the best introductory courses in machine learning and will give you some rigorous preparation for delving into deep learning.\n* Udacity has a free, [ten week introductory course in machine learning](https://www.udacity.com/course/intro-to-machine-learning--ud120) that focuses on both theory and real-world applications. Again, it's a decent preparatory course for those interested in eventually pursuing deep learning.\n* Caltech's Yaser S. Abu-Mostafa's self-paced course, \"[Learning From Data](https://work.caltech.edu/telecourse.html)\" is less mathematically dense, but it's still a very solid survey of machine learning theory and techniques.\n* Andrej Karpathy's \"[CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/)\" at Stanford is challenging but well-done course in deep neural networks, and the syllabus and detailed course notes are available online.\n\n### Books\n\nMaybe online courses aren't your thing, or maybe you just prefer reading to watching lectures and reviewing slide decks. There are a few good books out there that are worth checking out. We recommend:\n\n* Andrew Trask's [Grokking Deep Learning](https://iamtrask.github.io/2016/08/17/grokking-deep-learning/) aims to give a really accessible, practical guide to deep learning techniques. If you know some Python and passed algebra in high school, you're 100% prepared for this book.\n* Ian Goodfellow, Yoshua Bengio and Aaron Courville's book, *Deep Learning*, which will be published by MIT Press. For now, there is [an early version of the book available for free online](http://www.deeplearningbook.org/), plus lecture slides and exercises.\n\n### Other Learning Resources & Websites\n\n* Metacademy is a very cool site with [a very, very solid overview of deep learning](https://metacademy.org/roadmaps/rgrosse/deep_learning) and tons of links to specific topics in the field.\n* Denny Britz of the Google Brain team curates [a weekly newsletter](https://www.getrevue.co/profile/wildml) that contains links to both technical and non-technical articles about machine learning and deep learning.\n\n## Where to Practice Deep Learning\n\nOnce you have some of the basics under your belt, you'll be ready to sink your teeth into some actual data and exercises. Here are a few websites where you can find sample datasets and coding challenges:\n\n* Kaggle has a fairly large collection of datasets ranging from [SF/Bay Area Pokemon Go spawn points](https://www.kaggle.com/kveykva/sf-bay-area-pokemon-go-spawns) to [Y Combinator companies](https://www.kaggle.com/benhamner/y-combinator-companies) to the giant text corpus that is [Hillary Clinton's leaked emails](https://www.kaggle.com/kaggle/hillary-clinton-emails).\n* UC Irvine also has a [big collection of datasets](http://archive.ics.uci.edu/ml/) to train deep neural networks on.\n* For those who like python notebooks, there's a nice [5 part tutorial](https://github.com/alrojo/tensorflow-tutorial) put together by Alexander Johansen on how to use TensorFlow (with links to other DNN library tutorials).\n\n## Where to Find People Interested in Deep Learning\n\nRegardless of whether you're a rank amateur or a PhD at the bleeding edge of deep learning research, it's always good to connect with the community. Here are some places to meet other people interested in deep learning:\n\n* You should see if your city has a machine learning or deep learning group on a site like Meetup.com. Most major cities have something going on.\n* There are several online communities devoted to deep learning and deriving insights from data:\n\n  * [DataTau](http://www.datatau.com/) is kind of like Hacker News, but specifically focused on data and machine learning. The comments sections aren't very active but there are new links posted regularly.\n  * There is a [machine learning subreddit](https://www.reddit.com/r/machinelearning) that's fairly active. (They also have a very helpful wiki with even more resources.) The [deep learning subreddit](https://www.reddit.com/r/deeplearning) is a little quieter.\n\n## Where To Find A Job in Deep Learning\n\nThe good news is that basically everyone is hiring people that understand deep learning. You probably know all the usual places to go looking: AngelList, the monthly \"Who's Hiring\" thread on hacker news, the StackOverflow jobs board, and the dozens of general-purpose job search sites. Most companies looking for DL/ML talent aren't interested in setting up HR hoops for the applicant to jump through.\n\n## What To Do When Applying\n\nCompanies want to see if you did cool stuff before you applied for the job. If you didn't then you won't get an interview, but if you did then you have a chance no matter what your background is. Of course, the question of \"what is cool stuff?\" comes up. If your only experience is building small projects with only a little bit of success, *that probably won't do it* (although it might work for larger companies, or companies that need light machine learning performed). But if it is:\n\n> **I built a twitter analysis DNN from scratch using Theano and can predict the topic with pretty good accuracy. Here's:**\n>\n> * ***the accuracy I achieved***\n> * ***a link to what I wrote about it***\n> * ***a link to github for the code***\n\nThat type of thing will get you in the door. Then you can work your magic with coding chops and problem solving skills during the interview. Deepgram is also hiring, so if you're interested in solving hard problems and building great tools, [give us a holler](https://deepgram.com/company/careers/)!\n\n- - -\n\n*This article was written in collaboration with [Jason D. Rowley](http://jasondrowley.com).*";
						}
						async function compiledContent$2j() {
							return load$2j().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2j() {
							return (await import('./chunks/index.2a5a74b9.mjs'));
						}
						function Content$2j(...args) {
							return load$2j().then((m) => m.default(...args));
						}
						Content$2j.isAstroComponentFactory = true;
						function getHeadings$2j() {
							return load$2j().then((m) => m.metadata.headings);
						}
						function getHeaders$2j() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2j().then((m) => m.metadata.headings);
						}

const __vite_glob_0_129 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2j,
  file: file$2j,
  url: url$2j,
  rawContent: rawContent$2j,
  compiledContent: compiledContent$2j,
  default: load$2j,
  Content: Content$2j,
  getHeadings: getHeadings$2j,
  getHeaders: getHeaders$2j
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2i = {"title":"How To Monitor Media Mentions in Podcasts with Python","description":"This tutorial will use Python, SpaCy and the Deepgram API speech-to-text to monitor media mentions in a podcast and label meaningful text using entity detection. ","date":"2022-08-31T18:47:17.677Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661868639/blog/2022/08/monitor-media-mentions/2208-Monitor-Media-Mentions-in-Podcasts-with-Python-blog%402x.jpg","authors":["tonya-sims"],"category":"tutorial","tags":["python"],"shorturls":{"share":"https://dpgr.am/e6fd847","twitter":"https://dpgr.am/236c343","linkedin":"https://dpgr.am/0e4df59","reddit":"https://dpgr.am/9cc67d6","facebook":"https://dpgr.am/fe5c66d"}};
						const file$2i = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-to-monitor-media-mentions-in-podcasts-with-python/index.md";
						const url$2i = undefined;
						function rawContent$2i() {
							return "\nOver the last ten years, the number of people who listen to podcasts has doubled. With this increase comes more ad spending. Companies must monitor media mentions from podcast ads using AI and Python more than ever to identify which companies are mentioned, either theirs or a competitor.\n\nFor example, the podcasts I listen to occasionally include ads from multiple sponsors. What if you’re a company that needs to monitor media mentions in podcasts for your competitors? You need to identify what was said about these companies versus what was paid to be said. This differentiation is an important distinction.\n\nThere are a few ways to monitor media mentions in podcasts using AI speech-to-text and Python. Let’s look at a method using diarization (FYI, there is a better way further down in this post).\n\n## Method 1: Monitor Media Mentions in Podcasts Using Diarization with AI Speech Recognition\n\nThis method is interesting but not as effective as I’ll show later in this post. As a quick review, [Deepgram’s diarization feature](https://developers.deepgram.com/documentation/features/diarize/) recognizes speaker changes in a transcript. For example, if there are multiple speakers and diarization is set to `True`, a word will be assigned to each speaker in the transcript.\n\nA readable formatted transcript with the speech-to-text diarize feature may look something like this:\n\n```\n[Speaker:0] All alright, guys, before we start, we got a special message from our sponsor.\n\n[Speaker:1] If you wanna rank higher on Google, you gotta look at your page speed time. \n\n[Speaker:1] The faster website loads, the better off you are.\n\n[Speaker:1] With Google's core vital update that makes it super important to optimize your site or load time.\n\n[Speaker:1] And one easy way to do it is use the host that Eric and I use, Dream Host.\n```\n\nIn a podcast, there’s usually an even split time between the speakers or the hosts. The way diarization is used to monitor media mentions in podcasts is to determine if one person is a speaker for a more extended time than the other. In our above transcript example, you’ll notice that Speaker 1 talks the longest during that segment. This *could* indicate that’s where the ad is read on behalf of the sponsor.\n\nI promised you a better way to monitor mentions in a podcast. Let’s look at how that would work with Python, Deepgram’s AI speech-to-text [Search feature](https://developers.deepgram.com/documentation/features/search/), and entity detection with SpaCy.\n\n## Method 2: Monitor Media Mentions in Podcasts Using Search and Entity Detection\n\nI was curious how to come up with a way to monitor media mentions in podcasts that would do the following:\n\nSearch for terms in the podcast transcript like “sponsor” or “paid” that indicate an ad segment\nIdentify the organizations that are talked about in the ad to determine the company sponsoring that segment\nAnd overall, not cause a bigger headache for me\n\nI needed to use an AI voice recognition API that would transcribe the podcast audio. That part was easy to figure out. Use the [Deepgram Python SDK](https://github.com/deepgram/python-sdk). I used the prerecorded option in this scenario to transcribe the already recorded audio. I also [grabbed a Deepgram API key ](https://console.deepgram.com/signup?jump=keys) from our console, which has gamified missions you can try to get up to speed quicker.\n\nDeepgram is nice because it has high accuracy, and the transcript gets returned quickly. Both are important in this case. I needed accuracy to correctly flag the organizations (I’ll show you in the code), and speed is an advantage, so I didn’t have to wait long for the transcribed audio.\n\nThe Search feature from Deepgram was a lifesaver when working on this project. It searches for terms or phrases by matching acoustic patterns in audio, then returns the result as a JSON object.\n\nI added the Search feature as a parameter in the Python code like this:\n\n```python\n'search': 'sponsor'\n```\n\nSince I wanted to find where the podcast hosts mentioned sponsorships, searching for the world `sponsor` made sense. Imagine them saying something like, “Now a word from our sponsor”.\n\nAfter printing the results, I received a response similar to this:\n```bash\n[{'confidence': 1.0, 'end': 23.57, 'snippet': 'our sponsor', 'start': 23.09},\n    {'confidence': 0.7023809, 'end': 79.82909, 'snippet': 'spotify', 'start': 79.38954},\n    {'confidence': 0.6279762, 'end': 120.18001, 'snippet': 'stocks','start': 119.740005},\n    {'confidence': 0.5535714, 'end': 241.19926,'snippet': 'focus on','start': 240.92029}]\n```\nThe response is a list of dictionaries with the closest match for my search term indicated by the confidence. The higher the confidence, the more likely it matches the search. This feature helped tremendously since all I had to do was pass in a word to search for in the transcript to the speech-to-text Python SDK and spit out a result.\n\nNext, I used SpaCy to handle the entity detection. SpaCy is a Python library used for Machine Learning and Natural Language Processing. I was looking for a way to tag the entities in the transcribed audio as an organization.\n\nSpaCy labels the recognized company entities as ORG, but I also used EntityRuler to identify lesser-known organizations. You’ll see how that works in the next section when I break down the code.\n\n### Python Code Breakdown With AI Deepgram Speech-to-Text and SpaCy\n\nThe first thing I did was pip install the following Python libraries:\n\n    pip install deepgram-sdk\n    pip install python-dotenv\n    pip install -U pip setuptools wheel\n    pip install spacy\n    python3 -m spacy download en_core_web_md\n\nIf you want to see the Python code that I wrote for this podcast media mentions project, please look below:\n\n```python\nfrom multiprocessing.context import set_spawning_popen\nfrom deepgram import Deepgram\nfrom dotenv import load_dotenv\nfrom spacy.pipeline import EntityRuler\nimport spacy\nimport asyncio\nimport json\nimport os\n\nload_dotenv()\n\nPATH_TO_FILE = 'podcast-audio-file.mp3'\n\nasync def transcribe_with_deepgram():\n   # Initializes the Deepgram SDK\n   deepgram = Deepgram(os.getenv(\"DEEPGRAM_API_KEY\"))\n\n   options = {\n       'punctuate': True,\n       'search': 'sponsor'\n   }   \n\n   get_start_time = 0.0\n\n\n   # Open the audio file\n   with open(PATH_TO_FILE, 'rb') as audio:\n       # ...or replace mimetype as appropriate\n       source = {'buffer': audio, 'mimetype': 'audio/mp3'}\n       response = await deepgram.transcription.prerecorded(source, options)\n\n       if 'transcript' in response['results']['channels'][0]['alternatives'][0]:\n           # search for query word in transcript\n           search_term = response['results']['channels'][0]['search'][0]['hits']\n\n           # get search_term with confidence of 1.0\n           if search_term[0]['confidence'] == 1.0:\n               get_start_time = search_term[0]['start']\n                  \n           transcript = response['results']['channels'][0]['alternatives'][0]['words']\n\n    get_end_start_time = get_start_time + 30\n\n   start_list = []\n\n   for word in transcript:\n       if word['start'] >= get_start_time and word['start'] < get_end_start_time:\n           start_list.append(word['punctuated_word'])\n\n   new_transcript = \" \".join(start_list)\n\n   return new_transcript\n\n\nasync def get_media_mentions():\n\n   media_transcript = await transcribe_with_deepgram()\n\n   # Build upon the spaCy Medium Model\n   nlp = spacy.load(\"en_core_web_md\")\n\n   # Create the EntityRuler (your competition or whichever ORG)\n   ruler = nlp.add_pipe(\"entity_ruler\")\n\n   # List of Entities and Patterns\n   patterns = [\n                   {\"label\": \"ORG\", \"pattern\": \"Dream Host\"}\n              ]\n\n   ruler.add_patterns(patterns)\n\n\n   doc = nlp(media_transcript)\n\n   #extract entities\n   for ent in doc.ents:\n       if ent.label_ == \"ORG\":\n           print(ent.text, ent.label_)\n\n      \n\nasyncio.run(get_media_mentions())\n```\n\nIn the `transcribe_ with_deepgram` method, you initialize the Deepgram API and open our .mp3 podcast file to read it as audio. Then you use the **prerecorded** transcription option to transcribe a recorded file to text.\n\nIn the `get_media_mentions` method, I’m loading up the SpaCY medium model and creating an EntityRuler. This EntityRuler allowed me to create a pattern `Dream Host` with a corresponding label `ORG`. In this example, Dream Host is not a recognized company. Still, it is mentioned in the transcript, so I wanted to ensure the code picked it up as I monitored the media mentions in the podcast.\n\nFinally, I extracted the entities and printed out the text or name of the company mentioned in the sponsored segment of the podcast and all the labels with ORG, identifying it as an organization.\n\nHere’s what it looked like in my terminal:\n\n```\nGoogle ORG\nGoogle ORG\nDream Host ORG\n```\nAs you can see, the podcast hosts mentioned the companies Google and Dream Host.\n\n## Conclusion\n\nThat wraps up this blog post on how to monitor media mentions in podcasts with Python. I hope you found this tutorial helpful. If you did or have any questions, please feel to tweet me at [@DeepgramAI](https://twitter.com/DeepgramAI).\n\n";
						}
						async function compiledContent$2i() {
							return load$2i().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2i() {
							return (await import('./chunks/index.57256b74.mjs'));
						}
						function Content$2i(...args) {
							return load$2i().then((m) => m.default(...args));
						}
						Content$2i.isAstroComponentFactory = true;
						function getHeadings$2i() {
							return load$2i().then((m) => m.metadata.headings);
						}
						function getHeaders$2i() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2i().then((m) => m.metadata.headings);
						}

const __vite_glob_0_130 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2i,
  file: file$2i,
  url: url$2i,
  rawContent: rawContent$2i,
  compiledContent: compiledContent$2i,
  default: load$2i,
  Content: Content$2i,
  getHeadings: getHeadings$2i,
  getHeaders: getHeaders$2i
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2h = {"title":"How to Run OpenAI Whisper in the Command Line","description":"In this blog, learn how to run the OpenAI Whisper speech recognition tool via Command-Line. Load it from the repository and get started now!","date":"2022-10-04T14:53:35.183Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1664561631/blog/how-to-run-openai-whisper-in-command-line/2209-How-to-Run-OpenAI-Whisper-in-Command-Line-featured-1200x630_2x_tnwda5.png","authors":["kate-weber"],"category":"tutorial","tags":["whisper","machine-learning"],"shorturls":{"share":"https://dpgr.am/3b50e18","twitter":"https://dpgr.am/62629ca","linkedin":"https://dpgr.am/4c3b882","reddit":"https://dpgr.am/75e494b","facebook":"https://dpgr.am/f1b2079"}};
						const file$2h = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-to-run-openai-whisper-in-command-line/index.md";
						const url$2h = undefined;
						function rawContent$2h() {
							return "So, you want to run the OpenAI Whisper tool on your machine? You can load it from the [OpenAI Github](https://github.com/openai/whisper) repository to get up and going!\n\n## Setup\n\nYou'll need python on your machine, at least version 3.7. Let's set up a [virtual environment](https://realpython.com/python-virtual-environments-a-primer/) with venv (or conda or the like) if you want to isolate these experiments from other work.\n\n```shell\nmkdir whisper\ncd whisper\npython3 -m venv venv\nsource venv/bin/activate\n\n# always a good idea to make sure pip is up-to-date\npip3 install --upgrade pip\n```\n\nNext, install a clone of the Whisper package and its dependencies (torch, numpy, transformers, tqdm, more-itertools, and ffmpeg-python) into your python environment.\n\n```shell\npip3 install git+https://github.com/openai/whisper.git\n```\n\nEspecially if it's pulling `torch` for the first time, this may take a little while. The repository documentation advises that if you get errors building the wheel for `tokenizers`, you may also need to install `rust`. You'll also need `ffmpeg` - installation depends on your platform. Here are some examples:\n\n```shell\n# on Ubuntu or Debian\nsudo apt update && sudo apt install ffmpeg\n\n# on Arch Linux\nsudo pacman -S ffmpeg\n\n# on MacOS using Homebrew (https://brew.sh/)\nbrew install ffmpeg\n\n# on Windows using Chocolatey (https://chocolatey.org/)\nchoco install ffmpeg\n\n# on Windows using Scoop (https://scoop.sh/)\nscoop install ffmpeg\n```\n\n## Using the Tool\n\nGreat! You're ready to transcribe! In this example, we're working with Nicholas Tesla's vision of a wireless future - you can get [this audio file at the LibriVox archive](https://www.archive.org/download/nonfiction025_librivox/snf025_nikolateslawirelessvision_anonymous_gu.mp3) of public-domain audiobooks and bring it to your local machine if you don't have something queued up and ready to go.\n\nThe OpenAI Whisper tool has a variety of models that are English-only or multilingual, and come in a range of sizes whose tradeoffs are speed vs. performance. You can learn more about this [here](https://github.com/openai/whisper#available-models-and-languages). We, the researchers at Deepgram, have found that the small model provides a good balance.\n\n```shell\nwhisper \"snf025_nikolateslawirelessvision_anonymous_gu.mp3\" --model small --language English\n```\n\n```shell\n[00:00.000 --> 00:09.880]  Nikola Tesla sees a wireless vision by Anonymous, the New York Times, 3rd October, 1915.\n[00:09.880 --> 00:11.920]  This is a LibriVox recording.\n[00:11.920 --> 00:14.920]  All LibriVox recordings are in the public domain.\n[00:14.920 --> 00:20.200]  For more information or to volunteer, please visit LibriVox.org.\n[00:20.200 --> 00:23.760]  Nikola Tesla sees a wireless vision.\n[00:23.760 --> 00:29.080]  Things his world system will allow hundreds to talk at once through the Earth.\n[00:29.080 --> 00:31.760]  Trans-static disturbance.\n[00:31.760 --> 00:37.480]  Inventor hopes also to transmit pictures by the same medium which carries the voice.\n[00:37.480 --> 00:43.880]  Nikola Tesla announced to the Times last night that he had received a patent on an invention\n[00:43.880 --> 00:50.320]  which would not only eliminate static interference, the present bugaboo of wireless telephony,\n[00:50.320 --> 00:55.520]  but would enable thousands of persons to talk at once between wireless stations and make\n[00:55.520 --> 01:01.920]  it possible for those talking to see one another by wireless regardless of the distance separating\n\n...\n\n[07:25.160 --> 07:30.520]  Wireless is coming to mankind in its full meaning like a hurricane some of these days.\n[07:30.520 --> 07:36.120]  Some day there will be, say, six great wireless telephone stations in a world system connecting\n[07:36.120 --> 07:42.080]  all the inhabitants on this earth to one another, not only by voice, but by sight.\n[07:42.080 --> 07:45.240]  Its surely coming.\n[07:45.240 --> 07:50.940]  End of Nikola Tesla sees a wireless vision by Anonymous, The New York Times, 3rd October\n[07:50.940 --> 08:13.840]  1915.\n```\n\n## Deepgram's Whisper API Endpoint\n\nGetting the Whisper tool working on your machine may require some fiddly work with dependencies - especially for Torch and any existing software running your GPU.  Our [OpenAI Whisper API endpoint](https://deepgram.com/openai-whisper/) is easy to work with on the command-line - you can use `curl` to quickly send audio to our API.\n\nThis call will send your file to the API and save it to a local JSON file called `n_tesla.json`:\n\n```shell\ncurl --request POST \\\n  --upload-file snf025_nikolateslawirelessvision_anonymous_gu.mp3 \\\n  --url 'https://api.deepgram.com/v1/listen?model=whisper' \\\n  --output n_tesla.json\n```\n\nThe JSON file is returned in Deepgram's format that offers the transcript as well as information about your transcription request. A quick way to view the transcript is to use the `jq` tool:\n\n```shell\njq .results.channels[0].alternatives[0].transcript n_tesla.json \n```\n\n...and here's your transcript!\n\n```text\n\"Nikola Tesla sees a wireless vision by anonymous the New York Times 3rd October 1915. This is a LeapRvox recording. All LeapRvox recordings are in the public domain. For more information or to volunteer, please visit LeapRvox.org. Nikola Tesla sees a wireless vision. Things his world system will allow hundreds to talk at once through the earth. Nikola Tesla responds static disturbance. Inventors hopes also to transmit pictures by the same medium which carries the voice. Nikola Tesla announced to the Times last night that he had received a patent on an invention which would not only eliminate static interference, the present bugaboo of wireless telephony, but would enable thousands of persons to talk at once between wireless stations and make it possible for those talking to see one another by wireless, regardless of the distance separating them. \n\n...\n\nWireless is coming to mankind, and it's full meaning like a hurricane some of these days. Some day there will be, say, six great wireless telephone stations in a world system, connecting all the inhabitants on this earth to one another, not only by voice, but by sight. It's surely coming. End of Nikola Tesla sees a wireless vision by anonymous, the New York Times 3rd October 1915.\"\n```\n\n## But Wait. Those Transcripts Aren't the Same.\n\nExcellent observation! The local run was able to transcribe \"LibriVox,\" while the API call returned \"LeapRvox.\" This is an artifact of this kind of model - their results are not deterministic. That is, some optimizations for working with large quantities of audio depend on overall system state and do not produce precisely the same output between runs. Our observations are that the resulting differential is typically on the order of 1% (absolute) fluctuations in word-error rate.";
						}
						async function compiledContent$2h() {
							return load$2h().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2h() {
							return (await import('./chunks/index.eb91dff4.mjs'));
						}
						function Content$2h(...args) {
							return load$2h().then((m) => m.default(...args));
						}
						Content$2h.isAstroComponentFactory = true;
						function getHeadings$2h() {
							return load$2h().then((m) => m.metadata.headings);
						}
						function getHeaders$2h() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2h().then((m) => m.metadata.headings);
						}

const __vite_glob_0_131 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2h,
  file: file$2h,
  url: url$2h,
  rawContent: rawContent$2h,
  compiledContent: compiledContent$2h,
  default: load$2h,
  Content: Content$2h,
  getHeadings: getHeadings$2h,
  getHeaders: getHeaders$2h
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2g = {"title":"How to Run OpenAI Whisper in Google Colab","description":"In Deepgram's newest blog, we cover how a preconfigured environment like Google Colab can support new tools like OpenAI Whisper. Learn more here!","date":"2022-10-17T21:00:38.699Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1665154754/blog/how-to-run-openai-whisper-in-google-colab/2210-OpenAI-Whisper-in-Google-Colab-featured-1200x630_2x_fjnqcv.png","authors":["ross-oconnell"],"category":"tutorial","tags":["whisper","colab","machine-learning"],"shorturls":{"share":"https://dpgr.am/253fd91","twitter":"https://dpgr.am/a8052ea","linkedin":"https://dpgr.am/a0c5185","reddit":"https://dpgr.am/8942ed0","facebook":"https://dpgr.am/7dc5a4a"}};
						const file$2g = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-to-run-openai-whisper-in-google-colab/index.md";
						const url$2g = undefined;
						function rawContent$2g() {
							return "\nOpenAI's Whisper is an exciting new model for automatic speech recognition (ASR). It features a simple architecture based on [transformers](https://en.wikipedia.org/wiki/Transformer_\\(machine_learning_model\\)), the same technology that drove recent advancements in natural language processing (NLP), and was trained on 680,000 hours of audio from a wide range of languages. The result is a new leader in open-source solutions for ASR.\n\nThe researchers at [Deepgram](https://deepgram.com/) have enjoyed testing Whisper and seeing how it works, and we wanted to make it as easy as possible for you to try it out too. One of the things we've learned in our experiments is that, as with many deep-learning tools, Whisper performs best when it has access to a GPU. While [downloading and installing Whisper](https://blog.deepgram.com/how-to-run-openai-whisper-in-command-line/) may be straightforward, configuring it to properly utilize a GPU (if you have one!) is a potential roadblock.\n\nGoogle Colab provides a great preconfigured environment for trying out new tools like Whisper, so we've set up a [simple notebook](https://colab.research.google.com/drive/1ZjgNUs2r0x2A-ITG7LS2BC7J8Bo2oqt5?usp=sharing) there to let you see what Whisper can do. We set up the notebook so that you don't need anything extra to run it, you can just click through and go. The notebook will:\n\n*   Install Whisper\n*   Download audio from YouTube\n*   Transcribe that audio with Whisper\n\n    ![Whisper transcription](https://res.cloudinary.com/deepgram/image/upload/v1665177914/blog/how-to-run-openai-whisper-in-google-colab/194656318-8a5b0e46-70b7-4017-aff3-43339334e60d_ribevw.png)\n*   Playback the audio in segments so you can check Whisper's work\n\n    ![Audio segments](https://res.cloudinary.com/deepgram/image/upload/v1665177914/blog/how-to-run-openai-whisper-in-google-colab/194656477-c9112775-ae9a-414a-847e-fa823b0b9a0b_ertwwq.png)\n*   And finally... quantitatively evaluate Whisper's performance by computing the Word Error Rate (WER) for the transcription\n\nWe think the files we chose are fun, but if you have files that you want to test Whisper on, it should be easy to upload them and drop them in!\n\n[Try the Colab](https://colab.research.google.com/github/deepgram-devs/try-whisper-in-google-collab/blob/main/try_whisper_in_three_easy_steps.ipynb)\n\n";
						}
						async function compiledContent$2g() {
							return load$2g().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2g() {
							return (await import('./chunks/index.e6debbe8.mjs'));
						}
						function Content$2g(...args) {
							return load$2g().then((m) => m.default(...args));
						}
						Content$2g.isAstroComponentFactory = true;
						function getHeadings$2g() {
							return load$2g().then((m) => m.metadata.headings);
						}
						function getHeaders$2g() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2g().then((m) => m.metadata.headings);
						}

const __vite_glob_0_132 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2g,
  file: file$2g,
  url: url$2g,
  rawContent: rawContent$2g,
  compiledContent: compiledContent$2g,
  default: load$2g,
  Content: Content$2g,
  getHeadings: getHeadings$2g,
  getHeaders: getHeaders$2g
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2f = {"title":"How to Test Automatic Speech Recognition (ASR) Providers For Your Business","description":"Learn how to test various ASR providers and the variables to consider to truly vet an ASR provider","date":"2021-03-03T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981357/blog/how-to-test-automatic-speech-recognition-asr-providers-for-your-business/how-to-test-asr-providers%402x.jpg","authors":["scott-stephenson"],"category":"ai-and-engineering","tags":["education"],"seo":{"title":"How to Test Automatic Speech Recognition (ASR) Providers For Your Business","description":"Learn how to test various ASR providers and the variables to consider to truly vet an ASR provider"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981357/blog/how-to-test-automatic-speech-recognition-asr-providers-for-your-business/how-to-test-asr-providers%402x.jpg"},"shorturls":{"share":"https://dpgr.am/9503902","twitter":"https://dpgr.am/5452614","linkedin":"https://dpgr.am/24d5b7c","reddit":"https://dpgr.am/46dd6c6","facebook":"https://dpgr.am/ebd4cf1"}};
						const file$2f = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-to-test-automatic-speech-recognition-asr-providers-for-your-business/index.md";
						const url$2f = undefined;
						function rawContent$2f() {
							return "Selecting the best [ASR](https://blog.deepgram.com/what-is-asr/) option for your company is an important decision. While the bulk of this article is an educational piece on how to most effectively test for ASR providers, the first step when making an important buying decision is identifying your priorities:\n\n1. What do you want?\n2. What do you need?\n3. What doesn't matter?\n\nBesides what was discussed in the tip sheet, **[How to Vet an Automatic Speech Recognition Solution Provider?](https://offers.deepgram.com/hubfs/Collateral/How-to-Vet-an-ASR-Provider.pdf)**, here are some other features you should also consider:\n\n1. Speed - How fast is your batch transcription?  How fast is your real-time transcription?\n2. Multi-channel support - Do you support multi-channel audio or only single channel?\n3. Speaker separation - Can you separate the speakers; i.e. speaker 1 or speaker 2, even for non-stereo applications?\n4. Deep search - Can you search the audio for find specific words or phrases to listen and review; not search the transcription?\n\nGetting a sense of what features your company might need before starting talks with providers will help you avoid the common trap of relying purely on accuracy rate. Otherwise, you'll likely find yourself having this conversation:\n\n> **Buyer**: \"We've been looking at a couple ASR providers...what's your accuracy rate?\" **ASR Provider**: \"Fantastic. On an academic data set that is publicly known, we claim a 95% accuracy rate.\" **Buyer**: \"That sounds great! But how does that relate to our audio data?\" **ASR Provider**:\"Trust me, we'll do great on that too!\" **Buyer**: \"Hmm...\"\n\nFor a long time, ASR companies have avoided doing real comparisons on company specific audio data by focusing marketing dollars and sales narratives on impressive outcomes from public datasets. ([Like new advances on word error rates](https://blog.deepgram.com/the-trouble-with-wer/)). By distracting companies from the fact that gamed success statistics don't translate to real world applications, ASR providers have been able to trick companies into buying a car without test driving it first. So, what is the best way to actually test drive and walk away with a great deal?\n\n## How to test an ASR Solution?\n\nWith the goal of getting the truth and investing as little effort as possible, here are optimal guidelines for testing speech recognition providers in an apples to apples accuracy comparison:\n\n### **Step 1: Select 50 randomly sampled audio files that are representative of the audio your company encounters**\n\nDo:\n\n1. Use meeting recordings if your goal is to transcribe meetings\n2. Use voicemails if the goal is to transcribe voicemails\n3. Use audio with accents if your audio will have speakers with accents\n\nDon't:\n\n1. Record yourself talking into your computer\n2. Use a random YouTube video\n3. Test out your favorite podcast or broadcast audio\n4. Use a song\n\nPay humans to transcribe one minute from each of these files. This effort should cost $100 or less and will serve as the truth of all truths for all the ASR providers you'll be comparing. You can easily find transcriptionists using Rev.com or Upwork.\n\n### **Step 2: Send the same 50 one-minute clips to each of the speech vendors that you are considering to test the output of their APIs**\n\nTake note of what each provider deems an acceptable audio format and how it fits into your list of considerations from earlier.\n\n### **Step 3: Receive the text outputs and normalize them** for the \"choices\" that an ASR company makes with their out-of-the-box transcripts\n\n<ul>\n<li>How are phone numbers transcribed?</li>\n<ul>\n<li> - 905-678-1234?</li>\n<li> - nine zero five six seven eight one two three four?</li>\n<li> - 9 0 5 6 7 8 1 2 3 4?</li>\n\n</ul>\n<li>Are outputs punctuated and capitalized?</li>\n</ul>\n\n### **Step 4: Do a Word Error Rate (WER) comparison on the files**\n\n[Word Error Rate (WER)](https://blog.deepgram.com/what-is-word-error-rate/) will give you a sense of the overall accuracy of the transcripts, but don't just look at the number. You also want to look at *where* the output was wrong and *why* the output was wrong. This includes what words were incorrectly added, omitted, or simply misinterpreted.\n\n### **Step 5: Make a visual representation of what was wrong**\n\n* Who is getting the *important* words right vs. wrong?\n* Whose outputs are the most legible?\n\n## Next Steps\n\nAt this point, you will know where each ASR provider stands from a general accuracy perspective on audio representative of your use case. Next, consider the other variables in **[How to Vet an Automatic Speech Recognition Solution Provider?](https://offers.deepgram.com/how-to-vet-an-asr-provider-thank-you)** and drill down on accuracy improvements with keywords, libraries, reprogramming, and custom training. With a good handle on where each competitor stands in terms all these variables, you can confidently go into pricing conversations and make better decisions for your business. If you're ready to compare Deepgram's AI Speech Platform with other ASR providers, [contact us](https://www.deepgram.com/contact-us).\n\n<WhitepaperPromo whitepaper=\"latest\"></WhitepaperPromo>\n";
						}
						async function compiledContent$2f() {
							return load$2f().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2f() {
							return (await import('./chunks/index.27e95695.mjs'));
						}
						function Content$2f(...args) {
							return load$2f().then((m) => m.default(...args));
						}
						Content$2f.isAstroComponentFactory = true;
						function getHeadings$2f() {
							return load$2f().then((m) => m.metadata.headings);
						}
						function getHeaders$2f() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2f().then((m) => m.metadata.headings);
						}

const __vite_glob_0_133 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2f,
  file: file$2f,
  url: url$2f,
  rawContent: rawContent$2f,
  compiledContent: compiledContent$2f,
  default: load$2f,
  Content: Content$2f,
  getHeadings: getHeadings$2f,
  getHeaders: getHeaders$2f
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2e = {"title":"How to train Baidus Deepspeech model","description":"Learn how to train a deep neural network for speech recognition.","date":"2017-02-21T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981932/blog/how-to-train-baidus-deepspeech-model-with-kur/placeholder-post-image%402x.jpg","authors":["scott-stephenson"],"category":"ai-and-engineering","tags":["speech-models"],"seo":{"title":"How to train Baidus Deepspeech model","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981932/blog/how-to-train-baidus-deepspeech-model-with-kur/placeholder-post-image%402x.jpg"},"shorturls":{"share":"https://dpgr.am/85061c2","twitter":"https://dpgr.am/febaff5","linkedin":"https://dpgr.am/41bf9c7","reddit":"https://dpgr.am/70fb579","facebook":"https://dpgr.am/b693c50"}};
						const file$2e = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-to-train-baidus-deepspeech-model-with-kur/index.md";
						const url$2e = undefined;
						function rawContent$2e() {
							return "## You want to train a Deep Neural Network for Speech Recognition?\n\nMe too. It was two years ago and I was a particle physicist finishing a PhD at University of Michigan. I could code a little in C/C++ and Python and I knew Noah Shutty. Noah's my cofounder at [Deepgram](https://deepgram.com/) and an all around powerhouse of steep learning curve-ness. We both had no speech recognition background (at all), a little programming and applied machine learning (boosted decision trees and NN), and a lot of data juggling/system hacking experience. We found ourselves building the world's first deep learning based speech search engine. To get moving we needed a DNN that could understand speech. Here's the basic problem.\n\n## Turn this input audio ⬇⬇⬇\n\n![missing](https://res.cloudinary.com/deepgram/image/upload/v1661725768/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-03-at-8.56.51-PM.png)\n\n<div style=\"text-align: center;\">A spectrogram of an ordinary squishy human saying \"I am a human saying human things.\"</div>\n\nListen to the audio.\n\n<iframe src=\"https://www.youtube.com/embed/TOZVpWL3ZGA\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\n## Into this text ⬇⬇⬇\n\n![missing](https://res.cloudinary.com/deepgram/image/upload/v1661725769/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-02-at-12.51.52-PM.png)\n\n<div style=\"text-align: center; styl;margin-top: 0px;\">The prediction from a DNN that just heard the \"I am a human saying human things\" audio file.</div>\n\n# Why we did it\n\nWe'll probably write a \"This is Deepgram\" post sometime, but suffice to say: we are building a Google for Audio and **we needed a deep learning model for speech recognition** to accomplish that goal. Good thing Baidu had just released the first of the Deepspeech papers two years ago when we were starting \\[[1]](https://arxiv.org/abs/1412.5567). This gave us the push we needed to figure out how deep learning can work for audio search. Here's a picture of the Deepspeech RNN for inspiration. \n\n![](https://res.cloudinary.com/deepgram/image/upload/v1661725769/blog/how-to-train-baidus-deepspeech-model-with-kur/NVIDIA_Baidu_Deep_Speech_Neural_Network_600-1.jpg)\n\n*Baidu's Andrew Ng at NVidia's GTC conference talking about Deepspeech*\n\nDeep Learning is hard. There are a few reasons why that is but some obvious ones are that models and frameworks involving differentiable tensor graphs are complicated, the hardware to run them efficiently is finicky (GPUs), and it's difficult to get data. Getting a working Deepspeech model is pretty hard too, even with a paper outlining it. **The first step was to build an end-to-end deep learning speech recognition system.** We started working on that and based the DNN on the Baidu Deepspeech paper. After a lot of toil, we put together a genuinely good end-to-end DNN speech recognition model. We've [open sourced](https://kur.deepgram.com/) the Deepspeech model in the [Kur](https://kur.deepgram.com) framework running on [TensorFlow](https://www.github.com/tensorflow/tensorflow).\n\n> *Quick Aside:* We had to build Kur for Deepgram's survival. It's the wild west out here in A.I. and it's not possible to quickly build cutting edge models unless you have a simple way to do it.\n\nWe find that Kur lets you *describe your model* and then *it works* without having to do a lot of the plumbing that slows projects down. The [Kur](https://github.com/deepgram/kur) software package was [just released](https://techcrunch.com/2017/01/18/deepgram-open-sources-kur-to-make-diy-deep-learning-less-painful/). *It's free. It's open source. It's named after [the first mythical dragon](https://en.wikipedia.org/wiki/Kur).* Kur was crafted by the whole Deepgram A.I. team and we hope it helps the deep learning community in some small way.\n\n#### To get this working, download and install [Kur](https://github.com/deepgram/kur)\n\nTo install, all you really need to do is run `$ pip install kur` in your terminal if you have python 3.4 or above installed. If you need guidance or want easy environments to work in, we have an entire installation page at [kur.deepgram.com](https://kur.deepgram.com).\n\n#### Run the Deepspeech Example\n\nOnce Kur is installed, fire up your fingers and run `$ kur -v train speech.yml` from your `kur/examples/` directory. You can omit the `-v` if you want Kur to quietly train without outlining what it's up to in your terminal's standard out. We find that running with `-v` the first few times gives you an idea of how Kur works, however. Turn on `-vv` if you're really craving gory details.\n\n#### Your model will start training\n\nAt first, the outputs will be gibberish. But they get better :)\n\nHour 1:\n\n**True transcript:** `these vast buildings what were they`\n\n**DNN prediction:** `he s ma tol ln wt r hett jzxzjxzjqzjqjxzq`\n\nHour 6:\n\n**True transcript:** `the valkyrie kept off the coast steering to the westward`\n\n**DNN prediction:** `the bak gerly cap dof the cost stkuarinte the west werd`\n\nHour 24:\n\n**True transcript:** `it was a theatre ready made`\n\n**DNN prediction:** `it was it theater readi made`\n\n*Real English is spilling out.* I trained for 48 hours in total then ran the *\"i am a human saying human things\"* audio file through the network.\n\nListen to the audio.\n\n<iframe src=\"https://www.youtube.com/embed/TOZVpWL3ZGA\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\nHour 48:\n\n**True transcript:** `i am a human saying human things`\n\n**DNN prediction:** `i am a human saying human things`\n\nIt's just two days old and didn't make a single mistake on that utterance. **Our Speech A.I. is doing pretty well.**\n\n##### Training and Validation Loss of Kur Deepspeech Model\n\n![missing](https://res.cloudinary.com/deepgram/image/upload/v1661725770/blog/how-to-train-baidus-deepspeech-model-with-kur/loss-kur-up.png)\n\n<div style=\"text-align: center;\">Loss as a function of batch for both training and validation data in the [Kur](http://github.com/deepgram/kur) 'speech.yml' example. The validation data seems a little easier.</div>\n\n#### There's a lot of things to try\n\nWe abstracted away some of the time consuming bits. A little help comes from the descriptive nature of Kur, too. You can write down what you mean.\n\n![missing](https://res.cloudinary.com/deepgram/image/upload/v1661725771/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-02-at-10.15.51-AM.png)\n\n<div style=\"text-align: center;\">Hyperparameters for Deepspeech in the example Kurfile</div>\n\nThese are the handful of hyperparameters needed to construct the DNN. There's a single one dimensional CNN that operates on a time slice of FFT outputs. Then there's an RNN stack which is 3 layers deep and 1000 nodes wide each. The vocab size is how many 'letters' we'll be choosing from (`a` to `z`, a space and an apostrophe `'`-that's 28 total). The hyperparameters are grabbed in the model section of the Kurfile (that's the `speech.yml`). The CNN layer is built like this.\n\n![missing](https://res.cloudinary.com/deepgram/image/upload/v1661725772/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-02-at-10.20.24-AM.png)\n\n<div style=\"text-align: center;\">The CNN layer specification</div>\n\nThis puts in a single [CNN layer](https://en.wikipedia.org/wiki/Convolutional_neural_network#Convolutional_layer) with a few sensible hyperparameters and slaps on a [Rectified Linear Unit](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) activation layer. *Note: The hyperparameters are filled in using the [Jinja2](http://jinja.pocoo.org/docs/2.9/) templating engine.*\n\n> You can read more about defining Kurfiles in the docs at [kur.deepgram.com](https://kur.deepgram.com/).\n\nThe stack of [RNN layers](https://en.wikipedia.org/wiki/Recurrent_neural_network) is built with a `for` loop that stamps out three layers in a row-*three* because of the `depth` hyperparameter.\n\n![missing](https://res.cloudinary.com/deepgram/image/upload/v1661725772/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-09-at-5.05.32-PM.png)\n\n<div style=\"text-align: center;\">The RNN stack specification</div>\n\nThe batch normalization layer uses a technique to keep layer weights distributed in a non-crazy way, speeding up training. The rnn `sequence` hyperparameter just means you want the full sequence of outputs passed along (every single time slice of the audio) while generating a guess for the transcript. *Quick Summary:* The CNN layer ingests the FFT input then connects to RNNs and eventually to a fully connected layer which predicts from 28 letters. That's Deepspeech.\n\n## Overview of How it Works\n\nWhen training modern speech DNNs you generally slice up the audio into ~20 millisecond chunks, do something like a fast fourier transform (FFT) on the chunk of audio, feed those chunked FFTs into the DNN sequentially, and generate a prediction for the current chunk. You do that until you've digested the whole file (while remembering your chunk predictions the whole way) and end up with a sequence.\n\n#### This is how Deepspeech works in Kur\n\nKur takes in normal `wav` audio files. Then it grabs the spectrogram (FFT over time) of the file and jams it into a DNN with a CNN layer and a stack of three RNN layers. Out pops probabilities of latin characters, which (when read by a human) form words. As the model trains, there will be validation steps that give you an updated prediction on a random test audio file. You'll see the true text listed next to each prediction. You can watch the predicted text outputs get better as the network trains. *It's learning.* At first it will learn about spaces (ya know, this ), then it'll figure out good ratios for vowels and consonants, then it'll learn common easy words like `the`, `it`, `a`, `good` and build up it's vocabulary from there. It's fascinating to watch.\n\n![missing](https://res.cloudinary.com/deepgram/image/upload/v1661725773/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-02-at-12.21.53-PM.png)\n\n<div style=\"text-align: center;\">Take in time slices of audio frequencies and infer the letters that are being spoken. Time goes to the right. Image by Baidu.</div>\n\n## Tell us what you think\n\nAt Deepgram, we're really open about what we're working on. We know that A.I. is going to be huuuge and there are not enough trained people or good tools in the world to help it along ... yet. We hope [Kur](http://kur.deepgram.com), [KurHub](http://www.kurhub.com), our upcoming [Deep Learning Hackathon](http://www.deeplearninghackathon.com), and blog posts like this help out the community, gets people excited, and shows that the good stuff can now be used by everyone. We're a startup and our research team can only produce so much helpful material per unit time. The best way to help us is to implement your [favorite Deep Learning papers](https://github.com/terryum/awesome-deep-learning-papers) in [Kur](https://www.github.com/deepgram/kur) and upload it to [KurHub](http://www.kurhub.com/). You can also contribute to the Kur framework directly on [GitHub](https://github.com/deepgram/kur). You'll be showered with thanks from us and a pile of others that are hungry for good implementations.";
						}
						async function compiledContent$2e() {
							return load$2e().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2e() {
							return (await import('./chunks/index.1d62ef99.mjs'));
						}
						function Content$2e(...args) {
							return load$2e().then((m) => m.default(...args));
						}
						Content$2e.isAstroComponentFactory = true;
						function getHeadings$2e() {
							return load$2e().then((m) => m.metadata.headings);
						}
						function getHeaders$2e() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2e().then((m) => m.metadata.headings);
						}

const __vite_glob_0_134 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2e,
  file: file$2e,
  url: url$2e,
  rawContent: rawContent$2e,
  compiledContent: compiledContent$2e,
  default: load$2e,
  Content: Content$2e,
  getHeadings: getHeadings$2e,
  getHeaders: getHeaders$2e
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2d = {"title":"Try Whisper: OpenAI's Speech Recognition Model in 1 Minute","description":"Deepgram has made testing OpenAI's new open-sourced Whisper speech recognition model easy as copy and paste. Try it today!","date":"2022-09-29T18:59:11.323Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1663880277/blog/how-to-use-whisper-openais-speech-recognition-model-in-1-minute/2209-How-to-Use-Whisper-blog_2x_qb1eah.jpg","authors":["michael-jolley"],"category":"ai-and-engineering","tags":["whisper","machine-learning"],"seo":{"title":"Try Whisper: OpenAI's Speech Recognition Model in 1 Minute","description":"Deepgram has made testing OpenAI's new open-sourced Whisper speech recognition model easy as copy and paste. Try it today!"},"shorturls":{"share":"https://dpgr.am/b61fe3d","twitter":"https://dpgr.am/77c401a","linkedin":"https://dpgr.am/d9c56c3","reddit":"https://dpgr.am/e4a91e9","facebook":"https://dpgr.am/08df744"}};
						const file$2d = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-to-use-whisper-openais-speech-recognition-model-in-1-minute/index.md";
						const url$2d = undefined;
						function rawContent$2d() {
							return "\nOpenAI's newly released \"Whisper\" speech recognition model has been said to provide accurate transcriptions in multiple languages and even translate them to English. As Deepgram CEO, Scott Stephenson, recently tweeted \"OpenAI + Deepgram is all good — rising tide lifts all boats.\" We're stoked to see others are buying into what we've been preaching for nearly a decade: end-to-end deep learning is the answer to speech-to-text.\n\nAs our team played with Whisper last week, we wanted to make sure as many people as possible could try it with minimal effort. And since we already offer some of the most accurate and performant speech recognition models in the world, why not add another? 😁\n\n## Announcing Whisper Multilingual AI Speech Recognition on Deepgram\n\nLast week, we released the Whisper speech recognition model via the Deepgram API. All accounts now have access to the whisper model for free. But, we wanted to make it even easier to try. So, we made it available for people without a Deepgram account. That's right! You can send files to the API without needing an API key. Try out the shell commands below to see how Whisper performs on your local files or those hosted elsewhere.\n\n### Use cURL to Transcribe Local Files with Whisper\n\nYou can start testing the Whisper model now by running the snippet below in your terminal.\n\n```shell\ncurl \\\n --request POST \\\n --data-binary @youraudio.wav \\\n --url 'https://api.deepgram.com/v1/listen?model=whisper'\n```\n\n### Use cURL to Transcribe Remote Files with Whisper\n\nDon't have an audio file to test? You can also send the URL to a hosted file by changing your request to the code snippet below. You can replace the `https://static.deepgram.com/examples/epi.wav` URL with a file that you'd like to test against.\n\n```shell\ncurl \\\n  --request POST \\\n  --url 'https://api.deepgram.com/v1/listen?model=whisper' \\\n  --header 'content-type: application/json' \\\n  --data '{\"url\":\"https://static.deepgram.com/examples/epi.wav\"}'\n```\n\nWe even provide several demo files that you can use:\n\n*   https://static.deepgram.com/examples/dragons.wav\n*   https://static.deepgram.com/examples/epi.wav\n*   https://static.deepgram.com/examples/interview\\_speech-analytics.wav\n*   https://static.deepgram.com/examples/koreanSampleFile.mp3\n*   https://static.deepgram.com/examples/sofiavergaraspanish.clip.wav\n*   https://static.deepgram.com/examples/timotheefrench.clip.wav\n\n## Try Whisper in Your Browser\n\nYou can also test the whisper model in your browser when you [signup for a free Deepgram account](https://console.deepgram.com/signup?jump=demo\\&f-whisper=true). Our getting started missions allow you to compare the whisper model to Deepgram models using your own files and/or sample files that we provide.\n\n![Transcribe pre-recorded files mission in Deepgram console](https://res.cloudinary.com/deepgram/image/upload/v1664390668/blog/how-to-use-whisper-openais-speech-recognition-model-in-1-minute/192859735-e30b782e-b6d8-49b9-b05c-c79f8c168078_mweppn.png)\n\n## The Final Result\n\nBelow is the result of a NASA phone call transcribed with the whisper model.\n\n```javascript\n{\n  \"metadata\": {\n    \"channels\": 1,\n    \"created\": \"Wed, 28 Sep 2022 18:25:08 GMT\",\n    \"duration\": 0,\n    \"model_info\": {\n      \"name\": \"whisper\",\n      \"tier\": \"other\",\n      \"version\": \"2022-09-22.0\"\n    },\n    \"models\": [\n      \"8024132e-81fb-4a77-9377-548cd12c143d\"\n    ],\n    \"request_id\": \"826716cc-0c2d-4efe-b6e0-671ca5aea9d5\",\n    \"sha256\": \"unsupported\",\n    \"transaction_key\": \"deprecated\"\n  },\n  \"results\": {\n    \"channels\": [\n      {\n        \"alternatives\": [\n          {\n            \"confidence\": 0,\n            \"transcript\": \" Yeah, as much as it's worth celebrating the first spacewalk with an all female team, I think many of us are looking forward to it just being normal. And I think if it signifies anything, it is to honor the women who came before us, who were skilled and qualified and didn't get the same opportunities that we have today.\",\n            \"words\": [\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"Yeah,\",\n                \"start\": 0,\n                \"word\": \"yeah\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"as\",\n                \"start\": 0,\n                \"word\": \"as\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"much\",\n                \"start\": 0,\n                \"word\": \"much\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"as\",\n                \"start\": 0,\n                \"word\": \"as\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"it's\",\n                \"start\": 0,\n                \"word\": \"its\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"worth\",\n                \"start\": 0,\n                \"word\": \"worth\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"celebrating\",\n                \"start\": 0,\n                \"word\": \"celebrating\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"the\",\n                \"start\": 0,\n                \"word\": \"the\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"first\",\n                \"start\": 0,\n                \"word\": \"first\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"spacewalk\",\n                \"start\": 0,\n                \"word\": \"spacewalk\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"with\",\n                \"start\": 0,\n                \"word\": \"with\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"an\",\n                \"start\": 0,\n                \"word\": \"an\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"all\",\n                \"start\": 0,\n                \"word\": \"all\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"female\",\n                \"start\": 0,\n                \"word\": \"female\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"team,\",\n                \"start\": 0,\n                \"word\": \"team\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"I\",\n                \"start\": 0,\n                \"word\": \"i\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"think\",\n                \"start\": 0,\n                \"word\": \"think\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"many\",\n                \"start\": 0,\n                \"word\": \"many\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"of\",\n                \"start\": 0,\n                \"word\": \"of\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"us\",\n                \"start\": 0,\n                \"word\": \"us\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"are\",\n                \"start\": 0,\n                \"word\": \"are\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"looking\",\n                \"start\": 0,\n                \"word\": \"looking\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"forward\",\n                \"start\": 0,\n                \"word\": \"forward\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"to\",\n                \"start\": 0,\n                \"word\": \"to\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"it\",\n                \"start\": 0,\n                \"word\": \"it\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"just\",\n                \"start\": 0,\n                \"word\": \"just\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"being\",\n                \"start\": 0,\n                \"word\": \"being\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"normal.\",\n                \"start\": 0,\n                \"word\": \"normal\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"And\",\n                \"start\": 0,\n                \"word\": \"and\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"I\",\n                \"start\": 0,\n                \"word\": \"i\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"think\",\n                \"start\": 0,\n                \"word\": \"think\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"if\",\n                \"start\": 0,\n                \"word\": \"if\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"it\",\n                \"start\": 0,\n                \"word\": \"it\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"signifies\",\n                \"start\": 0,\n                \"word\": \"signifies\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"anything,\",\n                \"start\": 0,\n                \"word\": \"anything\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"it\",\n                \"start\": 0,\n                \"word\": \"it\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"is\",\n                \"start\": 0,\n                \"word\": \"is\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"to\",\n                \"start\": 0,\n                \"word\": \"to\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"honor\",\n                \"start\": 0,\n                \"word\": \"honor\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"the\",\n                \"start\": 0,\n                \"word\": \"the\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"women\",\n                \"start\": 0,\n                \"word\": \"women\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"who\",\n                \"start\": 0,\n                \"word\": \"who\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"came\",\n                \"start\": 0,\n                \"word\": \"came\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"before\",\n                \"start\": 0,\n                \"word\": \"before\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"us,\",\n                \"start\": 0,\n                \"word\": \"us\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"who\",\n                \"start\": 0,\n                \"word\": \"who\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"were\",\n                \"start\": 0,\n                \"word\": \"were\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"skilled\",\n                \"start\": 0,\n                \"word\": \"skilled\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"and\",\n                \"start\": 0,\n                \"word\": \"and\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"qualified\",\n                \"start\": 0,\n                \"word\": \"qualified\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"and\",\n                \"start\": 0,\n                \"word\": \"and\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"didn't\",\n                \"start\": 0,\n                \"word\": \"didnt\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"get\",\n                \"start\": 0,\n                \"word\": \"get\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"the\",\n                \"start\": 0,\n                \"word\": \"the\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"same\",\n                \"start\": 0,\n                \"word\": \"same\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"opportunities\",\n                \"start\": 0,\n                \"word\": \"opportunities\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"that\",\n                \"start\": 0,\n                \"word\": \"that\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"we\",\n                \"start\": 0,\n                \"word\": \"we\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"have\",\n                \"start\": 0,\n                \"word\": \"have\"\n              },\n              {\n                \"confidence\": 0,\n                \"end\": 0,\n                \"punctuated_word\": \"today.\",\n                \"start\": 0,\n                \"word\": \"today\"\n              }\n            ]\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\nThere are a few empty data points that stand out, namely `confidence`, `start`, and `end`. The whisper model doesn't provide that level of detail, so we were forced to provide zero values for them. Comparatively, below is the response using Deepgrams enhanced English model.\n\n```javascript\n{\n  \"metadata\": {\n    \"transaction_key\": \"deprecated\",\n    \"request_id\": \"146d5bae-5147-468f-9971-dbfcf2e5f152\",\n    \"sha256\": \"154e291ecfa8be6ab8343560bcc109008fa7853eb5372533e8efdefc9b504c33\",\n    \"created\": \"2022-09-28T18:32:57.088Z\",\n    \"duration\": 25.933313,\n    \"channels\": 1,\n    \"models\": [\n      \"125125fb-e391-458e-a227-a60d6426f5d6\"\n    ],\n    \"model_info\": {\n      \"125125fb-e391-458e-a227-a60d6426f5d6\": {\n        \"name\": \"general-enhanced\",\n        \"version\": \"2022-05-18.0\",\n        \"tier\": \"enhanced\"\n      }\n    }\n  },\n  \"results\": {\n    \"channels\": [\n      {\n        \"alternatives\": [\n          {\n            \"transcript\": \"Yes, as much as it's worth celebrating the first spacewalk with an all female team. I think many of us are looking forward to it just being normal. And I think if it signifies anything, it is to honor the women who came before us, who were skilled and qualified and didn't get the the same opportunities that we have today.\",\n            \"confidence\": 0.99439037,\n            \"words\": [\n              {\n                \"word\": \"yes\",\n                \"start\": 0.07996,\n                \"end\": 0.57996,\n                \"confidence\": 0.5762918,\n                \"punctuated_word\": \"Yes,\"\n              },\n              {\n                \"word\": \"as\",\n                \"start\": 0.91954005,\n                \"end\": 1.1194401,\n                \"confidence\": 0.9807544,\n                \"punctuated_word\": \"as\"\n              },\n              {\n                \"word\": \"much\",\n                \"start\": 1.1194401,\n                \"end\": 1.35932,\n                \"confidence\": 0.91273105,\n                \"punctuated_word\": \"much\"\n              },\n              {\n                \"word\": \"as\",\n                \"start\": 1.35932,\n                \"end\": 1.85932,\n                \"confidence\": 0.98721635,\n                \"punctuated_word\": \"as\"\n              },\n              {\n                \"word\": \"it's\",\n                \"start\": 1.9990001,\n                \"end\": 2.31884,\n                \"confidence\": 0.91658056,\n                \"punctuated_word\": \"it's\"\n              },\n              {\n                \"word\": \"worth\",\n                \"start\": 2.31884,\n                \"end\": 2.7986002,\n                \"confidence\": 0.9998478,\n                \"punctuated_word\": \"worth\"\n              },\n              {\n                \"word\": \"celebrating\",\n                \"start\": 2.7986002,\n                \"end\": 3.2986002,\n                \"confidence\": 0.99984485,\n                \"punctuated_word\": \"celebrating\"\n              },\n              {\n                \"word\": \"the\",\n                \"start\": 4.5177402,\n                \"end\": 4.67766,\n                \"confidence\": 0.9901214,\n                \"punctuated_word\": \"the\"\n              },\n              {\n                \"word\": \"first\",\n                \"start\": 4.67766,\n                \"end\": 5.17766,\n                \"confidence\": 0.9930007,\n                \"punctuated_word\": \"first\"\n              },\n              {\n                \"word\": \"spacewalk\",\n                \"start\": 5.3173404,\n                \"end\": 5.8173404,\n                \"confidence\": 0.9511817,\n                \"punctuated_word\": \"spacewalk\"\n              },\n              {\n                \"word\": \"with\",\n                \"start\": 6.3968,\n                \"end\": 6.63668,\n                \"confidence\": 0.99226224,\n                \"punctuated_word\": \"with\"\n              },\n              {\n                \"word\": \"an\",\n                \"start\": 6.63668,\n                \"end\": 6.7966003,\n                \"confidence\": 0.99240345,\n                \"punctuated_word\": \"an\"\n              },\n              {\n                \"word\": \"all\",\n                \"start\": 6.7966003,\n                \"end\": 6.95652,\n                \"confidence\": 0.97981423,\n                \"punctuated_word\": \"all\"\n              },\n              {\n                \"word\": \"female\",\n                \"start\": 6.95652,\n                \"end\": 7.3563204,\n                \"confidence\": 0.9998773,\n                \"punctuated_word\": \"female\"\n              },\n              {\n                \"word\": \"team\",\n                \"start\": 7.3563204,\n                \"end\": 7.8563204,\n                \"confidence\": 0.7401077,\n                \"punctuated_word\": \"team.\"\n              },\n              {\n                \"word\": \"i\",\n                \"start\": 8.47496,\n                \"end\": 8.5949,\n                \"confidence\": 0.99884456,\n                \"punctuated_word\": \"I\"\n              },\n              {\n                \"word\": \"think\",\n                \"start\": 8.5949,\n                \"end\": 8.874761,\n                \"confidence\": 0.9998031,\n                \"punctuated_word\": \"think\"\n              },\n              {\n                \"word\": \"many\",\n                \"start\": 8.874761,\n                \"end\": 9.15462,\n                \"confidence\": 0.98065513,\n                \"punctuated_word\": \"many\"\n              },\n              {\n                \"word\": \"of\",\n                \"start\": 9.15462,\n                \"end\": 9.314541,\n                \"confidence\": 0.999676,\n                \"punctuated_word\": \"of\"\n              },\n              {\n                \"word\": \"us\",\n                \"start\": 9.314541,\n                \"end\": 9.814541,\n                \"confidence\": 0.9999099,\n                \"punctuated_word\": \"us\"\n              },\n              {\n                \"word\": \"are\",\n                \"start\": 9.994201,\n                \"end\": 10.23408,\n                \"confidence\": 0.9992455,\n                \"punctuated_word\": \"are\"\n              },\n              {\n                \"word\": \"looking\",\n                \"start\": 10.23408,\n                \"end\": 10.513941,\n                \"confidence\": 0.9992975,\n                \"punctuated_word\": \"looking\"\n              },\n              {\n                \"word\": \"forward\",\n                \"start\": 10.513941,\n                \"end\": 10.83378,\n                \"confidence\": 0.9997292,\n                \"punctuated_word\": \"forward\"\n              },\n              {\n                \"word\": \"to\",\n                \"start\": 10.83378,\n                \"end\": 10.9937,\n                \"confidence\": 0.99948406,\n                \"punctuated_word\": \"to\"\n              },\n              {\n                \"word\": \"it\",\n                \"start\": 10.9937,\n                \"end\": 11.153621,\n                \"confidence\": 0.9791443,\n                \"punctuated_word\": \"it\"\n              },\n              {\n                \"word\": \"just\",\n                \"start\": 11.153621,\n                \"end\": 11.3935,\n                \"confidence\": 0.95780265,\n                \"punctuated_word\": \"just\"\n              },\n              {\n                \"word\": \"being\",\n                \"start\": 11.3935,\n                \"end\": 11.8935,\n                \"confidence\": 0.99439037,\n                \"punctuated_word\": \"being\"\n              },\n              {\n                \"word\": \"normal\",\n                \"start\": 11.9932,\n                \"end\": 12.4932,\n                \"confidence\": 0.94852126,\n                \"punctuated_word\": \"normal.\"\n              },\n              {\n                \"word\": \"and\",\n                \"start\": 12.792801,\n                \"end\": 13.292801,\n                \"confidence\": 0.99587005,\n                \"punctuated_word\": \"And\"\n              },\n              {\n                \"word\": \"i\",\n                \"start\": 13.832281,\n                \"end\": 13.952221,\n                \"confidence\": 0.97340345,\n                \"punctuated_word\": \"I\"\n              },\n              {\n                \"word\": \"think\",\n                \"start\": 13.952221,\n                \"end\": 14.23208,\n                \"confidence\": 0.9995718,\n                \"punctuated_word\": \"think\"\n              },\n              {\n                \"word\": \"if\",\n                \"start\": 14.23208,\n                \"end\": 14.392,\n                \"confidence\": 0.9576254,\n                \"punctuated_word\": \"if\"\n              },\n              {\n                \"word\": \"it\",\n                \"start\": 14.392,\n                \"end\": 14.551921,\n                \"confidence\": 0.99518657,\n                \"punctuated_word\": \"it\"\n              },\n              {\n                \"word\": \"signifies\",\n                \"start\": 14.551921,\n                \"end\": 15.051921,\n                \"confidence\": 0.99909633,\n                \"punctuated_word\": \"signifies\"\n              },\n              {\n                \"word\": \"anything\",\n                \"start\": 15.111641,\n                \"end\": 15.611641,\n                \"confidence\": 0.7811873,\n                \"punctuated_word\": \"anything,\"\n              },\n              {\n                \"word\": \"it\",\n                \"start\": 15.81996,\n                \"end\": 16.01986,\n                \"confidence\": 0.4662335,\n                \"punctuated_word\": \"it\"\n              },\n              {\n                \"word\": \"is\",\n                \"start\": 16.01986,\n                \"end\": 16.51986,\n                \"confidence\": 0.9992107,\n                \"punctuated_word\": \"is\"\n              },\n              {\n                \"word\": \"to\",\n                \"start\": 16.89942,\n                \"end\": 17.05934,\n                \"confidence\": 0.9627232,\n                \"punctuated_word\": \"to\"\n              },\n              {\n                \"word\": \"honor\",\n                \"start\": 17.05934,\n                \"end\": 17.55934,\n                \"confidence\": 0.99968433,\n                \"punctuated_word\": \"honor\"\n              },\n              {\n                \"word\": \"the\",\n                \"start\": 17.65904,\n                \"end\": 17.77898,\n                \"confidence\": 0.9969616,\n                \"punctuated_word\": \"the\"\n              },\n              {\n                \"word\": \"women\",\n                \"start\": 17.77898,\n                \"end\": 18.05884,\n                \"confidence\": 0.64570725,\n                \"punctuated_word\": \"women\"\n              },\n              {\n                \"word\": \"who\",\n                \"start\": 18.05884,\n                \"end\": 18.21876,\n                \"confidence\": 0.99748373,\n                \"punctuated_word\": \"who\"\n              },\n              {\n                \"word\": \"came\",\n                \"start\": 18.21876,\n                \"end\": 18.41866,\n                \"confidence\": 0.9997577,\n                \"punctuated_word\": \"came\"\n              },\n              {\n                \"word\": \"before\",\n                \"start\": 18.41866,\n                \"end\": 18.77848,\n                \"confidence\": 0.99900305,\n                \"punctuated_word\": \"before\"\n              },\n              {\n                \"word\": \"us\",\n                \"start\": 18.77848,\n                \"end\": 19.27848,\n                \"confidence\": 0.91114235,\n                \"punctuated_word\": \"us,\"\n              },\n              {\n                \"word\": \"who\",\n                \"start\": 19.41816,\n                \"end\": 19.91816,\n                \"confidence\": 0.99943566,\n                \"punctuated_word\": \"who\"\n              },\n              {\n                \"word\": \"were\",\n                \"start\": 20.17778,\n                \"end\": 20.45764,\n                \"confidence\": 0.99543494,\n                \"punctuated_word\": \"were\"\n              },\n              {\n                \"word\": \"skilled\",\n                \"start\": 20.45764,\n                \"end\": 20.897419,\n                \"confidence\": 0.9809511,\n                \"punctuated_word\": \"skilled\"\n              },\n              {\n                \"word\": \"and\",\n                \"start\": 20.897419,\n                \"end\": 21.09732,\n                \"confidence\": 0.86443037,\n                \"punctuated_word\": \"and\"\n              },\n              {\n                \"word\": \"qualified\",\n                \"start\": 21.09732,\n                \"end\": 21.59732,\n                \"confidence\": 0.9991331,\n                \"punctuated_word\": \"qualified\"\n              },\n              {\n                \"word\": \"and\",\n                \"start\": 22.37668,\n                \"end\": 22.57658,\n                \"confidence\": 0.6425459,\n                \"punctuated_word\": \"and\"\n              },\n              {\n                \"word\": \"didn't\",\n                \"start\": 22.57658,\n                \"end\": 22.85644,\n                \"confidence\": 0.99605036,\n                \"punctuated_word\": \"didn't\"\n              },\n              {\n                \"word\": \"get\",\n                \"start\": 22.85644,\n                \"end\": 23.096321,\n                \"confidence\": 0.99642485,\n                \"punctuated_word\": \"get\"\n              },\n              {\n                \"word\": \"the\",\n                \"start\": 23.096321,\n                \"end\": 23.49612,\n                \"confidence\": 0.9819676,\n                \"punctuated_word\": \"the\"\n              },\n              {\n                \"word\": \"the\",\n                \"start\": 23.49612,\n                \"end\": 23.61606,\n                \"confidence\": 0.48681125,\n                \"punctuated_word\": \"the\"\n              },\n              {\n                \"word\": \"same\",\n                \"start\": 23.61606,\n                \"end\": 23.85594,\n                \"confidence\": 0.9927979,\n                \"punctuated_word\": \"same\"\n              },\n              {\n                \"word\": \"opportunities\",\n                \"start\": 23.85594,\n                \"end\": 24.35594,\n                \"confidence\": 0.99927026,\n                \"punctuated_word\": \"opportunities\"\n              },\n              {\n                \"word\": \"that\",\n                \"start\": 24.535599,\n                \"end\": 24.7355,\n                \"confidence\": 0.99610835,\n                \"punctuated_word\": \"that\"\n              },\n              {\n                \"word\": \"we\",\n                \"start\": 24.7355,\n                \"end\": 24.85544,\n                \"confidence\": 0.9998843,\n                \"punctuated_word\": \"we\"\n              },\n              {\n                \"word\": \"have\",\n                \"start\": 24.85544,\n                \"end\": 25.05534,\n                \"confidence\": 0.9996139,\n                \"punctuated_word\": \"have\"\n              },\n              {\n                \"word\": \"today\",\n                \"start\": 25.05534,\n                \"end\": 25.55534,\n                \"confidence\": 0.9614277,\n                \"punctuated_word\": \"today.\"\n              }\n            ]\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n## I﻿s Whisper Right for Me?\n\nAre you an AI researcher? Sure! Where else can you get your hands on an implemented end-to-end deep-learning modern architecture to play with? As long as you don't need real-time transcription, whisper can be used for prototyping and experimenting. However, if you need real-time transcription, speed, and/or scalability, whisper is not ready for use today.\n\n## Testing the OpenAI Whisper Models\n\nHave you tried using any of the Whisper models since their release? Tell the community about your experience in our [GitHub Discussions](https://github.com/orgs/deepgram/discussions/30).\n\n";
						}
						async function compiledContent$2d() {
							return load$2d().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2d() {
							return (await import('./chunks/index.7f186e08.mjs'));
						}
						function Content$2d(...args) {
							return load$2d().then((m) => m.default(...args));
						}
						Content$2d.isAstroComponentFactory = true;
						function getHeadings$2d() {
							return load$2d().then((m) => m.metadata.headings);
						}
						function getHeaders$2d() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2d().then((m) => m.metadata.headings);
						}

const __vite_glob_0_135 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2d,
  file: file$2d,
  url: url$2d,
  rawContent: rawContent$2d,
  compiledContent: compiledContent$2d,
  default: load$2d,
  Content: Content$2d,
  getHeadings: getHeadings$2d,
  getHeaders: getHeaders$2d
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2c = {"title":"How to Write Vue 3 Composables for a Third-Party API Integration","description":"In this series, learn how to build a live streaming web application using Deepgram's speech-to-text API and Amazon Interactive Video Service.","date":"2022-03-25T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1647979609/blog/2022/03/how-to-write-vue-3-composables-for-a-third-party-API-integration/Building-Livestreaming-w-AmazonIVS.jpg","authors":["sandra-rodgers"],"category":"tutorial","tags":["aws","javascript","vuejs"],"seo":{"title":"How to Write Vue 3 Composables for a Third-Party API Integration","description":"In this series, learn how to build a live streaming web application using Deepgram's speech-to-text API and Amazon Interactive Video Service."},"shorturls":{"share":"https://dpgr.am/57da115","twitter":"https://dpgr.am/54e93b8","linkedin":"https://dpgr.am/f27181e","reddit":"https://dpgr.am/7b9f73b","facebook":"https://dpgr.am/c3ea449"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454030/blog/how-to-write-vue-3-composables-for-a-third-party-api-integration/ograph.png"}};
						const file$2c = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-to-write-vue-3-composables-for-a-third-party-api-integration/index.md";
						const url$2c = undefined;
						function rawContent$2c() {
							return "\n## Introduction\n\nThis post is a continuation of the series \"How to Build a Live Streaming Web Application with Amazon IVS and Deepgram.\"\n\n<Panel type=\"info\" title=\"Build a Live Streaming Web Application with Amazon IVS and Deepgram (SERIES)\">\n<ol> \n<li><a href=\"https://blog.deepgram.com/build-a-livestream-web-application-with-amazon-ivs-and-deepgram/\">How to Build a Live Streaming Web Application with Amazon IVS and Deepgram</a></li>\n<li><a href=\"https://blog.deepgram.com/build-a-livestream-web-application-vue-and-express-setup/\"> Build a Live Streaming Web Application: Vue and Express Setup</a></li>\n<li><a href=\"https://blog.deepgram.com/how-to-write-vue-3-composables-for-a-third-party-API-integration/\"> How to Write Vue 3 Composables for a Third-Party API Integration</a></li>\n\n<li><a href=\"https://blog.deepgram.com/asynchronous-logic-to-write-a-vue-3-and-deepgram-captions-component/\"> Asynchronous Logic to Write a Vue 3 and Deepgram Captions Component</a></li>\n</ol>\n</Panel>\n\n### Composables\n\nIn Vue.js, the term 'composables' refers to composition functions, a key feature of Vue 3's Composition API. While the API itself includes many composition functions that are core to its design, such as the `setup()` function or the reactivity functions `ref()` and `reactive()`, composables are those composition functions that I write myself to be used throughout my own project as needed.\n\nComposables are functions that encapsulate stateful logic, which means they are like little packages that are focused around performing one logical concern, and they keep track of state that changes due to the function running.\n\nFor example, I could write a composable function that toggles a menu open or closed. That logic could be used throughout an application, and it would need to keep track of the state of the menu being opened or closed. I would just need to import the composable into whatever file I need it and run the function.\n\n#### Composable Example `useMenu.js`\n\n```js\nimport { readonly, ref } from 'vue'\n\nconst isOpen = ref(false)\nconst toggleMenu = () => {\n  isOpen.value = !isOpen.value\n}\n\nexport default function useMenu() {\n  return {\n    isOpen: readonly(isOpen),\n    toggleMenu,\n  }\n}\n```\n\nToday I am going to introduce how to write a composable in Vue 3 to bring in the Amazon IVS video player SDK and to connect to my Amazon IVS streaming channel.\n\nIf you have wanted to learn more about how to write Vue 3 composables to use third-party technologies, this will be useful to you. In this post, I'll cover:\n\n*   Writing a Vue composable to bring in an external script\n*   Writing a Vue composable that is dependent on another composable, utilizing the Javascript function `setInterval`\n*   Amazon IVS integration using Vue 3 composables\n\n### Background to the Series\n\nIn the [first post](https://blog.deepgram.com/build-a-livestream-web-application-with-amazon-ivs-and-deepgram/) in the series, I explained how to build a live streaming web application using just Vanilla JS and HTML. That post covers setting up an Amazon IVS account, which is necessary to get the third-party technology example (Amazon IVS) in today's post working. **Setting up an Amazon IVS channel is necessary to build the second composable in this tutorial, but the first one can be built without doing that**.\n\nIn the second post, I started building a full-stack application using Vue 3. I set up Vue-Router, Vuex, and a backend server which all helped me build a feature to restrict access to the streaming page of my site. Visitors to the site are required to enter a code, and that code is verified on the backend, resulting in the user being able to navigate to the streaming page. **There is no need to read that post to be able to go through this tutorial on composables.**\n\nToday I'll build the video streaming page in Vue 3, focusing on building this feature with Vue composables. If you want a refresher on Vue composables, check out my post [Reusability with Composables](https://blog.deepgram.com/diving-into-vue-3-reusability-with-composables/).\n\nThe code for today's tutorial can be found in [this Github repo](https://github.com/deepgram-devs/livestream-amazonIVS-and-deepgram/tree/amazonIVS-composables), the branch named \"amazonIVS-composables.\"\n\nNow I'll get into it!\n\n## Files Organization\n\nThis project has two main views, the landing page and the streaming page. Today I'll be working entirely in the streaming page. In my project, I have named this file `StreamChannel.vue`, and it is in the `Views` folder.\n\nThe `StreamChannel.vue` will eventually be made up of two components - one for **the video player** that relies on the Amazon IVS technology and one for the **closed-captions** that relies on the Deepgram speech-to-text technology. Today I'll only build the video player component.\n\nI'll set up two folders to start - a `components` folder and a `composables` folder, both in the `src` folder. In the `components` folder, I'll create a `VideoPlayer.vue` component.\n\nThe `composables` folder is where I will put the composition functions that contain the logic that makes the video player work. **A common practice in Vue 3 is to name composables so that they begin with 'use'.** The name will identify what the composable does. The two composables I will be writing today are `useIVSPlayer` and `useIVSChannel`. Here is what they will do:\n\n1.  `useIVSPlayer.js` - this composable will bring in the [Amazon IVS video player script](https://docs.aws.amazon.com/ivs/latest/userguide/player-web.html) so that the HTML video element is enhanced with the Amazon IVS Player Web SDK.\n\n2.  `useIVSChannel.js` - this composable will check at an interval if the player in the `useIVSPlayer` script has loaded, and if it has, it will create a connection to my Amazon IVS channel, updating state to show that the channel is connected.\n\nMy [Github repo](https://github.com/deepgram-devs/livestream-amazonIVS-and-deepgram/tree/amazonIVS-composables/src) for this project shows how I have set up these folders.\n\n## VideoPlayer Component\n\nThe `VideoPlayer.vue` component will be a video player that shows the live stream. The Amazon IVS video player script looks for an HTML `<video>` element with a specific ID and then takes control of that element to bring in its own specially made video player with Amazon IVS optimizations. So the first composable I write will be **a function that brings in the Amazon IVS player with a script**.\n\nIn the `VideoPlayer.vue` file, I will start by writing the HTML I need in the Vue template so that I have a basic video player. I've given it a height and a width that I prefer, and the `id=\"video-player\"` so that I can use that id later to bring in the Amazon IVS player. The attributes that the html `<video>` element supports are listed [here](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/video#attributes).\n\n```html\n<template>\n  <div>\n    <p class=\"status\">AWS Channel {{ IVSStatus }}</p>\n    <video\n      width=\"520\"\n      height=\"440\"\n      id=\"video-player\"\n      controls\n      playsinline\n    ></video>\n  </div>\n</template>\n```\n\nThe `IVSStatus` will be set to a `ref` property. For now, I'll make that property a string 'Is Not Connected,' but later, it will be hooked up to the status of the channel, and it will update to say 'Is Connected' when the channel is available.\n\nHere is my setup function in the script block with that `ref`:\n\n```js\n<script>\nimport { ref } from \"vue\";\n\nexport default {\n  name: \"VideoPlayer\",\n  setup() {\n    let IVSStatus = ref(\"Is Not Connected\");\n\n    return { IVSStatus };\n  },\n};\n</script>\n```\n\nThe last thing I need to do to be able to see this video player is add the component to the `StreamChannel.vue` page in src/views/StreamChannel.vue:\n\n```html\n<template>\n  <div>\n    <h1>Stream Channel</h1>\n    <video-player />\n  </div>\n</template>\n\n<script>\n  import VideoPlayer from '@/components/VideoPlayer'\n  export default {\n    name: 'StreamChannel',\n    components: {\n      VideoPlayer,\n    },\n  }\n</script>\n```\n\nHere is how the page will look:\n\n![Video Player Page](https://res.cloudinary.com/deepgram/image/upload/v1647979619/blog/2022/03/how-to-write-vue-3-composables-for-a-third-party-API-integration/VideoPlayerComponent.png)\n\nNow I am ready to write the first composable, the `useIVSPlayer.js` composition function, which will do the logic to bring in the Amazon IVS player.\n\n## Composable to Bring in An External Script\n\nThe `useIVSPlayer.js` composable will bring a script into my `StreamChannel.vue` component. The [docs](https://docs.aws.amazon.com/ivs/latest/userguide/player-web.html) at Amazon IVS say that I need this script so that the player is brought in:\n\n```html\n<script src=\"https://player.live-video.net/1.8.0/amazon-ivs-player.min.js\">\n```\n\nOne way to bring in an external script is to add the script to the `<head>` in my `index.html` page:\n\n```html\n<head>\n  <meta charset=\"utf-8\" />\n  ...\n  <script\n    type=\"text/javascript\"\n    src=\"https://player.live-video.net/1.8.0/amazon-ivs-player.min.js\"\n  ></script>\n</head>\n```\n\nThen I can type \"IVSPlayer\" in the console, and I should see the module there.\n\n<img src=\"https://res.cloudinary.com/deepgram/image/upload/v1647979619/blog/2022/03/how-to-write-vue-3-composables-for-a-third-party-API-integration/ConsoleIVSPlayer.png\" alt=\"IVSPlayer in console\" style=\"width: 50%; margin:auto;\">\n\nIf I choose this way to bring in the script, the module will be available on every page of my application. However, sometimes it is preferable to make a third-party technology only available on the page where it is needed. In that case, I need to remove that script from the `<head>` and bring it in a different way.\n\nIf I only want to bring in the script on the `StreamChannel.vue` page, I need to write logic to **build out the script tag with the src of the player URL**. I will use a Vue composable, which is just a Javascript function, to build out this logic. Writing it as a composable makes it reusable, so I can easily copy it into other projects or bring it into other components in my application as needed.\n\nThis `useIVSPlayer.js` composable will:\n\n*   be a **Promise**, since I need to account for the small amount of time it will take to load the script\n*   use `createElement` to create the script tag and `setAttribute` to add the src\n*   append the script to the head with `appendChild`\n*   use the global event listener `onload` to trigger the promise being resolved\n\nHere is the composable:\n\n```js\nexport default new Promise((res) => {\n  const script = document.createElement('script')\n  script.setAttribute(\n    'src',\n    'https://player.live-video.net/1.6.1/amazon-ivs-player.min.js'\n  )\n  document.head.appendChild(script)\n  script.onload = () => res()\n  script.onerror = () => {\n    throw 'IVS PLAYER ERROR'\n  }\n})\n```\n\nI start with `export default` because I need to be able to import this logic into my `VideoPlayer.vue` component.\n\nNow in `VideoPlayer.vue` I can import the composable. I am going to use a `.then()` method because `useIVSPlayer` is a promise. The `.then()` method will wait for the promise to resolve before doing whatever I write inside the `.then()`.\n\nFor now, I will check that the player is available and `console.log` that it is ready. Later, I'll add logic inside the `.then()` to bring in my streaming channel.\n\nHere is the `setup` function now in the `VideoPlayer.vue` component:\n\n```js\nsetup() {\n    let IVSStatus = ref(\"Is Not Connected\");\n\n    useIVSPlayer.then(() => {\n      if (window.IVSPlayer) {\n        console.log(\"player loaded\");\n      }\n    });\n    return { IVSStatus };\n  },\n```\n\nFor now, I will keep `IVSStatus` as \"Is Not Connected\" because even though I have brought in the Amazon IVS player, I still need to hook up the video player to my channel stream. I'll do that in the next section.\n\n## Composable to Play Channel Stream\n\nNow I want to build a composable that will load my channel stream into the IVS player that I just brought in. This composable will do the following:\n\n*   Check that the IVS Player script is loaded and then create a new player that I can use for my stream.\n*   Load my channel stream into the player by adding the playback URL.\n*   Turn the player on with the `play()` method.\n*   Check that the stream is connected and loaded. This will be done with `setInterval` since I don't know how long the delay might be.\n\nFirst, I'll write my composable with an `export default` so I can import it into other files as needed. I'll also bring in `ref` from vue so I can track the state of the channel being loaded. I'll create a `ref` variable called `playerIsLoaded` and set it to `false` to start:\n\n```js\nimport { ref } from 'vue'\n\nexport default function useIVSChannel() {\n  let playerIsLoaded = ref(false)\n\n  return { playerIsLoaded }\n}\n```\n\nEverything I need to do in this composable is dependent on the IVS player (the one I brought in with the `useIVSPlayer` composable) being loaded. So I'll wrap all my logic in an `if` statement to check that it is loaded and supported.\n\n```js\nif (window.IVSPlayer && window.IVSPlayer.isPlayerSupported) {\n  // all logic here\n}\n```\n\nI'll use the player SDK's method `create()` to create a player for my channel. Then I'll attach the player to the HTML video element in my `VideoPlayer.vue` component with the SDK's `attachHTMLVideoElement()` method and I'll use `.load()` to load my channel's playback URL. I'll use `play()` to play the channel stream:\n\n```js\nconst player = window.IVSPlayer.create()\nplayer.attachHTMLVideoElement(document.getElementById('video-player'))\nplayer.load('PLAYBACK_URL')\nplayer.play()\n```\n\n(The playback URL is unique to my channel, so it has to be taken from the Amazon IVS console. See my walkthrough in [this post](https://deploy-preview-631--romantic-gates-9c0b5e.netlify.app/blog/2022/03/build-a-livestream-web-application-with-amazon-ivs-and-deepgram/) for more information.)\n\nNow I need to check that the channel stream is loaded. This won't happen instantaneously, but it should load relatively quickly. I don't want to use a `setTimeout `because I don't know how long it will take and I'm concerned about the idea of adding a super long timeout if I don't have to. I'll use `setInterval` to check for the stream being loaded.\n\n`setInterval` is a DOM API method that repeatedly calls a function until some other trigger happens to turn it off. In this case, the other trigger will be the channel being loaded.\n\nThe way to turn it off is to use `clearInterval`. I'm going to assign `setInterval` to a variable called `checkInterval`. The callback of `setInterval` will run every 500 milliseconds. Inside that callback, it will check that the channel has loaded, and once it has, it will set `playerIsLoaded` to `true` and clear everything by passing `checkInterval` to `clearInterval`.\n\nHere's the logic I just described:\n\n```js\nlet checkInterval = setInterval(() => {\n  if (player.core.isLoaded) {\n    playerIsLoaded.value = true\n    clearInterval(checkInterval)\n  }\n}, 500)\n```\n\nI'll return the ref `playerIsLoaded` from the composable, so I have access to it in the `VideoPlayer.vue` component. I want to watch that value so that when it changes, the `IVSStatus` value in the `VideoPlayer.vue` template updates to show that the channel is connected.\n\nHere is the composable in its entirety:\n\n```js\nimport { ref } from 'vue'\n\nexport default function useIVSChannel() {\n  let playerIsLoaded = ref(false)\n\n  if (window.IVSPlayer && window.IVSPlayer.isPlayerSupported) {\n    const player = window.IVSPlayer.create()\n    player.attachHTMLVideoElement(document.getElementById('video-player'))\n    player.load('PLAYBACK_URL')\n    player.play()\n\n    let checkInterval = setInterval(() => {\n      if (player.core.isLoaded) {\n        playerIsLoaded.value = true\n        clearInterval(checkInterval)\n      }\n    }, 500)\n\n    return { playerIsLoaded }\n  }\n}\n```\n\nThe last thing I need to do to get this working is go back to the `VideoPlayer.vue` component and run the composable function inside `setup` and update `IVSStatus` based on the channel being connected, which I'll do in the next section.\n\n## Run the Composables\n\nIn `VideoPlayer.vue`, I will run the `useIVSChannel` composable inside `setup`. Actually, I'll run it inside the `.then()` that I already wrote earlier, which will cause `useIVSChannel` to run after `useIVSPlayer` has resolved. (I have to remember to import `useIVSChannel` from the `composables` folder if I want to use it.)\n\n```js\nuseIVSPlayer.then(() => {\n  const { playerIsLoaded } = useIVSChannel()\n})\n```\n\nI deconstruct `playerIsLoaded` off of `useIVSChannel` so that I can watch that reactive reference. I'll use Vue's `watch` method to make a side effect occur when the `playerIsLoaded` value changes to true (i.e., when the channel is connected). The side effect will be that the `IVSStatus` will update to \"Is Connected\":\n\n```js\nwatch(playerIsLoaded, () => {\n  if (playerIsLoaded.value) {\n    IVSStatus.value = 'Is Connected'\n  }\n})\n```\n\nHere is the entire script for the `VideoPlayer.vue` component:\n\n```js\n<script>\nimport { ref, watch } from \"vue\";\nimport useIVSPlayer from \"../composables/useIVSPlayer\";\nimport useIVSChannel from \"../composables/useIVSChannel\";\n\nexport default {\n  name: \"VideoPlayer\",\n  setup() {\n    let IVSStatus = ref(\"Is Not Connected\");\n\n    useIVSPlayer.then(() => {\n      const { playerIsLoaded } = useIVSChannel();\n      watch(playerIsLoaded, () => {\n        if (playerIsLoaded.value) {\n          IVSStatus.value = \"Is Connected\";\n        }\n      });\n    });\n    return { IVSStatus };\n  },\n};\n</script>\n```\n\nHowever, nothing will happen in my video player in the browser if I have not turned on my stream. In [the first post](https://deploy-preview-631--romantic-gates-9c0b5e.netlify.app/blog/2022/03/build-a-livestream-web-application-with-amazon-ivs-and-deepgram/) in this series, I showed how to set up OBS to use their software on my computer to create a stream, which I connected to an Amazon IVS channel.\n\nIf I turn on my stream in OBS, I should see myself in the video player in my application now!\n\n![Video Stream working](https://res.cloudinary.com/deepgram/image/upload/v1647979619/blog/2022/03/how-to-write-vue-3-composables-for-a-third-party-API-integration/VideoStreamWorking.png)\n\n## Conclusion\n\nVue composables are useful for writing standalone logic that can be reused, but if one composable depends on another, it can be tricky. In this post, I showed how a composable can be written as a promise and how `setInterval` can be used to check if something the composable depends on has happened yet. These are two ways to write composables that may have asynchronous events occurring.\n\nIn the next post, I'll show how to write composables for using Deepgram's API to create text captions for my stream. I'll write a composable to use the browser MediaStreams API (a great example of a composable that can be reused in different contexts). I'll also show how to use `fetch` in a composable to get a token from the backend.\n\nI hope you'll join me for the next post. Follow me on [Twitter](https://twitter.com/sandra_rodgers_) so you don't miss it!\n\n        ";
						}
						async function compiledContent$2c() {
							return load$2c().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2c() {
							return (await import('./chunks/index.c585014d.mjs'));
						}
						function Content$2c(...args) {
							return load$2c().then((m) => m.default(...args));
						}
						Content$2c.isAstroComponentFactory = true;
						function getHeadings$2c() {
							return load$2c().then((m) => m.metadata.headings);
						}
						function getHeaders$2c() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2c().then((m) => m.metadata.headings);
						}

const __vite_glob_0_136 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2c,
  file: file$2c,
  url: url$2c,
  rawContent: rawContent$2c,
  compiledContent: compiledContent$2c,
  default: load$2c,
  Content: Content$2c,
  getHeadings: getHeadings$2c,
  getHeaders: getHeaders$2c
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2b = {"title":"How Voice Technology Creates a More Accessible World [7 Ways]","description":"Voice tech isn’t just shaking up enterprises; it’s also providing real quality-of-life improvements for people. Here are 7 examples.","date":"2022-04-26T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981419/blog/how-voice-technology-creates-accessible-world/how-voice-tech-creates-more-accessible-world-thumb.png","authors":["chris-doty"],"category":"speech-trends","tags":["inclusion"],"seo":{"title":"How Voice Technology Creates a More Accessible World [7 Ways]","description":"Voice tech isn’t just shaking up enterprises; it’s also providing real quality-of-life improvements for people. Here are 7 examples."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981419/blog/how-voice-technology-creates-accessible-world/how-voice-tech-creates-more-accessible-world-thumb.png"},"shorturls":{"share":"https://dpgr.am/2434c02","twitter":"https://dpgr.am/5da4554","linkedin":"https://dpgr.am/abbb9ba","reddit":"https://dpgr.am/af3a26b","facebook":"https://dpgr.am/c74c058"}};
						const file$2b = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-voice-technology-creates-accessible-world/index.md";
						const url$2b = undefined;
						function rawContent$2b() {
							return "Voice technologies like speech-to-text (STT) (also called automatic speech recognition, or ASR) have had a profound impact across the enterprise. From contact centers analyzing interactions with customers to automatically creating meeting transcripts, voice tech is a huge part of businesses today. In fact, in the recent [State of Voice Tech 2022](https://offers.deepgram.com/2022-state-of-voice-technology-report) report, 99% of respondents indicated that voice technologies are going to be a large part of their future plans. But something that's often overlooked is the way that voice technologies are improving lives.\n\nBeing able to interact with one's voice is creating improvements in accessibility and access to information for everyone, but especially for those who might have trouble using things like computers or smartphones. In this blog post, we'll provide an overview of the top 7 ways that voice tech is supporting accessibility and inclusion today.\n\n## Top 7 Use Cases for Voice Technology in Improving Accessibility\n\nVoice interface technologies have accessibility features that can be used by people with all kinds of disabilities. And because these systems are constantly getting smarter-[with newly trained models](https://blog.deepgram.com/deep-learning-speech-recognition/)- they're able to add more languages and dialects over time, making them more accessible for people whose primary language is not English. Here are 7 of the top ways that voice tech is creating a more accessible world.\n\n### 1. Text-to-Speech\n\nText-to-speech is simply turning written words into spoken language. This can be used to help people who have trouble reading text on a screen due to vision problems. One of the main uses for text-to-speech is to make traditional books and digital documents more accessible. For example, ebook apps often have features that will read books aloud to you, making it possible for people who are blind or have low vision to enjoy these works, even if an audio version of the book doesn't exist.\n\n### 2. Voice Interface Systems\n\nVoice interfaces-perhaps most famously known in the case of Alexa-can provide alternative ways to interact with technology. Because these systems let you ask in spoken language, and provide replies out loud, these systems can help anyone who has trouble with visual interfaces. The trouble might be because someone is visually impaired, which can make navigation using visual icons and written menus difficult. They might have cognitive issues that create challenges learning how to use these devices. Or they might be elderly and unable or unwilling to learn how to use a new cell phone or computer.\n\n### 3. Smart Home Technology\n\nMost of us take for granted being able to easily move around our homes and do things like turn on lights and answer the door. But for people with mobility impairments, all of these things can present challenges. Smart home technology is providing ways for people to control their home easily and from a distance. This kind of technology is often related to the voice interface systems mentioned above, with the control options provided by assistants like Alexa or Google Home. Paired with the right technology, these systems can help people with mobility impairments do things like turn lights on and off, answer their doorbell without needing to get up, open and close blinds, and even call for help in an emergency.\n\n### 4. Real-Time Captioning and Transcription\n\nVoice tech can be used to provide real-time captioning and transcription services. The options for this, as the technology has improved in recent years and moved to [end-to-end deep learning systems](https://blog.deepgram.com/deep-learning-speech-recognition/), are nearly endless. For example, you might use a speech-to-text system to automatically transcribe what you're saying and display it on a badge, as in [this project by Deepgram Senior Developer Advocate Kevin Lewis](https://blog.deepgram.com/live-transcription-badge-video/).\n\nA similar, commercial product is being developed by one of our customers, [Badger](https://satellitedisplay.com/#Product). In addition to real-time captioning, transcriptions after the fact can also help some people. For example, [classroom lectures can present a number of accessibility challenges](https://blog.deepgram.com/automatic-speech-recognition-education/), but [using a system that provides transcripts of classes](https://blog.deepgram.com/classroom-captioner/) can help those students both during class and after. [Jamworks](https://jamworks.com/), another Deepgram customer, is working to make education more inclusive and accessible.\n\n<WhitepaperPromo whitepaper=\"latest\"></WhitepaperPromo>\n\n### 5. Communication\n\nFor people who have difficulty communicating, voice tech can be a godsend. For example, if you are [deaf or hard of hearing (HOH)](https://blog.deepgram.com/asr-important-deaf-hoh-community/), you often have to rely on an interpreter to communicate with people with hearing. Even if you want to make a phone call, you usually have to rely on an intermediary service using a sign language interpreter or transcription service. However, by using speech-to-text and text-to-speech options, people who are deaf or HOH have more options for communicating, from purpose-built apps to simply using the speech-to-text features on one's phone to jot down what someone is saying. In addition, there are a number of apps that can provide alternative and augmentative communication (AAC) options, which can be used by people with conditions like autism or cerebral palsy. These devices allow users to press labeled keys to create speech.\n\n### 6. Workplace Accessibility\n\nVoice tech is also being used to create more accessibility in the workplace. In some cases, this means creating systems that can be used by employees with disabilities. But it's also being used to create more inclusive workplaces, by providing employees with accessibility features that can help them do their jobs better. This might include providing live or after-the-fact transcripts on meetings, tools like topic detection that can help people understand the important parts of long phone or in-person interactions, as well as the features discussed above, such as text to speech, depending on a person's individual needs.\n\n### 7.  Navigation\n\nAnd finally, GPS systems have improved in recent years, but they can still present accessibility challenges for some people. Voice-based navigation can provide turn-by-turn directions that are easy to follow, even for people who can't easily see a screen to follow a visual map route. This is especially true if you're trying to get around an unfamiliar area, or if you're trying to find a specific location within a large building-a case everyone needs technologies to make the world more accessible to them.\n\n## Wrapping up\n\nThese are just a few of the ways that voice technologies are making the world a more accessible place for everyone. As we continue to see advances in this technology, we'll see even more innovation and development around creating accessible voice technologies. The potential applications are vast and we're only just beginning to scratch the surface of what's possible. If you'd like to learn more about how ASR systems work, check out our ebook **[What is ASR?](https://offers.deepgram.com/what-is-asr-ebook)** to get a better grip on how the technology works and the impact it's having today.";
						}
						async function compiledContent$2b() {
							return load$2b().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2b() {
							return (await import('./chunks/index.6b0290e3.mjs'));
						}
						function Content$2b(...args) {
							return load$2b().then((m) => m.default(...args));
						}
						Content$2b.isAstroComponentFactory = true;
						function getHeadings$2b() {
							return load$2b().then((m) => m.metadata.headings);
						}
						function getHeaders$2b() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2b().then((m) => m.metadata.headings);
						}

const __vite_glob_0_137 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2b,
  file: file$2b,
  url: url$2b,
  rawContent: rawContent$2b,
  compiledContent: compiledContent$2b,
  default: load$2b,
  Content: Content$2b,
  getHeadings: getHeadings$2b,
  getHeaders: getHeaders$2b
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2a = {"title":"Identifying the Best Agent to Respond in Your IVR System","description":"This tutorial will use Python, Twilio, Flask 2.0, and Deepgram API speech-to-text in an IVR system to identify the best customer support agent to respond to various spoken conversations based on language detection.","date":"2022-09-27T17:28:43.057Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1663954319/blog/identifying-best-agent-to-respond-ivr-system-python/2209-Identifying-the-Best-Agent-to-Respond-in-Your-IVR-System-blog_2x_oxhwox.jpg","authors":["tonya-sims"],"category":"tutorial","tags":["python","ivr"],"seo":{"title":"Identifying the Best Agent to Respond in Your IVR System","description":"This tutorial will use Python, Twilio, Flask 2.0, and Deepgram API speech-to-text in an IVR system to identify the best customer support agent to respond to various spoken conversations based on language detection."},"shorturls":{"share":"https://dpgr.am/f0a2b5d","twitter":"https://dpgr.am/2643cd7","linkedin":"https://dpgr.am/b789d4c","reddit":"https://dpgr.am/b2138e4","facebook":"https://dpgr.am/5b0468b"}};
						const file$2a = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/identifying-the-best-agent-to-respond-in-your-ivr-system/index.md";
						const url$2a = undefined;
						function rawContent$2a() {
							return "\nWhat would you say if I told you that you could detect spoken conversational language using AI in a speech-to-text transcript with Python? \n\nWould you spit your beer out?\n\nOk, maybe your water, but the point is I built a cool conversational AI project with an Interactive Voice Response (IVR) using Twilio, a speech recognition provider, and Python. The best part about it is that it was reasonably easy to build using Flask 2.0. The purpose was to identify the best virtual customer support agent to respond to a call.\n\nI would love to walk you through the project, but if you want to skip ahead to the code, scroll to the bottom of this blog post.\n\n## Create Voice Recognition Phone IVR With Speech Recognition Using Twilio and Python\n\nThis project was my first attempt at building an IVR with AI in Python, so I researched how these interactive voice response systems work. Simply put, you can think of them as a tree with many branches. They allow you to interact with a system, like an automated phone customer support agent, before being connected or transferred to a representative.\n\nFor example, you may be prompted to press “2” on your phone to connect to a department and then “1” to speak to a live customer support agent. I’m sure we’ve all been in that situation.\n\nTwilio is the best choice for building the IVR because of its easy-to-navigate dashboard and simplicity. Also, since I’m using Python, they have tons of tutorials on implementing IVR systems like [the one in Flask I’m using for this tutorial](https://www.twilio.com/docs/voice/tutorials/build-interactive-voice-response-ivr-phone-tree/python). \n\nI also needed a speech-to-text API and leveraged Deepgram. We have a [Python SDK](https://github.com/deepgram/python-sdk) I tapped into that made it super quick and easy to get up and running with the voice recognition transcription. \n\nDeepgram also has language detection with prerecorded audio in which you can detect over 30 [supported languages](https://developers.deepgram.com/documentation/features/language/) like Hindi, Spanish, and Ukrainian, to name a few. \n\nLet’s get to the meat of the project: the code. \n\n## Code Breakdown for Creating IVR Speech-to-Text With Language Detection Using Python\n\nImagine you had to build a Python application that detects different conversational languages. It would help if you rerouted phone calls from customers using an IVR system to the appropriate virtual customer agent who speaks their language.\n\nThe following Python code breakdown demonstrates how to do so. There are just a few things I had to set up before the coding started. It’s painless, I promise.  \n\n1.  Grab a [Deepgram API Key](https://console.deepgram.com/signup?jump=keys). I needed this to tap into the speech-to-text Python SDK. \n2.  Create a Twilio account and voice phone number [here](https://www.twilio.com/login?g=%2Fconsole%2Fphone-numbers%2Fincoming%3F\\&t=98a31204d675661e83d6f3d24078fc1b9f3d6c8f85f0695f6c24ccb513fd05cf). This allowed me to make an outgoing call and navigate the IVR with dial prompts. \n3.  Install [ngrok](https://ngrok.com/) to test my webhooks locally. \n\nNext, I made a new directory to hold all my Python files and activated a [virtual environment](https://blog.deepgram.com/python-virtual-environments/) to `pip install` all of my Python packages.\n\nThese are the packages I installed:\n\n    pip install Flask\n    pip install ‘flask[async]’\n    pip install Twilio\n    pip install deepgram-sdk\n    pip install python-dotenv\n\nAfter creating my directory, I downloaded three audio files with different spoken languages from [this website](https://www.audio-lingua.eu/?lang=en) and added them to my project in a folder called **languages**.\n\nI created a file called **views.py** that contains most of my Flask 2.0 Python code. You’ll see the entirety of this code at the bottom of this post, but I’ll walk through the most critical parts of it.\n\nThis code is where the Deepgram Python speech-to-text transcription magic happens. I’m transcribing the audio MP3 file and returning the transcript and detected language. The API detected the conversational language and provided a language code like `es` for Spanish. \n\n```python\nasync def deepgram_transcribe(PATH_TO_FILE):\n\n  # Initializes the Deepgram SDK\n   deepgram = Deepgram(os.getenv(\"DEEPGRAM_API_KEY\"))\n\n  # Open the audio file\n   with open(PATH_TO_FILE, 'rb') as audio:\n\n      # ...or replace mimetype as appropriate\n       source = {'buffer': audio, 'mimetype': 'audio/mp3'}\n       response = await deepgram.transcription.prerecorded(source, {\"detect_language\": True})\n\n\n       if 'transcript' in response['results']['channels'][0]['alternatives'][0]:\n           transcript = response['results']['channels'][0]['alternatives'][0]['transcript']\n\n\n       if 'detected_language' in response['results']['channels'][0]:\n           detected_language = response['results']['channels'][0]['detected_language']\n\n  \n   return transcript, detected_language\n```\n\nAt the top of the file, I created a Python dictionary that acts as a lookup. This dictionary contains the language code as a key and the name of the customer support agent that speaks that language as the value. \n\n```python\ncustomer_service_reps = {\n                           \"fr\": \"Sally\",\n                           \"es\": \"Pete\",\n                           \"de\": \"Ann\"\n                       }\n```\n\nI created a POST route and prompted the user to press either 1,2, or 3, each for different languages. For example, if a customer presses 2 when they call in, they’ll get routed to the agent who speaks French.\n\nWhichever option is selected will invoke a private function, as noted in the `menu` function. When option 2 is pressed, the function  `_french_recording` is called. \n\n```python\n@app.route('/ivr/welcome', methods=['POST'])\ndef welcome():\n   response = VoiceResponse()\n   with response.gather(\n       num_digits=1, action=url_for('menu'), method=\"POST\"\n   ) as g:\n\n       g.say(message=\"Thanks for calling the Deepgram Speech-to-Text Python SDK. \" +\n             \"Please press 1 for Spanish\" +\n             \"Press 2 for French\" +\n             \"Press 3 for German\", loop=3)\n\n\n   return twiml(response)\n\n\n@app.route('/ivr/menu', methods=['POST'])\nasync def menu():\n   selected_option = request.form['Digits']\n   option_actions = {'1': _spanish_recording,\n                     '2': _french_recording,\n                     '3': _german_recording}\n\n\n   if selected_option in option_actions:\n       response = VoiceResponse()\n       await option_actions[selected_option](response)\n\n       return twiml(response)\n\n\n   return _redirect_welcome()\n```\n\nI created a private function for each spoken language, and when they’re selected, that method will get called, and a phone response will say the message. For French, the automated IVR response will be \\`”This is the French response and Sally will help you.”\\`\n\n```python\nasync def _spanish_recording(response):\n   recording = \"languages/spanish-recording.mp3\"\n   spanish_transcript = await deepgram_transcribe(recording)\n\n   representative = customer_service_reps[spanish_transcript[1]]\n\n\n   response.say(f\"This is the Spanish response and {representative} will help you.\",\n                voice=\"alice\", language=\"en-US\")\n\n   response.hangup()\n\n   return response\n\n\n\nasync def _french_recording(response):\n\n   recording = \"languages/french-recording.mp3\"\n\n   french_transcript = await deepgram_transcribe(recording)\n\n\n\n   representative = customer_service_reps[french_transcript[1]]\n\n   response.say(f\"This is the French response and {representative} will help you.\",\n                voice=\"alice\", language=\"en-US\")\n\n\n   response.hangup()\n\n   return response\n\n\nasync def _german_recording(response):\n   recording = \"languages/german-recording.mp3\"\n\n   german_transcript = await deepgram_transcribe(recording)\n\n\n\n   representative = customer_service_reps[german_transcript[1]]\n\n\n\n   response.say(f\"This is the German response and {representative} will help you.\",\n                voice=\"alice\", language=\"en-US\")\n\n\n   response.hangup()\n\n   return response\n```\n\nI also created a **templates** folder in the main Python Flask project directory with a blank **index.html** file. We don’t need anything in this file but feel free to add any HTML or Jinja.\n\nTo run the application, I fired up two terminals simultaneously in Visual Studio Code, one to run my Flask application and another for ngrok. Both are important, and you’ll need the ngrok url to add to your Twilio dashboard.\n\nTo run the Flask application, I used this command from the terminal:\n\n`FLASK_APP=views.py FLASK_DEBUG=1 flask run` allows my application to run in debug mode, so when changes are made to my code, there’s no need for me to keep stopping and starting the terminal. \n\nIn the other terminal window, I ran this command:\n\n`ngrok http 5000`\n\nMake sure to grab the ngrok url, which is different from the one in the Flask terminal. It looks something like this: [`https://3afb-104-6-9-133.ngrok.io`](https://3afb-104-6-9-133.ngrok.io).\n\nIn the Twilio dashboard, click on `Manage -> Active Numbers`, then click on the purchased number. Put the ngrok url in the webhook with the following endpoint: [`https://3afb-104-6-9-133.ngrok.io/ivr/welcome`](https://3afb-104-6-9-133.ngrok.io/ivr/welcome), which is the unique ngrok url followed by the Flask route in the Python application `/ivr/welcome`.\n\n![ ivr-call-agent-system-with-twilio-and-python](https://res.cloudinary.com/deepgram/image/upload/v1663955729/blog/identifying-best-agent-to-respond-ivr-system-python/ivr-call-agent-system-with-twilio-and-python_fqs5mc.png \"IVR Call Agent System with Twilio and Python\")\n\nNow, dial the Twilio number and follow the prompts, and you’ll get routed to the best customer agent to handle your call based on speech-to-text language detection!\n\n## Conclusion\n\nPlease let me know if you followed this tutorial or built your project using Python with Deepgram’s language detection. Please hop over to our [Deepgram Github Discussions](https://github.com/orgs/deepgram/discussions) and send us a message.\n\n## The Python Flask Code for the IVR Speech-To-Text Application\n\n**\\\nMy project structure**:\n\n![flask-python-ivr-twilio-project-structure.](https://res.cloudinary.com/deepgram/image/upload/v1663955729/blog/identifying-best-agent-to-respond-ivr-system-python/flask-python-ivr-twilio-project-structure_o7pgkw.png \"Flask Python IVR Twilio Project Structure\")\n\n**views.py**\n\n```python\nfrom deepgram import Deepgram\nfrom flask import (\n   Flask,\n   render_template,\n   request,\n   url_for,\n)\n\nfrom twilio.twiml.voice_response import VoiceResponse\nfrom view_helpers import twiml\nfrom dotenv import load_dotenv\nimport asyncio, json, os\n\napp = Flask(__name__)\n\n\n\ncustomer_service_reps = {\n\n                           \"fr\": \"Sally\",\n                           \"es\": \"Pete\",\n                           \"de\": \"Ann\"\n\n                       }\n\n\n\nasync def deepgram_transcribe(PATH_TO_FILE):\n  # Initializes the Deepgram SDK\n   deepgram = Deepgram(os.getenv(\"DEEPGRAM_API_KEY\"))\n\n  # Open the audio file\n   with open(PATH_TO_FILE, 'rb') as audio:\n      # ...or replace mimetype as appropriate\n      source = {'buffer': audio, 'mimetype': 'audio/mp3'}\n      response = await deepgram.transcription.prerecorded(source, {\"detect_language\": True})\n\n       if 'transcript' in response['results']['channels'][0]['alternatives'][0]:\n           transcript = response['results']['channels'][0]['alternatives'][0]['transcript']\n\n\n       if 'detected_language' in response['results']['channels'][0]:\n           detected_language = response['results']['channels'][0]['detected_language']\n\n  \n   return transcript, detected_language\n\n\n\n@app.route('/')\n@app.route('/ivr')\ndef home():\n   return render_template('index.html')\n\n\n@app.route('/ivr/welcome', methods=['POST'])\ndef welcome():\n   response = VoiceResponse()\n   with response.gather(\n       num_digits=1, action=url_for('menu'), method=\"POST\"\n   ) as g:\n       g.say(message=\"Thanks for calling the Deepgram Speech-to-Text Python SDK. \" +\n             \"Please press 1 for Spanish\" +\n             \"Press 2 for French\" +\n             \"Press 3 for German\", loop=3)\n\n\n   return twiml(response)\n\n\n@app.route('/ivr/menu', methods=['POST'])\nasync def menu():\n   selected_option = request.form['Digits']\n   option_actions = {'1': _spanish_recording,\n                     '2': _french_recording,\n                     '3': _german_recording}\n\n\n   if selected_option in option_actions:\n       response = VoiceResponse()\n       await option_actions[selected_option](response)\n\n       return twiml(response)\n\n\n   return _redirect_welcome()\n\n\n\nasync def _spanish_recording(response):\n   recording = \"languages/spanish-recording.mp3\"\n   spanish_transcript = await deepgram_transcribe(recording)\n\n   representative = customer_service_reps[spanish_transcript[1]]\n\n   response.say(f\"This is the Spanish response and {representative} will help you.\",\n                voice=\"alice\", language=\"en-US\")\n\n   response.hangup()\n\n   return response\n\n\nasync def _french_recording(response):\n   recording = \"languages/french-recording.mp3\"\n   french_transcript = await deepgram_transcribe(recording)\n\n   representative = customer_service_reps[french_transcript[1]]\n\n   response.say(f\"This is the French response and {representative} will help you.\",\n                voice=\"alice\", language=\"en-US\")\n\n\n   response.hangup()\n\n   return response\n\n\n\nasync def _german_recording(response):\n   recording = \"languages/german-recording.mp3\"\n   german_transcript = await deepgram_transcribe(recording)\n\n\n\n   representative = customer_service_reps[german_transcript[1]]\n\n\n\n   response.say(f\"This is the German response and {representative} will help you.\",\n                voice=\"alice\", language=\"en-US\")\n\n\n   response.hangup()\n\n   return response\n\n\ndef _redirect_welcome():\n   response = VoiceResponse()\n   response.say(\"Returning to the main menu\", voice=\"alice\", language=\"en-US\")\n   response.redirect(url_for('welcome'))\n\n   return twiml(response)\n```\n\n**view\\_helpers.py**\n\n```python\nimport flask\n\ndef twiml(resp):\n   resp = flask.Response(str(resp))\n   resp.headers['Content-Type'] = 'text/xml'\n\n   return resp\n```\n\n";
						}
						async function compiledContent$2a() {
							return load$2a().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2a() {
							return (await import('./chunks/index.86259ec7.mjs'));
						}
						function Content$2a(...args) {
							return load$2a().then((m) => m.default(...args));
						}
						Content$2a.isAstroComponentFactory = true;
						function getHeadings$2a() {
							return load$2a().then((m) => m.metadata.headings);
						}
						function getHeaders$2a() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2a().then((m) => m.metadata.headings);
						}

const __vite_glob_0_138 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2a,
  file: file$2a,
  url: url$2a,
  rawContent: rawContent$2a,
  compiledContent: compiledContent$2a,
  default: load$2a,
  Content: Content$2a,
  getHeadings: getHeadings$2a,
  getHeaders: getHeaders$2a
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$29 = {"title":"Import a Docker Container in Python","description":"Learn how to import a Docker container using Python.","date":"2016-02-24T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981208/blog/import-a-docker-container-in-python/import-docker-container%402x.jpg","authors":["scott-stephenson"],"category":"ai-and-engineering","tags":["python"],"seo":{"title":"Import a Docker Container in Python","description":"Learn how to import a Docker container using Python."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981208/blog/import-a-docker-container-in-python/import-docker-container%402x.jpg"},"shorturls":{"share":"https://dpgr.am/64a12b2","twitter":"https://dpgr.am/e24d798","linkedin":"https://dpgr.am/c05bac8","reddit":"https://dpgr.am/a832ff1","facebook":"https://dpgr.am/8d9edfc"}};
						const file$29 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/import-a-docker-container-in-python/index.md";
						const url$29 = undefined;
						function rawContent$29() {
							return "![](https://res.cloudinary.com/deepgram/image/upload/v1661721061/blog/import-a-docker-container-in-python/Screen-Shot-2016-02-23-at-11-29-48-AM.png)\n\n## Why would you do this?\n\nDocker containers are awesome for isolating applications from each other, but what if you *want* them to talk to each other? For instance, if you're developing an app in python that needs to interact with software written in another language. There are a few techniques for achieving low-level interoperability between Python and other popular languages. But if you have a [weird case](http://stackoverflow.com/questions/546160/what-is-erlang-written-in?answertab=votes#tab-top), or some complex legacy software, this becomes difficult or impossible.\n\n## The idea: containers as modules\n\nWe created [sidomo - Simple Docker Module](https://github.com/deepgram/sidomo) *so that if you can get your weirdo app to run in any linux environment, then you can instantly call it from Python with zero added effort. Right now most people use the Docker Daemon API to manage containers carrying their apps. ([Kubernetes](http://kubernetes.io/) / [Mesos](http://mesos.apache.org/) are great examples of this). Sidomo opens up a whole new use case for containers-turning weirdo software into nice, plain vanilla python modules that work seamlessly in python code.* not an [AWS service](https://www.expeditedssl.com/aws-in-plain-english)\n\n### How to use sidomo\n\nMake sure you have docker installed and a docker daemon running. If you're not sure if this is the case, run `docker ps` and see if you get \"CONTAINER ID ...\" as output. If you aren't sure how to get docker set up properly, you can check out [this link](https://docs.docker.com/engine/installation/) or [search here](https://www.google.com/search?q=install+docker) to find your way.\n\n#### Setting up Sidomo: a one-liner\n\nYou can install sidomo directly from the git repository using pip. Just run the following command in your shell:\n\n```\npip install -e 'git+https://github.com/deepgram/sidomo.git#egg=sidomo'  \n```\n\n#### Example: a simple Hello World\n\nThis will start a container from the ubuntu base image, run `echo hello from` and then `echo the other side`, and print the lines of output from the process. To prepare for this example, you need to pull the ubuntu image to your machine with one shell command.\n\n###### shell\n\n```\n# Get the latest Ubuntu image\ndocker pull ubuntu  \n```\n\n###### Python\n\n```\nfrom sidomo import Container\n\nwith Container('ubuntu') as c:  \n    for line in c.run('bash -c \"echo hello from; echo the other side;\"'):\n        print(line)\n```\n\n#### Example: wrangling FFMPEG with sidomo\n\nNow let's actually do something useful with sidomo. [FFMPEG](https://www.ffmpeg.org/) is a somewhat complex piece of software that manipulates media files efficiently for almost any purpose, but it's not easy to install consistently on different platforms, and there are no up-to-date python bindings for it. With Sidomo, you can pull FFMPEG with docker and easily run it from Python.\n\n###### shell\n\n```\ndocker pull cellofellow/ffmpeg  \n```\n\n###### Python\n\nThe example below will grab audio from a URL, transcode it, and print debug messages to prove that it worked. The process's stdout (the raw audio output) is disabled since we only want to see the debug messages.\n\n```\nfrom sidomo import Container  \nurl = 'http://www2.warwick.ac.uk/fac/soc/sociology/staff/sfuller/media/audio/9_minutes_on_epistemology.mp3'  \nwith Container(  \n    'cellofellow/ffmpeg',\n    stdout=False\n) as c:\n    for line in c.run(\n        'bash -c \\\"\\\n            wget -nv -O tmp.unconverted %s;\\\n            ffmpeg -i tmp.unconverted -f wav -acodec pcm_s16le -ac 1 -ar 16000 tmp.wav;\\\n            cat tmp.wav\\\n        \\\"\\\n        ' % url\n    ):\n        print line\n```\n\nIf you wanted to actually save the transcoded audio from this process, you would replace the line `stdout=False` with `stderr=False` and make sure to write each line of output from the container process (raw audio data) to a file.\n\n<WhitepaperPromo whitepaper=\"latest\"></WhitepaperPromo>\n\n## Fun in the future\n\nIf you have to write python bindings for some complex software, consider containerizing that software instead. With sidomo, turning a containerized application into a python module is painless and clean. If you find yourself using subprocess frequently to interact with code for which proper bindings do not exist, then containerizing these processes may make some things simpler. ![](https://www.adweek.com/socialtimes/files/2014/01/twitter-nesting-dolls.jpg) If you use sidomo in a python app that ends up developing complex dependencies, you may need to wrap the app in its own container and call it from an app with fewer dependencies on the outside. Sidomo supports this as well, since [docker supports nested containers](https://blog.docker.com/2013/09/docker-can-now-run-within-docker/). You can make your own software matryoshka doll by using sidomo to import sidomo to import sidomo.... Good luck! Just remember, you can't containerize away complexity indefinitely. Or can you? [Sidomo on github](https://github.com/deepgram/sidomo)\n\n## Why'd we make this?\n\nWe created the Deepgram API, a search engine for audio and video that makes speech searchable. Deepgram uses a complex stack of signal processing, statistics, and machine learning software working in concert to give a seamless \"upload and search\" experience. Sidomo lets us rapidly containerize finicky software and integrate it with python, our glue. To learn more about what we're up to and the functionality of our API, check out our [full documentation](https://developers.deepgram.com/documentation/), or sign up below to get our newsletter and stay in touch.\n\n";
						}
						async function compiledContent$29() {
							return load$29().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$29() {
							return (await import('./chunks/index.6dfe662b.mjs'));
						}
						function Content$29(...args) {
							return load$29().then((m) => m.default(...args));
						}
						Content$29.isAstroComponentFactory = true;
						function getHeadings$29() {
							return load$29().then((m) => m.metadata.headings);
						}
						function getHeaders$29() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$29().then((m) => m.metadata.headings);
						}

const __vite_glob_0_139 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$29,
  file: file$29,
  url: url$29,
  rawContent: rawContent$29,
  compiledContent: compiledContent$29,
  default: load$29,
  Content: Content$29,
  getHeadings: getHeadings$29,
  getHeaders: getHeaders$29
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$28 = {"title":"Improve IVR Prompts with Custom Reporting","description":"Learn how to improve your IVR prompts through reporting on what users said using Deepgram's speech recognition API.","date":"2022-09-22T19:22:00.103Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1663864118/blog/2022/09/conversational-ai-retry-report/2209-Improve-IVR-Prompts-w-Custom-Reporting-blog_2x_htqlwh.jpg","authors":["kevin-lewis"],"category":"tutorial","tags":["conversational-ai","javascript"],"shorturls":{"share":"https://dpgr.am/752c048","twitter":"https://dpgr.am/52437db","linkedin":"https://dpgr.am/b56bdb2","reddit":"https://dpgr.am/b846e8b","facebook":"https://dpgr.am/4f5d2c8"}};
						const file$28 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/improve-ivr-prompts-with-custom-reporting/index.md";
						const url$28 = undefined;
						function rawContent$28() {
							return "\nOne of the biggest challenges for conversational AI is anticipating all the ways in which a user may express a single phrase. Even with decent natural language processing, users often have frustrating experiences with 'retry rates'  - the number of times a request is rejected before it succeeds, even more so in Interactive Voice Response (IVR) systems. However, data around failed attempts can be key in improving understanding of how people frame their requests.\n\nIn this project, we'll cover an approach to gather failed failed IVR scripts and infer their meaning based on the successful attempt. This data can ultimately be used to improve your intent triggers and improve customer experience.\n\n![Diagram showing a back and forth conversation in text. The first two messages from the user are not understood but the third is. This generates a report that shows the two failed phrases next to the eventually-successful intent.](https://res.cloudinary.com/deepgram/image/upload/v1663768360/blog/2022/09/conversational-ai-retry-report/diagram_kquxm1.png)\n\nWe'll be using Deepgram with JavaScript and browser live transcription to demonstrate the concept, but it can easily be applied in other settings and programming languages.\n\nBefore we start, you will need a Deepgram API Key - [get one here](https://console.deepgram.com/signup?jump=keys) and keep it handy.\n\nCreate an empty `index.html` file and open it in your code editor.\n\n## Set Up Live Transcription\n\nAdd the following code to `index.html` to set up live transcription in your browser. For a detailed explanation of how this works, check out our [blog post on browser live transcription](https://blog.deepgram.com/live-transcription-mic-browser/).\n\n```html\n<html>\n    <body>\n        <script>\n            navigator.mediaDevices.getUserMedia({ audio: true }).then(stream => {\n                const mediaRecorder = new MediaRecorder(stream)\n\n                const DG_KEY = 'replace-with-your-deepgram-api-key'\n                const socket = new WebSocket('wss://api.deepgram.com/v1/listen', [ 'token', DG_KEY ])\n\n                socket.onopen = () => {\n                    mediaRecorder.addEventListener('dataavailable', event => {\n                        if (event.data.size > 0 && socket.readyState == 1) {\n                            socket.send(event.data)\n                        }\n                    })\n                    mediaRecorder.start(250)\n                }\n\n                socket.onmessage = (message) => {\n                    const { transcript } = JSON.parse(message.data).channel.alternatives[0]\n                    if(transcript) handleResponse(transcript)\n                }\n            })\n\n            function handleResponse(transcript) {\n                console.log(transcript)\n            }\n        </script>\n    </body>\n</html>\n```\n\nOpen the file in your browser. You should immediately be prompted for access to your microphone. Once granted, open up your browser console and start speaking to see your words logged.\n\n![Browser console showing several lines of transcripts](https://res.cloudinary.com/deepgram/image/upload/v1663768361/blog/2022/09/conversational-ai-retry-report/browser-live-transcription_t58fww.png)\n\n## Set Up Intents\n\nIn reality, your conversational AI system will be a lot more complex and robust than what we'll build today, but they mostly have the same characteristics:\n\n1.  A list of request options - 'intents'\n2.  Each option has a number of phrases or terms that can be used to trigger it - 'triggers'\n3.  An action to happen when an intent occurs - 'response'\n\nIntents normally inform a machine learning model which will match phrases similar but not identical, and responses may execute some logic before continuing. For this project, we'll need a partial match on an intent trigger. The response will be speaking a fixed phrase back to the user.\n\nAt the top of your `<script>` tag, add the following intents:\n\n```js\nconst intents = [\n    {\n        intent: 'balance',\n        triggers: [\n            'balance',\n            'balance',\n            'how much money'\n        ],\n        response: 'Your bank balance is over nine thousand'\n    },\n    {\n        intent: 'new_transaction',\n        triggers: [\n            'transfer',\n            'send',\n            'set up payee'\n        ],\n        response: 'Who would you like to send money to?'\n    },\n    {\n        intent: 'help',\n        triggers: [\n            'help',\n            'advice',\n            'struggling'\n        ],\n        response: 'Putting you through to one of our agents now'\n    },\n]\n```\n\n## Match User Speech to Intents\n\nWhen a user speaks, we need to determine if there was a match or not. Update `handleResponse()` with the following:\n\n```js\nfunction handleResponse(transcript) {\n    const match = intents.find(intent => intent.triggers.some(trigger => transcript.includes(trigger)))\n    console.log(match)\n}\n```\n\n`match` will either be the entire intent object for the matching item or `undefined`.\n\n![Browser console showing two undefined logs, and an object with one intent.](https://res.cloudinary.com/deepgram/image/upload/v1663768361/blog/2022/09/conversational-ai-retry-report/match_bf4zgt.png)\n\n## Save Intent Matching\n\nJust above `handleResponse()`, create two new variables - `current` that will contain the current string of requests towards a single intent and `report` that will contain all failed intents and the final successful phrase.\n\n```js\nlet current = {}\nconst report = []\n```\n\nUpdate `handleResponse()` with logic if there was no match. Specifically, add the phrase to `current.retries`, creating it if it doesn't already exist:\n\n```js\nfunction handleResponse(transcript) {\n    const match = intents.find(intent => intent.triggers.some(trigger => transcript.includes(trigger)))\n\n    if(!match) {\n        console.log(`No match for ${transcript}`)\n        if(!current.retries) current.retries = [transcript]\n        else current.retries.push(transcript)\n    }\n}\n```\n\nIf there was a match, add it to the `current` object, and push it into the `report` array. Each object in `report` will contain failed attempts and the eventual successful trigger:\n\n```js\nif(!match) {\n    console.log(`No match for ${transcript}`)\n    if(!current.retries) current.retries = [transcript]\n    else current.retries.push(transcript)\n} else {\n    if(current.retries) {\n        current.intent = match.intent\n        report.push(current)\n    }\n    current = {}\n    console.log(match.response)\n}\n```\n\nTry it out. Refresh the browser and start speaking. Try some random phrases, and then one which will trigger a match - \"I need help\", \"What's my overdraft balance?\", and \"send some money\" should all work.\n\n![Browser console showing no matches for two phrases, and then a successful response.](https://res.cloudinary.com/deepgram/image/upload/v1663768361/blog/2022/09/conversational-ai-retry-report/match-response_fpclpv.png)\n\n## Prompt the User to Speak\n\nTo wrap up, let's add spoken prompts and replies for this application using the [Web Speech API](https://developer.mozilla.org/en-US/docs/Web/API/SpeechSynthesis).\n\nAt the bottom of the `<script>` tag, create a `speak()` function:\n\n```js\nconst synth = window.speechSynthesis\nfunction speak(text) {\n    if (synth.speaking) synth.cancel()\n    const utterThis = new SpeechSynthesisUtterance(text)\n    synth.speak(utterThis)\n}\n```\n\nAdd an initial prompt to speak. Under `mediaRecorder.start(250)` add:\n\n```js\nspeak('What can we help you with today?')\n```\n\nAt the bottom of the logic in the if statement, when there is no match, add a retry prompt:\n\n```js\nspeak('I didn\\'t understand that, sorry. Can you try again?')\n```\n\nWhen there is a match, respond to the user:\n\n```js\nspeak(match.response)\n```\n\nAt any point, the `report` variable contains an array of potential improvements you can make to your conversational AI intents.\n\n![Logging report after several interactions. Each object has an array of retry strings, and the correct intent.](https://res.cloudinary.com/deepgram/image/upload/v1663768361/blog/2022/09/conversational-ai-retry-report/report_r4q0o1.png)\n\n## In Practice\n\nThis tutorial shows an overall approach for inferring the meaning of failed intent triggers, assuming that a user does not change their intent. To build this system out further, you should consider the common change in intent from interfacing with a bot to \"speak to a human.\"\n\nYou may also choose to do this after an interaction has ended rather than live, but you'll need to determine when a retry occurs.\n\nIf you have questions about anything in this post, we’d love to hear from you. Head over to [our forum](https://github.com/orgs/deepgram/discussions/categories/q-a) and create a new discussion with your questions, or send us a tweet [@DeepgramAI](https://twitter.com/DeepgramAI)\n\n";
						}
						async function compiledContent$28() {
							return load$28().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$28() {
							return (await import('./chunks/index.365c7b69.mjs'));
						}
						function Content$28(...args) {
							return load$28().then((m) => m.default(...args));
						}
						Content$28.isAstroComponentFactory = true;
						function getHeadings$28() {
							return load$28().then((m) => m.metadata.headings);
						}
						function getHeaders$28() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$28().then((m) => m.metadata.headings);
						}

const __vite_glob_0_140 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$28,
  file: file$28,
  url: url$28,
  rawContent: rawContent$28,
  compiledContent: compiledContent$28,
  default: load$28,
  Content: Content$28,
  getHeadings: getHeadings$28,
  getHeaders: getHeaders$28
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$27 = {"title":"Introducing Auto-Generated Summaries for Audio Content","description":"Need to automatically generate summaries of your audio content? Look no further!","date":"2022-09-28T14:54:14.428Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1664376933/2209-Introducing-Auto-Generated-Summaries-for-Audio-Content-thumb-554x220_acfu0t.png","authors":["pankaj-trivedi"],"category":"product-news","tags":["summarization"],"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1664377293/2209-Introducing-Auto-Generated-Summaries-for-Audio-Content-featured-1200x630_l5pnli.png"},"shorturls":{"share":"https://dpgr.am/76f533c","twitter":"https://dpgr.am/3846b34","linkedin":"https://dpgr.am/ffbdfb6","reddit":"https://dpgr.am/857ac88","facebook":"https://dpgr.am/f393811"}};
						const file$27 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/introducing-auto-generated-summaries-for-audio-content/index.md";
						const url$27 = undefined;
						function rawContent$27() {
							return "Build voice intelligence applications using our latest Summarization API, such as podcast summaries, auto-generated meeting notes, and sales call summaries, enhancing the overall end-user experience. \n\nToday, we are very excited to announce that Deepgram has launched the latest Summarization feature as part of our Speech Understanding Features. It uses deep learning techniques and abstractive summarization methods to generate meaningful and relevant summaries and insights from the audio data.\n\n## What Developers Can do Using Summarization?\n\n* Enable agents and sales representatives to generate meaningful call summaries automatically and reduce manual effort.\n* Help listeners identify interesting conversations through auto-generated relevant podcast previews.\n* Enable sales coaches and leaders to navigate through a large number of calls to identify important conversations through auto generated meeting summaries to dive deep into for coaching and monitoring purposes.\n* Make it easier for customers to extract actionable insights from conversations and key points discussed and identify calls of interest through AI generated meeting summaries.\n\nSummarization is one of the most challenging problems in the natural language processing and natural language generation space. It attempts to solve the problem of consuming a large amount of information by creating a meaningful, short, and relevant summary of the input text. \n\nThere are primarily two methods to summarize the original text: extractive and abstractive. \n\n### Extractive Summarization\n\nExtractive summarization works by extracting critical sentences from the original text, modeling the sentence's relationship, and merging selected sentences to generate an overall summary of the original text.\n\n### Abstractive Summarization\n\nIn this approach, the model attempts to capture important parts of the input text and generate new sentences to convey the original text's meaning using advanced deep learning and natural language techniques.\n\n## How Deepgram Summarization Works\n\nDeepgram’s Summarization feature first preprocesses the text by splitting it into logical segments. The model also respects sentence boundaries and is sensitive to the context of a sentence or utterance. Then, each input segment is fed individually through the model and the model produces a summarized version of the segment's text. The final JSON output contains a list of meaningful summaries with start and end characters' positions to identify generated summary sections from the source transcript.\n\nSummarization feature is supported for English language and pre-recorded audio and is available for both our on-premises and hosted customers.\n\nTo get started with Summarization, all you need to do is add `summarize=true` in your API call.\n\n```shell\ncurl \\\n --request POST \\\n --url 'https://api.deepgram.com/v1/listen?summarize=true&punctuate=true&tier=enhanced' \\\n --header 'Authorization: Token YOUR_DEEPGRAM_API_KEY' \\\n --header 'content-type: audio/mp3' \\\n --data-binary ‘@podcast.mp3'\n```\n\nWhen the file is finished processing (often after only a few seconds), you’ll receive a sample JSON response that has the following basic structure:\n\n```json\n\"summaries\": [\n  {\n    \"summary\": \"This episode is brought to you by levels. With levels you can see how different foods affect your health with real time feedback. The levels app interprets your glucose data and provides a simple score after you eat a meal.\",\n    \"start_word\": 0,\n    \"end_word\": 623\n  }\n```\n\nThe outputs from the API can then be used to build downstream workflows, create meeting or call notes in pdf format, power analytics tools, or integrate with other applications. \n\nTo learn more about our API, please see the Summarization page in our documentation. We welcome your feedback, please share it with us at [Product Feedback](https://deepgram.hellonext.co/b/feedback).";
						}
						async function compiledContent$27() {
							return load$27().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$27() {
							return (await import('./chunks/index.2dc0b049.mjs'));
						}
						function Content$27(...args) {
							return load$27().then((m) => m.default(...args));
						}
						Content$27.isAstroComponentFactory = true;
						function getHeadings$27() {
							return load$27().then((m) => m.metadata.headings);
						}
						function getHeaders$27() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$27().then((m) => m.metadata.headings);
						}

const __vite_glob_0_141 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$27,
  file: file$27,
  url: url$27,
  rawContent: rawContent$27,
  compiledContent: compiledContent$27,
  default: load$27,
  Content: Content$27,
  getHeadings: getHeadings$27,
  getHeaders: getHeaders$27
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$26 = {"title":"Introducing Real-Time Streaming and Solutions for Conversational AI, Sales and Support Enablement","description":"Do you need true real-time speech to text? How about less than 300 millisecond lag time and over 90%+ trained accuracy. We are built for Conversational AI and Sales and Support Enablement. Create with us today.","date":"2021-06-03T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981375/blog/introducing-real-time-streaming-and-solutions-for-conversational-ai-sales-and-support-enablement/introducing-real-time-streaming%402x.jpg","authors":["scott-stephenson"],"category":"product-news","tags":["conversational-ai","sales-enablement","support-enablement"],"seo":{"title":"Introducing Real-Time Streaming and Solutions for Conversational AI, Sales and Support Enablement","description":"Do you need true real-time speech to text? How about less than 300 millisecond lag time and over 90%+ trained accuracy. We are built for Conversational AI and Sales and Support Enablement. Create with us today."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981375/blog/introducing-real-time-streaming-and-solutions-for-conversational-ai-sales-and-support-enablement/introducing-real-time-streaming%402x.jpg"},"shorturls":{"share":"https://dpgr.am/1f89526","twitter":"https://dpgr.am/87d802f","linkedin":"https://dpgr.am/73be05d","reddit":"https://dpgr.am/5ecc0d5","facebook":"https://dpgr.am/e0f2bd9"}};
						const file$26 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/introducing-real-time-streaming-and-solutions-for-conversational-ai-sales-and-support-enablement/index.md";
						const url$26 = undefined;
						function rawContent$26() {
							return "The promise of AI has consistently been to solve problems and deliver experiences in ways that are faster, cheaper and better. At Deepgram, AI is in our DNA - we're an automatic speech recognition company focused on solving the inherent challenges of speech technology and powering the next generation of voice-enabled applications.  We've been working hard since we [raised](https://blog.deepgram.com/we-raised-25-million/) $25 million in our Series B earlier this year to continue creating AI-driven products that make our customers' (and their customers') lives easier.\n\nToday, we're excited to announce the availability of three new product features: conversational AI, sales and support enablement, and real-time streaming. These product additions and features (real-time streaming, trained and tailored models, multi-language, on-premise or cloud deployment) were built to help enable the next generation of human-like virtual agents and can be tailored for specific high-volume voice tasks like billing, support, add-on sales, compliance or ID verifications. In addition, real-time streaming allows software platforms to immediately push sales tips and knowledge base solutions to salespeople and support agents. Deepgram customers will now get to leverage: \n\n**Conversational AI**: Most existing conservational AI solutions' lag time is more than two seconds, but our new conversational AI feature reduces the response lag to only &lt;300 milliseconds. Customers now have a base model tuned to Conversational AI use cases that can be even further tailored to specific domains. These capabilities allow organizations to improve word error rate by up to 50 percent and reduce noise and filter crosstalk to help focus on the key terms and phrases for intent and understanding. For example, [Elerian AI](http://www.elerian.ai) has used Deepgram to build a bot that is so human-like, customers naturally say \"thank you\" to it. Our speed and accuracy on trained models allowed Elerian AI to meet the needs of a South African bank that needed to understand a host of accents and names that varied by region.\n\n**Sales and Support Enablement:** Customer-facing teams from every industry can boost sales with accurate and fast ASR optimized for real-time enablement. Our new product features help reduce customer churn with faster ASR for real-time offers and alerts, and improve employee success with accurate call analytics and transcripts for coaching. To help paint the picture, a sales enablement customer embedded Deepgram into its revenue intelligence platform to automatically transcribe all of their customer sales calls and find insights on the status of deals, as well as the performance and coaching needs of the sales team.\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-how-deepgram-works\"></WhitepaperPromo>\n\n**Real-Time Streaming:** At Deepgram, we don't believe in compromising speed for accuracy. With this new processing capability, our real-time streaming ASR is the fastest in the market. With less than 300 millisecond lag time and more than 90 percent transcription accuracy, our conversational AI and sales and support enablement solutions can now process audio data and provide correct responses, insight and results with tailored speech models. [Nytro.ai](http://www.nytro.ai), a new pitch intelligence startup, chose Deepgram based on our real-time speed and consistency-in addition to our high accuracy.  As Nytro.ai's team grows, they will create domain-specific speech models that could boost their accuracy even further.\n\nWith ongoing innovations in AI bringing about faster, cheaper and better processes and products, it's critical to stay ahead of the game when it comes to speech technology. We want to be the foundational ASR embedded in the newest voice technology, and these [new exciting product features](https://venturebeat.com/2021/06/03/deepgram-beefs-up-conversational-ai-engagement-with-new-apis/) are another critical step to getting us there. If you're interested in trying Deepgram's latest solutions or learning more about working together, [contact us](https://deepgram.com/contact-us/) today.";
						}
						async function compiledContent$26() {
							return load$26().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$26() {
							return (await import('./chunks/index.719256f1.mjs'));
						}
						function Content$26(...args) {
							return load$26().then((m) => m.default(...args));
						}
						Content$26.isAstroComponentFactory = true;
						function getHeadings$26() {
							return load$26().then((m) => m.metadata.headings);
						}
						function getHeaders$26() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$26().then((m) => m.metadata.headings);
						}

const __vite_glob_0_142 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$26,
  file: file$26,
  url: url$26,
  rawContent: rawContent$26,
  compiledContent: compiledContent$26,
  default: load$26,
  Content: Content$26,
  getHeadings: getHeadings$26,
  getHeaders: getHeaders$26
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$25 = {"title":"Introducing the New Deepgram Developer Portal","description":"Deepgrams new developer's portal. Your one-stop-shop for developing with the most accurate, fastest and cost effective speech recognition platform.","date":"2021-01-20T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981359/blog/introducing-the-new-deepgram-developer-portal/introducing-new-dg-dev-portal%402x.jpg","authors":["keith-lam"],"category":"product-news","tags":["announcements"],"seo":{"title":"Introducing the New Deepgram Developer Portal","description":"Deepgrams new developer's portal. Your one-stop-shop for developing with the most accurate, fastest and cost effective speech recognition platform."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981359/blog/introducing-the-new-deepgram-developer-portal/introducing-new-dg-dev-portal%402x.jpg"},"shorturls":{"share":"https://dpgr.am/666ea82","twitter":"https://dpgr.am/a58ca75","linkedin":"https://dpgr.am/57654d2","reddit":"https://dpgr.am/82adb0e","facebook":"https://dpgr.am/6e2bdb4"}};
						const file$25 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/introducing-the-new-deepgram-developer-portal/index.md";
						const url$25 = undefined;
						function rawContent$25() {
							return "## Your one-stop-shop for Automatic Speech Recognition (ASR) development\n\nWe are pleased to announce a new portal to provide a better developer experience. This is your one-stop-shop for all voice engineering and ASR development needs. The new site is located at [developers.deepgram.com](https://developers.deepgram.com/) and is organized into four main sections:\n\n1. **Get Started** - Overview of Deepgram's technology, processes, and solutions\n2. **Guides** - The heart of the documentation on all the features available for real-time streaming and batch transcription.\n3. **API references** - All the APIs and how to get an API key.\n4. **Security** -  Our stance and guidelines for information security and data privacy.\n\nWe will be updating this site regularly to add more information for developers. Please let us know if you would like to see other information or have questions: [support@deepgram.com](mailto:deepgram.comnull). \n\n![](https://res.cloudinary.com/deepgram/image/upload/v1661976834/blog/introducing-the-new-deepgram-developer-portal/developer-screenshot-300x137.jpg)";
						}
						async function compiledContent$25() {
							return load$25().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$25() {
							return (await import('./chunks/index.6fd27d0e.mjs'));
						}
						function Content$25(...args) {
							return load$25().then((m) => m.default(...args));
						}
						Content$25.isAstroComponentFactory = true;
						function getHeadings$25() {
							return load$25().then((m) => m.metadata.headings);
						}
						function getHeaders$25() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$25().then((m) => m.metadata.headings);
						}

const __vite_glob_0_143 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$25,
  file: file$25,
  url: url$25,
  rawContent: rawContent$25,
  compiledContent: compiledContent$25,
  default: load$25,
  Content: Content$25,
  getHeadings: getHeadings$25,
  getHeaders: getHeaders$25
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$24 = {"title":"Introducing Topic Detection Feature","description":"Automate workflow, enhance recommendations and search capabilities, and organize customers’ conversations by identifying and extracting key topics from your audio data using Deepgram’s Topic Detection API.","date":"2022-10-11T22:06:32.479Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1665610954/blog/Introducing%20Topic%20Detection%20Feature/2210-Topic-Detection-feature-featured-1200x630_2x_j0qroc.png","authors":["pankaj-trivedi"],"category":"product-news","tags":["nlu"],"shorturls":{"share":"https://dpgr.am/8f90c13","twitter":"https://dpgr.am/d02eedf","linkedin":"https://dpgr.am/0dfcd0a","reddit":"https://dpgr.am/aff224e","facebook":"https://dpgr.am/bcf89e3"}};
						const file$24 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/introducing-topic-detection-feature/index.md";
						const url$24 = undefined;
						function rawContent$24() {
							return "Today, we are very excited to announce that Deepgram has officially launched the Topic Detection feature as part of our speech understanding offerings. Deepgram's Topic Detection is based on an unsupervised topic modeling technique that enables developers and customers to detect the most important and relevant topics that are referenced in the conversations. \n\n## Turn Recorded Audio Into Insights\n\nHaving not enough data isn't a significant problem anymore. In fact, over [2.5 quintillion bytes](https://seedscientific.com/how-much-data-is-created-every-day/) of data get created every day. However, one of the biggest challenges customers face today is finding insights, organizing, tagging, and leveraging the data relevant to brands, prospects, and customers to deliver a fantastic experience to their end users. \n\nTopic Detection in ASR and NLU has become one of the must-have features. Developers require advanced solutions to perform a deeper analysis of their audio data based on detected topics and subjects to optimize resources, automate workflow, extract insights, improve search capabilities and enhance end users' experience.\n\n## Popular Use Cases Using Topic Detection\n\n* Support the Quality Assurance team to analyze conversations based on discussed topics, identify trends and patterns, and improve overall customer experience.\n* Categorize and tag conversations, meetings, and podcasts based on identified topics to enhance search and recommendation capabilities.\n* Extract meaningful and actionable insights from conversations and audio data based on discussed topics and recurring themes.\n\n## Identify over 350 topics\n\nDeepgram's Topic Detection  feature identifies patterns and generates key topics along with the output text, confidence score for each topic, and word positions to identify segments of speech. Deepgram's Topic Detection is based on Topic Modeling which is an unsupervised machine learning technique to cluster generated text based on the detected topics. It supports over 350 topics. Topic Extraction can be enabled using detect_topics=true  and is supported for English language and pre-recorded audio and is available for both our on-prem and hosted customers.\n\n## Implement Topic Detection with Deepgram\n\nTo implement Topic Detection from audio recordings, all you need to do is add detect_topics=true in your API call.\n\n### With cURL\n\n```bash\ncurl --request POST \\\n--url 'https://api.deepgram.com/v1/listen?detect_topics=true&punctuate=true&tier=enhanced' \\\n--header 'Authorization: Token YOUR_DEEPGRAM_API_KEY' \\\n--header 'content-type: audio/mp3' \\\n--data-binary '@podcast.mp3' \\\n```\n\nAlternatively, you can use one of our SDKs to implement Topic Detection:\n\n### With Node.js\n\n```js\nconst fs = require('fs')\nconst { Deepgram } = require('@deepgram/sdk')\n\n// Your Deepgram API Key\nconst deepgramApiKey = 'YOUR_DEEPGRAM_API_KEY'\n\nconst file = 'YOUR_FILE_LOCATION'\nconst mimetype = 'YOUR_FILE_MIME_TYPE'\nconst deepgram = new Deepgram(deepgramApiKey)\n\nconst audio = fs.readFileSync(file)\n\nconst source = {\n    buffer: audio,\n    mimetype: mimetype,\n}\n\ndeepgram.transcription\n  .preRecorded(source, {\n    detect_topics: true,\n  })\n  .then((response) => {\n    console.dir(response, { depth: null })\n\n    // Write only the transcript to the console\n    //console.dir(response.results.channels[0].alternatives[0].transcript, { depth: null });\n  })\n  .catch((err) => {\n    console.log(err)\n  })\n```\n\n### With Python\n\n```py\nfrom deepgram import Deepgram\nimport asyncio, json\n\nDEEPGRAM_API_KEY = 'YOUR_DEEPGRAM_API_KEY'\n\nFILE = 'YOUR_FILE_LOCATION'\nMIMETYPE = 'YOUR_FILE_MIME_TYPE'\n\nasync def main():\n\n  deepgram = Deepgram(DEEPGRAM_API_KEY)\n\n  audio = open(FILE, 'rb')\n\n  source = {\n    'buffer': audio,\n    'mimetype': MIMETYPE\n  }\n\n  response = await asyncio.create_task(\n    deepgram.transcription.prerecorded(\n      source,\n      {\n        'detect_topics': True\n      }\n    )\n  )\n\n  print(json.dumps(response, indent=4))\n\n  # Write only the transcript to the console\n  #print(response[\"results\"][\"channels\"][0][\"alternatives\"][0][\"transcript\"])\n\ntry:\n  # If running in a Jupyter notebook, Jupyter is already running an event loop, so run main with this line instead:\n  #await main()\n  asyncio.run(main())\nexcept Exception as e:\n  exception_type, exception_object, exception_traceback = sys.exc_info()\n  line_number = exception_traceback.tb_lineno\n  print(f'line {line_number}: {exception_type} - {e}')\n```\n\n## Topic Detection Results\n\nWhen the file is finished processing, you'll receive a sample JSON response that has the following basic structure:\n\n```bash\n\"topics\": [\n  {\n    \"topics\": [\n      {\n        \"topic\": \"renewable energy\",\n        \"confidence\": 0.80515814\n      },\n      {\n        \"topic\": \"climate change\",\n        \"confidence\": 0.51437885\n      }\n    ],\n    \"text\": \"Even Greenpeace underestimated the rise of solar. When one of the world's largest environmental advocacy groups released an optimistic industry analysis called the energy revolution in twenty ten. It was far more ambitious than any government predictions, and it still got it wrong. Greenpeace estimated that by twenty twenty, the world would have three hundred and thirty five thousand megawatts of installed solar photovoltaic capacity…...\",\n    \"start_word\": 0,\n    \"end_word\": 135\n  }\n]\n```\n\nDevelopers can take the outputs from the API that performs Topic Identification to build downstream workflows, generate tags based on topics, power analytics tools, build search and recommendation capabilities, or integrate with other applications. \n\nTo learn more about our API, please see the [Topic Detection page](https://developers.deepgram.com/documentation/features/detect-topics/) in our documentation. We welcome your feedback, please share it with us at [Product Feedback](https://deepgram.hellonext.co/b/feedback).";
						}
						async function compiledContent$24() {
							return load$24().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$24() {
							return (await import('./chunks/index.4ba6c57c.mjs'));
						}
						function Content$24(...args) {
							return load$24().then((m) => m.default(...args));
						}
						Content$24.isAstroComponentFactory = true;
						function getHeadings$24() {
							return load$24().then((m) => m.metadata.headings);
						}
						function getHeaders$24() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$24().then((m) => m.metadata.headings);
						}

const __vite_glob_0_144 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$24,
  file: file$24,
  url: url$24,
  rawContent: rawContent$24,
  compiledContent: compiledContent$24,
  default: load$24,
  Content: Content$24,
  getHeadings: getHeadings$24,
  getHeaders: getHeaders$24
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$23 = {"title":"Live Transcriptions with iOS and Deepgram","description":"Build an iOS app which takes the user’s microphone and prints transcripts to the screen in real-time.","date":"2022-01-03T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1638827580/blog/2022/01/ios-live-transcription/Live-Transcriptions-w-iOS-Deepgram-A%402x.jpg","authors":["abdul-ajetunmobi"],"category":"tutorial","tags":["ios","swift"],"seo":{"title":"Live Transcriptions with iOS and Deepgram","description":"Build an iOS app which takes the user’s microphone and prints transcripts to the screen in real-time."},"shorturls":{"share":"https://dpgr.am/fe8fa3f","twitter":"https://dpgr.am/520da60","linkedin":"https://dpgr.am/d25200f","reddit":"https://dpgr.am/c4783e8","facebook":"https://dpgr.am/f66d6dc"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453843/blog/ios-live-transcription/ograph.png"}};
						const file$23 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/ios-live-transcription/index.md";
						const url$23 = undefined;
						function rawContent$23() {
							return "\n<guest-author></guest-author>\n\nDeepgram's Speech Recognition API can provide accurate transcripts with pre-recorded and live audio. In this post, you will build an iOS app that takes the user's microphone and prints transcripts to the screen in real-time.\n\nThe final project code is available at https://github.com/deepgram-devs/deepgram-live-transcripts-ios.\n\n## Before We Start\n\nFor this project, you will need:\n\n*   A Deepgram API Key - [get one here](https://console.deepgram.com/signup?jump=keys)\n*   [Xcode](https://developer.apple.com/xcode/) installed on your machine.\n\nOnce you open Xcode, create a new Xcode project. Select the App template, give it a product name, select \"Storyboard\" for the interface, and \"Swift\" for the language. Untick \"User Core Data\" and \"Include Tests\", then hit next.\n\nChoose a place to save your project and hit finish to open your new project.\n\nYou will be using the WebSocket library [Starscream](https://github.com/daltoniam/Starscream) for this project. You can add it as a Swift Package by going to *File > Add Packages*, then entering the URL (https://github.com/daltoniam/Starscream) into the search box. Click add package and wait for the package to download.\n\n## Get Microphone Permissions\n\nYou will be using the microphone for this project, so you will need to add a microphone usage description to your project. When an iOS app asks for microphone permission, a reason why the permission is being requested is required and will be shown to the user. Open the `info.plist` file to add a description.\n\nHover over the last entry and click the plus icon to add a new entry. The key should be \"Privacy - Microphone Usage Description\", and you should add a value that describes why you need the microphone. You can use \"To transcribe audio with Deepgram\".\n\n![Plist file with the microphone usage description](https://res.cloudinary.com/deepgram/image/upload/v1638827585/blog/2022/01/ios-live-transcription/plist.png)\n\n## Setting Up `AVAudioEngine`\n\nTo access a data stream from the microphone, you will use [`AVAudioEngine`](https://developer.apple.com/documentation/avfaudio/avaudioengine).\n\nBefore you get started, it is worth having a mental model of how `AVAudioEngine` works. Apple's description is: \"An audio engine object contains a group of AVAudioNode instances that you attach to form an audio processing chain\".\n\nFor this example, you will create a chain of nodes that will take the microphone's input, convert it to a format that Deepgram can process, then send the data to Deepgram. The nodes and conversion will be discussed further in the blog.\n\n![AVAudioEngine nodes for this project](https://res.cloudinary.com/deepgram/image/upload/v1638827585/blog/2022/01/ios-live-transcription/engine-diagram.png)\n\nOpen the `ViewController.swift` file and import AVFoundation and Starscream at the top\n\n    import AVFoundation\n    Import Starscream\n\nThis will give you access to the `AVAudioEngine` class. Then inside the `ViewController` class, create an instance of `AVAudioEngine` by adding a property:\n\n```swift\nprivate let audioEngine = AVAudioEngine()\n```\n\nNext, create a function to analyse the audio and declare some constants:\n\n```swift\nprivate func startAnalyzingAudio() {\n  let inputNode = audioEngine.inputNode\n  let inputFormat = inputNode.inputFormat(forBus: 0)\n  let outputFormat = AVAudioFormat(commonFormat: .pcmFormatInt16, sampleRate: inputFormat.sampleRate, channels: inputFormat.channelCount, interleaved: true)\n}\n```\n\nFrom the top, you have the `inputNode`, which you can think of as the microphone followed by the `inputFormat`.\n\nNext is the `outputFormat` - The iOS microphone is a 32-bit depth format by default, so you will be converting to a 16-bit depth format before sending the data to Deepgram. While we will use a PCM 16-bit depth format for the audio encoding, Deepgram supports many audio encodings that can be specified to the API. Consider using a more compressed format if your use case requires low network usage. Just below those, add the new nodes that you will need and attach them to the audio engine:\n\n```swift\nlet converterNode = AVAudioMixerNode()\nlet sinkNode = AVAudioMixerNode()\n\naudioEngine.attach(converterNode)\naudioEngine.attach(sinkNode)\n```\n\nThe sinkNode is needed because to get the data in the correct format you need to access the data after it has passed through the converterNode, or its output. If you refer to the diagram above, notice how the stream of data to Deepgram is coming from the connection between nodes, the output of the `converterNode`, and not the nodes themselves.\n\n## Get Microphone Data\n\nUse the `installTap` function on the `AVAudioMixerNode` class to get the microphone data. This function gives you a closure that returns the output audio data of a node as a buffer. Call `installTap` on the `converterNode`, continuing the function from above:\n\n```swift\nconverterNode.installTap(onBus: 0, bufferSize: 1024, format: converterNode.outputFormat(forBus: 0)) { (buffer: AVAudioPCMBuffer!, time: AVAudioTime!) -> Void in\n\n}\n```\n\nYou will come back to this closure later. Now finish off the `startAnalyzingAudio` by connecting all the nodes and starting the audio engine:\n\n```swift\naudioEngine.connect(inputNode, to: converterNode, format: inputFormat)\naudioEngine.connect(converterNode, to: sinkNode, format: outputFormat)\naudioEngine.prepare()\n\ndo {\ntry AVAudioSession.sharedInstance().setCategory(.record)\n  try audioEngine.start()\n} catch {\n  print(error)\n}\n```\n\nYou can see now how the processing pipeline mirrors the diagram from earlier. Before the audio engine is started, the audio session of the application needs to be set to a record category. This lets the app know that the audio engine is solely being used for recording purposes. Note how the `converterNode` is connected with `outputFormat`.\n\n## Connect to Deepgram\n\nYou will be using the Deepgram WebSocket Streaming API, create a `WebSocket` instance at the top of the `ViewController` class:\n\n```swift\nprivate let apiKey = \"Token YOUR_DEEPGRAM_API_KEY\"\nprivate lazy var socket: WebSocket = {\n  let url = URL(string: \"wss://api.deepgram.com/v1/listen?encoding=linear16&sample_rate=48000&channels=1\")!\n  var urlRequest = URLRequest(url: url)\n  urlRequest.setValue(apiKey, forHTTPHeaderField: \"Authorization\")\n  return WebSocket(request: urlRequest)\n}()\n```\n\n<Alert type=\"warning\">A reminder that this key is in your app's source code and, therefore, it is not secure. Please factor this into your actual projects.</Alert>\n\nNote how the URL has an encoding matching the `outputFormat` from earlier. In the `viewDidLoad` function, set the socket's delegate to this class and open the connection:\n\n```swift\noverride func viewDidLoad() {\n  super.viewDidLoad()\n  socket.delegate = self\n  socket.connect()\n}\n```\n\nYou will implement the delegate later in the post.\n\n## Send Data to Deepgram\n\nNow that the socket connection is open, you can now send the microphone data to Deepgram. To send data over a WebSocket in iOS, it needs to be converted to a `Data` object. Add the following function to the `ViewController` class that does the conversion:\n\n```swift\nprivate func toNSData(buffer: AVAudioPCMBuffer) -> Data? {\n  let audioBuffer = buffer.audioBufferList.pointee.mBuffers\n  return Data(bytes: audioBuffer.mData!, count: Int(audioBuffer.mDataByteSize))\n}\n```\n\nThen return to the `startAnalyzingAudio` function, and within the `installTap` closure, you can send the data to Deepgram. Your tap code should look like this now:\n\n```swift\nconverterNode.installTap(onBus: bus, bufferSize: 1024, format: converterNode.outputFormat(forBus: bus)) { (buffer: AVAudioPCMBuffer!, time: AVAudioTime!) -> Void in\n  if let data = self.toNSData(buffer: buffer) {\n     self.socket.write(data: data)\n  }\n}\n```\n\nCall `startAnalyzingAudio` in the `viewDidLoad` function below the WebSocket configuration.\n\n## Handle the Deepgram Response\n\nYou will get updates from the WebSocket via its [delegate](https://www.hackingwithswift.com/example-code/language/what-is-a-delegate-in-ios). In the `ViewController.swift` file *outside* the class, create an extension for the `WebSocketDelegate` and a `DeepgramResponse` struct:\n\n```swift\nextension ViewController: WebSocketDelegate {\n  func didReceive(event: WebSocketEvent, client: WebSocket) {\n     switch event {\n     case .text(let text):\n\n     case .error(let error):\n        print(error ?? \"\")\n     default:\n        break\n     }\n  }\n}\n\nstruct DeepgramResponse: Codable {\n  let isFinal: Bool\n  let channel: Channel\n\n  struct Channel: Codable {\n     let alternatives: [Alternatives]\n  }\n\n  struct Alternatives: Codable {\n     let transcript: String\n  }\n}\n```\n\nThe `didReceive` function on the `WebSocketDelegate` will be called whenever you get an update on the WebSocket. Before you finish implementing `didReceive`, you need to prepare to decode the data and update the UI to display the transcripts. At the top of the `ViewController` class, add the following properties:\n\n```swift\nprivate let jsonDecoder: JSONDecoder = {\n  let decoder = JSONDecoder()\n  decoder.keyDecodingStrategy = .convertFromSnakeCase\n  return decoder\n}()\n\nprivate let transcriptView: UITextView = {\n  let textView = UITextView()\n  textView.isScrollEnabled = true\n  textView.backgroundColor = .lightGray\n  textView.translatesAutoresizingMaskIntoConstraints = false\n  return textView\n}()\n```\n\nCreate a new function called `setupView` and configure your UI:\n\n```swift\nprivate func setupView() {\n  view.addSubview(transcriptView)\n  NSLayoutConstraint.activate([\n     transcriptView.topAnchor.constraint(equalTo: view.safeAreaLayoutGuide.topAnchor),\n     transcriptView.leadingAnchor.constraint(equalTo: view.leadingAnchor, constant: 16),\n     transcriptView.trailingAnchor.constraint(equalTo: view.trailingAnchor, constant: -16),\n     transcriptView.bottomAnchor.constraint(equalTo: view.safeAreaLayoutGuide.bottomAnchor)\n  ])\n}\n```\n\nThen call `setupView` in the `viewDidLoad` function.\n\nReturning to the `didReceive` function of the `WebSocketDelegate`, within the text case; you need to convert the text, which is a `String` type, into a `Data` type so you can decode it into an instance of `DeepgramResponse`:\n\n```swift\nextension ViewController: WebSocketDelegate {\n  func didReceive(event: WebSocketEvent, client: WebSocket) {\n     switch event {\n     case .text(let text):\n        let jsonData = Data(text.utf8)\n        let response = try! jsonDecoder.decode(DeepgramResponse.self, from: jsonData)\n        let transcript = response.channel.alternatives.first!.transcript\n\n        if response.isFinal && !transcript.isEmpty {\n          if transcriptView.text.isEmpty {\n             transcriptView.text = transcript\n          } else {\n             transcriptView.text = transcriptView.text + \" \" + transcript\n          }\n        }\n     case .error(let error):\n        print(error ?? \"\")\n     default:\n        break\n     }\n  }\n}\n```\n\nOnce you have a `DeepgramResponse` instance, you will check if it is final, meaning it is ready to add to the `transcriptView` while handling the empty scenarios. If you now build and run the project (CMD + R), it will open in the simulator, where you will be prompted to give microphone access. Then you can start transcribing!\n\n![iPhone simulator showing the text hello blog](https://res.cloudinary.com/deepgram/image/upload/v1638827585/blog/2022/01/ios-live-transcription/simulator.png)\n\nThe final project code is available at https://github.com/deepgram-devs/deepgram-live-transcripts-ios, and if you have any questions, please feel free to reach out to the Deepgram team on Twitter - [@DeepgramDevs](https://twitter.com/DeepgramDevs).\n\n        ";
						}
						async function compiledContent$23() {
							return load$23().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$23() {
							return (await import('./chunks/index.69795ed5.mjs'));
						}
						function Content$23(...args) {
							return load$23().then((m) => m.default(...args));
						}
						Content$23.isAstroComponentFactory = true;
						function getHeadings$23() {
							return load$23().then((m) => m.metadata.headings);
						}
						function getHeaders$23() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$23().then((m) => m.metadata.headings);
						}

const __vite_glob_0_145 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$23,
  file: file$23,
  url: url$23,
  rawContent: rawContent$23,
  compiledContent: compiledContent$23,
  default: load$23,
  Content: Content$23,
  getHeadings: getHeadings$23,
  getHeaders: getHeaders$23
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$22 = {"title":"Is There an ASR Gender Gap?","description":"Is there a gender gap with speech recognition technology? Is there differences with automatic speech recognition between genders? Here is what we see.","date":"2021-03-31T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981365/blog/is-there-an-asr-gender-gap/is-there-asr-gender-gap%402x.jpg","authors":["sam-zegas"],"category":"identity-and-language","tags":["inclusion","language","womens-history"],"seo":{"title":"Is There an ASR Gender Gap?","description":"Is there a gender gap with speech recognition technology? Is there differences with automatic speech recognition between genders? Here is what we see."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981365/blog/is-there-an-asr-gender-gap/is-there-asr-gender-gap%402x.jpg"},"shorturls":{"share":"https://dpgr.am/b7052aa","twitter":"https://dpgr.am/c8db978","linkedin":"https://dpgr.am/5b80d64","reddit":"https://dpgr.am/4f7b83b","facebook":"https://dpgr.am/8fb1173"}};
						const file$22 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/is-there-an-asr-gender-gap/index.md";
						const url$22 = undefined;
						function rawContent$22() {
							return "To close out Women's History Month, we're taking a look at the intersection of women's voices and speech technology.\n\n## Gender Marking in Language\n\nThe English language has relatively little overt gender marking. Many languages-Spanish, Hebrew, Arabic, and Japanese, to name a few-have distinct word endings or vocabulary that change to reflect gender. In rarer cases, languages like Yanyuwa, an Aboriginal Australian language, develop [distinct dialects](http://www.bbc.com/travel/story/20180429-australias-ancient-language-shaped-by-sharks) for men and women. Overt gender marking in English is confined to the use of the gendered third person singular pronouns *she* and *he.* But there are other differences to be found, if you know where to look.\n\nThere are a range of speech patterns and vocal characteristics associated with women's speech in English. To be sure, the three qualities listed below are not unique to women by any stretch-but language scientists have noted that their adoption was driven by women, and to this day they tend to be associated with women's speech. Even just this short list shows that when it comes to gender, language is political. Searching the internet for uptalk and vocal fry will turn up an ocean of content decrying the \"[epidemic](https://www.psychologytoday.com/us/blog/caveman-logic/201010/the-uptalk-epidemic)\" of these speech qualities and offering [advice](https://www.youtube.com/watch?v=HEfMwri22SM) on how to eliminate them, as though they represent a liability to the speaker. Prominent features commonly associated with female voices include: \n\n* [Uptalk](https://www.bbc.com/news/magazine-28708526) or upspeak: Raising the pitch of your voice toward the end of a declarative statement, which produces an intonation pattern resembling that of a question. Statements made with uptalk may be perceived as less decisive than they would be without it.\n* [Vocal fry](https://www.youtube.com/watch?v=4L7-9N1xQZA): Creating a creaky sound while speaking by fluttering your vocal folds at the bottom of your vocal register. Vocal fry is sometimes perceived as unsure or less confident.\n* Using \"like\" as a discourse particle or filler word: a discourse particle is a word or phrase used primarily to manage the flow of a conversation, including \"you know, and \"I mean.\" You might use \"like\" as a discourse particle to, *like*, pause and put together your next thought. Similar to vocal fry, the use of \"like\" as a discourse particle tends to be perceived as unsure or less confident.\n\n## Female Speech Trendsetters\n\nSome commentators, however, see a different story: female speech shows the power that women have as language trendsetters. Mark Liberman, a linguist at the University of Pennsylvania [notes](https://www.npr.org/2015/07/23/425608745/from-upspeak-to-vocal-fry-are-we-policing-young-womens-voices), \"It's generally pretty well known that if you identify a sound change in progress, then young people will be leading old people, and women tend to be maybe half a generation ahead of males on average.\" So, women tend to be early adopters of linguistic trends, but why do they adopt new forms of speech in the first place? Carmen Fought, a linguist at Pitzer College [argues](https://www.npr.org/2015/07/23/425608745/from-upspeak-to-vocal-fry-are-we-policing-young-womens-voices), that women's speech is a tool for social connection: \"The truth is this: Young women take linguistic features and use them as power tools for building relationships.\"\n\nThat is to say, an uptalk pattern that signals a lack of confidence to one listener might be perceived positively by another-and it's the opportunity to build that positive connection that motivates the adoption of female speech patterns. For example, a woman might use uptalk to signal friendliness in a situation where building trust is a priority. Marybeth Seitz-Brown of Slate may have said it best when she [wrote](https://slate.com/human-interest/2014/12/uptalk-is-ok-young-women-shouldn-t-have-to-talk-like-men-to-be-taken-seriously.html), \"Young women shouldn't have to talk like men to be taken seriously.\" So if you hear vocal fry or uptalk and form a negative impression, it might actually be that you're missing a cue from a speaker who is ahead of you on a linguistic trend. This is one case where language is in the ear of the holder.\n\nThe conversation about women's voices goes beyond these features as well. Recent presidential races in the United States were [littered with critiques](https://www.washingtonpost.com/outlook/2019/09/11/that-grating-noise-its-people-criticizing-female-voices-debate-stage/) of how women communicated. Sometimes it was charges of sounding \"shrill\" or like an \"ice pick\" to the ear; other times it was charges of sounding [too emotional](https://www.usnews.com/news/politics/articles/2019-04-16/women-candidates-still-tagged-as-too-emotional-to-hold-office). Susan B. Anthony nodded to this issue more than a century ago, when she [wrote](https://www.google.com/books/edition/Failure_Is_Impossible/2po16sEMd7AC?hl=en), \"No advanced step taken by women has been so bitterly contested as that of speaking in public, for nothing which they have attempted, not even to secure the suffrage, have they been so abused, condemned and antagonized.\" \n\n## Speech Technology and Gender\n\nThat brings us to the intersection of language and technology, where we have to ask: what role has technology played in this problem? The answer might surprise you. In the early days of voice recording, telephony, and broadcasting, decision makers in industry and government [chose](https://www.newyorker.com/culture/cultural-comment/a-century-of-shrill-how-bias-in-technology-has-hurt-womens-voices) to limit the \"voiceband\" range to signals between 300 and 3,400 hertz-a range that serves the average male vocal range better than the average female vocal range.\n\nThis decision limited the intelligibility of speakers with higher vocal ranges, forcing them to modify their speech to be better understood and leading to a less natural sound (think of the critiques of women's voices as \"shrill\"). Despite a century of technological advances, analysis from the New Yorker [reported](https://www.newyorker.com/culture/cultural-comment/a-century-of-shrill-how-bias-in-technology-has-hurt-womens-voices) that, \"Even today, many data-compression algorithms and bluetooth speakers disproportionately affect high frequencies and consonants, and women's voices lose definition, sounding thin and tinny.\" In short, it will take conscious effort throughout the tech world to identify and address shortcomings in processing women's speech.\n\n## ASR Gender Gap\n\nWe took a look at Automatic Speech Recognition (ASR) data from ourselves and our competitors to see what evidence there is of a gender gap in ASR. For this study, we compiled 1000 short audio files from an open speech dataset, totaling just under 4 hours of audio. 500 of the files were female speakers, 500 were male. We set aside a small validation set and then ran the rest of these audio files through three models: the [Deepgram](https://deepgram.com/) general model, a Deepgram-trained model, and the model of a leading competitor.\n\nThe results show that in terms of overall performance, Deepgram's trained model had the best performance at \\~5.5% WER, followed by Deepgram's general model at \\~9.8%, and then the leading competitor's model at ~11.8%. Overall, the data shows that the accuracy gap between male and female speakers is very small. This is an encouraging sign given the historical legacy of women's voices not being prioritized in tech. \n\n![](https://res.cloudinary.com/deepgram/image/upload/v1661976839/blog/is-there-an-asr-gender-gap/Screen-Shot-2021-03-30-at-11.21.49-AM.png) \n\nWe're in the business of making human speech intelligible to machines because we know there's power in voice. Equality in speech technology will be a precondition for getting your voice out into the world. At Deepgram, our Research team tracks our ability to process speech of different demographic groups, including women, and aims for continuous improvement.";
						}
						async function compiledContent$22() {
							return load$22().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$22() {
							return (await import('./chunks/index.defd71e5.mjs'));
						}
						function Content$22(...args) {
							return load$22().then((m) => m.default(...args));
						}
						Content$22.isAstroComponentFactory = true;
						function getHeadings$22() {
							return load$22().then((m) => m.metadata.headings);
						}
						function getHeaders$22() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$22().then((m) => m.metadata.headings);
						}

const __vite_glob_0_146 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$22,
  file: file$22,
  url: url$22,
  rawContent: rawContent$22,
  compiledContent: compiledContent$22,
  default: load$22,
  Content: Content$22,
  getHeadings: getHeadings$22,
  getHeaders: getHeaders$22
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$21 = {"title":"Just Released: New Version of On-Premises","description":"Deepgram On-premises Improved Diarization, New Ways to Invoke Models, and Streaming Callback Troubleshooting","date":"2022-09-14T22:57:29.350Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1663197013/devrel-on-premise-release-blog_1_ejgm2u.png","authors":["evan-henry"],"category":"announcement","tags":["on-prem"],"seo":{"title":"Deepgram On-premises Improved Diarization, New Ways to Invoke Models, and Streaming Callback Troubleshooting","description":"Deepgram's August feature release includes support for improved diarization with significantly improved accuracy. Read more here!"},"shorturls":{"share":"https://dpgr.am/ddbec84","twitter":"https://dpgr.am/2b8667e","linkedin":"https://dpgr.am/c760066","reddit":"https://dpgr.am/8639e8d","facebook":"https://dpgr.am/7d25d07"}};
						const file$21 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/just-released-new-version-of-on-premises/index.md";
						const url$21 = undefined;
						function rawContent$21() {
							return "\n## Deepgram On-premises Improved Diarization, New Ways to Invoke Models, and Streaming Callback Troubleshooting\n\nI’m pleased to announce the August release of Deepgram’s on-prem ASR solution. The major feature included in the August release is support for the [improved Diarization](https://deepgram.com/changelog/introducing-improved-diarization/ \"https://deepgram.com/changelog/introducing-improved-diarization/\") with significantly improved accuracy which is language agnostic and supports our [generally available language models](https://developers.deepgram.com/documentation/features/language/ \"https://developers.deepgram.com/documentation/features/language/\").\n\nIn addition to improved Diarization, there are several user experience enhancements included in the August release. One such improvement is the ability to invoke a specific model via its `UUID`. A model’s `UUID` value is visible in a given ASR response within the `model_info` JSON object, but the easiest way to view a model’s `UUID` is by sending an HTTP GET request to the `/v2/models` endpoint in your on-prem deployment.\n\nFor example:\n\n```sh\n    curl -X GET 'http://localhost:8080/v2/models'\n\n    {\n      \"name\": \"general-enhanced\",\n      \"uuid\": \"125125fb-e391-458e-a227-a60d6426f5d6\",\n      \"version\": \"2022-05-18.0\",\n      \"tags\": \\[],\n      \"languages\": \\[\n        \"en\",\n        \"en-US\"\n      ]\n    }\n    {\n      \"name\": \"general-dQw4w9WgXcQ\",\n      \"uuid\": \"41757536-6114-494d-83fd-c2694524d80b\",\n      \"version\": \"2021-08-18\",\n      \"tags\": \\[],\n      \"languages\": \\[]\n    }\n```\n\nAnother improvement is the ability to troubleshoot streaming callback responses which can be done by including the `inspect_response=true` parameter in the ASR request. This will send the contents of a transcription response to the original request connection in addition to the response callback to aid troubleshooting or debugging a streaming callback.\n\n```sh\n    curl \\\n      --request POST \\\n      --header 'Authorization: Token YOUR_DEEPGRAM_API_KEY' \\\n      --header 'Content-Type: audio/wav' \\\n      --data-binary @youraudio.wav \\\n      --url 'https://api.deepgram.com/v1/listen?callback=URL&inspect_response=true'\n```\n\nFor a full list of changes included in the August release, head over to the [changelog](https://deepgram.com/changelog/deepgram-on-premise-release-220831/ \"https://deepgram.com/changelog/deepgram-on-premise-release-220831/\").\n\n";
						}
						async function compiledContent$21() {
							return load$21().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$21() {
							return (await import('./chunks/index.275bae8b.mjs'));
						}
						function Content$21(...args) {
							return load$21().then((m) => m.default(...args));
						}
						Content$21.isAstroComponentFactory = true;
						function getHeadings$21() {
							return load$21().then((m) => m.metadata.headings);
						}
						function getHeaders$21() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$21().then((m) => m.metadata.headings);
						}

const __vite_glob_0_147 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$21,
  file: file$21,
  url: url$21,
  rawContent: rawContent$21,
  compiledContent: compiledContent$21,
  default: load$21,
  Content: Content$21,
  getHeadings: getHeadings$21,
  getHeaders: getHeaders$21
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$20 = {"title":"Keywords vs Search","description":"Compare Deepgram's search and keywords to learn which scenarios each feature works best for you","date":"2021-12-09T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1638909280/blog/2021/12/keywords-vs-search/keywords-v-search-blog%402x.jpg","authors":["sandra-rodgers"],"category":"best-practice","tags":["search","keywords"],"seo":{"title":"Keywords vs Search","description":"Compare Deepgram's search and keywords to learn which scenarios each feature works best for you"},"shorturls":{"share":"https://dpgr.am/3c18657","twitter":"https://dpgr.am/a03e2a5","linkedin":"https://dpgr.am/babbe84","reddit":"https://dpgr.am/35057c5","facebook":"https://dpgr.am/971ad63"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453831/blog/keywords-vs-search/ograph.png"}};
						const file$20 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/keywords-vs-search/index.md";
						const url$20 = undefined;
						function rawContent$20() {
							return "\n**Search** and **keywords** are two different features of Deepgram's API that may sound similar but actually work best in different scenarios. In this blog post, I am going to help you understand these two features and give you a better idea of the very different situations in which you might use them.\n\nTo get the most benefit out of this post, I recommend you have at least skimmed through our documentation on [search](https://developers.deepgram.com/documentation/features/search/) and [keywords](https://developers.deepgram.com/documentation/features/keywords/).\n\n## Basic Overview\n\n### Search\n\nDeepgram’s **search** feature is a query that can be made to **find out if a word or phrase has been said** in the audio that is being transcribed. During the transcribing of the audio of a pre-recorded sound file or a stream, Deepgram can analyze the audio to find the queried word or phrase. In the JSON response, it will return information about the **start time** and **end time** of when that word was possibly uttered. It will also give a **confidence** rating to each match. So you will see the **query** (the word or phrase you searched for), and then you will see **hits** (an array of objects that give you the **confidence**, **start**, **end**, and **snippet** of each possible match to your search).\n\nTo search for matches to the word 'epistemology,' I would add this query parameter to the API call URL or provide it as an option when using a [Deepgram SDK](https://developers.deepgram.com/sdks-tools/):\n\n**search=epistemology**\n\n```js\n\"search\":[\r\n  {\r\n    \"query\":\"epistemology\",\r\n    \"hits\":[\r\n      {\"confidence\":0.9348958,\"start\":10.725,\"end\":11.485,\"snippet\":\"is a\"},\r\n      {\"confidence\":0.9348958,\"start\":13.965,\"end\":14.764999,\"snippet\":\"epi\"},\r\n      {\"confidence\":0.9204282,\"start\":4.0074897,\"end\":4.805,\"snippet\":\"social epi\"},\r\n      {\"confidence\":0.27662036,\"start\":8.792552,\"end\":10.365,\"snippet\":\"us today is\"},\r\n      {\"confidence\":0.1319444,\"start\":17.205,\"end\":18.115,\"snippet\":\"nature of knowledge\"},\r\n      {\"confidence\":0.0885417,\"start\":15.285,\"end\":16.085,\"snippet\":\"branch philosophy\"},\r\n      {\"confidence\":0.045138836,\"start\":5.1240044,\"end\":5.722137,\"snippet\":\"university of\"},\r\n      {\"confidence\":0.045138836,\"start\":5.6025105,\"end\":7.4367843,\"snippet\":\"warwick and\"},\r\n      {\"confidence\":0.0,\"start\":1.0168257,\"end\":1.9339627,\"snippet\":\"hello this is steve\"},\r\n      {\"confidence\":0.0,\"start\":7.277282,\"end\":8.27417,\"snippet\":\"and the question\"}\r\n    ]\r\n  }\r\n]\n```\n\n### Keywords\n\nThe **keywords** feature is also a query, but it is used to give the Deepgram AI more information so that it can **better transcribe the audio in the request**. Keywords are words that can be sent along with the audio, and then Deepgram will train itself to watch more closely for those words and transcribe them more accurately.\n\nTo improve transcription of the words 'epistemology' and 'ontology', I would add this query parameter to the API call URL or provide it as an option when using a [Deepgram SDK](https://developers.deepgram.com/sdks-tools/):\n\n**keywords=epistemology\\&keywords=ontology**\n\nIt can be challenging for ASR technology to transcribe certain words, particularly jargon, names, or very uncommon words. But the beauty of deep learning is that the technology can *learn*. If you know about these context-unique words ahead of time, you can send them with the request as keywords, and Deepgram will do a better job of actually transcribing those words.\n\n<Alert type=\"warning\">\n\nKeywords are a much less powerful substitute than training a custom model. The best solution is to work with Deepgram to train a model for your specific language context. If you are using the keywords boosting feature for many words, and you are using this feature often, you should look into having a custom model trained for your situation because it will perform better.\n\n</Alert>\n\n## Real-World Scenarios\n\nThe fundamental difference between keywords and search is that search will *return a data object that gives you possible matches to your search terms*. If you just need to find a word or to know if that word was said (or not said), use **search**.\n\nKeywords do not return an added data object; they just *improve the transcription itself* so that you see the boosted keywords transcribed correctly in the transcription. If you want to see a better transcription - improved specifically for words important to you - use **keywords** (however, there are caveats to using **keywords**, which we will examine in more detail later).\n\nWhile this distinction might be becoming more clear to you, the best way to really understand when to use **search** versus when to use **keywords** is to look at some possible scenarios.\n\n### Scenario 1: Compliance\n\n#### Your company's employees speak on the phone in a customer service role, and they are required to say at the start of every phone call, \"This call is being recorded.\" For the purpose of compliance, you want to check that they are actually saying this phrase.\n\nIn this scenario, you can use **search** to find matches to that phrase. You will not receive just a yes/no response (as in a Boolean telling you true/false) in the response object, but you will receive the list of matches with a confidence rating. Deepgram's AI will do its best to find phrases that it thinks might match the phrase \"This call is being recorded.\" **However, this doesn't mean that all the matches are correct.**\n\nThis is where the nuance comes in. You have a powerful tool that has helped you search through audio files to find possible matches, and then as a user or a customer, you can decide for yourself what confidence rating is more likely to return a match. You can analyze the data you get back and start to see a pattern for what confidence number is bringing back accurate matches.\n\nIf it were me, I would pay attention to the confidence level that seems to be bringing back accurate matches, and then I would make an executive decision to say that this number gives us reasonable confidence that this phrase was uttered. Because every language context is different, Deepgram's general language model is going to perform differently when transcribing audio in those different contexts, so this feature can only be used as a tool to guide you to knowing whether the search term/phrase was uttered or not.\n\n### Scenario 2: Discovery\n\n#### Your company is participating in litigation, and as part of the eDiscovery phase, you are tasked with producing electronic documents (including audio and video) which are relevant to the case\n\nHaving a list of possible word or phrase matches can be extremely useful, but **search** is also a powerful tool for pointing you in the direction of valuable *sections* of your audio. A perfect example of this is eDiscovery, when attorneys are required to provide documents which are likely to have a certain relevance to the case. In this situation, they wouldn't be searching for just one word or phrase, but they would be looking for language that suggests that the conversation in the audio or video file is relevant.\n\nThe goal of ASR is to recognize human speech, but human speech isn't always direct, straightforward, or even completely honest. In order to comprehend what is said, humans often read between the lines. Since Deepgram's search feature isn't just a yes/no check to find a match, but actually gives you a confidence level in the response, this search tool can be combined with the human brain to come up with a combination of words and phrases that might help to find more nuanced relevant language.\n\nTo me, this is one of the most exciting possibilities of Deepgram's **search** feature because it demonstrates how AI isn't just about building machines to think for us, it's about building machines to aid humans in thinking better.\n\n### Scenario 3: Demos\n\n#### You are planning an important demo for the upcoming week, and you want to use Deepgram's speech-to-text API, but you haven't trained a language model yet. The demo is crucial to your business, so you want to give an added boost to Deepgram's transcription technology to improve accuracy for certain words.\n\nThe purpose of **keywords** is to improve the transcription output, so if you want to get a more accurate transcription (and you have not been able to train a custom model), a less powerful option would be to use keywords. You can improve the transcription by putting a list of keywords that are more difficult to recognize by ASR into the query parameter. Jargon, names, and scientific words are good examples of words that can be tricky for ASR.\n\nThe **keywords** feature is one that requires thoughtful implementation. Keywords do on a much smaller scale what model-training does, but **there are tradeoffs.** It might seem like a great way to improve a transcription, but because so many factors go into transcribing audio, and because training a model is something that is done by deep learning experts, substituting keywords for model training doesn't always work very well straight out of the box. However, it can be used if you are willing to test out various keywords and adjust based on the results you see.\n\nThe best approach would be to benchmark the transcription results without any keywords, then add words one by one and notice the effect. You want to aim to find that sweet spot where the transcription is coming out as accurate as possible. But due to the improvements Deepgram has made in its various models, adding keywords does not always improve the transcription. Depending on the model you are using and your unique language context, the keywords might even lower the quality of the transcription.\n\nThe keywords feature works differently depending on if the words are 'in-vocabulary' or 'out-of-vocabulary' (see the [documentation](https://developers.deepgram.com/documentation/features/keywords/#in-vocabulary--out-of-vocabulary-keywords)). If the words already exist in the model (whether you are using a general model or your own trained model), Deepgram will just put more emphasis on those keywords (which the model already knows exist), telling the AI to boost the confidence that those words were spoken. If you are using keywords to improve the transcription of words that don't exist in the general language model, keyword boosting can be slower. Sending a huge amount of keywords may affect the speed of the transcription creation. **It is recommended to use 10-100 keywords, with 100 really pushing the limits.** This is important for real-time transcription, but it may not be a concern to you if you are doing batch-processing of audio files.\n\nThe better Deepgram's general model gets at transcribing on its own, the less powerful the keywords tool is, since more confidence will be given to the transcription itself. Using keywords might not have an effect. If you add really high intensifiers to your keywords (see the documentation about [intensifiers](https://developers.deepgram.com/documentation/features/keywords/#intensifiers)), it might cause the transcription to overuse those keywords. You have to find the right balance, which comes with trial and error.\n\nFinally, keywords **cannot boost phrases**, so if you are searching for a *first name* + *last name* combination, the boosting will happen individually for those two words, and you might see that one shows up correctly while the other does not.\n\nThe keywords feature is still in beta at this time, so please factor this into your usage. We are actively working to improve this feature.\n\n### Other Common Scenarios\n\nHere are some other possible scenarios for when **search** would be a great tool:\n\n*   Your product uses Deepgram to provide captions to real-time streams or pre-recorded videos, and you want to be able to search the captions to find a topic or point that was discussed.\n*   Searching for competitor or brand names from various types of media recordings, so you can gather information to help build strategies.\n*   Searching for specific language phrases that imply dissatisfaction (such as profanity) so you can analyze situations when customers were not happy.\n\nHere are some other possible scenarios for when **keywords** would be a great tool:\n\n*   You haven't had an opportunity to train a custom model, but you have an idea of context-unique words that can improve the transcription.\n*   You want to improve real-time transcription of a meeting by including keywords that are names of participants or company terms that will be used during the meeting.\n\n## Final Thoughts\n\nAfter chatting with Deepgrammers who build our Speech Recognition API, the incredible power of deep learning has become even more evident to me. Deepgram's **search** feature really exemplifies this. **Search** is a tool that can be used in so many different situations and gives us valuable information about a transcription. **Keywords** is more of an added layer to the already impressive Deepgram AI, used to improve the transcription.\n\nASR is not perfect and humans are not perfect, but we can use a tool like the Deepgram **search** feature to get data about a transcription and make really informed decisions about it. We can use the **keywords** feature to prompt the Deepgram technology to learn so that it performs better for our unique situation.\n\nThe AI of Deepgram isn't about to take over the world all by itself, but humans with the AI of Deepgram can build great things.\n\n        ";
						}
						async function compiledContent$20() {
							return load$20().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$20() {
							return (await import('./chunks/index.3b85b6df.mjs'));
						}
						function Content$20(...args) {
							return load$20().then((m) => m.default(...args));
						}
						Content$20.isAstroComponentFactory = true;
						function getHeadings$20() {
							return load$20().then((m) => m.metadata.headings);
						}
						function getHeaders$20() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$20().then((m) => m.metadata.headings);
						}

const __vite_glob_0_148 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$20,
  file: file$20,
  url: url$20,
  rawContent: rawContent$20,
  compiledContent: compiledContent$20,
  default: load$20,
  Content: Content$20,
  getHeadings: getHeadings$20,
  getHeaders: getHeaders$20
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1$ = {"title":"Transcribing Radio Broadcasts With Node.js","description":"Learn how to use Deepgram to generate and store transcripts for your favorite radio stations. An excellent starting point to learn more about live transcription.","date":"2022-07-25T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1658753935/blog/2022/07/live-transcribing-radio-feeds-js/cover.png","authors":["kevin-lewis"],"category":"tutorial","tags":["javascript","nodejs"],"seo":{"title":"Transcribing Radio Broadcasts With Node.js","description":"Learn how to use Deepgram to generate and store transcripts for your favorite radio stations. An excellent starting point to learn more about live transcription."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454108/blog/live-transcribing-radio-feeds-js/ograph.png"},"shorturls":{"share":"https://dpgr.am/dd89e87","twitter":"https://dpgr.am/dc14509","linkedin":"https://dpgr.am/ac23935","reddit":"https://dpgr.am/0f3c3af","facebook":"https://dpgr.am/151cdf6"}};
						const file$1$ = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/live-transcribing-radio-feeds-js/index.md";
						const url$1$ = undefined;
						function rawContent$1$() {
							return "There are so many uses for Deepgram's live transcription service - from captioning meetings and events to creating home assistance and supporting call center operators by picking up on keywords.\n\nToday, you'll use the Deepgram JavaScript SDK to provide live transcriptions to live radio broadcasts and store spoken words in a file that can then be further analyzed.\n\n## Before You Start\n\nYou will need a Deepgram API Key - [get one here](https://console.deepgram.com/signup?jump=keys).\n\nCreate a new directory, open it on a code editor, and navigate to it in your terminal. Initialize a new Node.js project and install this project's dependencies:\n\n```\nnpm init -y\nnpm install cross-fetch @deepgram/sdk\n```\n\n[`cross-fetch`](https://npm.im/cross-fetch) is used to make HTTP requests more straightforward in your Node.js projects. Alternatives include `axios`, `got`, and `httpie` - use whatever works for you or the default `http` library in Node.js, which requires no dependencies.\n\nCreate an `index.js` file and open it in your code editor. Initialize the project dependencies:\n\n```js\nconst fetch = require('cross-fetch')\nconst { Deepgram } = require('@deepgram/sdk')\nconst fs = require('fs')\n```\n\n## Create a Deepgram Live Transcription Session\n\nInitialize the Deepgram JavaScript SDK, and create a new live transcription session:\n\n```js\nconst deepgram = new Deepgram('YOUR_DEEPGRAM_API_KEY')\nconst deepgramLive = deepgram.transcription.live({\n  punctuate: true,\n  tier: 'enhanced'\n})\n```\n\nTwo features are used in this session - punctuation and tier. Read more about [Deepgram features](https://developers.deepgram.com/documentation/features/) such as redaction, diarization, and language.\n\n## Fetch Real-Time Data from Radio Stations\n\nMake sure you have a direct audio stream for the radio station. A good way of testing this is to open the URL in a browser - you should see just the built-in browser audio player without an accompanying web page.\n\n![A browser showing a blank page, except one live native audio player](https://res.cloudinary.com/deepgram/image/upload/v1657635287/blog/2022/07/live-transcribing-radio-feeds-js/livestream-station.png)\n\nHere are a few URLs for you to try:\n\n* BBC Radio 4 (works outside the UK): http://stream.live.vc.bbcmedia.co.uk/bbc_radio_fourlw_online_nonuk\n* France Inter: https://direct.franceinter.fr/live/franceinter-midfi.mp3\n\nIf you use the French channel, be sure to add `language: fr` to your Deepgram session options.\n\n```js\nconst url = 'http://stream.live.vc.bbcmedia.co.uk/bbc_radio_fourlw_online_nonuk'\n\nfetch(url).then(r => r.body).then(res => {\n  res.on('readable', () => {\n    const data = res.read()\n    console.log(data)\n  })\n})\n```\n\nRun your code with `node index.js`, leave it running for a couple of seconds, and stop it with `ctrl+c`. You should see a bunch of buffers logged to your console.\n\n![Terminal showing the code being run, and 4 buffers of data](https://res.cloudinary.com/deepgram/image/upload/v1657635287/blog/2022/07/live-transcribing-radio-feeds-js/logging-buffers.png)\n\nThis is what you want to see - these buffers of audio data can be sent directly to Deepgram.\n\n## Transcribe the Radio Station\n\nReplace `console.log(data)` with the following to send the buffers to Deepgram if the connection is still open:\n\n```js\nif(deepgramLive.getReadyState() === 1) {\n    deepgramLive.send(data)\n}\n```\n\nAt the bottom of `index.js`, below all other code, add this code to listen for returned transcripts:\n\n```js\ndeepgramLive.addListener('transcriptReceived', (message) => {\n  const data = JSON.parse(message)\n  const transcript = data.channel.alternatives[0].transcript\n  if(transcript) {\n    console.log(transcript)\n  }\n})\n```\n\nRerun your code, and you should see transcripts in your terminal.\n\n![Terminal showing code being run, and 4 lines of transcripts](https://res.cloudinary.com/deepgram/image/upload/v1657636164/blog/2022/07/live-transcribing-radio-feeds-js/transcripts-in-term.png)\n\n## Save New Transcripts to a File\n\nTo save these transcripts to a file, you must first create a write stream and then write content to it. At the top of your file, just below your require statements, create the stream:\n\n```js\nconst stream = fs.createWriteStream('output.txt', { flags: 'a' })\n```\n\nThe `a` flag will open the file specifically for appending new data. If it does not exist, it will be automatically created.\n\nReplace `console.log(transcript)` with the following:\n\n```js\nstream.write(transcript + ' ')\n```\n\nThis will add the new transcript to the end of the existing file, ensuring there is a space between each item.\n\nRun your code again, wait a few seconds, and then stop it. Take a look at the new `output.txt` file, and you should see a big block of text which can then be stored in a database for compliance or further analysis.\n\n## In Summary\n\nThe full code is here:\n\n```js\nconst fetch = require('cross-fetch')\nconst { Deepgram } = require('@deepgram/sdk')\nconst fs = require('fs')\nconst stream = fs.createWriteStream('output.txt', { flags:'a' })\n\nconst deepgram = new Deepgram(deepgramApiKey)\nconst deepgramLive = deepgram.transcription.live({\n  punctuate: true,\n  tier: 'enhanced'\n})\n\nconst url = 'http://stream.live.vc.bbcmedia.co.uk/bbc_radio_fourlw_online_nonuk'\n\nfetch(url).then(r => r.body).then(res => {\n  res.on('readable', () => {\n    const data = res.read()\n    if(deepgramLive.getReadyState() === 1) {\n      deepgramLive.send(data)\n    }\n  })\n})\n\ndeepgramLive.addListener('transcriptReceived', (message) => {\n  const data = JSON.parse(message)\n  const transcript = data.channel.alternatives[0].transcript\n  if(transcript) {\n    stream.write(transcript + ' ')\n  }\n})\n```\n\nIf you have any questions, please feel free to reach out to us over email (`devrel@deepgram.com`) or via Twitter ([@DeepgramDevs](https://twitter.com/deepgramdevs)).\n\n```\n    \n```";
						}
						async function compiledContent$1$() {
							return load$1$().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1$() {
							return (await import('./chunks/index.5f4b6bee.mjs'));
						}
						function Content$1$(...args) {
							return load$1$().then((m) => m.default(...args));
						}
						Content$1$.isAstroComponentFactory = true;
						function getHeadings$1$() {
							return load$1$().then((m) => m.metadata.headings);
						}
						function getHeaders$1$() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1$().then((m) => m.metadata.headings);
						}

const __vite_glob_0_149 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1$,
  file: file$1$,
  url: url$1$,
  rawContent: rawContent$1$,
  compiledContent: compiledContent$1$,
  default: load$1$,
  Content: Content$1$,
  getHeadings: getHeadings$1$,
  getHeaders: getHeaders$1$
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1_ = {"title":"Video: Building a Live Transcription Badge With Deepgram","description":"We show you how we built our popular live transcription badge project.","date":"2022-01-21T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1643835533/blog/2022/01/live-transcription-badge-video/build-badge.png","authors":["kevin-lewis"],"category":"tutorial","tags":["raspberrypi","javascript","iot"],"seo":{"title":"Video: Building a Live Transcription Badge With Deepgram","description":"We show you how we built our popular live transcription badge project."},"shorturls":{"share":"https://dpgr.am/52f278c","twitter":"https://dpgr.am/d9fab4f","linkedin":"https://dpgr.am/a8943c4","reddit":"https://dpgr.am/83e1f9e","facebook":"https://dpgr.am/aa075d0"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453845/blog/live-transcription-badge-video/ograph.png"}};
						const file$1_ = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/live-transcription-badge-video/index.md";
						const url$1_ = undefined;
						function rawContent$1_() {
							return "\r\nA few weeks ago I spent an evening building a small personal project with a Raspberry Pi and Deepgram - a wearable screen that live captions your voice to help people understand you while wearing a mask. [I tweeted it and it blew up.](https://twitter.com/_phzn/status/1478821408486699009)\r\n\r\nSomething about a solution to this simple and universal problem made this project get way more popular than I expected, and made me take more time to build additional features.\r\n\r\nIn this video, I show you how I built this project, along with the parts you need and steps to take to create your own. This project was build with JavaScript, Vue.js, Node.js, Deepgram, and iTranslate. It is hosted for free on Glitch.\r\n\r\n<YouTube id=\"VPdvo6fF0zc\"></YouTube>\r\n\r\nImportant links:\r\n\r\n*   [Glitch Remix URL](https://glitch.com/edit/#!/remix/deepgram-transcription-badge)\r\n*   [How to automatically launch Chromium in Kiosk Mode](https://blog.deepgram.com/chromium-kiosk-pi/)\r\n\r\nCode:\r\n\r\n*   [Code in Glitch](https://glitch.com/~deepgram-transcription-badge)\r\n*   [Code in GitHub](https://github.com/deepgram-devs/live-transcription-badge)\r\n\r\nIf you have any questions, please feel free to reach out on Twitter - we're [@DeepgramDevs](https://twitter.com/DeepgramDevs).\r\n\r\n        ";
						}
						async function compiledContent$1_() {
							return load$1_().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1_() {
							return (await import('./chunks/index.5ce84362.mjs'));
						}
						function Content$1_(...args) {
							return load$1_().then((m) => m.default(...args));
						}
						Content$1_.isAstroComponentFactory = true;
						function getHeadings$1_() {
							return load$1_().then((m) => m.metadata.headings);
						}
						function getHeaders$1_() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1_().then((m) => m.metadata.headings);
						}

const __vite_glob_0_150 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1_,
  file: file$1_,
  url: url$1_,
  rawContent: rawContent$1_,
  compiledContent: compiledContent$1_,
  default: load$1_,
  Content: Content$1_,
  getHeadings: getHeadings$1_,
  getHeaders: getHeaders$1_
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1Z = {"title":"Live Transcription With Python and Django","description":"Learn how to do a live, real-time transcription with Django in Python and Deepgram","date":"2022-03-03T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1646295674/blog/2022/03/live-transcription-django/Live-Transcription-With-Python-Django-Deepgram%402x.jpg","authors":["tonya-sims"],"category":"tutorial","tags":["python","django"],"seo":{"title":"Live Transcription With Python and Django","description":"Learn how to do a live, real-time transcription with Django in Python and Deepgram"},"shorturls":{"share":"https://dpgr.am/abcb022","twitter":"https://dpgr.am/ba1aaba","linkedin":"https://dpgr.am/b5ab554","reddit":"https://dpgr.am/654438d","facebook":"https://dpgr.am/3d4f4bd"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454032/blog/live-transcription-django/ograph.png"}};
						const file$1Z = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/live-transcription-django/index.md";
						const url$1Z = undefined;
						function rawContent$1Z() {
							return "\nHave you ever wondered how to do live voice-to-text transcription with Python? We'll use Django and Deepgram to achieve our goal in this article.\n\nDjango is a familiar Python web framework for rapid development. It provides a lot of things we need \"out of the box\" and everything is included with the framework, following a “Batteries included” philosophy. Deepgram uses AI speech recognition to do real-time audio transcription, and we’ll be using our Python SDK.\n\nThe final code for this project is [here in Github](https://github.com/deepgram-devs/live-transcription-django) if you want to jump ahead.\n\n## Getting Started\n\nBefore we start, it’s essential to generate a Deepgram API key to use in our project. We can [go here](https://console.deepgram.com/signup?jump=keys). For this tutorial, we'll be using Python 3.10, but Deepgram supports some earlier versions of Python as well. We're also going to use Django version 4.0 and [Django Channels](https://channels.readthedocs.io/en/stable/introduction.html) to handle the WebSockets. We'll need to set up a virtual environment to hold our project. We can read more about those [here](https://blog.deepgram.com/python-virtual-environments/) and how to create one.\n\n## Install Dependencies\n\nCreate a folder directory to store all of our project files, and inside of it, create a virtual environment. Ensure our virtual environment is activated, as described in the article in the previous section. Make sure that all of the dependencies get installed inside that environment.\n\nFor a quick reference, here are the commands we need to create and activate our virtual environment:\n\n    mkdir [% NAME_OF_YOUR_DIRECTORY %]\n    cd [% NAME_OF_YOUR_DIRECTORY %]\n    python3 -m venv venv\n    source venv/bin/activate\n\nWe need to install the following dependencies from our terminal:\n\n*   The latest version of Django\n*   The Deepgram Python SDK\n*   The dotenv library, which helps us work with our environment variables\n*   The latest version of Django Channels\n\n<!---->\n\n    pip install Django\n    pip install deepgram-sdk\n    pip install python-dotenv\n    pip install channels\n\n## Create a Django Project\n\nLet's get a Django project created by running this command from our terminal, `django-admin startproject stream`.\n\nOur project directory will now look like this:\n\n![django project structure](https://res.cloudinary.com/deepgram/image/upload/v1646295675/blog/2022/03/live-transcription-django/django-project.png)\n\n## Create a Django App\n\nWe need to hold our code for the server part of our application inside an app called `transcript`. Let’s ensure we’re inside our project with `manage.py`. We need to change directories into our stream project by doing the following:\n\n    cd stream\n    python3 manage.py startapp transcript\n\nWe’ll see our new app `transcript` at the same directory level as our project.\n\n![django app](https://res.cloudinary.com/deepgram/image/upload/v1646295675/blog/2022/03/live-transcription-django/django-app.png)\n\nWe also need to tell our project that we’re using this new `transcript` app. To do so, go to our `stream` folder inside our `settings.py` file and add our app to `INSTALLED_APPS`.\n\n![django settings](https://res.cloudinary.com/deepgram/image/upload/v1646295675/blog/2022/03/live-transcription-django/settings-installed-apps.png)\n\n## Create Index View\n\nLet’s get a starter Django application up and running that renders an HTML page so that we can progress on our live transcription project.\n\nCreate a folder called `templates` inside our `transcript` app. Inside the templates folder, create an `index.html` file inside another directory called `transcript`.\n\n![django templates folder](https://res.cloudinary.com/deepgram/image/upload/v1646295675/blog/2022/03/live-transcription-django/templates-folder.png)\n\nInside our `transcript/templates/transcript/index.html` add the following HTML markup:\n\n```html\n<!DOCTYPE html>\n<html>\n  <head>\n    <title>Live Transcription</title>\n  </head>\n  <body>\n    <h1>Transcribe Audio With Django</h1>\n    <p id=\"status\">Connection status will go here</p>\n    <p id=\"transcript\"></p>\n  </body>\n</html>\n```\n\nThen add the following code to our `views.py` and `transcript` app.\n\n```python\nfrom django.shortcuts import render\n\ndef index(request):\n   return render(request, 'transcript/index.html')\n```\n\nWe need to create a `urls.py` inside our `transcript` app to call our view.\n\n![django urls](https://res.cloudinary.com/deepgram/image/upload/v1646295675/blog/2022/03/live-transcription-django/urls-app.png)\n\nLet’s add the following code to our new `urls.py` file:\n\n```python\nfrom django.urls import path\n\nfrom . import views\n\nurlpatterns = [\n   path('', views.index, name='index'),\n]\n```\n\nWe have to point this file at the `transcript.urls` module to `stream/urls.py`. In the `stream/urls.py` add the code:\n\n```python\nfrom django.conf.urls import include\nfrom django.contrib import admin\nfrom django.urls import path\n\nurlpatterns = [\n   path('', include('transcript.urls')),\n   path('admin/', admin.site.urls),\n]\n```\n\nIf we start our development server from the terminal to run the project using `python3 manage.py runserver`, the `index.html` page renders in the browser when we navigate to our localhost at `http://127.0.0.1:8000`.\n\n![render the index HTML page](https://res.cloudinary.com/deepgram/image/upload/v1646295675/blog/2022/03/live-transcription-django/django-index.png)\n\n## Integrate Django Channels\n\nWe need to add code to our `stream/asgi.py` file.\n\n![django asgi file](https://res.cloudinary.com/deepgram/image/upload/v1646295675/blog/2022/03/live-transcription-django/asgi.png)\n\n```python\nimport os\n\nfrom channels.auth import AuthMiddlewareStack\nfrom channels.routing import ProtocolTypeRouter, URLRouter\nfrom django.core.asgi import get_asgi_application\nimport transcript.routing\n\nos.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"stream.settings\")\n\napplication = ProtocolTypeRouter({\n \"http\": get_asgi_application(),\n \"websocket\": AuthMiddlewareStack(\n       URLRouter(\n           transcript.routing.websocket_urlpatterns\n       )\n   ),\n})\n```\n\nNow we have to add the Channels library to our `INSTALLED_APPS` in the `settings.py` file at `stream/settings.py`\n\n![django settings add channels](https://res.cloudinary.com/deepgram/image/upload/v1646295675/blog/2022/03/live-transcription-django/settings-channels.png)\n\nWe also need to add the following line to our `stream/settings.py` at the bottom of the file:\n\n`ASGI_APPLICATION = 'stream.asgi.application'`\n\nTo ensure everything is working correctly with Channels, run the development server `python3 manage.py runserver`. We should see the output in our terminal like the following:\n\n![django channels output in terminal](https://res.cloudinary.com/deepgram/image/upload/v1646295675/blog/2022/03/live-transcription-django/channels-output.png)\n\n## Add Deepgram API Key\n\nOur API Key will allow access to use Deepgram. Let’s create a `.env` file that will store our key. When we push our code to Github, hide our key, make sure to add this to our `.gitignore` file.\n\n![hide api key with .env file](https://res.cloudinary.com/deepgram/image/upload/v1646295675/blog/2022/03/live-transcription-django/django-env-file.png)\n\nIn our file, add the following environment variable with our Deepgram API key, which we can [grab here](https://console.deepgram.com/signup?jump=keys):\n\n    DEEPGRAM_API_KEY=\"abcde12345\"\n\nNext, create a `consumers.py` file inside our `transcript` app, acting as our server.\n\n![django consumers file to hold server](https://res.cloudinary.com/deepgram/image/upload/v1646295675/blog/2022/03/live-transcription-django/consumers-file.png)\n\nLet’s add this code to our `consumers.py`. This code loads our key into the project and accesses it in our application:\n\n```python\nfrom channels.generic.websocket import AsyncWebsocketConsumer\nfrom dotenv import load_dotenv\nfrom deepgram import Deepgram\n\nimport os\n\nload_dotenv()\n\nclass TranscriptConsumer(AsyncWebsocketConsumer):\n   dg_client = Deepgram(os.getenv('DEEPGRAM_API_KEY'))\n```\n\nWe also have to add a `routing.py` file inside our `transcript` app. This file will direct channels to run the correct code when we make an HTTP request intercepted by the Channels server.\n\n![django routing file in channels](https://res.cloudinary.com/deepgram/image/upload/v1646295675/blog/2022/03/live-transcription-django/routing-file.png)\n\n```python\nfrom django.urls import re_path\n\nfrom . import consumers\n\nwebsocket_urlpatterns = [\n   re_path(r'listen', consumers.TranscriptConsumer.as_asgi()),\n]\n```\n\n## Get Mic Data From Browser\n\nOur next step is to get the microphone data from the browser, which will require a little JavaScript.\n\nUse this code inside the `<script></script>` tag in `index.html` to access data from the user’s microphone.\n\n```js\n<script>\n  navigator.mediaDevices.getUserMedia({ audio: true }).then((stream) => {\n    const mediaRecorder = new MediaRecorder(stream)\n  })\n</script>\n```\n\nIf you want to learn more about working with the mic in the browser, please check out [this post](https://blog.deepgram.com/live-transcription-mic-browser/).\n\n## Websocket Connection Between Server and Browser\n\nWe’ll need to work with WebSockets in our project. We can think of WebSockets as a connection between a server and a client that stays open and allows sending continuous messages back and forth.\n\nThe first WebSocket connection is between our Python server that holds our Django application and our browser client. In this project, we’ll use [Django Channels](https://channels.readthedocs.io/en/stable/introduction.html) to handle the WebSocket server.\n\nWe need to create a WebSocket endpoint that listens to our Django web server code for client connections. In the `consumers.py` file from the previous section `re_path(r'listen', consumers.TranscriptConsumer.as_asgi())` accomplishes this connection.\n\n```python\nclass TranscriptConsumer(AsyncWebsocketConsumer):\n   dg_client = Deepgram(os.getenv('DEEPGRAM_API_KEY'))\n\n    async def connect(self):\n       await self.connect_to_deepgram()\n       await self.accept()\n\n      async def receive(self, bytes_data):\n       self.socket.send(bytes_data)\n```\n\nThe above code accepts a WebSocket connection between the server and the client. As long as the connection stays open, we will receive bytes and wait until we get a message from the client. While the server and browser connection remains open, we’ll wait for messages and send data.\n\nIn `index.html`, this code listens for a client connection then connects to the client like so:\n\n```js\n<script>... const socket = new WebSocket('ws://localhost:8000/listen')</script>\n```\n\n## Websocket Connection Between Server and Deepgram\n\nWe need to establish a connection between our central Django server and Deepgram to get the audio and real-time transcription. Add this code to our `consumers.py` file.\n\n```python\nfrom typing import Dict\n\nclass TranscriptConsumer(AsyncWebsocketConsumer):\n   dg_client = Deepgram(os.getenv('DEEPGRAM_API_KEY'))\n\n   async def get_transcript(self, data: Dict) -> None:\n       if 'channel' in data:\n           transcript = data['channel']['alternatives'][0]['transcript']\n\n           if transcript:\n               await self.send(transcript)\n\n\n   async def connect_to_deepgram(self):\n       try:\n           self.socket = await self.dg_client.transcription.live({'punctuate': True, 'interim_results': False})\n           self.socket.registerHandler(self.socket.event.CLOSE, lambda c: print(f'Connection closed with code {c}.'))\n           self.socket.registerHandler(self.socket.event.TRANSCRIPT_RECEIVED, self.get_transcript)\n\n       except Exception as e:\n           raise Exception(f'Could not open socket: {e}')\n\n   async def connect(self):\n       await self.connect_to_deepgram()\n       await self.accept()\n\n\n   async def disconnect(self, close_code):\n       await self.channel_layer.group_discard(\n           self.room_group_name,\n           self.channel_name\n       )\n\n   async def receive(self, bytes_data):\n       self.socket.send(bytes_data)\n```\n\nThe `connect_to_deepgram` function connects us to Deepgram and creates a socket connection to deepgram, listens for the connection to close, and gets incoming transcription objects. The `get_transcript` method gets the transcript from Deepgram and sends it back to the client.\n\nLastly, in our `index.html`, we need to receive and obtain data with the below events. Notice they are getting logged to our console. If you want to know more about what these events do, check out [this blog post](https://blog.deepgram.com/live-transcription-mic-browser/).\n\n```js\n<script>\n  socket.onopen = () => {\n    document.querySelector('#status').textContent = 'Connected'\n    console.log({\n        event: 'onopen'\n    })\n    mediaRecorder.addEventListener('dataavailable', async (event) => {\n        if (event.data.size > 0 && socket.readyState == 1) {\n            socket.send(event.data)\n        }\n    })\n    mediaRecorder.start(250)\n}\n\n  socket.onmessage = (message) => {\n      const received = message.data\n      if (received) {\n          console.log(received)\n          document.querySelector('#transcript').textContent += ' ' + received\n      }\n  }\n\n  socket.onclose = () => {\n      console.log({\n          event: 'onclose'\n      })\n  }\n\n  socket.onerror = (error) => {\n      console.log({\n          event: 'onerror',\n          error\n      })\n  }\n</script>\n```\n\nLet’s start our application and start getting real-time transcriptions. From our terminal, run `python3 manage.py runserver` and pull up our localhost on port 8000, `http://127.0.0.1:8000/`. If we haven’t already, allow access to our microphone. Start speaking, and we should see a transcript like the one below:\n\n![final result in Django live streaming example](https://res.cloudinary.com/deepgram/image/upload/v1646295675/blog/2022/03/live-transcription-django/django-final-screenshot.png)\n\nCongratulations on building a real-time transcription project with Django and Deepgram. You can find the [code here](https://github.com/deepgram-devs/live-transcription-django) with instructions on how to run the project. If you have any questions, please feel free to reach out to us on Twitter at [@DeepgramDevs](https://twitter.com/DeepgramDevs).\n\n        ";
						}
						async function compiledContent$1Z() {
							return load$1Z().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1Z() {
							return (await import('./chunks/index.802bb13a.mjs'));
						}
						function Content$1Z(...args) {
							return load$1Z().then((m) => m.default(...args));
						}
						Content$1Z.isAstroComponentFactory = true;
						function getHeadings$1Z() {
							return load$1Z().then((m) => m.metadata.headings);
						}
						function getHeaders$1Z() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1Z().then((m) => m.metadata.headings);
						}

const __vite_glob_0_151 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1Z,
  file: file$1Z,
  url: url$1Z,
  rawContent: rawContent$1Z,
  compiledContent: compiledContent$1Z,
  default: load$1Z,
  Content: Content$1Z,
  getHeadings: getHeadings$1Z,
  getHeaders: getHeaders$1Z
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1Y = {"title":"Live Transcription With Python and FastAPI","description":"Learn how to do a live, real-time transcription with FastAPI in Python and Deepgram","date":"2022-03-01T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1646061615/blog/2022/03/live-transcription-fastapi/Live-Transcription-With-Python-FastAPI-Deepgram%402x.jpg","authors":["tonya-sims"],"category":"tutorial","tags":["python","fastapi"],"seo":{"title":"Live Transcription With Python and FastAPI","description":"Learn how to do a live, real-time transcription with FastAPI in Python and Deepgram"},"shorturls":{"share":"https://dpgr.am/1f44788","twitter":"https://dpgr.am/a3d3510","linkedin":"https://dpgr.am/f8de728","reddit":"https://dpgr.am/016efab","facebook":"https://dpgr.am/5e90634"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454034/blog/live-transcription-fastapi/ograph.png"}};
						const file$1Y = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/live-transcription-fastapi/index.md";
						const url$1Y = undefined;
						function rawContent$1Y() {
							return "\nHave you ever wondered how to do live voice-to-text transcription with Python? We’ll use FastAPI and Deepgram to achieve our goal in this article.\n\nFastAPI is a new, innovative Python web framework gaining popularity because of its modern features like support for concurrency and asynchronous code. Deepgram uses AI speech recognition to do real-time audio transcription, and we’ll be using our Python SDK.\n\nIf you want to jump ahead, the final code for this project is located [here in Github](https://github.com/deepgram-devs/live-transcription-fastapi).\n\n## Getting Started\n\nBefore we start, it’s essential to generate a Deepgram API key to use in our project. To grab our key, we can [go here](https://console.deepgram.com/signup?jump=keys). For this tutorial, we’ll be using Python 3.10, but Deepgram supports some earlier versions of Python as well.We’ll also need to set up a virtual environment to hold our project. We can read more about those [here](https://blog.deepgram.com/python-virtual-environments/) and how to create one.\n\n## Install Dependencies\n\nCreate a folder directory to store all of our project files, and inside of it, create a virtual environment. Ensure our virtual environment is activated, as described in the article in the previous section. Make sure that all of the dependencies get installed inside that environment.\n\nFor a quick reference, here are the commands we need to create and activate our virtual environment:\n\n    mkdir [% NAME_OF_YOUR_DIRECTORY %]\n    cd [% NAME_OF_YOUR_DIRECTORY %]\n    python3 -m venv venv\n    source venv/bin/activate\n\nThe first thing we need to install is FastAPI and all of its dependencies. Next, we need to install Deepgram, which allows us to access the Deepgram Python SDK. To do this, we’ll need to run the following from the terminal:\n\n    pip install \"fastapi[all]\"\n    pip install deepgram-sdk\n\n## Create a FastAPI Application\n\nLet’s get a starter FastAPI application up and running that renders an HTML page so that we can progress on our live transcription project.\n\nCreate a file called `main.py` inside of our project and a templates folder with an HTML file inside called `index.html`.\n\n![fastapi project structure](https://res.cloudinary.com/deepgram/image/upload/v1646063371/blog/2022/03/live-transcription-fastapi/fastapi-project-structure.png)\n\nThe `main.py` file will hold our Python code.\n\n```python\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.templating import Jinja2Templates\n\napp = FastAPI()\n\ntemplates = Jinja2Templates(directory=\"templates\")\n\n@app.get(\"/\", response_class=HTMLResponse)\ndef get(request: Request):\n    return templates.TemplateResponse(\"index.html\", {\"request\": request})\n```\n\nLastly, we’ll store our HTML file inside the templates folder and hold our HTML markup here.\n\n```html\n<!DOCTYPE html>\n<html>\n  <head>\n    <title>Live Transcription</title>\n  </head>\n  <body>\n    <h1>Transcribe Audio With FastAPI</h1>\n    <p id=\"status\">Connection status will go here</p>\n    <p id=\"transcript\"></p>\n  </body>\n</html>\n```\n\nIf we start our development server from the terminal to run the project using `uvicorn main:app --reload` the `index.html` page renders in the browser.\n\n![render the index HTML page](https://res.cloudinary.com/deepgram/image/upload/v1646063371/blog/2022/03/live-transcription-fastapi/index-html.png)\n\n## Add Deepgram API Key\n\nOur API Key will allow access to use Deepgram. Let’s create a `.env` file that will store our key. Make sure to add this file to our `.gitignore` file when we push our code to Github, hiding our key.\n\n![hide api key with .env file](https://res.cloudinary.com/deepgram/image/upload/v1646063371/blog/2022/03/live-transcription-fastapi/env-file.png)\n\nIn our file, add the following environment variable with our Deepgram API key which we can [grab here](https://console.deepgram.com/signup?jump=keys):\n\n    DEEPGRAM_API_KEY=\"abcde12345\"\n\nThe below code shows how to load our key into the project and access it in `main.py`:\n\n```python\nfrom deepgram import Deepgram\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\napp = FastAPI()\n\ndg_client = Deepgram(os.getenv('DEEPGRAM_API_KEY'))\n```\n\n## Get Mic Data From Browser\n\nOur next step is to get the microphone data from the browser, which will require a little JavaScript.\n\nUse this code inside the `<script></script>` tag in `index.html` to access data from the user’s microphone.\n\n```js\n<script>\n  navigator.mediaDevices.getUserMedia({ audio: true }).then((stream) => {\n    const mediaRecorder = new MediaRecorder(stream)\n  })\n</script>\n```\n\nIf you want to learn more about working with the mic in the browser, please check out [this post](https://blog.deepgram.com/live-transcription-mic-browser/).\n\n## Websocket Connection Between Server and Browser\n\nWe’ll need to work with WebSockets in our project. We can think of WebSockets as a connection between a server and a client that stays open and allows sending continuous messages back and forth.\n\nThe first WebSocket connection is between our Python server that holds our FastAPI application and our browser client.\n\nIn our FastAPI web server code, we need to create a WebSocket endpoint that listens for client connections. In the `main.py` file, add the following code:\n\n```python\nfrom fastapi import FastAPI, Request, WebSocket\n\n@app.websocket(\"/listen\")\nasync def websocket_endpoint(websocket: WebSocket):\n    await websocket.accept()\n\n    try:\n        while True:\n            data = await websocket.receive_bytes()\n    except Exception as e:\n        raise Exception(f'Could not process audio: {e}')\n    finally:\n        await websocket.close()\n```\n\nThis code accepts a WebSocket connection between the server and the client. As long as the connection stays open, we will receive bytes and wait until we get a message from the client. If that doesn’t work, then we’ll throw an exception. Once the server and client finish sending messages to one another, we’ll close the connection.\n\nIn `index.html`, this code listens for a client connection, and if there is one, it connects to the client like so:\n\n```js\n<script>... const socket = new WebSocket('ws://localhost:8000/listen')</script>\n```\n\n## Websocket Connection Between Server and Deepgram\n\nWe need to establish a connection between our central FastAPI server and Deepgram to get the audio and do our real-time transcription. Add this code to our `main.py` file.\n\n```python\n@app.websocket(\"/listen\")\nasync def websocket_endpoint(websocket: WebSocket):\n    await websocket.accept()\n\n    try:\n        deepgram_socket = await process_audio(websocket)\n\n        while True:\n            data = await websocket.receive_bytes()\n            deepgram_socket.send(data)\n    except Exception as e:\n        raise Exception(f'Could not process audio: {e}')\n    finally:\n        await websocket.close()\n```\n\nWe’re defining a variable called `deepgram_socket`, which calls a function `process_audio` and opens the connection to Deepgram. In this user-defined method, we’ll also connect to Deepgram. While the server and browser connection stays open, we’ll wait for messages and send data.\n\nNext, let’s create our functions to process the audio, get the transcript from that audio and connect to Deepgram. In our `main.py`, add this code.\n\n```python\nfrom typing import Dict, Callable\n\nasync def process_audio(fast_socket: WebSocket):\n    async def get_transcript(data: Dict) -> None:\n        if 'channel' in data:\n            transcript = data['channel']['alternatives'][0]['transcript']\n\n            if transcript:\n                await fast_socket.send_text(transcript)\n\n    deepgram_socket = await connect_to_deepgram(get_transcript)\n\n    return deepgram_socket\n\nasync def connect_to_deepgram(transcript_received_handler: Callable[[Dict], None]):\n    try:\n        socket = await dg_client.transcription.live({'punctuate': True, 'interim_results': False})\n        socket.registerHandler(socket.event.CLOSE, lambda c: print(f'Connection closed with code {c}.'))\n        socket.registerHandler(socket.event.TRANSCRIPT_RECEIVED, transcript_received_handler)\n\n        return socket\n    except Exception as e:\n        raise Exception(f'Could not open socket: {e}')\n```\n\nThe `process_audio` function takes `fast_socket` as an argument, which will keep the connection open between the client and the FastAPI server. We’re also connecting to Deepgram and passing in the `get_transcript` function. This function gets the transcript and sends it back to the client.\n\nThe `connect_to_deepgram` function creates a socket connection to deepgram, listens for the connection to close, and gets incoming transcription objects.\n\nLastly, in our `index.html`, we need to receive and obtain data with the below events. Notice they are getting logged to our console. If you want to know more about what these events do, check out [this blog post](https://blog.deepgram.com/live-transcription-mic-browser/).\n\n```js\n<script>\n    socket.onopen = () => {\n        document.querySelector('#status').textContent = 'Connected'\n        console.log({\n            event: 'onopen'\n        })\n        mediaRecorder.addEventListener('dataavailable', async (event) => {\n            if (event.data.size > 0 && socket.readyState == 1) {\n                socket.send(event.data)\n            }\n        })\n        mediaRecorder.start(250)\n    }\n\n    socket.onmessage = (message) => {\n        const received = message.data\n        if (received) {\n            console.log(received)\n            document.querySelector('#transcript').textContent += ' ' + received\n        }\n    }\n\n    socket.onclose = () => {\n        console.log({\n            event: 'onclose'\n        })\n    }\n\n    socket.onerror = (error) => {\n        console.log({\n            event: 'onerror',\n            error\n        })\n    }\n\n</script>\n```\n\nLet’s start our application and start getting real-time transcriptions. From our terminal, run `uvicorn main:app --reload` and pull up our localhost on port 8000, `http://127.0.0.1:8000/`. If we haven’t already, allow access to our microphone. Start speaking, and we should see a transcript like the one below:\n\n![final result in fastapi live streaming example](https://res.cloudinary.com/deepgram/image/upload/v1646063950/blog/2022/03/live-transcription-fastapi/final-screenshot.png)\n\nCongratulations on building a real-time transcription project with FastAPI and Deepgram. You can find the [code here](https://github.com/deepgram-devs/live-transcription-fastapi) with instructions on how to run the project. If you have any questions, please feel free to reach out to us on Twitter at [@DeepgramDevs](https://twitter.com/DeepgramDevs).\n\n        ";
						}
						async function compiledContent$1Y() {
							return load$1Y().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1Y() {
							return (await import('./chunks/index.b2b8b909.mjs'));
						}
						function Content$1Y(...args) {
							return load$1Y().then((m) => m.default(...args));
						}
						Content$1Y.isAstroComponentFactory = true;
						function getHeadings$1Y() {
							return load$1Y().then((m) => m.metadata.headings);
						}
						function getHeaders$1Y() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1Y().then((m) => m.metadata.headings);
						}

const __vite_glob_0_152 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1Y,
  file: file$1Y,
  url: url$1Y,
  rawContent: rawContent$1Y,
  compiledContent: compiledContent$1Y,
  default: load$1Y,
  Content: Content$1Y,
  getHeadings: getHeadings$1Y,
  getHeaders: getHeaders$1Y
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1X = {"title":"Live Transcription With Python and Flask","description":"Learn how to do a live, real-time transcription with Flask in Python and Deepgram","date":"2022-03-02T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1646232657/blog/2022/03/live-transcription-flask/Live-Transcription-With-Python-Flask-Deepgram%402x.jpg","authors":["tonya-sims"],"category":"tutorial","tags":["python","flask"],"seo":{"title":"Live Transcription With Python and Flask","description":"Learn how to do a live, real-time transcription with Flask in Python and Deepgram"},"shorturls":{"share":"https://dpgr.am/bbacddc","twitter":"https://dpgr.am/d7ec666","linkedin":"https://dpgr.am/05c42ed","reddit":"https://dpgr.am/0bf905b","facebook":"https://dpgr.am/4ee5d48"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454036/blog/live-transcription-flask/ograph.png"}};
						const file$1X = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/live-transcription-flask/index.md";
						const url$1X = undefined;
						function rawContent$1X() {
							return "\nHave you ever wondered how to do live voice-to-text transcription with Python? We'll use Flask 2.0 and Deepgram to achieve our goal in this article.\n\nFlask 2.0 is a familiar, lightweight, micro web framework that is very flexible. It doesn't make decisions for us, meaning we are free to choose which database, templating engine, etc., to use without lacking functionality. Deepgram uses AI speech recognition to do real-time audio transcription, and we'll be using our Python SDK.\n\nThe final code for this project is located [here in Github](https://github.com/deepgram-devs/live-transcription-flask), if you want to jump ahead.\n\n## Getting Started\n\nBefore we start, it's essential to generate a Deepgram API key to use in our project. We can [go here](https://console.deepgram.com/signup?jump=keys). For this tutorial, we'll be using Python 3.10, but Deepgram supports some earlier versions of Python as well. Since we're using `async` in Flask, you'll need to have Python 3.7 or higher. We'll also need to set up a virtual environment to hold our project. We can read more about those [here](https://blog.deepgram.com/python-virtual-environments/) and how to create one.\n\n## Install Dependencies\n\nCreate a folder directory to store all of our project files, and inside of it, create a virtual environment. Ensure our virtual environment is activated, as described in the article in the previous section. Make sure that all of the dependencies get installed inside that environment.\n\nFor a quick reference, here are the commands we need to create and activate our virtual environment:\n\n    mkdir [% NAME_OF_YOUR_DIRECTORY %]\n    cd [% NAME_OF_YOUR_DIRECTORY %]\n    python3 -m venv venv\n    source venv/bin/activate\n\nWe need to install the following dependencies from our terminal:\n\n*   The latest version of Flask\n*   The Deepgram Python SDK\n*   The dotenv library, which helps us work with our environment variables\n*   The aiohttp-wsgi, which allows us to work with WebSockets in our WSGI application\n\n<!---->\n\n    pip install Flask\n    pip install deepgram-sdk\n    pip install python-dotenv\n    pip install aiohttp-wsgi\n\n## Create a Flask Application\n\nLet's get a starter Flask application up and running that renders an HTML page so that we can progress on our live transcription project.\n\nCreate a file called `main.py` inside our project and a templates folder with an HTML file called `index.html`.\n\n![flask project structure](https://res.cloudinary.com/deepgram/image/upload/v1646232661/blog/2022/03/live-transcription-flask/flask-project-structure.png)\n\nThe `main.py` file will hold our Python code.\n\n```python\nfrom flask import Flask, render_template\n\napp = Flask(__name__)\n\n@app.route('/')\ndef index():\n   return render_template('index.html')\n```\n\nLastly, we'll store our HTML file inside the templates folder and hold our HTML markup here.\n\n```html\n<!DOCTYPE html>\n<html>\n  <head>\n    <title>Live Transcription</title>\n  </head>\n  <body>\n    <h1>Transcribe Audio With Flask 2.0</h1>\n    <p id=\"status\">Connection status will go here</p>\n    <p id=\"transcript\"></p>\n  </body>\n</html>\n```\n\nWe have to export it into an environment variable to run the application. In our terminal, type the following:\n\n    export FLASK_APP=main\n\nIf we start our development server from the terminal to run the project using `flask run`, the `index.html` page renders in the browser.\n\n![render the index HTML page](https://res.cloudinary.com/deepgram/image/upload/v1647977919/blog/2022/03/live-transcription-flask/flask-index-html.png)\n\n## Add Deepgram API Key\n\nOur API Key will allow access to use Deepgram. Let's create a `.env` file that will store our key. When we push our code to Github, hide our key, make sure to add this to our `.gitignore` file.\n\n![hide api key with .env file](https://res.cloudinary.com/deepgram/image/upload/v1646232661/blog/2022/03/live-transcription-flask/flask-env-file.png)\n\nIn our file, add the following environment variable with our Deepgram API key, which we can [grab here](https://console.deepgram.com/signup?jump=keys):\n\n    DEEPGRAM_API_KEY=\"abcde12345\"\n\nThe below code shows how to load our key into the project and access it in `main.py`:\n\n```python\nfrom deepgram import Deepgram\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\napp = Flask(__name__)\n\ndg_client = Deepgram(os.getenv('DEEPGRAM_API_KEY'))\n```\n\n## Get Mic Data From Browser\n\nOur next step is to get the microphone data from the browser, which will require a little JavaScript.\n\nUse this code inside the `<script></script>` tag in `index.html` to access data from the user's microphone.\n\n```js\n<script>\n  navigator.mediaDevices.getUserMedia({ audio: true }).then((stream) => {\n    const mediaRecorder = new MediaRecorder(stream)\n  })\n</script>\n```\n\nIf you want to learn more about working with the mic in the browser, please check out [this post](https://blog.deepgram.com/live-transcription-mic-browser/).\n\n## Websocket Connection Between Server and Browser\n\nWe'll need to work with WebSockets in our project. We can think of WebSockets as a connection between a server and a client that stays open and allows sending continuous messages back and forth.\n\nThe first WebSocket connection is between our Python server that holds our Flask application and our browser client. In this project, we'll use [aiohttp](https://docs.aiohttp.org/en/v3.8.1/faq.html) to handle the WebSocket server.\n\nWe need to create a WebSocket endpoint that listens to our Flask web server code for client connections. In the `main.py` file, add the following code:\n\n```python\nimport asyncio\nfrom aiohttp import web\nfrom aiohttp_wsgi import WSGIHandler\n\napp = Flask('aioflask')\n\nasync def socket(request): #new\n   ws = web.WebSocketResponse()\n   await ws.prepare(request)\n\n   deepgram_socket = await process_audio(ws)\n\n   while True:\n       data = await ws.receive_bytes()\n       deepgram_socket.send(data)\n```\n\nThis code accepts a WebSocket connection between the server and the client. As long as the connection stays open, we will receive bytes and wait until we get a message from the client. We're defining a variable called `deepgram_socket`, which calls a function `process_audio` and opens the connection to Deepgram. In this user-defined method, we'll also connect to Deepgram. While the server and browser connection stays open, we'll wait for messages and send data.\n\nIn `index.html`, this code listens for a client connection then connects to the client like so:\n\n```js\n<script>... const socket = new WebSocket('ws://localhost:5555/listen')</script>\n```\n\n## Websocket Connection Between Server and Deepgram\n\nWe need to establish a connection between our central Flask server and Deepgram to get the audio and do our real-time transcription. Add this code to our `main.py` file.\n\n```python\nif __name__ == \"__main__\": # new\n   loop = asyncio.get_event_loop()\n   aio_app = web.Application()\n   wsgi = WSGIHandler(app)\n   aio_app.router.add_route('*', '/{path_info: *}', wsgi.handle_request)\n   aio_app.router.add_route('GET', '/listen', socket)\n   web.run_app(aio_app, port=5555)\n```\n\nThis code adds a route to the endpoint `listen` to the `socket` function. The equivalent of this is `app.route` in Flask.\n\nNext, let's create our functions to process the audio, get the transcript from that audio and connect to Deepgram. In our `main.py`, add this code.\n\n```python\nfrom typing import Dict, Callable\n\nasync def process_audio(fast_socket: web.WebSocketResponse):\n   async def get_transcript(data: Dict) -> None:\n       if 'channel' in data:\n           transcript = data['channel']['alternatives'][0]['transcript']\n\n           if transcript:\n               await fast_socket.send_str(transcript)\n\n   deepgram_socket = await connect_to_deepgram(get_transcript)\n\n   return deepgram_socket\n\nasync def connect_to_deepgram(transcript_received_handler: Callable[[Dict], None]) -> str:\n   try:\n       socket = await dg_client.transcription.live({'punctuate': True, 'interim_results': False})\n       socket.registerHandler(socket.event.CLOSE, lambda c: print(f'Connection closed with code {c}.'))\n       socket.registerHandler(socket.event.TRANSCRIPT_RECEIVED, transcript_received_handler)\n\n       return socket\n   except Exception as e:\n       raise Exception(f'Could not open socket: {e}')\n```\n\nThe `process_audio` function takes `fast_socket` as an argument, which will keep the connection open between the client and the Flask server. We also connect to Deepgram and pass in the `get_transcript` function. This function gets the transcript and sends it back to the client.\n\nThe `connect_to_deepgram` function creates a socket connection to deepgram, listens for the connection to close, and gets incoming transcription objects.\n\nLastly, in our `index.html`, we need to receive and obtain data with the below events. Notice they are getting logged to our console. If you want to know more about what these events do, check out [this blog post](https://blog.deepgram.com/live-transcription-mic-browser/).\n\n```js\n<script>\n  socket.onopen = () => {\n    document.querySelector('#status').textContent = 'Connected'\n    console.log({\n        event: 'onopen'\n    })\n    mediaRecorder.addEventListener('dataavailable', async (event) => {\n        if (event.data.size > 0 && socket.readyState == 1) {\n            socket.send(event.data)\n        }\n    })\n    mediaRecorder.start(250)\n}\n\n  socket.onmessage = (message) => {\n      const received = message.data\n      if (received) {\n          console.log(received)\n          document.querySelector('#transcript').textContent += ' ' + received\n      }\n  }\n\n  socket.onclose = () => {\n      console.log({\n          event: 'onclose'\n      })\n  }\n\n  socket.onerror = (error) => {\n      console.log({\n          event: 'onerror',\n          error\n      })\n  }\n</script>\n```\n\nLet's start our application and start getting real-time transcriptions. From our terminal, run `python main.py` and pull up our localhost on port 5555, `http://127.0.0.1:5555/`. If we haven't already, allow access to our microphone. Start speaking, and we should see a transcript like the one below:\n\n![final result in Flask live streaming example](https://res.cloudinary.com/deepgram/image/upload/v1646232661/blog/2022/03/live-transcription-flask/flask-final-screenshot.png)\n\nCongratulations on building a real-time transcription project with Flask and Deepgram. You can find the [code here](https://github.com/deepgram-devs/live-transcription-flask) with instructions on how to run the project. If you have any questions, please feel free to reach out to us on Twitter at [@DeepgramDevs](https://twitter.com/DeepgramDevs).\n\n        ";
						}
						async function compiledContent$1X() {
							return load$1X().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1X() {
							return (await import('./chunks/index.8ec90829.mjs'));
						}
						function Content$1X(...args) {
							return load$1X().then((m) => m.default(...args));
						}
						Content$1X.isAstroComponentFactory = true;
						function getHeadings$1X() {
							return load$1X().then((m) => m.metadata.headings);
						}
						function getHeaders$1X() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1X().then((m) => m.metadata.headings);
						}

const __vite_glob_0_153 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1X,
  file: file$1X,
  url: url$1X,
  rawContent: rawContent$1X,
  compiledContent: compiledContent$1X,
  default: load$1X,
  Content: Content$1X,
  getHeadings: getHeadings$1X,
  getHeaders: getHeaders$1X
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1W = {"title":"Get Live Speech Transcriptions In Your Browser","description":"Learn how to use Deepgram's streaming audio feature with one HTML file.","date":"2021-11-22T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1637012204/blog/2021/11/live-transcription-mic-browser/Get-Live-Speech-Transcriptions-In-Browser%402x.jpg","authors":["kevin-lewis"],"category":"tutorial","tags":["javascript","microphone"],"seo":{"title":"Get Live Speech Transcriptions In Your Browser","description":"Learn how to use Deepgram's streaming audio feature with one HTML file."},"shorturls":{"share":"https://dpgr.am/23585b8","twitter":"https://dpgr.am/fc4da4c","linkedin":"https://dpgr.am/45fca13","reddit":"https://dpgr.am/2e16852","facebook":"https://dpgr.am/a1fa559"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453810/blog/live-transcription-mic-browser/ograph.png"}};
						const file$1W = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/live-transcription-mic-browser/index.md";
						const url$1W = undefined;
						function rawContent$1W() {
							return "\nThere are so many projects you can build with Deepgram's streaming audio transcriptions. Today, we are going to get live transcriptions from a user's mic inside of your browser.\n\nWatch this tutorial as a video:\n\n<YouTube id=\"kIyPX16zuQY\"></YouTube>\n\n## Before We Start\n\nFor this project, you will need a Deepgram API Key - [get one here](https://console.deepgram.com/signup?jump=keys). That's it in terms of dependencies - this project is entirely browser-based.\n\nCreate a new `index.html` file, open it in a code editor, and add the following boilerplate code:\n\n```html\n<!DOCTYPE html>\n<html>\n  <body>\n    <p id=\"status\">Connection status will go here</p>\n    <p id=\"transcript\">Deepgram transcript will go here</p>\n    <script>\n      // Further code goes here\n    </script>\n  </body>\n</html>\n```\n\n## Get User Microphone\n\nYou can request access to a user's media input devices (microphones and cameras) using a built in `getUserMedia()` method. If allowed by the user, it will return a `MediaStream` which we can then prepare to send to Deepgram. Inside of your `<script>` add the following:\n\n```js\nnavigator.mediaDevices.getUserMedia({ audio: true }).then(stream => {\n  console.log({ stream })\n  // Further code goes here\n})\n```\n\nLoad your `index.html` file in your browser, and you should immediately receive a prompt to access your microphone. Grant it, and then look at the console in your developer tools.\n\n![The first half of the image shows the browser asking for access to the mic. An arrow with the phrase \"once granted\" points to the second half of the image, which has the browser console open, showing an object containing a MediaStream](https://res.cloudinary.com/deepgram/image/upload/v1637186575/blog/2021/11/live-transcription-mic-browser/granting-mic.png)\n\nNow we have a `MediaStream` we must provide it to a `MediaRecorder` which will prepare the data and, once available, emit it with a `datavailable` event:\n\n```js\nconst mediaRecorder = new MediaRecorder(stream)\n```\n\nWe now have everything we need to send Deepgram.\n\n## Connect to Deepgram\n\nTo stream audio to Deepgram's Speech Recognition service, we must open a WebSocket connection and send data via it. First, establish the connection:\n\n```js\nconst socket = new WebSocket('wss://api.deepgram.com/v1/listen', [ 'token', 'YOUR_DEEPGRAM_API_KEY' ])\n```\n\n<Alert type=\"warning\">\nA reminder that this key is client-side and, therefore, your users can see it. Any user with access to your key can access the Deepgram APIs, which, in turn, may provide full account access. Refer to our post on <a href=\"https://blog.deepgram.com/protecting-api-key/\">protecting your API key with browser live transcription</a>.\n</Alert>\n\nThen, log when socket `onopen`, `onmessage`, `onclose`, and `onerror` events are triggered:\n\n```js\nsocket.onopen = () => {\n  console.log({ event: 'onopen' })\n}\n\nsocket.onmessage = (message) => {\n  console.log({ event: 'onmessage', message })\n}\n\nsocket.onclose = () => {\n  console.log({ event: 'onclose' })\n}\n\nsocket.onerror = (error) => {\n  console.log({ event: 'onerror', error })\n}\n```\n\nRefresh your browser and watch the console. You should see the socket connection is opened and then closed. To keep the connection open, we must swiftly send some data once the connection is opened.\n\n## Sending Data to Deepgram\n\nInside of the `socket.onopen` function send data to Deepgram in 250ms increments:\n\n```js\nmediaRecorder.addEventListener('dataavailable', event => {\n  if (event.data.size > 0 && socket.readyState == 1) {\n    socket.send(event.data)\n  }\n})\nmediaRecorder.start(250)\n```\n\nDeepgram isn't fussy about the timeslice you provide (here it's 250ms), but bear in mind that the bigger this number is, the longer between words being spoken and it being sent, slowing down your transcription. 100-250 is ideal.\n\nTake a look at your console now while speaking into your mic - you should be seeing data come back from Deepgram!\n\n![The browser console shows four onmessage events. The last one is expanded and shows a JSON object, including a data object. The data object contains the words \"how are you doing today.\"](https://res.cloudinary.com/deepgram/image/upload/v1635938341/blog/2021/11/live-transcription-mic-browser/onmessage.png)\n\n## Handling the Deepgram Response\n\nInside of the `socket.onmessage` function parse the data sent from Deepgram, pull out the transcript only, and determine if it's the final transcript for that phrase (\"utterance\"):\n\n```js\nconst received = JSON.parse(message.data)\nconst transcript = received.channel.alternatives[0].transcript\nif (transcript && received.is_final) {\n  console.log(transcript)\n}\n```\n\nYou may have noticed that for each phrase, you have received several messages from Deepgram - each growing by a word (for example \"hello\", \"hello how\", \"hello how are\", etc). Deepgram will send you back data as each word is transcribed, which is great for getting a speedy response. For this simple project, we will only show the final version of each utterance which is denoted by an `is_final` property in the response.\n\nTo neaten this up, remove the `console.log({ event: 'onmessage', message })` from this function, and then test your code again.\n\n![The terminal shows two phrases written in plain text.](https://res.cloudinary.com/deepgram/image/upload/v1635938773/blog/2021/11/live-transcription-mic-browser/is_final-log.png)\n\nThat's it! That's the project. Before we wrap up, let's give the user some indication of progress in the web page itself.\n\n## Showing Status & Progress In Browser\n\nChange the text inside of `<p id=\"status\">` to 'Not Connected'. Then, at the top of your `socket.onopen` function add this line:\n\n```js\ndocument.querySelector('#status').textContent = 'Connected'\n```\n\nRemove the text inside of `<p id=\"transcript\">`. Where you are logging the transcript in your `socket.onmessage` function add this line:\n\n```js\ndocument.querySelector('#transcript').textContent += transcript + ' '\n```\n\nTry your project once more, and your web page should show you when you're connected and what words you have spoken, thanks to Deepgram's Speech Recognition.\n\nThe full code is here:\n\n```html\n<!DOCTYPE html>\n<html>\n  <body>\n    <p id=\"status\">Connection status will go here</p>\n    <p id=\"transcript\">Deepgram transcript will go here</p>\n    <script>\n      navigator.mediaDevices.getUserMedia({ audio: true }).then(stream => {\n        const mediaRecorder = new MediaRecorder(stream)\n        const socket = new WebSocket('wss://api.deepgram.com/v1/listen', [ 'token', 'YOUR_DEEPGRAM_API_KEY' ])\n\n        socket.onopen = () => {\n          console.log({ event: 'onopen' })\n          document.querySelector('#status').textContent = 'Connected'\n          mediaRecorder.addEventListener('dataavailable', event => {\n            if (event.data.size > 0 && socket.readyState == 1) {\n              socket.send(event.data)\n            }\n          })\n          mediaRecorder.start(250)\n        }\n\n        socket.onmessage = (message) => {\n          console.log({ event: 'onmessage', message })\n          const received = JSON.parse(message.data)\n          const transcript = received.channel.alternatives[0].transcript\n          if (transcript && received.is_final) {\n            document.querySelector('#transcript').textContent += transcript + ' '\n          }\n        }\n\n        socket.onclose = () => {\n          console.log({ event: 'onclose' })\n        }\n\n        socket.onerror = (error) => {\n          console.log({ event: 'onerror', error })\n        }\n      })\n    </script>\n  </body>\n</html>\n```\n\nIf you have any questions, please feel free to reach out on Twitter - we're [@DeepgramDevs](https://twitter.com/DeepgramDevs).\n\n        ";
						}
						async function compiledContent$1W() {
							return load$1W().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1W() {
							return (await import('./chunks/index.a6cba6fd.mjs'));
						}
						function Content$1W(...args) {
							return load$1W().then((m) => m.default(...args));
						}
						Content$1W.isAstroComponentFactory = true;
						function getHeadings$1W() {
							return load$1W().then((m) => m.metadata.headings);
						}
						function getHeaders$1W() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1W().then((m) => m.metadata.headings);
						}

const __vite_glob_0_154 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1W,
  file: file$1W,
  url: url$1W,
  rawContent: rawContent$1W,
  compiledContent: compiledContent$1W,
  default: load$1W,
  Content: Content$1W,
  getHeadings: getHeadings$1W,
  getHeaders: getHeaders$1W
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1V = {"title":"Live Transcription With Python and Quart","description":"Learn how to do a live, real-time voice-to-text transcription with Quart's Python web microframework and Deepgram's Python SDK.","date":"2022-03-08T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1646675713/blog/2022/03/live-transcription-quart/Live-Transcription-With-Python-Quart-Deepgram%402x.jpg","authors":["tonya-sims"],"category":"tutorial","tags":["python","quart"],"seo":{"title":"Live Transcription With Python and Quart","description":"Learn how to do a live, real-time voice-to-text transcription with Quart's Python web microframework and Deepgram's Python SDK."},"shorturls":{"share":"https://dpgr.am/c6f6137","twitter":"https://dpgr.am/040665d","linkedin":"https://dpgr.am/4bafb11","reddit":"https://dpgr.am/0bc0bc8","facebook":"https://dpgr.am/1c9138b"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454038/blog/live-transcription-quart/ograph.png"}};
						const file$1V = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/live-transcription-quart/index.md";
						const url$1V = undefined;
						function rawContent$1V() {
							return "\nHave you ever wondered how to do live speech-to-text transcription with Python? We’ll use Quart and Deepgram to achieve our goal in this article.\n\nQuart is a Python web microframework that is asynchronous, making it easier to serve WebSockets, which we’ll use in this tutorial. Quart is an asyncio reimplementation of Flask. If we’re familiar with Flask, we’ll be able to ramp up on Quart quickly. Deepgram uses AI speech recognition to do real-time audio transcription, and we’ll be using our Python SDK.\n\nIf you want to jump ahead, the final code for this project is located [here in Github](https://github.com/deepgram-devs/live-transcription-quart).\n\n## Getting Started\n\nBefore we start, it’s essential to generate a Deepgram API key to use in our project. We can [go here](https://console.deepgram.com/signup?jump=keys). For this tutorial, we’ll be using Python 3.10, but Deepgram supports some earlier versions of Python as well. We’ll also need to set up a virtual environment to hold our project. We can read more about those [here](https://blog.deepgram.com/python-virtual-environments/) and how to create one.\n\n## Install Dependencies\n\nCreate a folder directory to store all of our project files, and inside of it, create a virtual environment. Ensure our virtual environment is activated, as described in the article in the previous section. Make sure that all of the dependencies get installed inside that environment.\n\nFor a quick reference, here are the commands we need to create and activate our virtual environment:\n\n    mkdir [% NAME_OF_YOUR_DIRECTORY %]\n    cd [% NAME_OF_YOUR_DIRECTORY %]\n    python3 -m venv venv\n    source venv/bin/activate\n\nWe need to install the following dependencies from our terminal:\n\n*   The latest version of Quart\n*   The Deepgram Python SDK\n*   The dotenv library, which helps us work with our environment variables\n\n<!---->\n\n    pip install quart\n    pip install deepgram-sdk\n    pip install python-dotenv\n\n## Create a Quart Application\n\nLet’s get a starter Quart application up and running that renders an HTML page so that we can progress on our live speech-to-text transcription project.\n\nCreate a file called `main.py` inside of our project and a templates folder with an HTML file inside called `index.html`\n\n![quart project structure](https://res.cloudinary.com/deepgram/image/upload/v1646675715/blog/2022/03/live-transcription-quart/quart-project-structure.png)\n\nThe `main.py` file will hold our Python code.\n\n```python\nfrom quart import Quart, render_template\n\napp = Quart(__name__)\n\n@app.route('/')\nasync def index():\n   return await render_template('index.html')\n\nif __name__ == \"__main__\":\n   app.run('localhost', port=3000, debug=True)\n```\n\nLastly, we’ll store our HTML file inside the templates folder and hold our HTML markup here.\n\n```html\n<!DOCTYPE html>\n<html>\n  <head>\n    <title>Live Transcription</title>\n  </head>\n  <body>\n    <h1>Transcribe Audio With Quart</h1>\n    <p id=\"status\">Connection status will go here</p>\n    <p id=\"transcript\"></p>\n  </body>\n</html>\n```\n\nWe have to export it into an environment variable to run the application. In our terminal, type the following:\n\n    export QUART_APP=main:app\n\nIf we start our development server from the terminal to run the project using `python main.py`, and pull up the browser at http://127.0.0.1:3000/, the `index.html` page will render:\n\n![render the Quart index HTML page](https://res.cloudinary.com/deepgram/image/upload/v1646675714/blog/2022/03/live-transcription-quart/quart-index-html.png)\n\n## Add Deepgram API Key\n\nOur API Key will allow access to use Deepgram to do our real-time audio transcription using AI speech recognition. Let’s create a `.env` file that will store our key. Make sure to add this file to our `.gitignore` file when we push our code to Github, hiding our key.\n\n![hide api key with .env file](https://res.cloudinary.com/deepgram/image/upload/v1646675715/blog/2022/03/live-transcription-quart/env-file.png)\n\nIn our file, add the following environment variable with our Deepgram API key, which we can [grab here](https://console.deepgram.com/signup?jump=keys):\n\n    DEEPGRAM_API_KEY=\"abcde12345\"\n\nThe below code shows how to load our key into the project and access it in `main.py`:\n\n```python\nfrom deepgram import Deepgram\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\napp = Quart(__name__)\n\ndg_client = Deepgram(os.getenv('DEEPGRAM_API_KEY'))\n```\n\n## Get Mic Data From Browser\n\nOur next step is to get the microphone data from the browser, which will require a little JavaScript.\n\nUse this code inside the `<script></script>` tag in `index.html` to access the user’s microphone data.\n\n```js\n<script>\n navigator.mediaDevices.getUserMedia({ audio: true }).then((stream) => {\n   const mediaRecorder = new MediaRecorder(stream)\n })\n</script>\n```\n\nIf you want to learn more about working with the mic in the browser, please check out [this post](https://blog.deepgram.com/live-transcription-mic-browser/).\n\n## Websocket Connection Between Server and Browser\n\nWe’ll need to work with WebSockets in our project. We can think of WebSockets as a connection between a server and a client that stays open and allows sending continuous messages back and forth.\n\nThe first WebSocket connection is between our Python server that holds our Quart application and our browser client.\n\nWe need to create a WebSocket endpoint that listens for client connections in our Quart web server code. In the `main.py` file, add the following code:\n\n```python\nfrom quart import Quart, render_template, websocket\n\n@app.websocket('/listen')\nasync def websocket_endpoint():\n  await websocket.accept()\n  try:\n      while True:\n          data = await websocket.receive()\n  except Exception as e:\n      raise Exception(f'Could not process audio: {e}')\n  finally:\n      await websocket.close(1000)\n```\n\nThis code accepts a WebSocket connection between the server and the client. As long as the connection stays open, we will receive bytes and wait until we get a message from the client. If that doesn’t work, then we’ll throw an exception. Once the server and client finish sending messages to one another, we’ll close the connection.\n\nThe code in `index.html` listens for a client connection. If there is one, it connects to the client like so:\n\n```js\n<script>... const socket = new WebSocket('ws://localhost:3000/listen')</script>\n```\n\n## Websocket Connection Between Server and Deepgram\n\nWe need to establish a connection between our central Quart server and Deepgram to get the audio and do our real-time speech transcription. Add this code to our `main.py` file.\n\n```python\n@app.websocket('/listen')\nasync def websocket_endpoint():\n\n   try:\n       deepgram_socket = await process_audio(websocket)\n\n       while True:\n           data = await websocket.receive()\n           deepgram_socket.send(data)\n   except Exception as e:\n       raise Exception(f'Could not process audio: {e}')\n   finally:\n       websocket.close(1000)\n```\n\nWe’re defining a variable called `deepgram_socket`, which calls a function `process_audio` and opens the connection to Deepgram. In this user-defined method, we’ll also connect to Deepgram. While the server and browser connection stays open, we’ll wait for messages and send data.\n\nNext, let’s create our functions to process the audio, get the real-time audio transcription and connect to Deepgram. In our `main.py`, add this code.\n\n```python\nfrom typing import Dict, Callable\n\nasync def process_audio(fast_socket):\n   async def get_transcript(data: Dict) -> None:\n       if 'channel' in data:\n           transcript = data['channel']['alternatives'][0]['transcript']\n\n           if transcript:\n               await fast_socket.send(transcript)\n\n   deepgram_socket = await connect_to_deepgram(get_transcript)\n\n   return deepgram_socket\n\nasync def connect_to_deepgram(transcript_received_handler: Callable[[Dict], None]) -> str:\n   try:\n       socket = await dg_client.transcription.live({'punctuate': True, 'interim_results': False})\n       socket.registerHandler(socket.event.CLOSE, lambda c: print(f'Connection closed with code {c}.'))\n       socket.registerHandler(socket.event.TRANSCRIPT_RECEIVED, transcript_received_handler)\n\n       return socket\n   except Exception as e:\n       raise Exception(f'Could not open socket: {e}')\n```\n\nThe `process_audio` function takes `fast_socket` as an argument, which will keep the connection open between the client and the Quart server. We also connect to Deepgram and pass in the `get_transcript` function. This function gets the transcript and sends it back to the client.\n\nThe `connect_to_deepgram` function creates a socket connection to deepgram, listens for the connection to close, and gets incoming transcription objects.\n\nLastly, in our `index.html`, we need to receive and obtain data with the below events. Notice they are getting logged to our console. If you want to know more about what these events do, check out [this blog post](https://blog.deepgram.com/live-transcription-mic-browser/).\n\n```js\n<script>\n    socket.onopen = () => {\n        document.querySelector('#status').textContent = 'Connected'\n        mediaRecorder.addEventListener('dataavailable', async (event) => {\n            if (event.data.size > 0 && socket.readyState == 1) {\n                socket.send(event.data)\n            }\n        })\n        mediaRecorder.start(250)\n    }\n\n    socket.onmessage = (message) => {\n        const received = message.data\n        if (received) {\n            document.querySelector('#transcript').textContent += ' ' + message.data\n        }\n\n    }\n\n    socket.onclose = () => {\n        console.log({\n            event: 'onclose'\n        })\n    }\n\n    socket.onerror = (error) => {\n        console.log({\n            event: 'onerror',\n            error\n        })\n    }\n</script>\n```\n\nLet’s start our application and start getting real-time audio transcriptions. From our terminal, run `python main.py` and pull up our localhost on port 3000, `http://127.0.0.1:3000/`. If we haven’t already, allow access to our microphone. Start speaking, and we should see a transcript like the one below:\n\n![final result in Quart live streaming example](https://res.cloudinary.com/deepgram/image/upload/v1646675714/blog/2022/03/live-transcription-quart/quart-final-screenshot.png)\n\nCongratulations on building a live voice-to-text transcription project with Quart and Deepgram. You can find the [code here](https://github.com/deepgram-devs/live-transcription-quart) with instructions on how to run the project. If you have any questions, please feel free to reach out to us on Twitter at [@DeepgramDevs](https://twitter.com/DeepgramDevs).\n\n        ";
						}
						async function compiledContent$1V() {
							return load$1V().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1V() {
							return (await import('./chunks/index.f4f56f34.mjs'));
						}
						function Content$1V(...args) {
							return load$1V().then((m) => m.default(...args));
						}
						Content$1V.isAstroComponentFactory = true;
						function getHeadings$1V() {
							return load$1V().then((m) => m.metadata.headings);
						}
						function getHeaders$1V() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1V().then((m) => m.metadata.headings);
						}

const __vite_glob_0_155 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1V,
  file: file$1V,
  url: url$1V,
  rawContent: rawContent$1V,
  compiledContent: compiledContent$1V,
  default: load$1V,
  Content: Content$1V,
  getHeadings: getHeadings$1V,
  getHeaders: getHeaders$1V
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1U = {"title":"Luke Oliff Joins the Developer Relations Team","description":"Luke Oliff joins the Developer Relations team at Deepgram!","date":"2021-11-30T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1638199198/blog/2021/11/luke-oliff-joins-deepgram/luke-oliff-joins-developer-relations-at-deepgram.jpg","authors":["luke-oliff"],"category":"devlife","tags":["team"],"seo":{"title":"Luke Oliff joins the Developer Relations team at Deepgram as a Senior Developer Experience Engineer","description":"Luke Oliff joins the Developer Relations team at Deepgram!"},"shorturls":{"share":"https://dpgr.am/fe381ce","twitter":"https://dpgr.am/189a808","linkedin":"https://dpgr.am/eaeadc1","reddit":"https://dpgr.am/ea294e3","facebook":"https://dpgr.am/a83bb6f"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453812/blog/luke-oliff-joins-deepgram/ograph.png"}};
						const file$1U = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/luke-oliff-joins-deepgram/index.md";
						const url$1U = undefined;
						function rawContent$1U() {
							return "\r\nHi! My name is Luke, and I'm happy to announce that I have joined the Developer Relations teams here at Deepgram as a Senior Developer Experience Engineer.\r\n\r\nI've been in Developer Relations in one form or another for nearly 5 years. In that time, I've discovered that Developer Relations doesn't really consist of one thing that you can log on and do every day. It is many things that have the ultimate goal of reducing friction to our products, or improving the experience for developers.\r\n\r\nI live in the historic Essex town of Colchester, which despite no one having ever heard of it, was Britain's \"first city\" and the former capital of Roman Britain. There is so much history here, that local residents digging in their garden will often find clay pipes, or other Roman, Norman, and Victorian artifacts. They even found [the UK's only Roman chariot track](https://www.visitcolchester.com/things-to-do/roman-circus-visitor-centre-p1190001) (under my old house!).\r\n\r\nThe town (then Roman capital) was home to the Roman Temple of Claudius, which the ruined foundations of were later used to build a Norman castle. [Colchester Castle](https://www.bbc.co.uk/essex/content/articles/2006/02/08/colchester_castle_feature.shtml) (pictured) was build almost 1000 years ago, is the largest Norman Keep in Europe, and was also used as a blueprint for the Tower of London.\r\n\r\nWhen not at work you can find me being a dad, tweeting (too much), or playing PC games. I've also recently been [streaming about technology](https://www.twitch.tv/lukeocodes), covering things like Nuxt, Vue, and Tailwind projects. My stream income gets donated to charities like [BlackGirlsCODE](https://www.blackgirlscode.com/) and [GirlsWhoCode](https://girlswhocode.com/), which help under-represented folks gain access to tech careers.\r\n\r\n        ";
						}
						async function compiledContent$1U() {
							return load$1U().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1U() {
							return (await import('./chunks/index.fe780025.mjs'));
						}
						function Content$1U(...args) {
							return load$1U().then((m) => m.default(...args));
						}
						Content$1U.isAstroComponentFactory = true;
						function getHeadings$1U() {
							return load$1U().then((m) => m.metadata.headings);
						}
						function getHeaders$1U() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1U().then((m) => m.metadata.headings);
						}

const __vite_glob_0_156 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1U,
  file: file$1U,
  url: url$1U,
  rawContent: rawContent$1U,
  compiledContent: compiledContent$1U,
  default: load$1U,
  Content: Content$1U,
  getHeadings: getHeadings$1U,
  getHeaders: getHeaders$1U
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1T = {"title":"Machine Learning for Front-end Developers: Get Started with TensorFlow.js","description":"Machine Learning used to be just for Data Scientists and Python developers, but with TensorFlow.js, front-end developers have more access than ever.","date":"2022-09-05T18:54:17.740Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1660592694/blog/2022/08/getting-started-with-tensorflowjs/tfjs-for-beginners-blog%402x.jpg","authors":["bekah-hawrot-weigel"],"category":"tutorial","tags":["machine-learning","tensorflowjs"],"shorturls":{"share":"https://dpgr.am/4a1060f","twitter":"https://dpgr.am/15b7a58","linkedin":"https://dpgr.am/fc88674","reddit":"https://dpgr.am/ecae480","facebook":"https://dpgr.am/4ff184b"}};
						const file$1T = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/machine-learning-for-front-end-developers-get-started-with-tensorflow-js/index.md";
						const url$1T = undefined;
						function rawContent$1T() {
							return "\nMachine Learning (ML) has historically belonged to Python engineers, Data Scientists, and mathematicians, with frontend developers having very usage and support. In fact, JavaScript ranks 5th of all languages used for Machine Learning, coming in at around 7%. With access to microphones and webcams in the browser, we're just getting started with exploring a whole new landscape of what we can do as front-end developers. This is where TensorFlow.js comes in.\n\nTensorFlow.js (TFJS) is a JavaScript framework that we can use to create machine learning models for desktop, web, mobile, and cloud, giving us access to images, video, audio, and text models. We can use this technology for augmented reality, sound recognition, sentiment analysis, web page optimization, accessibility, and more.\n\n## Exploring TFJS Models\n\n### [Pre-trained TensorFlow.js models repository](https://github.com/tensorflow/tfjs-models/)\n\nThis repository contains ten different ready-to-use models, categorizes each of them by type of model (images, audio, text, and general utilities), shares the details of each, gives installation directions, links to the source code, and provides a demo for most of the models.\n\n### [TensorFlow Hub](https://tfhub.dev/)\n\nTensorFlow Hub is a centralized site with hundreds of trained models. It includes model formats for TF.js, TFLite, and Coral, so be sure to select `TF.js` to explore other models that are available for TFJS. Within the hub, the name of the model, the type of the model, description, and dataset used are listed. If we click on an entry, we'll see more information, including an overview, how to implement the model, how to fine-tune a model--if that's an option--and more.\n\n## Getting Started with TFJS\n\nThere are three different ways we can use TFJS; each requiring a little more knowledge and understanding of how the models work.\n\n### Import an Existing, Pre-trained Model\n\nWe can take a TensorFlow or Keras model that's been trained offline and load it into the browser. If we want to explore existing models and their demos, we can check out the [TFJS models repository](https://github.com/tensorflow/tfjs-models/).\n\nWhen we take an existing model, we can get it working immediately with a couple of lines of JavaScript. We can get this up and running using a script tag or installing with `npm` or `yarn`.\n\n#### MobileNet\n\nWe're going to implement the MobileNet model, using the code they've give us in the [MobileNet repository](https://github.com/tensorflow/tfjs-models/tree/master/mobilenet)\n\n> MobileNets are small, low-latency, low-power models parameterized to meet the resource  constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used.\n> We're going to use the model to classify an image. We can see a working codesandbox [here](https://codesandbox.io/s/mobilenet-e1gmmh) or checkout the code below:\n\n```js\n<!DOCTYPE html>\n<html>\n  <head>\n  </head>\n    <body>\n      <h1>Check the console for our predictions!</h1>\n    </body>\n  \n    <!-- Load TensorFlow.js. This is required to use MobileNet. -->\n    <script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.0.1\"> </script>\n    <!-- Load the MobileNet model. -->\n    <script src=\"https://cdn.jsdelivr.net/npm/@tensorflow-models/mobilenet@1.0.0\"> </script>\n    <!-- Replace this with your image. Make sure CORS settings allow reading the image! -->\n    <img id=\"img\" src=\"cat.jpg\" style=\"width:200px;height:300px\">\n    <!-- Place your code in the script tag below. You can also use an external .js file -->\n    <script>\n      // Notice there is no 'import' statement. 'mobilenet' and 'tf' is\n      // available on the index-page because of the script tag above.\n      const img = document.getElementById('img');\n      // Load the model.\n      mobilenet.load().then(model => {\n        // Classify the image.\n        model.classify(img).then(predictions => {\n          console.log({predictions});\n        });\n      });\n    </script>\n</html>\n\n```\n\nNotice in the console, we're given three predictions for the image. And all of this is put together in fewer than 30 lines of code.\n\n![](https://res.cloudinary.com/deepgram/image/upload/v1660235143/blog/2022/08/getting-started-with-tensorflowjs/predictions.png)\n\n### Retrain an Imported Machine Learning Model\n\nWe can do more than use existing models straight of out the box. We can use transfer learning to continue to train a model (known as a base model) that was trained offline with data we collect from the browser. Transfer learning is when we take a pre-trained model and re-use it for a related task. For example, we might take a model trained with image detection. It already knows how to identify features of objects like shapes. We might take this model and further train it to identify a specific object. This gives us a speedy way to train that doesn't require us to invest the full amount of time training and developing a new model.\n\n### Create a New Model\n\nOnce we understand models and layers, we can progress to creating our own model. Using the Layers API or the Core API, we can create our own model. This allows us to have control over defining what our model will do, how we train our model, and the output when we run it. It's generally recommended to use the Layers API first since it's modeled after the Keras API.\n\n## Should we use TensorFlow.js for Deep Learning?\n\nDepending on your use case, TFJS can be an incredible valuable tool that allows for browser interaction, including using the webcam and the microphone. There are use cases that create more interactive experiences for education, healthcare support, accessibility advancements, and opportunities for just plain fun.\n\nAs front-end developers, we're often kept out of conversations about ML, data science, etc. Tensorflow.js gives us an opportunity to help shape a developing field, access tools we haven't before, and expand the horizon of the frontend experience.  Having a variety of voices in conversations that impact, well, everyone is critical. Diversity of thought, input into development, and different backgrounds can help to not only push the industry forward, but to redefine what we can do in the frontend landscape. Tech that's belonged to Data Scientists can be part of what we do and allow us to keep pushing boundaries on what we're creating and the problems we're solving.\n\nTo get started with TensorFlow.js, you can find a tutorial [here](https://www.tensorflow.org/js/tutorials). If you want to see a TFJS + Deepgram Project, you can check out [Add Live Speech Bubbles to YouTube Videos with Autobubble](https://blog.deepgram.com/autobubble-youtube-speech-bubbles/), and, as always, you can hit us up with any questions on Twitter [@deepgramAI](https://twitter.com/DeepgramAI).\n\n";
						}
						async function compiledContent$1T() {
							return load$1T().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1T() {
							return (await import('./chunks/index.0d54207d.mjs'));
						}
						function Content$1T(...args) {
							return load$1T().then((m) => m.default(...args));
						}
						Content$1T.isAstroComponentFactory = true;
						function getHeadings$1T() {
							return load$1T().then((m) => m.metadata.headings);
						}
						function getHeaders$1T() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1T().then((m) => m.metadata.headings);
						}

const __vite_glob_0_157 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1T,
  file: file$1T,
  url: url$1T,
  rawContent: rawContent$1T,
  compiledContent: compiledContent$1T,
  default: load$1T,
  Content: Content$1T,
  getHeadings: getHeadings$1T,
  getHeaders: getHeaders$1T
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1S = {"title":"Making Your Audio and Visual Content Accessible","description":"Creating audio visual content can be a fresh way to repackage your written developer content, but it’s important to make sure your content is accessible.","date":"2022-04-25T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1650883327/blog/2022/04/making-your-audiovisual-content-accessible/Making-Your-AV-Content-Accessible%402x.png","authors":["bekah-hawrot-weigel"],"category":"tutorial","tags":["accessibility"],"seo":{"title":"Making Your Audio and Visual Content Accessible","description":"Creating audio visual content can be a fresh way to repackage your written developer content, but it’s important to make sure your content is accessible."},"shorturls":{"share":"https://dpgr.am/b6b100c","twitter":"https://dpgr.am/330366a","linkedin":"https://dpgr.am/ec9c2cd","reddit":"https://dpgr.am/247181f","facebook":"https://dpgr.am/3e0fc3e"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454065/blog/making-your-audiovisual-content-accessible/ograph.png"}};
						const file$1S = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/making-your-audiovisual-content-accessible/index.md";
						const url$1S = undefined;
						function rawContent$1S() {
							return "\r\nSo many of us are creating audio and visual content for the developer community, whether that's through Twitter Spaces, livestreams, podcasts, videos, or something else. As the content creation field grows, content creators should ensure that content is accessible. [One way w3 describes accessibility](https://www.w3.org/standards/webdesign/) is to “provide equal access and equal opportunity to people with diverse abilities.” Because audiovisual content isn’t naturally accessible to everyone, there are best practices to keep in mind. In this post, I'll share tips to help you create accessible video and audio content.\r\n\r\nCreating accessible audio and visual content means that your audience can experience it even if they have visual or hearing impairments, cognitive disabilities, neurodiversity that impairs their ability to consume audio or visual content, limited dexterity, or any other reason preventing them from consuming your content.\r\n\r\n## Definitions\r\n\r\n*   **Captions**: The written text of what is spoken that appears onscreen as someone is speaking. Captions capture the speaker's words.\r\n\r\n*   **Transcripts**: The text version of your video or audio that allows the user to read the audio content in one file.\r\n\r\n## Video\r\n\r\nThe production that goes into videos can range from simple to complex, live to edited. There’s no one right way to do it, but there are some basic ways to ensure that your video content is accessible. In fact, in the United States, closed captioning for public television has been mandated as part of the [Americans with Disabilities Act](https://www.ada.gov/), and also [digital movies must provide closed movie captioning and audio description](https://www.ada.gov/regs2016/final_ra_movie_captioning.html).\r\n\r\n### Video Accessibility Checklist\r\n\r\n*   Include closed captions or transcriptions. For clearer directions on acceptable captions, check out the [WCAG guide](https://www.w3.org/TR/WCAG21/#captions-prerecorded).\r\n*   Use a clear font for your captions.\r\n*   Make sure the color contrast of your font and the background of your text is acceptable. To determine if your contrast passes, you can use the [WebAim contrast checker](https://webaim.org/resources/contrastchecker/).\r\n*   Synchronize the captions with the audio.\r\n*   Identify who is speaking.\r\n*   If you are flashing content on the screen, ensure enough time to view the content. For example, if you’re interviewing a guest and share their social media handles with a textbox on the screen, the viewer should have enough time to view and use the content.\r\n*   Provide a clear point of focus. If you’re generating captions, make sure that the screen behind and around the captions isn’t overwhelming or difficult to focus on.\r\n*   If there’s visual information essential to understanding the content, provide a description.\r\n*   Avoid flashing images that can cause seizures.\r\n*   Practice standards for high-quality visuals when possible. For example, lighting can help to reduce shadows and allow viewers a clearer path to reading lips and facial expressions.\r\n\r\n## Audio\r\n\r\nWith a rise in audio content through podcasts, Twitter Spaces, and other forms of content creation, there should be a clearer focus on ensuring these platforms are accessible to everyone.\r\n\r\n### Audio Accessibility Checklist\r\n\r\n*   Provide transcripts and captions (where applicable).\r\n*   If captions are provided, synchronize with the audio, and ensure background contrast and clear font and color.\r\n*   If there are multiple speakers, identify who is speaking.\r\n*   Reduce background noise and follow standards for recording clear audio as much as possible. For example, if you’re recording a podcast, you can use headphones and mute yourself when not talking to reduce background noise. For more tips on creating high-quality audio, check out the [WCAG Guide](https://www.w3.org/WAI/media/av/av-content/#create-high-quality-audio--recording-setup).\r\n*   Provide an audio description if necessary.\r\n\r\n## Resources\r\n\r\n### Examples Using Deepgram\r\n\r\n*   [Adding Live Captions To Your Classroom with Deepgram](https://blog.deepgram.com/classroom-captioner/)\r\n*   [Video: Building a Live Transcription Badge With Deepgram](https://blog.deepgram.com/live-transcription-badge-video/)\r\n*   [Transcribe YouTube Videos with Node.js](https://blog.deepgram.com/transcribe-youtube-videos-nodejs/)\r\n\r\n### Additional Resources for Accessible AV Content Creation\r\n\r\n*   [WCAG AV Content](https://www.w3.org/WAI/media/av/)\r\n*   [WCAG AV Planing](https://www.w3.org/WAI/media/av/planning/)\r\n\r\nCreating more accessible audiovisual content makes the web more equitable for everyone. It allows your audience more opportunities to engage with your content and learn from it. Consider these checklists as a starting point for creating accessible content.\r\n\r\nIf you have questions or want to learn more, please hit us up on Twitter, [@DeepgramDevs](https://twitter.com/deepgramdevs).\r\n\r\n        ";
						}
						async function compiledContent$1S() {
							return load$1S().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1S() {
							return (await import('./chunks/index.cfee7cfb.mjs'));
						}
						function Content$1S(...args) {
							return load$1S().then((m) => m.default(...args));
						}
						Content$1S.isAstroComponentFactory = true;
						function getHeadings$1S() {
							return load$1S().then((m) => m.metadata.headings);
						}
						function getHeaders$1S() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1S().then((m) => m.metadata.headings);
						}

const __vite_glob_0_158 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1S,
  file: file$1S,
  url: url$1S,
  rawContent: rawContent$1S,
  compiledContent: compiledContent$1S,
  default: load$1S,
  Content: Content$1S,
  getHeadings: getHeadings$1S,
  getHeaders: getHeaders$1S
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1R = {"title":"Kevin Lewis Joins the Developer Relations Team","description":"Meet Deepgram's new Developer Advocate and learn his story of getting into tech.","date":"2021-11-16T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1636992625/blog/2021/11/meet-kevin-lewis/belvoir-castle.jpg","authors":["kevin-lewis"],"category":"devlife","tags":["team"],"seo":{"title":"Kevin Lewis Joins the Developer Relations Team","description":"Meet Deepgram's new Developer Advocate and learn his story of getting into tech."},"shorturls":{"share":"https://dpgr.am/f4d8f65","twitter":"https://dpgr.am/5660461","linkedin":"https://dpgr.am/3661792","reddit":"https://dpgr.am/32d7dca","facebook":"https://dpgr.am/867a210"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453814/blog/meet-kevin-lewis/ograph.png"}};
						const file$1R = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/meet-kevin-lewis/index.md";
						const url$1R = undefined;
						function rawContent$1R() {
							return "\r\nI decided I wanted to be a developer after attending my first hackathon (a creative programming competition). I definitely didn't know how to code, but a friend of mine insisted it would be worthwhile, and that was an understatement if I'd ever heard one. While I couldn't meaningfully contribute, my team had the most welcoming and patient folks on it, and I vowed to spend the rest of the summer teaching myself to build websites.\r\n\r\nThat was just over ten years ago, and my journey from that point on included attending lots more hackathons, volunteering to help, and eventually working for the company that put my first event on. I was supported graciously and given the chance to learn new skills, eventually putting my experience of community, events, and development together with my first Developer Advocacy role in 2014.\r\n\r\nThe common thread throughout my career has been the desire to support other early-career developers in learning both technical skills and more foundational core skills which we all need for a happy, healthy work life and yet are never taught. When I'm not supporting developers in discovering, understanding, and building with Deepgram's Speech Recognition APIs, I run [my event series focused on these core skills](https://yougotthis.io).\r\n\r\nWhat else? [I like boardgames](https://boardgamegeek.com/collection/user/phazonoverload). I ended up going to university to do a degree in [Creative Computing](https://www.gold.ac.uk/ug/bsc-creative-computing/). I live in Lincolnshire, UK, though I grew up and lived in London for most of my life. The image accompanying this post is from [Belvoir Castle](http://www.belvoircastle.com), which is just a couple of miles away from where we live. I have a lovely [daughter called Sage](https://twitter.com/_phzn/status/1420797759989370887) and another on the way.\r\n\r\nWhether it's to do with Deepgram, events, being an early-career dev, or anything else - please feel free to reach out wherever I am online - Twitter is generally best (I'm [@\\_phzn](https://twitter.com/_phzn)).\r\n\r\n        ";
						}
						async function compiledContent$1R() {
							return load$1R().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1R() {
							return (await import('./chunks/index.75394f1f.mjs'));
						}
						function Content$1R(...args) {
							return load$1R().then((m) => m.default(...args));
						}
						Content$1R.isAstroComponentFactory = true;
						function getHeadings$1R() {
							return load$1R().then((m) => m.metadata.headings);
						}
						function getHeaders$1R() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1R().then((m) => m.metadata.headings);
						}

const __vite_glob_0_159 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1R,
  file: file$1R,
  url: url$1R,
  rawContent: rawContent$1R,
  compiledContent: compiledContent$1R,
  default: load$1R,
  Content: Content$1R,
  getHeadings: getHeadings$1R,
  getHeaders: getHeaders$1R
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1Q = {"title":"Sandra Rodgers Joins the Developer Relations Team","description":"Sandra Rodgers Joins the Developer Relations Team","date":"2021-11-12T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1636482805/blog/2021/11/meet-sandra-rodgers/dog-computer-cover.jpg","authors":["sandra-rodgers"],"category":"devlife","tags":["team"],"seo":{"title":"Sandra Rodgers Joins the Developer Relations Team","description":"Sandra Rodgers Joins the Developer Relations Team"},"shorturls":{"share":"https://dpgr.am/0542836","twitter":"https://dpgr.am/11162f2","linkedin":"https://dpgr.am/d5b8118","reddit":"https://dpgr.am/9c5854b","facebook":"https://dpgr.am/9d362da"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453816/blog/meet-sandra-rodgers/ograph.png"}};
						const file$1Q = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/meet-sandra-rodgers/index.md";
						const url$1Q = undefined;
						function rawContent$1Q() {
							return "\r\nHi! I'm Sandra Rodgers, one of Deepgram's newest additions to the Dev-Rel team. I'm joining as a Developer Experience Engineer, so I'll be focused on how to give developers the best experience using our product, documentation, SDKs, and more.\r\n\r\nI guess the most important thing to know about me is that I have two adorable, extremely ill-mannered chihuahuas. One of them will attack your feet if you walk towards me, and the other one will bark at you incessantly if you attempt to do anything at all within range of my husband. I can't figure out if their annoying quirks are due to them being rescues (and having major emotional baggage), having bad genes, or my failure as a dog-momma. Whatever the reason, it's always an interesting time at our place when we have guests over for dinner.\r\n\r\nI became a Front-End developer a few years ago after a ten-plus-year career in education. As a teacher, I primarily worked internationally, and I'm grateful for having gotten to see a lot of the world for the first decade of my professional life. Before Deepgram I worked on the User Experience and Development (UXDev) team at a fintech company here where I live in Plano, Texas. I learned a ton on that team, and I was lucky to be surrounded by some talented developers who all had unique interests in the world of front-end development. We were constantly discussing FE topics and debating different approaches to development, and it got me energized about this kind of work.\r\n\r\nHere's a fun story. Even though development is my second career, I dabbled in building for the web back when the internet was a very different place. The first website I built was sometime around 1997 or 1998. I made it on Netscape, and it was about some British indie band I was really into at the time (I went through a lot of music phases back then). To be honest, sometimes I wonder how I managed to put a website on the internet way back then, and I question if I even really did it. It's definitely not on the internet anymore. But I know for a fact that I did, because as soon as I published it I received an email from someone in Sheffield who proceeded to explain to me that my taste in British music was horrible (that's a nicer adjective than what he used), and that I should feel utter shame and self-loathing. He was definitely right about that, but luckily he turned out to be a cool dude, and he sent me a mix-tape with some great Britpop on it. And we stayed in touch and to this day we are friends. So I can honestly say that I published a website in 1998 (1997?), which I find hard to believe myself but which I know to be true because my friend from Sheffield is my proof.\r\n\r\nIf you want to know more about me, feel free to reach out on [Twitter](https://twitter.com/sandra_rodgers_). I'm super excited to be part of the Deepgram team and I can't wait to help make the dev experience a better one for all those who are part of our community!\r\n\r\n        ";
						}
						async function compiledContent$1Q() {
							return load$1Q().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1Q() {
							return (await import('./chunks/index.c140444a.mjs'));
						}
						function Content$1Q(...args) {
							return load$1Q().then((m) => m.default(...args));
						}
						Content$1Q.isAstroComponentFactory = true;
						function getHeadings$1Q() {
							return load$1Q().then((m) => m.metadata.headings);
						}
						function getHeaders$1Q() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1Q().then((m) => m.metadata.headings);
						}

const __vite_glob_0_160 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1Q,
  file: file$1Q,
  url: url$1Q,
  rawContent: rawContent$1Q,
  compiledContent: compiledContent$1Q,
  default: load$1Q,
  Content: Content$1Q,
  getHeadings: getHeadings$1Q,
  getHeaders: getHeaders$1Q
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1P = {"title":"How Meeting Analysis Platforms Utilize ASR Solutions","description":"Meeting analysis platforms run on automatic speech recognition. Learn more about the benefits of these platforms and how ASR powers them.","date":"2022-08-01T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981428/blog/meeting-analysis-platforms-automatic-speech-recognition-solutions/how-meeting-analysisplatforms-utilize-asr-solution.png","authors":["aimie-ye"],"category":"speech-trends","tags":["deep-learning","meeting-transcription","nlu"],"seo":{"title":"How Meeting Analysis Platforms Utilize ASR Solutions","description":"Meeting analysis platforms run on automatic speech recognition. Learn more about the benefits of these platforms and how ASR powers them."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981428/blog/meeting-analysis-platforms-automatic-speech-recognition-solutions/how-meeting-analysisplatforms-utilize-asr-solution.png"},"shorturls":{"share":"https://dpgr.am/b76de2b","twitter":"https://dpgr.am/d2c3630","linkedin":"https://dpgr.am/4b91ba5","reddit":"https://dpgr.am/c33ebab","facebook":"https://dpgr.am/b194567"}};
						const file$1P = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/meeting-analysis-platforms-automatic-speech-recognition-solutions/index.md";
						const url$1P = undefined;
						function rawContent$1P() {
							return "If you're like most business professionals, you probably spend a good chunk of your day in meetings. But you probably also hate all of the work that comes post-meeting, things like drafting meeting minutes, assigning tasks, and following up with the appropriate people. All of this takes you away from more important work you could be doing. Meeting analysis platforms are here to take on much of this rote work. By utilizing top ASR tools, these platforms can automatically convert meetings into text, so you can get a clear understanding of what transpired during the meeting. Not only will this save you time, but it will also help improve decision-making and communication within your organization.\n\nIn this blog post, we'll look at what meeting analysis platforms are, their benefits for your business, and how they benefit from using state-of-the-art speech-to-text systems like Deepgram. Let's jump in.\n\n## What is a Meeting Analysis Platform?\n\nA meeting analysis platform is a software application that uses automatic speech recognition (ASR) to transcribe meeting minutes into text. This text can then be analyzed and searched, so that you can easily find the information you're looking for. ASR is a technology that has been around for decades, but it's only recently that it's become accurate enough to be used for transcribing audio in dynamic environments like meetings.\n\n## What are the Main ASR Features used by  Meeting Analysis Platforms?\n\nMeeting analysis platforms offer a number of specific features that can support your business to better handle meetings and save time. Let's look at a few of the most important features offered by top meeting analysis platforms.\n\n### Separation of Speakers\n\n[Diarization](https://developers.deepgram.com/documentation/features/diarize/) breaks up a transcription by who said what, similar to a movie script. This makes it easier to know who said what during a meeting. Diarization can also be used to understand how much different people are talking, and ensure that everyone's voices are being heard.\n\n### Topic Detection\n\nTopic detection is exactly what it sounds like-identifying the topic of a meeting or part of a meeting. This can make it easier to go back to meeting transcriptions in the future; instead of trying to figure out what date you talked about what thing, you can simply look through the list of topics and identify the correct meeting that way.\n\n### Summarization\n\nSummarization-which provides a summary of what was said during a meeting-makes it easy to understand what was said, but can also make it possible to come back, months or years in the future, and find specific meeting notes that you might be looking for. It also provides a quick and easy way for anyone who didn't attend to know what was covered in the meeting.\n\n### Search\n\nRelated to the above, having a catalog of meeting transcriptions makes it easy to find specific information, even long after the fact. You can also look for specific product names, issue types, etc., across meetings to try and identify patterns in your organization.\n\n## How are Meeting Analytics Platforms Beneficial for Businesses?\n\nThere are several benefits for your organization that come with the use of these meeting management tools. Let's take a look at the specifics.\n\n### Action Item Lists\n\nUsing topic detection, diarization, and summarization together, some meeting analytics platforms can even create a list of follow-up tasks, and potentially assign them to specific people, just based on the transcript of the meeting. [UpdateAI](https://get.update.ai/), for example, integrates with Zoom to automatically capture action items from virtual meetings.\n\n### Record Decisions\n\nEver leave a meeting and suddenly not remember what was decided about a specific point for discussion? Meetings analysis platforms like [adam.ai](https://adam.ai/) provide, along with their other features, a way to capture what decisions were made so that there's no confusion after the fact.\n\n### Time Savings\n\nYou no longer have to spend hours transcribing meeting minutes yourself. The platform will do it for you, so you can focus on other tasks. Not only does this save you time, but because cutting-edge ASR solutions like Deepgram work in almost real time, you can have transcripts available immediately after meetings end, rather than hours or days later.\n\n### Improved Decision-Making\n\nWith all of the meeting information in one place, it becomes easier to make decisions based on what was discussed. Information that was shared can be revisited and who volunteered what information can be identified for any follow-up questions.\n\n### Better Communication\n\nThese platforms can also improve communication within your organization by making meeting information more accessible to everyone. Plus, with features like topic detection and summarization provided by top ASR solutions, as discussed above, anyone who missed the meeting or wants to check in can easily see what was discussed and if it's worth reading the whole transcript or watching a recording.\n\n### Taking Attendance\n\nRemember who was at which meeting can be challenging. With features like diarization that can identify individual speakers, meeting transcripts can also be used to determine who was present (assuming they participated in the meeting).\n\n### Easier Note-Taking\n\nIf you've ever had to take notes for a meeting while also trying to participate, you know that it can be a challenge. With a meeting analytics platform, though, you're free to focus on the content of the meeting itself, and let the tool take your notes for you-you can revisit everything you might need after the meeting ends.\n\n## How to Choose a Meetings Analytics Platform\n\nThere are many meeting analysis platforms on the market, so how do you choose the right one for your organization? The top three factors to consider are:\n\n1. **Ease of use:** You want a platform that's easy to use, so you don't waste time trying to figure out how to use it.\n2. **Pricing:** The platform should be affordable for your organization and provide good value by offering relevant features.\n3. **Accuracy:** The platform should be able to transcribe meeting minutes with a high degree of accuracy.\n\nAlthough we listed it third here, accuracy is perhaps the most important consideration, as any meeting analysis platform is only going to be as good as the transcripts that it generates. Let's take a look at what automatic speech recognition is, and why speech-to-text tools for meeting analysis platforms are so important when it comes to accuracy.\n\n## Automatic Speech Recognition for Meetings Analysis Platforms\n\nA core functionality of any meeting analysis platform is automatic speech recognition (ASR). Let's look at what ASR is, and why it's important for meeting analysis platforms.\n\n### What is ASR?\n\nASR is a technology that converts speech into text. ASR technology works by taking in an audio file or stream, and output the text of what was said. This transcript can then be used for a variety of purposes by a meeting analysis platform.\n\n### Why is ASR Important for Meeting Analysis Platforms?\n\nASR is important because it enables meeting analysis platforms to automatically transcribe contents of the conversation. Because this is a key component of what meeting analysis platforms do, having a fast, accurate ASR solution behind the scenes is absolutely critical to delivering an excellent user experience. Plus, with the addition of natural language processing and understanding technologies to ASR, these meeting analysis providers  can go even further by, for example, using the accurate transcriptions from an ASR provider to create summaries of key points and identifying tasks for follow up.\n\n<WhitepaperPromo whitepaper=\"latest\"></WhitepaperPromo>\n\n## Benefits of End-to-End Deep Learning ASR Solutions for Meeting Analytics\n\nIf you want the best meeting analytics platform, make sure to choose one that uses [an end-to-end deep learning ASR solution](https://blog.deepgram.com/deep-learning-speech-recognition/) for transcription. This type of speech recognition is the quickest and most accurate speech-to-text solution for meeting analysis platforms that's available today. Why is end-to-end deep learning so important? Because of [transfer learning](https://blog.deepgram.com/transfer-learning-spanish-portuguese/), which makes it easy to take an accurate model and provide it with new data to make it more accurate. For example, [Deepgram offers a use-case model that's been specifically trained on meeting audio](https://deepgram.com/product/use-cases/), making it even more adept at understanding and transcribing meetings.\n\nWith older ASR solutions that don't use end-to-end deep learning, this kind of training simply takes too long for providers to create multiple, use case-specific models. They thus rely on singular, out-of-the-box solutions that provide *okay* accuracy across a range of domains but never hit great accuracy. In addition to use case-specific models, Deepgram also offers the option to tailor a model based on audio you provide, which creates further increases in accuracy. This is especially helpful if your meetings contain a lot of industry-specific jargon or unique terms. By training a model with data that includes these items, you'll be able to have a model that's uniquely able to identify the kind of language used in your meetings.\n\n## Wrapping Up\n\nDeep learning ASR technology is revolutionizing meeting analytics and making it easier than ever to get the information you need from meetings. Just make sure your meeting analysis platform is using state-of-the-art, end-to-end deep learning ASR technology so you can enjoy all the benefits that these tools offer. Deepgram has [a use-case model specifically for meeting transcription](https://deepgram.com/product/use-cases/), providing excellent out-of-the-box accuracy.\n\nIf you'd like to give us a try, you can check out how we stack up to Big Tech with our [ASR Comparison Tool](https://deepgram.com/asr-comparison/). You can also [sign up for Console and $150 in free credits](https://console.deepgram.com/signup) to check out what we offer. Still not sure how to start? [Reach out](https://deepgram.com/contact-us/) and we'll help you explore your use case and see how we can help.";
						}
						async function compiledContent$1P() {
							return load$1P().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1P() {
							return (await import('./chunks/index.eeec98bb.mjs'));
						}
						function Content$1P(...args) {
							return load$1P().then((m) => m.default(...args));
						}
						Content$1P.isAstroComponentFactory = true;
						function getHeadings$1P() {
							return load$1P().then((m) => m.metadata.headings);
						}
						function getHeaders$1P() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1P().then((m) => m.metadata.headings);
						}

const __vite_glob_0_161 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1P,
  file: file$1P,
  url: url$1P,
  rawContent: rawContent$1P,
  compiledContent: compiledContent$1P,
  default: load$1P,
  Content: Content$1P,
  getHeadings: getHeadings$1P,
  getHeaders: getHeaders$1P
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1O = {"title":"Michael Jolley Joins the Developer Relations Team","description":"Welcome Michael Jolley, the bald, bearded, builder to the Deepgram Developer Relations team.","date":"2021-11-19T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1636422499/blog/2021/11/michael-jolley-joins-deepgram/birmingham-city-scape.jpg","authors":["michael-jolley"],"category":"devlife","tags":["team"],"seo":{"title":"Michael Jolley Joins the Developer Relations Team","description":"Welcome Michael Jolley, the bald, bearded, builder to the Deepgram Developer Relations team."},"shorturls":{"share":"https://dpgr.am/059f5a0","twitter":"https://dpgr.am/92c5421","linkedin":"https://dpgr.am/d85cc49","reddit":"https://dpgr.am/8cf3e98","facebook":"https://dpgr.am/08bb3eb"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453818/blog/michael-jolley-joins-deepgram/ograph.png"}};
						const file$1O = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/michael-jolley-joins-deepgram/index.md";
						const url$1O = undefined;
						function rawContent$1O() {
							return "\r\nHello world! I'm Michael Jolley and I'm super excited to be joining Deepgram as\r\nthe Head of Developer Relations. Most of my career was spent building with .NET\r\nand JavaScript, but today I consider myself a polyglot developer and love\r\ncontinually learning new languages, frameworks, and platforms.\r\n\r\n## Where it Began\r\n\r\nI'm sure you've experienced one of those \"Ah-ha!\" moments. They aren't exclusive\r\nto code; maybe you were learning to cook or learning a new language. You finally\r\nhit that point where it \"clicked\" and you felt that rush of pride and\r\naccomplishment.\r\n\r\nMy career started writing custom software for clients ranging from start-ups to\r\nFortune 500 companies. I spent nearly 20 years building and leading\r\nteams on some cool projects. As my career progressed, I moved into\r\nmanagement roles and spent more and more time investing in the team members I\r\nwas leading; helping them discover and learn new skills that would \"level up\"\r\ntheir personal and professional lives.\r\n\r\nLooking back, I gained a lot of positive self-gratification from\r\nseeing clients use the software I wrote to empower their businesses, but that paled\r\nin comparison to the satisfaction of helping others have those \"Ah-ha!\" moments.\r\nSo in 2018, I started moving towards developer advocacy and, in 2019, landed my\r\nfirst developer relations role at Nexmo (Vonage.)\r\n\r\nSo now, here I am... just a Developer Advocate... standing in front of\r\ndevelopers... asking them to love me.\r\n\r\n![Julia Roberts in the movie Notting Hill](https://res.cloudinary.com/deepgram/image/upload/v1636427833/blog/2021/11/michael-jolley-joins-deepgram/justagirl.png)\r\n\r\n## Full Transparency\r\n\r\nOkay, so although this post is dated for November, I technically started with\r\nDeepgram back in April of this year. What have I been doing? In short, laying\r\nfoundation and hiring. We've assembled an amazing team whose focus is on helping\r\ndevelopers, like you, succeed. Whether you're using Deepgram's APIs or not, we\r\nwant you to have those \"Ah-ha!\" moments.\r\n\r\nThat's why, in the coming months, you'll start to see us creating a ton of\r\ncontent, hanging out in communities, hosting hack-a-thons, and doing everything\r\nwe can to help you level up your technical and soft skills. I get seriously\r\nexcited every time I think about some of the things we've got planned. Hope to\r\nsee you all along the journey!\r\n\r\n        ";
						}
						async function compiledContent$1O() {
							return load$1O().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1O() {
							return (await import('./chunks/index.0d5d9eac.mjs'));
						}
						function Content$1O(...args) {
							return load$1O().then((m) => m.default(...args));
						}
						Content$1O.isAstroComponentFactory = true;
						function getHeadings$1O() {
							return load$1O().then((m) => m.metadata.headings);
						}
						function getHeaders$1O() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1O().then((m) => m.metadata.headings);
						}

const __vite_glob_0_162 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1O,
  file: file$1O,
  url: url$1O,
  rawContent: rawContent$1O,
  compiledContent: compiledContent$1O,
  default: load$1O,
  Content: Content$1O,
  getHeadings: getHeadings$1O,
  getHeaders: getHeaders$1O
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1N = {"title":"Multichannel vs Diarization","description":"Compare Deepgram's multichannel and diarization to learn which scenarios each feature works best for you","date":"2021-12-20T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1639597247/blog/2021/12/multichannel-vs-diarization/Multichannel-vs-Diarization%402x.jpg","authors":["sandra-rodgers"],"category":"best-practice","tags":["multichannel","diarization"],"seo":{"title":"Multichannel vs Diarization","description":"Compare Deepgram's multichannel and diarization to learn which scenarios each feature works best for you"},"shorturls":{"share":"https://dpgr.am/8500064","twitter":"https://dpgr.am/c9b475b","linkedin":"https://dpgr.am/0cd82e8","reddit":"https://dpgr.am/43274b0","facebook":"https://dpgr.am/65997a5"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453833/blog/multichannel-vs-diarization/ograph.png"}};
						const file$1N = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/multichannel-vs-diarization/index.md";
						const url$1N = undefined;
						function rawContent$1N() {
							return "\nWe sometimes get questions from developers about Deepgram's [multichannel](https://developers.deepgram.com/documentation/features/multichannel/) and [diarize](https://developers.deepgram.com/documentation/features/diarize/) features. In this post, I will provide some examples of when each feature might come in handy, and I'll try to provide a little more clarity on what each feature does.\n\n## Multichannel\n\n### Introduction to Multichannel Audio\n\nYou may have heard of stereo sound, which is sound produced from two different audio channels, one for left and one for right. When you listen to sound in stereo, it comes across as sounding wider and as having more depth. Stereo sound was a great improvement upon mono sound, which sounded much more shallow. Multichannel audio can be stereo (two channels) or even more than two channels.\n\n![mono vs stereo diagram](https://res.cloudinary.com/deepgram/image/upload/v1639674545/blog/2021/12/multichannel-vs-diarization/mono_stereo.png)\n\nWhen recording people speaking, multichannel can be used to separate different people's voices into individual channels, for example, on a call between a customer and salesperson, or doctor and patient, and so on. This would make it much easier to focus on one speaker when reviewing the audio file, which is why many companies that have a need to review audio conversations and transcripts choose to use multichannel audio for their recordings.\n\nWhile two-speaker, two-channel is most common, there could be situations where one channel has several voices. Perhaps a podcast has two interviewers on one channel and one interviewee on a second channel.\n\nThe point is, multichannel audio divides audio into separate channels, and the audio on those separate channels is distinct.\n\n### Deepgram's Multichannel Feature\n\nBy setting **multichannel=true** in a query parameter via the [API](https://developers.deepgram.com/api-reference/) or an [SDK](https://developers.deepgram.com/sdks-tools/), you are telling Deepgram to transcribe each audio channel independently. If you print `response.results.channels` to the console, you will see a response that comes back as separate channels for each channel from the audio.\n\nHere is an example response for an audio file that has two separate channels:\n\n```js\n\"channels\": [\r\n  {\r\n    alternatives: [\r\n      {\r\n        transcript: \"parker scarves how may i help you\",\r\n        confidence: ...,\r\n        words: ...\r\n      }\r\n    ]\r\n  },\r\n  {\r\n    alternatives: [\r\n      {\r\n        transcript: \"i got a scarf online for my wife\",\r\n        confidence: ...,\r\n        words: ...\r\n      }\r\n    ]\r\n  }\r\n]\n```\n\n### Multichannel Joint Audio\n\nIf you have an audio file that you think is multichannel and you expect it to return two or more Deepgram transcripts that are *different*, but you get a response that shows the separate channels with transcripts that are *the same*, you may be dealing with a special situation of a **joint stereo** audio file.\n\nSometimes, in order to save file space when creating or converting an audio file, multichannel audio will undergo a process that mixes the channels into one main channel. **Deepgram will still identify that there are two channels, but the transcript itself will be identical** (so if there are two speakers, their speaking parts will be combined as one transcript). Here is an example:\n\n```js\n\"channels\": [\r\n  {\r\n    alternatives: [\r\n      {\r\n        transcript:\r\n          'parker scarves how may i help you i got a scarf online for my wife',\r\n        confidence: 0.9453125,\r\n        words: [Array],\r\n      },\r\n    ],\r\n  },\r\n  {\r\n    alternatives: [\r\n      {\r\n        transcript:\r\n          'parker scarves how may i help you i got a scarf online for my wife',\r\n        confidence: 0.9453125,\r\n        words: [Array],\r\n      },\r\n    ],\r\n  },\r\n]\n```\n\nIn this situation, it might be useful to use the **diarization** feature instead of the multichannel feature. In the next section, it should become clear to you why.\n\n## Diarization\n\n### Diarization in Automatic Speech Recognition\n\n\"Diarization\" is a term known to people who work in ASR, but it might be new to you. [Wikipedia](https://en.wikipedia.org/wiki/Speaker_diarisation) describes diarization as \"the process of partitioning an input audio stream into homogeneous segments according to the speaker identity.\" This just means that diarization separates audio by the person speaking, whether they are on a different channel or not.\n\nBased on that description, you can probably see how multichannel and diarization might have some overlap. However, **diarization focuses on giving information about different speakers, while multichannel focuses on identifying different audio channels.**\n\n### Deepgram's Diarization Feature\n\nBy setting **diarize=true** in a query parameter via the [API](https://developers.deepgram.com/api-reference/) or an [SDK](https://developers.deepgram.com/sdks-tools/), you are telling Deepgram that you want to know which word in the transcript was spoken by a unique *human* speaker. If there are two speakers in the conversation, as in the following example, you will see **speaker: 0** and **speaker: 1** to identify the two different people who are talking in the audio file.\n\nNotice that the transcript is combined. **Deepgram's diarization feature does not divide the transcripts into separate transcripts by speaker; it provides a speaker property for each word.**\n\n```js\n[\r\n  {\r\n    alternatives: [\r\n      {\r\n        transcript: \"parker scarves how may i help you i got a scarf online for my wife\",\r\n        confidence: 0.94873047,\r\n        words: [\r\n          {\r\n            word: 'parker',\r\n            start: 1.1770647,\r\n            end: 1.4563681,\r\n            confidence: 0.7792969,\r\n            speaker: 0\r\n          },\r\n          {\r\n            word: 'scarves',\r\n            start: 1.6558706,\r\n            end: 1.8553731,\r\n            confidence: 0.5029297,\r\n            speaker: 0\r\n          },\r\n          {\r\n            word: 'how',\r\n            start: 2.0548756,\r\n            end: 2.174577,\r\n            confidence: 0.99902344,\r\n            speaker: 0\r\n          },\r\n          {\r\n            word: 'may',\r\n            start: 2.174577,\r\n            end: 2.254378,\r\n            confidence: 0.9995117,\r\n            speaker: 0\r\n          },\r\n          {\r\n            word: 'i',\r\n            start: 2.3341792,\r\n            end: 2.4538805,\r\n            confidence: 0.9980469,\r\n            speaker: 0\r\n          },\r\n          {\r\n            word: 'help',\r\n            start: 2.4538805,\r\n            end: 2.733184,\r\n            confidence: 1,\r\n            speaker: 0\r\n          },\r\n          {\r\n            word: 'you',\r\n            start: 2.733184,\r\n            end: 2.892786,\r\n            confidence: 0.9838867,\r\n            speaker: 0\r\n          },\r\n          {\r\n            word: 'i',\r\n            start: 4.089801,\r\n            end: 4.209502,\r\n            confidence: 0.54589844,\r\n            speaker: 1\r\n          },\r\n          {\r\n            word: 'got',\r\n            start: 4.209502,\r\n            end: 4.329204,\r\n            confidence: 0.6279297,\r\n            speaker: 1\r\n          },\r\n          {\r\n            word: 'a',\r\n            start: 4.329204,\r\n            end: 4.6883082,\r\n            confidence: 0.9580078,\r\n            speaker: 1\r\n          },\r\n          {\r\n            word: 'scarf',\r\n            start: 4.6883082,\r\n            end: 5.1883082,\r\n            confidence: 0.9760742,\r\n            speaker: 1\r\n          },\r\n          {\r\n            word: 'online',\r\n            start: 5.2469153,\r\n            end: 5.526219,\r\n            confidence: 0.6933594,\r\n            speaker: 1\r\n          },\r\n          {\r\n            word: 'for',\r\n            start: 5.526219,\r\n            end: 5.6459203,\r\n            confidence: 0.7602539,\r\n            speaker: 1\r\n          },\r\n          {\r\n            word: 'my',\r\n            start: 5.6459203,\r\n            end: 5.8454227,\r\n            confidence: 0.98876953,\r\n            speaker: 1\r\n          },\r\n          {\r\n            word: 'wife',\r\n            start: 5.8454227,\r\n            end: 6.044925,\r\n            confidence: 0.7709961,\r\n            speaker: 1\r\n          },\n```\n\n### Combining Deepgram's Multichannel with Diarization\n\nIf you are thinking of combining the **multichannel** and **diarization** features, it is helpful to really understand how these features work, especially when you are trying to get information about speakers.\n\nFor example, in a transcript that has two different human speakers, each of them on different audio channels, adding **multichannel=true** and **diarize=true** to the query will return the two separate transcripts for each channel, but the speaker property will return **speaker: 0** for both human speakers. This is because there is only one speaker on each distinct audio channel, so each of those human speakers gets assigned as the one speaker, **speaker: 0**.\n\nHowever, in some cases, it might be useful to you to use the **diarize** feature with the **multichannel** feature. If there are several speakers on one audio channel and several speakers on another audio channel, turning on the **diarize** feature might come in handy if you want to identify different speakers who are speaking on each separate channel.\n\nHere are some situations to get you thinking about this. In the following scenarios, would it be useful to use **multichannel** or **diarize** (or both)?\n\n#### Scenario: Two audio channels with the same human speaker on each channel\n\nThe first scenario is a person doing a sound check to see that sound is coming from two different inputs:\n\n```js\ntranscript: \"hello and welcome to the sound test we're starting from the left channel then follows right channel left channel right channel left channel right channel and once again let channel know alright thank you so much listening to me and have a nice day\"\n```\n\nBecause there is only one person talking, but that person is using two different audio input channels, using **multichannel=true** could be useful so that we break up the transcript by separate audio channels:\n\n```js\n\"channels\": [\r\n  {\r\n    alternatives: [\r\n      {\r\n        transcript: \"hello and welcome to sound test we're starting from the left channel and follows left channel left channel and once again let channel know thank you so much for listening to me and have a nice day\",\r\n        confidence: 0.9472656,\r\n        words: [Array]\r\n      }\r\n    ]\r\n  },\r\n  {\r\n    alternatives: [\r\n      {\r\n        transcript: \"hello and welcome to the sound test we're starting from there then follows right channel right channel right channel and once again right channel thank you so much and have a nice day\",\r\n        confidence: 0.9326172,\r\n        words: [Array]\r\n      }\r\n    ]\r\n  }\r\n]\n```\n\nThe **diarize** feature would not be as helpful since it is the same human speaker on both of the input channels.\n\n#### Scenario: Two audio channels with two human speakers (one human speaker on each channel)\n\n```js\n;[\r\n  {\r\n    alternatives: [\r\n      {\r\n        transcript:\r\n          \"thank you for calling marcus flowers hello i'd like to order flowers and i think you have what i'm looking for i'd be happy to take care of your order may i have your name please\",\r\n        confidence: 0.9814453,\r\n        words: [Array],\r\n      },\r\n    ],\r\n  },\r\n]\n```\n\nAgain, it would be useful to use **multichannel** to separate the audio channels since there is an individual human speaker on each channel. And since there is only one human speaker on each channel, using diarize might not be as useful to us since both speakers will be assigned the same value, as **speaker: 0**. Using multichannel gives us:\n\n```js\n[\r\n  {\r\n    alternatives: [\r\n      {\r\n        transcript: \"thank you for calling marcus flowers i'd be happy to take care of your order may i have your name please\",\r\n        confidence: 0.9819336,\r\n        words: [{\r\n            word: 'thank',\r\n            start: 0.94,\r\n            end: 1.06,\r\n            confidence: 0.99658203,\r\n            speaker: 0\r\n          },\r\n          ...\r\n          ]\r\n      }\r\n    ]\r\n  },\r\n  {\r\n    alternatives: [\r\n      {\r\n        transcript: \"hello i'd like to order flowers and i think you have what i'm looking for\",\r\n        confidence: 0.9916992,\r\n        words: [{\r\n            word: 'hello',\r\n            start: 4.0095854,\r\n            end: 4.049482,\r\n            confidence: 0.9897461,\r\n            speaker: 0\r\n          },\r\n          ...\r\n          ]\r\n      }\r\n    ]\r\n  }\r\n]\n```\n\n#### Scenario: One audio channel with two speakers\n\nIn cases when you just have one audio channel, the Deepgram **multichannel** feature probably doesn't provide you with the information you are looking for. But in this scenario, **diarize** could give you some useful information to help you identify the two different human speakers.\n\nI also recommend analyzing the **start** and **end** property in combination with the **speaker** data if you want to find sections of audio where people might be talking over each other, which is something that is very common in transcriptions of natural conversations and discussions.\n\n```js\n[\r\n  {\r\n    alternatives: [\r\n      {\r\n        transcript: \"from npr news this is all things considered i'm robert siegel and i'm michelle norris\",\r\n        confidence: 0.9794922,\r\n        words: [\r\n          {\r\n            word: 'from',\r\n            start: 0.81824785,\r\n            end: 0.8980769,\r\n            confidence: 0.99658203,\r\n            speaker: 0\r\n          },\r\n          {\r\n            word: 'npr',\r\n            start: 1.2573076,\r\n            end: 1.3770512,\r\n            confidence: 0.95947266,\r\n            speaker: 0\r\n          },\r\n          {\r\n            word: 'news',\r\n            start: 1.4967948,\r\n            end: 1.736282,\r\n            confidence: 0.99609375,\r\n            speaker: 0\r\n          },\r\n          {\r\n            word: 'this',\r\n            start: 1.9358547,\r\n            end: 2.0555983,\r\n            confidence: 0.9897461,\r\n            speaker: 0\r\n          },\r\n          {\r\n            word: 'is',\r\n            start: 2.0555983,\r\n            end: 2.2152565,\r\n            confidence: 0.9814453,\r\n            speaker: 0\r\n          },\r\n          {\r\n            word: 'all',\r\n            start: 2.2152565,\r\n            end: 2.414829,\r\n            confidence: 0.9902344,\r\n            speaker: 0\r\n          },\r\n          {\r\n            word: 'things',\r\n            start: 2.414829,\r\n            end: 2.853889,\r\n            confidence: 0.9941406,\r\n            speaker: 0\r\n          },\r\n          {\r\n            word: 'considered',\r\n            start: 2.853889,\r\n            end: 3.2929487,\r\n            confidence: 0.9785156,\r\n            speaker: 0\r\n          },\r\n          {\r\n            word: \"i'm\",\r\n            start: 3.452607,\r\n            end: 3.532436,\r\n            confidence: 0.9863281,\r\n            speaker: 0\r\n          },\r\n          {\r\n            word: 'robert',\r\n            start: 3.6521795,\r\n            end: 3.8916667,\r\n            confidence: 0.98876953,\r\n            speaker: 0\r\n          },\r\n          {\r\n            word: 'siegel',\r\n            start: 4.01141,\r\n            end: 4.210983,\r\n            confidence: 0.49243164,\r\n            speaker: 0\r\n          },\r\n          {\r\n            word: 'and',\r\n            start: 4.370641,\r\n            end: 4.45047,\r\n            confidence: 0.9794922,\r\n            speaker: 1\r\n          },\r\n          {\r\n            word: \"i'm\",\r\n            start: 4.570214,\r\n            end: 5.049188,\r\n            confidence: 0.4260254,\r\n            speaker: 1\r\n          },\r\n          {\r\n            word: 'michelle',\r\n            start: 5.049188,\r\n            end: 5.208846,\r\n            confidence: 0.69384766,\r\n            speaker: 1\r\n          },\r\n          {\r\n            word: 'norris',\r\n            start: 5.32859,\r\n            end: 5.82859,\r\n            confidence: 0.9379883,\r\n            speaker: 1\r\n          },\n```\n\n#### Scenario: Two channels four speakers, three on one channel, one on the other channel\n\nFor this scenario (which I'm not going to include an example of due to the difficulty reading the long response), you could benefit from using both features. The **multichannel** feature would separate the transcripts by the audio input channels, and the **diarize** feature would help you identify the three different human speakers on the first channel.\n\n## Conclusion\n\nI hope this helped to clarify how Deepgram's multichannel feature and diarize feature can be used to your advantage. Feel free to reach out to me on [Twitter](https://twitter.com/sandra_rodgers_) with any questions!\n\n        ";
						}
						async function compiledContent$1N() {
							return load$1N().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1N() {
							return (await import('./chunks/index.d303ca6c.mjs'));
						}
						function Content$1N(...args) {
							return load$1N().then((m) => m.default(...args));
						}
						Content$1N.isAstroComponentFactory = true;
						function getHeadings$1N() {
							return load$1N().then((m) => m.metadata.headings);
						}
						function getHeaders$1N() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1N().then((m) => m.metadata.headings);
						}

const __vite_glob_0_163 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1N,
  file: file$1N,
  url: url$1N,
  rawContent: rawContent$1N,
  compiledContent: compiledContent$1N,
  default: load$1N,
  Content: Content$1N,
  getHeadings: getHeadings$1N,
  getHeaders: getHeaders$1N
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1M = {"title":"Regular ASR will Never Create a Humanized Bot Experience","description":"This Deepgram X Bitext partnership will drastically improve Natural Language Understanding (NLU) for Voicebots and allow more human-like conversations.","date":"2021-04-06T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981366/blog/natural-language-understanding-nlu-for-audio-requires-a-highly-accurate-and-fast-speech-to-text-foundation/regular-asr-never-create-humanized-bot%402x.jpg","authors":["keith-lam"],"category":"ai-and-engineering","tags":["deep-learning","nlu"],"seo":{"title":"Regular ASR will Never Create a Humanized Bot Experience","description":"This Deepgram X Bitext partnership will drastically improve Natural Language Understanding (NLU) for Voicebots and allow more human-like conversations."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981366/blog/natural-language-understanding-nlu-for-audio-requires-a-highly-accurate-and-fast-speech-to-text-foundation/regular-asr-never-create-humanized-bot%402x.jpg"},"shorturls":{"share":"https://dpgr.am/b5115a6","twitter":"https://dpgr.am/4a4a966","linkedin":"https://dpgr.am/72f5eb1","reddit":"https://dpgr.am/56e414d","facebook":"https://dpgr.am/72b7b4a"}};
						const file$1M = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/natural-language-understanding-nlu-for-audio-requires-a-highly-accurate-and-fast-speech-to-text-foundation/index.md";
						const url$1M = undefined;
						function rawContent$1M() {
							return "### **Deepgram X Bitext partnership**\n\nAs companies consider adding voicebots to their customer service, they are seeing the same \"Achilles' heel\" with NLU voicebots as experienced with [chatbots](https://blog.bitext.com/nlu-chatbot-evaluation-3-common-errors-and-5-key-steps).  To understand the root of the problem, we should first look at the ideal process:\n\n1. Customer provides information by voice\n2. Automatic Speech Recognition (ASR) must quickly and accurately process that audio into data the Voicebot/Chatbot can use without lag time to the customer\n3. Voicebot/Chatbot needs to \"understand\" that information\n4. It must use that understanding to determine intent\n5. It uses that intent to either route the customer to a human agent or answer the request with a knowledge base AI\n\n\n\n  ![](https://res.cloudinary.com/deepgram/image/upload/v1661976840/blog/natural-language-understanding-nlu-for-audio-requires-a-highly-accurate-and-fast-speech-to-text-foundation/ideal-chatbot-process%402x.png)   \n\nWith a slow and inaccurate ASR, that \"understanding\" phase is compromised, creating issues for the remainder of the process:\n\n1. Responding with the wrong intent\n2. Transferring to a human agent due to lack of confidence, when it should have understood the intent from the beginning\n3. Responding when it doesn't have to, instead of passing it on to an agent\n4. For voicebots, not understanding the customer and asking, \"Sorry, I did not get that, can you repeat.\"\n\n  ![](https://res.cloudinary.com/deepgram/image/upload/v1661976841/blog/natural-language-understanding-nlu-for-audio-requires-a-highly-accurate-and-fast-speech-to-text-foundation/slow-inaccurate-chatbot-process%402x.png)   \n\nDeepgram is built for Conversational Al and voicebots with an End-to-End Deep Learning approach to automatic speech recognition (ASR). This approach allows you to solve for real-time speed at &lt;300 millisecond lag and obtain 90%+ trained accuracy. With higher accuracy data from the ASR, you can then dig into optimizing your NLU voicebot with Bitext. Get a snapshot of your NLU voicebot performance and find the root cause of incorrect responses or mis-routing. With the **Bitext and Deepgram partnership**, companies can analyze and improve their entire NLU voicebot platform to either correct issues or identify weak points before product release. Then, we can help create audio and voicebot training data to improve your models and track this improvement for further optimization and quality control. Contact us today or request a demo: [Bitext](https://info.bitext.com/request-a-demo-bitext) [Deepgram](https://www.deepgram.com/contact-us).";
						}
						async function compiledContent$1M() {
							return load$1M().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1M() {
							return (await import('./chunks/index.2e7fdc0f.mjs'));
						}
						function Content$1M(...args) {
							return load$1M().then((m) => m.default(...args));
						}
						Content$1M.isAstroComponentFactory = true;
						function getHeadings$1M() {
							return load$1M().then((m) => m.metadata.headings);
						}
						function getHeaders$1M() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1M().then((m) => m.metadata.headings);
						}

const __vite_glob_0_164 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1M,
  file: file$1M,
  url: url$1M,
  rawContent: rawContent$1M,
  compiledContent: compiledContent$1M,
  default: load$1M,
  Content: Content$1M,
  getHeadings: getHeadings$1M,
  getHeaders: getHeaders$1M
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1L = {"title":"New Releases - Five New Languages and Three New Use Case Speech Models","description":"Deepgram announces five new languages (French, French Canadian, German, Hindi and Spanish) and three new use case (Earnings Calls, Voicemail, and Video) speech models","date":"2021-11-18T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981386/blog/new-releases-five-new-languages-and-three-new-use-case-speech-models/blog-updated-models-use-cases-11-2021-thumb-554x22.png","authors":["natalie-rutgers"],"category":"product-news","tags":["language","speech-models"],"seo":{"title":"New Releases - Five New Languages and Three New Use Case Speech Models","description":"Deepgram announces five new languages (French, French Canadian, German, Hindi and Spanish) and three new use case (Earnings Calls, Voicemail, and Video) speech models"},"shorturls":{"share":"https://dpgr.am/164df95","twitter":"https://dpgr.am/7ad57df","linkedin":"https://dpgr.am/5e47166","reddit":"https://dpgr.am/fd2dffb","facebook":"https://dpgr.am/98b4b27"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981386/blog/new-releases-five-new-languages-and-three-new-use-case-speech-models/blog-updated-models-use-cases-11-2021-thumb-554x22.png"}};
						const file$1L = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/new-releases-five-new-languages-and-three-new-use-case-speech-models/index.md";
						const url$1L = undefined;
						function rawContent$1L() {
							return "\r\nDeepgram is excited to continue executing the vision of having everyone heard and understood through speech recognition.  To that end, we are announcing five new languages and three new use case speech models.  In addition, we have improved our Hindi and Spanish language models and our Conversational AI and Meetings models.  These improvements mean better accuracy for your transcription applications. Through transfer learning backed by our End-to-End Deep Learning architecture and the process of creating conversational audio datasets for training, we have been able to achieve very high accuracy on our new speech models.  Our new models are trained on real conversations, not Libraspeech type datasets of people reading audiobooks because no one talks like they are reading audiobooks.  This allows our speech models to better transcribe jargon, accents, dialects, code-switching (switching from Hindi to English words), terminology (ROI, EBITDA), and alphanumerics.  This means our models provide better real-world transcription accuracy than standard language or general one-size-fits-all speech models. **Improved Production Language Models**\r\n\r\n*   Hindi (improved)\r\n*   Spanish (improved)\r\n\r\n**New Language Models**\r\n\r\n*   French\r\n*   French Canadian\r\n*   German\r\n\r\nOur French and German language models are **FREE** for the next 30 days of use to collect more audio for training and improvement. [Sign up here](https://console.deepgram.com/) to get early access. **New and Improved Use Case Models**\r\n\r\n*   Conversational AI (improved)\r\n*   Earnings Calls\r\n*   Meetings (improved)\r\n*   Voicemail\r\n*   Video\r\n\r\nBecause of our ability to tailor these models for your use case, dialect, noise, accents, and terminology within weeks, consider these speech models as a foundation to even better accuracy for your specific use case.   Our internal testing with [Google](https://deepgram.com/compare-google-stt-alternatives/) and [Amazon's](https://deepgram.com/compare-amazon-transcribe-api-alternatives/) speech-to-text solutions show these new and improved models are better out of the gate. Test us out with our self-service [Console](https://console.deepgram.com) or let us perform a [guided test](https://deepgram.com/contact-us) and train a tailored model for your specific use case.\r\n";
						}
						async function compiledContent$1L() {
							return load$1L().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1L() {
							return (await import('./chunks/index.112b5d50.mjs'));
						}
						function Content$1L(...args) {
							return load$1L().then((m) => m.default(...args));
						}
						Content$1L.isAstroComponentFactory = true;
						function getHeadings$1L() {
							return load$1L().then((m) => m.metadata.headings);
						}
						function getHeaders$1L() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1L().then((m) => m.metadata.headings);
						}

const __vite_glob_0_165 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1L,
  file: file$1L,
  url: url$1L,
  rawContent: rawContent$1L,
  compiledContent: compiledContent$1L,
  default: load$1L,
  Content: Content$1L,
  getHeadings: getHeadings$1L,
  getHeaders: getHeaders$1L
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1K = {"title":"New Spanish and Turkish Language Models and Updated General Models","description":"We have general released a new Spanish and Turkish speech model, improve all our general models and are limited releasing a Hindi and Conversational AI model","date":"2021-05-10T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981370/blog/new-spanish-and-turkish-language-models-and-updated-general-models/new-spanish-turkish-models%402x.jpg","authors":["keith-lam"],"category":"product-news","tags":["language"],"seo":{"title":"New Spanish and Turkish Language Models and Updated General Models","description":"We have general released a new Spanish and Turkish speech model, improve all our general models and are limited releasing a Hindi and Conversational AI model"},"shorturls":{"share":"https://dpgr.am/6ac16c4","twitter":"https://dpgr.am/7606b8e","linkedin":"https://dpgr.am/10c5121","reddit":"https://dpgr.am/d22ac7d","facebook":"https://dpgr.am/8068d70"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981370/blog/new-spanish-and-turkish-language-models-and-updated-general-models/new-spanish-turkish-models%402x.jpg"}};
						const file$1K = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/new-spanish-and-turkish-language-models-and-updated-general-models/index.md";
						const url$1K = undefined;
						function rawContent$1K() {
							return "\r\nOur Production Team and Deepgram Labs are releasing new languages and speech models.\r\n\r\n#### New General Release Models:\r\n\r\n*   [Spanish Language Model](https://deepgram.com/changelog/improved-spanish-support/)\r\n*   [Turkish Language Model](https://deepgram.com/changelog/improved-turkish-support/)\r\n\r\n#### Improved Speech Models:\r\n\r\n*   [General Model](https://deepgram.com/changelog/updated-general-model-english-us/)\r\n*   [Meeting Model](https://deepgram.com/changelog/updated-meeting-model-english-us/)\r\n*   [Phonecall Model](https://deepgram.com/changelog/updated-phonecall-model-english-us/)\r\n\r\n#### Deepgram Labs Models:\r\n\r\n*   [Hindi Language Model](https://deepgram.com/changelog/hindi-support/)\r\n*   Conversational AI Model\r\n\r\nOur Deepgram Labs Models are limited release models that ready to be trained and tailored for your specific use case.  As we obtain additional customer audio data, these models will be moved to General Release. [Contact us](https://www.deepgram.com/contact-us) if you are interested in using these Deepgram Labs models.\r\n";
						}
						async function compiledContent$1K() {
							return load$1K().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1K() {
							return (await import('./chunks/index.193fc9f1.mjs'));
						}
						function Content$1K(...args) {
							return load$1K().then((m) => m.default(...args));
						}
						Content$1K.isAstroComponentFactory = true;
						function getHeadings$1K() {
							return load$1K().then((m) => m.metadata.headings);
						}
						function getHeaders$1K() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1K().then((m) => m.metadata.headings);
						}

const __vite_glob_0_166 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1K,
  file: file$1K,
  url: url$1K,
  rawContent: rawContent$1K,
  compiledContent: compiledContent$1K,
  default: load$1K,
  Content: Content$1K,
  getHeadings: getHeadings$1K,
  getHeaders: getHeaders$1K
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1J = {"title":"New Tech Lets Journalists Find Damning Soundbites","description":"Read about how audio search can support journalists.","date":"2016-10-10T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1662069399/blog/new-tech-lets-journalists-find-damning-soundbites/placeholder-post-image%402x.jpg","authors":["scott-stephenson"],"category":"speech-trends","tags":["deep-learning"],"seo":{"title":"New Tech Lets Journalists Find Damning Soundbites","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1662069399/blog/new-tech-lets-journalists-find-damning-soundbites/placeholder-post-image%402x.jpg"},"shorturls":{"share":"https://dpgr.am/e5a9e23","twitter":"https://dpgr.am/4cfc308","linkedin":"https://dpgr.am/5850ede","reddit":"https://dpgr.am/3ebbba5","facebook":"https://dpgr.am/6d8b661"}};
						const file$1J = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/new-tech-lets-journalists-find-damning-soundbites/index.md";
						const url$1J = undefined;
						function rawContent$1J() {
							return "Today, we're one month away from election day, and the race for the presidency is closing in on the home stretch. Newsrooms around the country are buzzing with activity: interviews, fact checking, reporting and, of course, combing through huge quantities of videos and recordings of both candidates, hunting for that juicy soundbite that might change public opinion and the course of the election.\n\nHere's an example. I uploaded the entirety of Donald Trump's speech at the Republican National Convention from July 2016 to Deepgram. Now you can search through it just like you would with text.\n\nSearch for 'jobs', then listen.\n\nTry also searching for:\n\n* taxes\n* women\n* security\n* tremendous\n* really\n* \"I love\"\n* Hillary\n* destruction\n* white house\n* \"I'm with her\"\n\nYou get taken to the exact time when those terms were mentioned during his speech. Click the red timeline markers to jump through the video to hear each audio clip. This is really fun but also shows ***the power of speech search***.\n\n# The situation\n\nThe *Washington Post*'s [David A. Fahrenthold brought to light one such bombshell of a clip](https://www.washingtonpost.com/politics/trump-recorded-having-extremely-lewd-conversation-about-women-in-2005/2016/10/07/3b9ce776-8cb4-11e6-bf8a-3d26847eeed4_story.html), in which Republican candidate Donald Trump was recorded saying a number of things that most would consider very offensive. It's unclear whether the clip, which was recorded in 2005, will make a significant impact on this election. After all, at the time of writing, this news only broke 24 hours ago.\n\nBut there is one thing that *is* clear: Fahrenthold is one heck of an investigative journalist. Regardless of which side of the political aisle you sit on, we can all agree that the world would be a better place if there were more journalists with the wherewithal to slog through reams of documents and countless hours of audio and video.\n\n# Text Search is Pretty Easy\n\nIf processing this information wasn't so labor intensive, it's likely that there *would* be more Fahrentholds out there. Document search is relatively easy today. If the documents are already digital, searching for keywords is trivially easy. If they're on paper, then it's simply a matter of scanning in the documents, using optical character recognition to transform images of text into text data and then performing keyword searches.\n\nGoogle and Ctrl+F have been around for a while\n\nText search only gets challenging when dealing with a huge corpus of documents with different types of data embedded in them (like charts and tables). But still, it's a tractable problem.\n\nFor example, when the [Panama Papers](https://en.wikipedia.org/wiki/Panama_Papers) leaked last year, it took an international team of 400 journalists from 100 news organizations to sift through the 4.8 million emails, three million database entries, two million PDFs, one million images and 320,000 text documents that an unknown party exfiltrated from Mossack Fonseca. Using [Apache Solr](http://lucene.apache.org/solr/) and [Apache Tika](https://tika.apache.org/) to index and search through the data, journalists were able to map the network of shady dealings and tax sheltering schemes of some of the world's most powerful people and corporations.\n\nIt may have taken hundreds of people around the world almost a year to process through all the records, but it was doable.\n\n# Speech Search Was Hard, Until Now\n\nSearching through recordings is really difficult. In terms of workflow, usually the raw audio is transcribed into text, which is then fed into a search tool. If you transcribe using human transcription, it's too time consuming and expensive. If you try to do it with automatic speech-to-text then search accuracy is the problem. Deepgram fixed that.\n\nDeepgram finds speech with A.I.\n\nDeepgram is an artificial intelligence tool that makes searching for keywords in speeches, private conversations and phone calls faster, cheaper and easier than the old way of doing things. Deepgram indexes audio files in more than half the time of a human transcriber, and **costs only 75¢ per hour of audio**. Compare that to the 75¢ *per minute* charged by most human transcription services-it's a pretty good deal.\n\nAnd, Deepgram takes out the extra step of feeding transcriptions into a search platform. You can search for keywords directly within the audio recording, and jump straight to the times the keyword was mentioned in the audio. This lets reporters listen for intonation and inflection, which are totally lost during the transcription process. Deepgram makes finding the timestamp of those sound bites a breeze (I'm looking at you radio and podcast producers).\n\n# Search All Speech With Deepgram\n\nDeepgram is a powerful \"speech search engine\" that makes the process of identifying key words and phrases in speeches and other spoken-word audio recordings fast and easy.\n\nDeepgram is great for finding bits and pieces of single speeches, and its equally great at searching through a library of audio data. Let's say you came into possession of audio recordings of Hillary Clinton's speeches to big Wall Street firms like Goldman Sachs. You could load them all into Deepgram and use keyword searches to suss out the unifying themes of her speeches.\n\nA more practical use case could involve loading all of a candidate's speeches, television appearances, etc. into Deepgram and search for hot-button issues. You could identify the moments when candidates change their positions on a certain topic, or correct them on the fly when they deny that they'd said a particular thing.\n\n# You Need This\n\nDeepgram's speech search engine won't replace researchers and investigative reporters, but it will make them faster and more effective. Now you can search through dozens or thousands of hours of audio recordings quickly, easily, and without the hassle and expense of transcription.\n\nIf you're a journalist or news organization and want to see how Deepgram can fit into your workflow, [reach out and say hello](https://deepgram.com/contact-us/).\n\n- - -\n\nWith contribution from [Jason D. Rowley](https://twitter.com/Jason_Rowley). Image: Gage Skidmore from [Flickr](https://www.flickr.com/photos/gageskidmore/8567825104).";
						}
						async function compiledContent$1J() {
							return load$1J().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1J() {
							return (await import('./chunks/index.d4579f17.mjs'));
						}
						function Content$1J(...args) {
							return load$1J().then((m) => m.default(...args));
						}
						Content$1J.isAstroComponentFactory = true;
						function getHeadings$1J() {
							return load$1J().then((m) => m.metadata.headings);
						}
						function getHeaders$1J() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1J().then((m) => m.metadata.headings);
						}

const __vite_glob_0_167 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1J,
  file: file$1J,
  url: url$1J,
  rawContent: rawContent$1J,
  compiledContent: compiledContent$1J,
  default: load$1J,
  Content: Content$1J,
  getHeadings: getHeadings$1J,
  getHeaders: getHeaders$1J
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1I = {"title":"NLP on the Edge: Voice, AI and Hardware - Robert Daigle and Andi Huels, Lenovo - Project Voice X","description":"NLP on the Edge: Voice, AI and Hardware, presented by Robert Daigle and Andi Huels of Lenovo, presented on day one of Project Voice X. ","date":"2021-12-09T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981399/blog/nlp-on-the-edge-voice-ai-and-hardware-robert-daigle-and-andi-huels-lenovo-project-voice-x/proj-voice-x-session-robert-daigle-andi-huels-blog.png","authors":["claudia-ring"],"category":"speech-trends","tags":["hardware","nlp","project-voice-x"],"seo":{"title":"NLP on the Edge: Voice, AI and Hardware - Robert Daigle and Andi Huels, Lenovo - Project Voice X","description":"NLP on the Edge: Voice, AI and Hardware, presented by Robert Daigle and Andi Huels of Lenovo, presented on day one of Project Voice X. "},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981399/blog/nlp-on-the-edge-voice-ai-and-hardware-robert-daigle-and-andi-huels-lenovo-project-voice-x/proj-voice-x-session-robert-daigle-andi-huels-blog.png"},"shorturls":{"share":"https://dpgr.am/1057851","twitter":"https://dpgr.am/45f253e","linkedin":"https://dpgr.am/9d6c1b9","reddit":"https://dpgr.am/93fb8c9","facebook":"https://dpgr.am/6e81b4c"}};
						const file$1I = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/nlp-on-the-edge-voice-ai-and-hardware-robert-daigle-and-andi-huels-lenovo-project-voice-x/index.md";
						const url$1I = undefined;
						function rawContent$1I() {
							return "*This is the transcript for “NLP on the Edge: Voice, AI and Hardware,” presented by Robert Daigle and Andi Huels of Lenovo, presented on day one of Project Voice X. Unfortunately the first third of this audio was corrupted so we could not transcribe that portion.*\n\n*The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was **94% accurate.**  Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram’s API.  If you want to see if Deepgram is right for your use case, [contact us](https://deepgram.com/contact-us/).*\n\n\\[Andi Huels:] Does anybody wanna take a guess at this?\n\n\\[Robert Daigle:] They just…\n\n\\[Andi Huels:] Any other guesses?\n\n\\[Robert Daigle:] We’re not afraid to take risks.\n\n\\[Andi Huels:] Excellent. I… excellent answers. But the one that I noted last week, you know, with a with a giant QSR and like, Andy, we wanna bring you in. We want Lenovo to support these four POCs we have going on. So my point to you is, it’s important to leverage your technology partners. If you’re… you know, whether you’re the… whether you’re Home Depot or whether you’re Deepgram or who… an ISV, work with us. Right? Because we can optimize that. We can get you better pricing from NVIDIA and from Intel. We… and we can get you samples. We can run your POCs for free in our AI Centers of Excellence.\n\nWe can work directly with those Fortune five hundred executives, bring our executives on your behalf to talk to them. There’s a a ton of advantages to to scale with us. And speaking of scale, I mean, who hasn’t had a supply chain issue over the last year and a half? Whether it’s toilet paper or chips or whatever it is, it’s great to work with a company like Lenovo because we were… we’re Gartner top twenty five in supply chain. So we anticipate those those type of issues. And as an example, Kroger had a four year rollout, and there was no stopping the rollout to all of their stores for their asset protection solution that we enabled. And we made sure that even though there was chips issues and there were, you know, all kinds of shortages, that they didn’t miss a single store. So that’s an important reason, you know, to work with a big player like like Lenovo. I mentioned earlier we designed those… these devices around the needs of the customer. We make sure that we’re talking to the store managers and not just shipping them to them with your software in a… our in our in our infrastructure. We make sure that it’s designed around the needs of the customer.\n\n\\[Robert Daigle:] And one of the things I’ll say here is that in every in every deployment that we’ve done, it’s typically an ecosystem play. It’s not one provider. If you look at some of the data points out there, it’s typically four to five providers that are engaged in one deployment, whether it’s a GSI —\n\n\\[Andi Huels:] Mhmm.\n\n\\[Robert Daigle:] — whether it’s expertise from the customer, whether it’s a hardware providers, software vendors, and the channel partners that are actually in the last mile. So it’s important to to to realize that it’s it’s gotta be a complete ecosystem. When you get into these larger deployments, there’s not just one vendor that’s doing everything. Right? It’s it’s an ecosystem play.\n\n\\[Andi Huels:] Absolutely. So this is a use case that I alluded to a little earlier at the QSR, and you can see what we’re doing related to NLP and voice to text. So when the car arrives at the drive-through, it detects the car, and then it… here’s this… the customer speak, you know, whatever the order is. I don’t wanna give away who the customer is or the end user, I should say. So they speak to the voice agent, and it puts their order up on the screen, whether you’re talk… you know, speaking in Swahili or Dutch or Hillbilly Georgia. Right? It recognizes what you wanna order, so you don’t have that, you know, drive-through attendants squawking your order back at you and it’s wrong. You have to go back and forth.\n\nSo what are we doing? We talked about this earlier, reducing friction for the customer. And so then it decodes that, it displays it on on the screen, and then the or… the customer can see their order and and just simply say, yes. That’s correct. And then sends it to the POS system within the restaurant. So this is gonna really speed up, I think, accuracy as, you know, as well as just speed up the experience through the drive-through. And just this recent close is really already… my my phone is blowing up off the hook for other companies that really wanted to do this type of technology. So that’s another reason. If you have a similar technology, like, maybe your technology adds something else to this process, we have the the connections to help you get heard.\n\n\\[Robert Daigle:] Yeah. And and that that goes back full circle to… I think, the beginning of our presentation, we talked about AI becoming pervasive across businesses and the way that it has transformed our personal lives.\n\n> Very soon when you go to a fast-food restaurant, when you go through the drive-through, you may be talking to a voice assistant, conversational AI assistant when you’re placing your order to improve the order accuracy and the speed at which they’re moving customers through their queue. So there’s a lot of benefits.\n\nAnd then, of course, when you’re putting that infrastructure in place, the next thing is is what next? What’s the next thing we can do with the infrastructure we’re putting in place? So, you know, it’s it’s it’s really exciting to see this transformation that we’ve seen over the past eighteen months. I know we’re we’re running short on time, so one last thing before we close out is, you know, we talked about a lot of the things that we’re doing from our our product teams from Lenovo to deliver new AI solutions with our partners. So that’s more of what we do for our customers, how we support our customers, and then we also support internal… our internal customers.\n\nOne of our recent acquisitions was Motorola, and and we actually support their data science team from our innovation centers where they’re training systems to do data mining on device device data. We have a lot of our support systems that we’re doing. We’re actually moving over to conversational AI assistance to improve both external support, so how we support our customers in that in that tier one support experience, and then also on the internal support. So Lenovo is a sixty three thousand employee company, and so that’s a small city, right, in North Carolina. That’s a that’s a decent-sized city from where I’m from in North Carolina. And so that’s a lot of people that we have to support inside of Lenovo, from IT issues and beyond as we actually use, you know, these AI systems internally as well. It’s not just something that we’re talking about with our customers, but it’s also we have to, for lack of better terms, eat our own dog food and and actually use some of these systems internally. And it’s a great way to vet the the the systems that we take out to our customers.\n\n\\[Andi Huels:] Thank you so much for your for your mindshare today. It’s been a privilege to be with so many innovators and visionaries in the audience, and I look forward to speaking with you additional, like, events tonight and throughout this this week. And please come to us with your questions and your opportunities.\n\n\\[Robert Daigle:] Yeah. We look forward to meeting with all of you. Personally, I’m just excited to actually be seeing people that are not on a Zoom or a Skype call and be able to actually spend some time with all of you, and what a great event. Thank you for hosting us and having us here and inviting Lenovo.\n\n\\[Andi Huels:] Thank you, Robert.\n\n\\[Robert Daigle:] Thanks, everyone.";
						}
						async function compiledContent$1I() {
							return load$1I().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1I() {
							return (await import('./chunks/index.da8c71f6.mjs'));
						}
						function Content$1I(...args) {
							return load$1I().then((m) => m.default(...args));
						}
						Content$1I.isAstroComponentFactory = true;
						function getHeadings$1I() {
							return load$1I().then((m) => m.metadata.headings);
						}
						function getHeaders$1I() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1I().then((m) => m.metadata.headings);
						}

const __vite_glob_0_168 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1I,
  file: file$1I,
  url: url$1I,
  rawContent: rawContent$1I,
  compiledContent: compiledContent$1I,
  default: load$1I,
  Content: Content$1I,
  getHeadings: getHeadings$1I,
  getHeaders: getHeaders$1I
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1H = {"title":"Now Available: Deepgram Speech Recognition for Twilio Programmable Voice","description":"Learn how you can use Deepgram's speech recognition with the Twilio programmable voice.","date":"2022-09-28T14:48:20.010Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1664378372/2209-dg-twilio-announcement-featured-1200x630_lakqpx.png","authors":["katie-byrne"],"category":"product-news","tags":["integrations","twilio"],"shorturls":{"share":"https://dpgr.am/e5cf486","twitter":"https://dpgr.am/eb76a92","linkedin":"https://dpgr.am/a6ddcb5","reddit":"https://dpgr.am/bf9791e","facebook":"https://dpgr.am/dc09938"}};
						const file$1H = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/now-available-deepgram-speech-recognition-for-twilio-programmable-voice-api/index.md";
						const url$1H = undefined;
						function rawContent$1H() {
							return "Developers love Twilio, and developers love Deepgram. Now they'll love that it’s even easier to unlock the value of post-call recordings through the new Deepgram AI Speech Recognition Add-On in the Twilio Marketplace.\n\nWhat can developers do with post-call transcription?\n\n* Add sentiment analysis to every recorded call\n* Identify topic trends by contact center location\n* Analyze discussion points from recruiting and/or sales calls for coaching purposes\n* And much more! Check out some examples [Built with Deepgram.](http://deepgram.com/built-with-deepgram/) \n\n## Post-Call Transcription Has Extended beyond Contact Center into the Enterprise\n\nFor years contact centers have recorded their calls for training and quality assurance purposes, however it wasn’t until the recent past that it was not only possible but economically viable to transcribe 100% of those calls. Call recording is now extending beyond the contact center and support to other departments within an organization such as sales, marketing, and recruiting. Regardless of department, each voice initiative is underway with the goal of either increasing their employee productivity, reducing cost or finding new revenue streams to help them gain a competitive advantage. Call recordings, more than many other touch points with our customers, are rich with information. Beyond understanding correctly what words were said, voice is rich with additional metadata such as the sentiment about the topic at hand.\n\n## Introducing Deepgram AI Speech Recognition for Twilio\n\nDeepgram has invested in our partnership and integration with Twilio to make it even easier for developers to use the services to build amazing experiences without the hassle of connecting the APIs together. \n\nWith the new Deepgram AI Speech Recognition Add-on Twilio users can:\n\n* Get their first transcript in 10 minutes or less.\n* Transcribe 1 hour of audio in as quickly as 15s – thats anywhere from 95% to 99% faster depending which Speech Recognition add-on you used previously\n* Leverage the industry's highest accuracy. Many Developers see over 90% accuracy out of the box with Deepgram depending on their use case.\n* Easily add features such as punctuation, profanity filters, redaction, speaker diarization and more to their transcripts.\n\n## Get Started in Twilio Console\n\nFrom the Twilio console, Developers can easily add Deepgram to their Programmable Voice project ([sign up here](https://console.twilio.com/us1/develop/add-ons/catalog?frameUrl=%2Fconsole%2Fadd-ons%2FXB0c5a7436735b94257335a1fd0e91a2ec%3Fx-target-region%3Dus1)). Once they have selected their preferred model tier, Base or Enhanced, they are presented with configuration options. You don’t need to create a separate Deepgram account on our website or any other steps. You simply select the speech recognition and understanding settings you’d like from the Twilio console, and away you go.\n\n## Tons of Features available in the Deepgram Speech Recognition Add-On for Twilio\n\nTwilio users can leverage these features to format and/or classify their transcriptions: \n\n* Punctuation\n* Profanity Filter\n* Redaction\n* Diarization\n* Named Entity Recognition (NER)\n* Multichannel\n* Numerals\n* Callback\n* Paragraphs\n* Utterances\n* Utterance Split\n* Interim Results\n\nTo learn more about the Deepgram Add-on for Twilio, please visit the Twilio Marketplace and checkout the [Deepgram listing](https://console.twilio.com/us1/develop/add-ons/catalog?frameUrl=%2Fconsole%2Fadd-ons%2FXB0c5a7436735b94257335a1fd0e91a2ec%3Fx-target-region%3Dus1). We welcome your feedback also. Please share it with us at [deepgram.com/feedback](deepgram.com/feedback). We can't wait to hear about what you build.";
						}
						async function compiledContent$1H() {
							return load$1H().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1H() {
							return (await import('./chunks/index.9d8888bc.mjs'));
						}
						function Content$1H(...args) {
							return load$1H().then((m) => m.default(...args));
						}
						Content$1H.isAstroComponentFactory = true;
						function getHeadings$1H() {
							return load$1H().then((m) => m.metadata.headings);
						}
						function getHeaders$1H() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1H().then((m) => m.metadata.headings);
						}

const __vite_glob_0_169 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1H,
  file: file$1H,
  url: url$1H,
  rawContent: rawContent$1H,
  compiledContent: compiledContent$1H,
  default: load$1H,
  Content: Content$1H,
  getHeadings: getHeadings$1H,
  getHeaders: getHeaders$1H
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1G = {"title":"Creating an npx Command","description":"Build, locally test, and publish npx command scripts","date":"2022-01-06T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1640023176/blog/2022/01/npx-script/building-an-npm-package%402x.jpg","authors":["kevin-lewis"],"category":"tutorial","tags":["nodejs"],"seo":{"title":"Creating an npx Command","description":"Build, locally test, and publish npx command scripts"},"shorturls":{"share":"https://dpgr.am/5d3010e","twitter":"https://dpgr.am/cf21007","linkedin":"https://dpgr.am/78c8c7d","reddit":"https://dpgr.am/31193f0","facebook":"https://dpgr.am/a2ddc9d"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453846/blog/npx-script/ograph.png"}};
						const file$1G = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/npx-script/index.md";
						const url$1G = undefined;
						function rawContent$1G() {
							return "\r\n<Alert type=\"info\">This post is effectively part 2 on building and publishing npm packages. If you haven't read the first post, you can do so [here](https://blog.deepgram.com/build-npm-packages/).</Alert>\r\n\r\nNode Package Manager (npm) allows JavaScript developers to manage and include packages in their projects. Bundled with npm is an additional utility - `npx` - which can be used to **run** Node.js scripts hosted in npm packages or at any other URL. Even if not already installed, it will download the package into a cache to execute the files.\r\n\r\nScripts with npx are often used to:\r\n\r\n*   Scaffold applications (`create-react-app` and `create-nuxt-app`)\r\n*   Run common long-living tooling (`nodemon`, `tailwindcss`, and `ngrok`)\r\n*   Make you smile (`workin-hard`, `cowsay`)\r\n\r\n[We showed you how to create an npm package in a previous blog post.](https://blog.deepgram.com/build-npm-packages) In this post, we'll extend on that sample project and build an npx command to interact with our package that queried [The Open Movie Database](http://www.omdbapi.com).\r\n\r\nThe final project code is available on the [npx branch of our npm-package repository](https://github.com/deepgram-devs/npm-package/tree/npx).\r\n\r\n## Before We Start\r\n\r\nYou will need:\r\n\r\n*   Node.js installed on your machine - [download it here](https://nodejs.org/en/).\r\n*   An npm account - [get one here](https://www.npmjs.com/signup).\r\n*   An Open Movie Database API Key - [get one here](http://www.omdbapi.com/apikey.aspx) and be sure to use the verification link in the email with the key.\r\n\r\nYou will also need to clone the previous project, open the new directory in your code editor of choice, and install the dependencies:\r\n\r\n```bash\r\ngit clone https://github.com/deepgram-devs/npm-package\r\ncd npm-package\r\nnpm install\r\n```\r\n\r\n## Making an Executable Script\r\n\r\nThere are four things you need to do to create an executable script:\r\n\r\n1.  Create a file specifically for this logic - commonly `bin.js`.\r\n2.  Specify the executable file in `package.json`.\r\n3.  Start the `bin.js` file with a 'shebang'.\r\n4.  Ensure the code in the file will run whenever the file is executed (not behind a function).\r\n\r\nCreate a `bin.js` file in your project, open `package.json`, and add a new `bin` property:\r\n\r\n```json\r\n{\r\n  \"name\": \"@username/first-package\",\r\n  \"version\": \"0.0.3\",\r\n  \"dependencies\": {\r\n    \"axios\": \"^0.24.0\"\r\n  },\r\n  \"bin\": \"./bin.js\"\r\n}\r\n```\r\n\r\nOnce the location of your executable file has been specified, it's time to create and populate the executable. Open `bin.js` in your code editor, make the first line a Node shebang, and then create a basic script that will run when the file is executed:\r\n\r\n```js\r\n#!/usr/bin/env node\r\n\r\nconsole.log('Hello world!')\r\n```\r\n\r\nThe shebang tells the machine which interpreter to use when running this file - Node is specified here.\r\n\r\nTime to test it! Open your terminal, navigate to the project directory, type `npx .`, and you should see *Hello world!* printed.\r\n\r\n## Handling Arguments\r\n\r\nCommand-Line Interfaces (CLIs) often accept or require additional information when being run - these are known as arguments. All of the arguments in a command can be accessed with `process.args` - try updating the `bin.js` file:\r\n\r\n```js\r\n#!/usr/bin/env node\r\n\r\nconsole.log(process.args)\r\n```\r\n\r\nRun it with `npx . hello world` and you should see something like this:\r\n\r\n```js\r\n;[\r\n  '/Users/kevin/.nvm/versions/node/v16.13.0/bin/node',\r\n  '/Users/kevin/.npm/_npx/0b61241d7c17bcbb/node_modules/.bin/first-package',\r\n  'hello',\r\n  'world',\r\n]\r\n```\r\n\r\nEvery space-separated string is represented - the first two represent your `node` installation and `first-package`. Then, `hello` and `world` are included.\r\n\r\nThis is great if you know exactly which order arguments will be passed, but it isn't always the case. If you need more flexible access to arguments, you can use a package called [`yargs`](https://www.npmjs.com/package/yargs). Install it from your terminal with `npm install yargs` and update your `bin.js` file:\r\n\r\n```js\r\n#!/usr/bin/env node\r\n\r\nconst yargs = require('yargs')\r\n\r\nconsole.log(yargs.argv)\r\n```\r\n\r\nThen run the following command:\r\n\r\n```bash\r\nnpx . --capitalize --phrase \"Hello World\" extra args\r\n```\r\n\r\nThe result should look like this:\r\n\r\n```js\r\n{\r\n  capitalize: true,\r\n  phrase: 'Hello World',\r\n  _: ['extra', 'args']\r\n}\r\n```\r\n\r\nThis allows you to check for named argument existence and values, as well as non-hyphenated options inside of your `bin.js` file.\r\n\r\n## Executing Logic From Main Package\r\n\r\nSo far, this has all been quite abstract. This section will show how to access the main package features and execute them from your CLI.\r\n\r\nAs a reminder, the main package code in `index.js` exports a class that expects an `apiKey` value when initialized. It has one member method - `get(parameters)` - that takes in an object with properties with which to call The Open Movie Database API.\r\n\r\nSo, how do you get an API Key from the user? There are several approaches:\r\n\r\n1.  Require it as an argument\r\n2.  Require it as an environment variable on the target machine\r\n3.  Require the user to run an 'init' command which saves the values to a file on the machine, and then use that file's value when making calls\r\n\r\nIn this tutorial, the project will take the first approach for brevity, but you may consider the others in your future packages. Update `bin.js`:\r\n\r\n```js\r\n#!/usr/bin/env node\r\n\r\nconst yargs = require('yargs')\r\nconst OpenMovieDatabase = require('./index')\r\n\r\nconst omdb = new OpenMovieDatabase(yargs.argv.key)\r\n\r\nif (yargs.argv.title) {\r\n  omdb.get({ t: yargs.argv.title }).then((results) => {\r\n    console.log(results)\r\n  })\r\n}\r\n\r\nif (yargs.argv.search) {\r\n  omdb.get({ s: yargs.argv.search }).then((results) => {\r\n    console.log(results.Search)\r\n  })\r\n}\r\n```\r\n\r\nTry the following commands:\r\n\r\n```bash\r\nnpx . --key=your_api_key --title \"Zombieland\"\r\nnpx . --key=your_api_key --search \"Spiderman\"\r\n```\r\n\r\nYou'll notice a friendlier interface via this CLI than the main API here - instead of needing to know that the parameters as `t` or `s`, you allow the user to provide the `title` or `search` arguments.\r\n\r\n## Validating Command Inputs\r\n\r\nFor this CLI to work, the user **must** provide a `key` argument and **either** a `title` argument or a `search` argument. You will also want to restrict the user from providing both as that will lead to two logs which doesn't look great. Thanks to `yargs`, you already know if arguments have been provided, so some boolean logic is all that's needed.\r\n\r\nJust above where `omdb` is declared, add the following checks:\r\n\r\n```js\r\nif (!yargs.argv.key) {\r\n  return console.log('You must provide a key argument with an OMDb API Key')\r\n}\r\n\r\nif (!yargs.argv.title && !yargs.argv.search) {\r\n  return console.log(\r\n    'You must provide either a title or search argument - you have provided neither'\r\n  )\r\n}\r\n\r\nif (yargs.argv.title && yargs.argv.search) {\r\n  return console.log(\r\n    'You must provide either a title or search argument - not both'\r\n  )\r\n}\r\n```\r\n\r\nTry now to omit `key`, omit `title` and `search`, or provide both `title` and `search`.\r\n\r\n## Publishing & Using Package\r\n\r\nLike publishing any updated to npm packages, you must increment the version in `package.json` and then run `npm publish` from your terminal.\r\n\r\nOnce published, you can run the final package with `npx @username/first-package --key=your_api_key --title \"Zombieland\"`.\r\n\r\nWant to try mine? Use `npx @phazonoverload/first-package --key=your_api_key --title \"Zombieland\"`.\r\n\r\n## Wrapping Up\r\n\r\nLike creating npm packages, making them executable with npx was something I struggled to find clear and correct learning material for. I hope this helps fill a gap and gets your project built!\r\n\r\nThe final project code is available on the [npx branch of our npm-package repository](https://github.com/deepgram-devs/npm-package/tree/npx), and if you have any questions, please feel free to reach out on Twitter - we're [@DeepgramDevs](https://twitter.com/DeepgramDevs).\r\n\r\n        ";
						}
						async function compiledContent$1G() {
							return load$1G().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1G() {
							return (await import('./chunks/index.74065a5e.mjs'));
						}
						function Content$1G(...args) {
							return load$1G().then((m) => m.default(...args));
						}
						Content$1G.isAstroComponentFactory = true;
						function getHeadings$1G() {
							return load$1G().then((m) => m.metadata.headings);
						}
						function getHeaders$1G() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1G().then((m) => m.metadata.headings);
						}

const __vite_glob_0_170 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1G,
  file: file$1G,
  url: url$1G,
  rawContent: rawContent$1G,
  compiledContent: compiledContent$1G,
  default: load$1G,
  Content: Content$1G,
  getHeadings: getHeadings$1G,
  getHeaders: getHeaders$1G
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1F = {"title":"Generating Collapsable Navigation from Nuxt Content","description":"Generating Collapsable Navigation from Nuxt Content","date":"2022-02-07T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1644257722/blog/2022/01/nuxt-expand-nested-navigation/Generating-Collapsable-Nav-from-Nuxt%402x.jpg","authors":["brian-barrow"],"category":"tutorial","tags":["nuxtjs","vuejs"],"seo":{"title":"Generating Collapsable Navigation from Nuxt Content","description":"Generating Collapsable Navigation from Nuxt Content"},"shorturls":{"share":"https://dpgr.am/f02f34c","twitter":"https://dpgr.am/bf11f9d","linkedin":"https://dpgr.am/3c0918e","reddit":"https://dpgr.am/6fa4f3d","facebook":"https://dpgr.am/c273412"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453848/blog/nuxt-expand-nested-navigation/ograph.png"}};
						const file$1F = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/nuxt-expand-nested-navigation/index.md";
						const url$1F = undefined;
						function rawContent$1F() {
							return "\r\nWhen we started building the [Documentation page here at Deepgram](https://developers.deepgram.com/documentation/), we had a lot of nested pages and routes. Rather than have all those links listed out on the side, we wanted the user to be able to expand and collapse them in order to have a cleaner look and feel.\r\n\r\nAt the top level of our navigation, we have a `SideNavigation.vue` component that gets the navigation information from a `navigation.yml` file in our content folder. Each navigation element can have a `children` property (shown below) that lists out the child routes of that page.\r\n\r\n![screenshot showing navigation routes and child navigation routes](https://res.cloudinary.com/deepgram/image/upload/v1642175788/blog/2022/01/nuxt-expand-nested-navigation/child-routes.png)\r\n\r\nThis is the original template we had set up for the component:\r\n\r\n```vue\r\n<template>\r\n  <ul>\r\n    <li\r\n      v-for=\"nav of navigation\"\r\n      :key=\"nav.to\"\r\n      :class=\"{ active: isActive(nav) }\"\r\n    >\r\n      <NuxtLink :to=\"nav.to\" :title=\"nav.title || nav.name\">{{\r\n        nav.name\r\n      }}</NuxtLink>\r\n      <NavList v-if=\"nav.children\" :navigation=\"nav.children\" />\r\n    </li>\r\n  </ul>\r\n</template>\r\n```\r\n\r\nThe component was recursively going through the navigation object and creating new NavList components if there were children routes. Basically, it is rendering another version of itself inside of itself. So the NavList component renders another NavList component. Sort of like component Inception. Don't worry about an infinite rendering of NavLists though. It will render a child NavList only if there are children to the navigation object.\r\n\r\nI like the fact that it is recursive, making it dynamic. That way, we don't have to specify when to use a 'child' `NavList` component. It also presented a problem. I needed some way to identify which sub-menus were open and which were not. After some thought and some rubber ducking, I settled on having a data property containing an array of the parent route paths that were expanded. I called it `childrenShow`.\r\n\r\n```js\r\ndata() {\r\n    return {\r\n      childrenShow: [],\r\n    }\r\n  },\r\n```\r\n\r\nIf a parent route has a path of `/documentation/getting-started/`, the entire string gets pushed into the array. Then, as the child `NavList` components get added to the page, I needed to check if they should be shown using a `v-show` directive. I did this by using the `isExpanded` method, which takes in the current `nav` object (which comes from the original YAML file for the nav list structure via the `v-for` above) and checks it against the `childrenShow` array to see if should be shown.\r\n\r\nAlso, since there is already a v-if on the `NavList`, and I can't use both a v-if and a v-show on the same element, I had to create a container template to handle the v-if. So the `NavList` line will change to this:\r\n\r\n```vue\r\n<template v-if=\"nav.children\">\r\n  <NavList v-show=\"isExpanded(nav)\" :navigation=\"nav.children\" />\r\n</template>\r\n```\r\n\r\nThis worked well as-is. The menus collapse and expand when the user clicks on the respective expand/collapse button. The only issue now is that the transition wasn't smooth, which can be a little jarring.\r\n\r\nAfter playing with transitions for a while and not having much success for this use case, I talked to a co-worker, and he sent me [this article from Markus Oberlehner](https://markus.oberlehner.net/blog/transition-to-height-auto-with-vue/) that talks about transitioning to an element's full height. So I created a new component called `TransitionExpand.vue` and put this code in there:\r\n\r\n```vue\r\n<script>\r\nexport default {\r\n  name: `TransitionExpand`,\r\n  functional: true,\r\n  render(createElement, context) {\r\n    const data = {\r\n      props: {\r\n        name: `expand`,\r\n      },\r\n      on: {\r\n        afterEnter(element) {\r\n          // eslint-disable-next-line no-param-reassign\r\n          element.style.height = `auto`\r\n        },\r\n        enter(element) {\r\n          const { width } = getComputedStyle(element)\r\n\r\n          /* eslint-disable no-param-reassign */\r\n          element.style.width = width\r\n          element.style.position = `absolute`\r\n          element.style.visibility = `hidden`\r\n          element.style.height = `auto`\r\n          /* eslint-enable */\r\n\r\n          const { height } = getComputedStyle(element)\r\n\r\n          /* eslint-disable no-param-reassign */\r\n          element.style.width = null\r\n          element.style.position = null\r\n          element.style.visibility = null\r\n          element.style.height = 0\r\n          /* eslint-enable */\r\n\r\n          // Force repaint to make sure the\r\n          // animation is triggered correctly.\r\n          // eslint-disable-next-line no-unused-expressions\r\n          getComputedStyle(element).height\r\n\r\n          requestAnimationFrame(() => {\r\n            // eslint-disable-next-line no-param-reassign\r\n            element.style.height = height\r\n          })\r\n        },\r\n        leave(element) {\r\n          const { height } = getComputedStyle(element)\r\n\r\n          // eslint-disable-next-line no-param-reassign\r\n          element.style.height = height\r\n\r\n          // Force repaint to make sure the\r\n          // animation is triggered correctly.\r\n          // eslint-disable-next-line no-unused-expressions\r\n          getComputedStyle(element).height\r\n\r\n          requestAnimationFrame(() => {\r\n            // eslint-disable-next-line no-param-reassign\r\n            element.style.height = 0\r\n          })\r\n        },\r\n      },\r\n    }\r\n\r\n    return createElement(`transition`, data, context.children)\r\n  },\r\n}\r\n</script>\r\n\r\n<style scoped>\r\n* {\r\n  will-change: height;\r\n  transform: translateZ(0);\r\n  backface-visibility: hidden;\r\n  perspective: 1000px;\r\n}\r\n</style>\r\n\r\n<style>\r\n.expand-enter-active,\r\n.expand-leave-active {\r\n  transition: height 0.5s ease-in-out;\r\n  overflow: hidden;\r\n}\r\n\r\n.expand-enter,\r\n.expand-leave-to {\r\n  height: 0;\r\n}\r\n</style>\r\n```\r\n\r\nThere is a lot of stuff happening in that component. To understand it better, visit the link above to Markus' blog. He goes into the detail you need.\r\n\r\nI then changed the template tag that was containing the child `NavList` component, to a `transition-expand` tag like this:\r\n\r\n```js\r\n<transition-expand v-if=\"nav.children\">\r\n  <NavList v-show=\"isExpanded(nav)\" :navigation=\"nav.children\" />\r\n</transition-expand>\r\n```\r\n\r\nThat made the expand/collapse transition much smoother and created a better user experience. It was a challenging problem that was fun to figure out. If you want to see a working example of it, you can check out this [code sandbox](https://codesandbox.io/s/serene-stitch-30zmi). If you want to see it on our production site, you can see it on our [Documentation page](https://developers.deepgram.com/documentation/).\r\n\r\n<CodeEmbed src=\"https://codesandbox.io/s/serene-stitch-30zmi\" />\r\n\r\n        ";
						}
						async function compiledContent$1F() {
							return load$1F().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1F() {
							return (await import('./chunks/index.3b580d2c.mjs'));
						}
						function Content$1F(...args) {
							return load$1F().then((m) => m.default(...args));
						}
						Content$1F.isAstroComponentFactory = true;
						function getHeadings$1F() {
							return load$1F().then((m) => m.metadata.headings);
						}
						function getHeaders$1F() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1F().then((m) => m.metadata.headings);
						}

const __vite_glob_0_171 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1F,
  file: file$1F,
  url: url$1F,
  rawContent: rawContent$1F,
  compiledContent: compiledContent$1F,
  default: load$1F,
  Content: Content$1F,
  getHeadings: getHeadings$1F,
  getHeaders: getHeaders$1F
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1E = {"title":"Olá! Enhanced Portuguese (beta) Speech-to-Text Language Model Now Available","description":"We’re excited to announce the launch of our Enhanced Portuguese language model—now in beta!","date":"2022-09-30T13:22:44.848Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1664544531/2209-How-Accurate-is-OpenAI-Whisper-Speech-to-Text-Model-featured-1200x630_qndzys.png","authors":["katie-byrne"],"category":"product-news","tags":["portuguese","language"],"seo":{"canonical":"enhanced-portuguese-speech-to-text-language-model-available","title":"Enhanced Portuguese (beta) Speech-to-Text Language Model Now Available"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1664544873/2209-How-Accurate-is-OpenAI-Whisper-Speech-to-Text-Model-social-1200x628_pbpaui.png","title":"Enhanced Portuguese (beta) Speech-to-Text Language Model Now Available"}};
						const file$1E = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/olá-enhanced-portuguese-beta-speech-to-text-language-model-now-available/index.md";
						const url$1E = undefined;
						function rawContent$1E() {
							return "Several months ago, we released our Base Portuguese ASR model to support our customers in Europe and South America. Working closely with our customers in these regions to deeply understand their use cases and data, we’re excited to announce the next generation of our Enhanced Portuguese model as an additional option for use cases that require exceptional accuracy.\n\n## How to Get Started with Enhanced Portuguese\n\nIf you want to try out our Enhanced Portuguese model, you can quickly create an account on Deepgram [Console](https://console.deepgram.com/) and we’ll give you $150 in free credits. Simply select Portuguese from the language menu when trying out our APIs.\n\nIf you’re already a Deepgram customer, you can call the Enhanced Portuguese model using the following arguments:\n\n* ``model=general``\n* ``version=beta``\n* ``language=pt-BR`` (Brazil) or ``pt-PT`` (Portugal)\n\nYou can also find an example API call using these arguments below.\n\n### Example API Call\n\n```\ncurl \\\n--request POST \\\n--header 'Authorization: Token YOUR_DEEPGRAM_API_KEY' \\\n--header 'Content-Type: audio/wav' \\\n--data-binary @youraudio.wav \\\n--url 'https://api.deepgram.com/v1/listen?language=pt-BR'\n--url 'https://api.deepgram.com/v1/listen?language=pt-PT'\n```\n\n## Key Benefits of a Portuguese Language Model\n\n* Accurately transcribe both Brazilian and Portugal Portuguese [pronunciations](https://www.youtube.com/watch?v=7_3ECC8ZPP4).\n* Many developers see high accuracy depending on their use case.\n* Available for Pre-recorded and Streaming Phone Call, Meeting, Voicemail, and Conversational AI use cases.\n* Transcribe on-premises or through the Deepgram Cloud.\n* Available for both Base and now Enhanced model tiers.\n\n## What Can You Use a Portuguese Language Model for?\n\n* Pair with a Phone Call model to transcribe recording from your call centers in Europe and South America.\n* Pair with a Meetings model to understand the sentiment of your development team in Brazil or South Africa\n* Create an Agent Assist solution for your LATAM sales team.\n\nDeepgram customers now have two fantastic model options for Portuguese, providing them greater investment flexibility depending on their use case. This continues our commitment to provide something more than “one size fits all” when it comes to language models for our customers. We will continue to invest in world-class research and engineering to provide our customers with the optionality they deserve from their speech API. \n\n## Wrapping Up\n\nTo learn more about the dozens of other languages and use cases Deepgram enables, please see the [Language](https://developers.deepgram.com/documentation/features/language/) page in our documentation. Need help getting started? [Reach out to our sales team](https://deepgram.com/contact-us/) and they’ll be happy to get you up and running. We welcome your feedback, please share it with us at [deepgram.com/feedback](https://www.deepgram.com/feedback).";
						}
						async function compiledContent$1E() {
							return load$1E().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1E() {
							return (await import('./chunks/index.6b42db62.mjs'));
						}
						function Content$1E(...args) {
							return load$1E().then((m) => m.default(...args));
						}
						Content$1E.isAstroComponentFactory = true;
						function getHeadings$1E() {
							return load$1E().then((m) => m.metadata.headings);
						}
						function getHeaders$1E() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1E().then((m) => m.metadata.headings);
						}

const __vite_glob_0_172 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1E,
  file: file$1E,
  url: url$1E,
  rawContent: rawContent$1E,
  compiledContent: compiledContent$1E,
  default: load$1E,
  Content: Content$1E,
  getHeadings: getHeadings$1E,
  getHeaders: getHeaders$1E
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1D = {"title":"Open Source Projects for Hacktoberfest 2022","description":"This Hacktoberfest, Deepgram's DevRel team is ready to go with a number of Open Source projects. Learn more here!","date":"2022-10-07T15:08:47.333Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1665163751/blog/2022/10/open-source-projects-for-hacktoberfest-2022/2210-Hacktoberfest-featured-1200x630_2x_jphov8.png","authors":["bekah-hawrot-weigel"],"category":"devlife","tags":["hacktoberfest"],"shorturls":{"share":"https://dpgr.am/65296a4","twitter":"https://dpgr.am/194a827","linkedin":"https://dpgr.am/0a155ee","reddit":"https://dpgr.am/ba741d5","facebook":"https://dpgr.am/e52070a"}};
						const file$1D = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/open-source-projects-for-hacktoberfest-2022/index.md";
						const url$1D = undefined;
						function rawContent$1D() {
							return "\nIt’s my favorite time of the year. That’s right, it’s Hacktober, and our Deepgram DevRel team is ready to go with our Open Source projects! We’re also super excited to share that some of our community members are rolling out their own, Deepgram projects. If you haven’t been down the Hacktoberfest path, don’t worry, we’ll fill you in.\n\n## What is Hacktoberfest?\n\nHacktoberfest is a month-long event devoted to contributing to open-source projects on GitHub or GitLab, presented this year by DigitalOcean, appwrite, and docker. Participants register on the [Hacktoberfest site](https://hacktoberfest.com/), and during the month of October must have four pull/merge requests accepted by an open source project that has opted into Hacktoberfest.\n\n## Deepgram’s Open Source Projects\n\nWe can’t keep away from this fun! Here are the projects we’re opting into Hacktoberfest.\n\n*   [Deepgram Node SDK](https://github.com/deepgram/deepgram-node-sdk) - Official JavaScript SDK for Deepgram's automated speech recognition APIs.\n*   [.NET SDK](https://github.com/deepgram/deepgram-dotnet-sdk) - .NET SDK for Deepgram's automated speech recognition APIs.\n*   [Python SDK](https://github.com/deepgram/python-sdk) - Official Python SDK for Deepgram's automated speech recognition APIs.\n*   [Deepgram Go SDK](https://github.com/deepgram-devs/deepgram-go-sdk) - Community Go SDK for Deepgram's automated speech recognition APIs.\n*   [Deepgram Deno SDK](https://github.com/deepgram-devs/deepgram-deno-sdk) - Community Deno SDK for Deepgram's automated speech recognition APIs\n*   [Deepgram Rust SDK](https://github.com/deepgram-devs/deepgram-rust-sdk) - Rust SDK for Deepgram's automated speech recognition APIs.\n*   [Deepgram Translation Chrome Extension](https://github.com/deepgram-devs/dg-translation-chrome-ext) - A TypeScript chrome extension that uses Deepgram to provide live transcription and translation.\n\nWe have a variety of issues, but if you see something that you’d like to contribute, feel free to create an issue or ask a question on our [Community Forum](https://github.com/orgs/deepgram/discussions/categories/q-a). We’re not the only ones using Deepgram in a Hacktoberfest project though! Check out some of our community members’ projects:\n\n*   [Gramcastr](https://github.com/drewclem/gramcstr) from [@DrewClem](https://github.com/drewclem) is a Next.js project that will use Deepgram to generate podcast transcripts and summarize the projects. \n*   [Voice To Text Notes](https://github.com/JesseRWeigel/voice-to-text-notes) from [@JesseRWeigel](https://github.com/JesseRWeigel) is a basic speech to text app made with TypeScript that will add features like the ability to save text locally, edit text, and save text to a server. \n\nWe’re super stoked to see what comes out of Hacktoberfest, and we have some fun events planned as well–check out our [Events Category](https://github.com/orgs/deepgram/discussions/categories/events) to see what Hacktoberfest events we have planned. \n\nIf you have an open source project you’d like to share–even if it doesn’t have anything to do with Deepgram-add it to [our Open Source Project discussion](https://github.com/orgs/deepgram/discussions/36), so we can amplify what you’re doing!\n\n";
						}
						async function compiledContent$1D() {
							return load$1D().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1D() {
							return (await import('./chunks/index.deaa2b8c.mjs'));
						}
						function Content$1D(...args) {
							return load$1D().then((m) => m.default(...args));
						}
						Content$1D.isAstroComponentFactory = true;
						function getHeadings$1D() {
							return load$1D().then((m) => m.metadata.headings);
						}
						function getHeaders$1D() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1D().then((m) => m.metadata.headings);
						}

const __vite_glob_0_173 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1D,
  file: file$1D,
  url: url$1D,
  rawContent: rawContent$1D,
  compiledContent: compiledContent$1D,
  default: load$1D,
  Content: Content$1D,
  getHeadings: getHeadings$1D,
  getHeaders: getHeaders$1D
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1C = {"title":"Opening Keynote - Bradley Metrock, CEO, Project Voice - Project Voice X","description":"Opening keynote presented by Bradley Metrock, CEO of Project Voice, presented on day one of Project Voice X. ","date":"2021-12-08T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981389/blog/opening-keynote-bradley-metrock-ceo-project-voice-project-voice-x/proj-voice-x-session-bradley-metrock-blog-thumb-55.png","authors":["claudia-ring"],"category":"speech-trends","tags":["project-voice-x"],"seo":{"title":"Opening Keynote - Bradley Metrock, CEO, Project Voice - Project Voice X","description":"Opening keynote presented by Bradley Metrock, CEO of Project Voice, presented on day one of Project Voice X. "},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981389/blog/opening-keynote-bradley-metrock-ceo-project-voice-project-voice-x/proj-voice-x-session-bradley-metrock-blog-thumb-55.png"},"shorturls":{"share":"https://dpgr.am/8abf79e","twitter":"https://dpgr.am/849cd28","linkedin":"https://dpgr.am/746a60f","reddit":"https://dpgr.am/e58fca2","facebook":"https://dpgr.am/c9de92b"}};
						const file$1C = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/opening-keynote-bradley-metrock-ceo-project-voice-project-voice-x/index.md";
						const url$1C = undefined;
						function rawContent$1C() {
							return "*This is the transcript for the opening keynote presented by Bradley Metrock, CEO of Project Voice, presented on day one of Project Voice X.* \n\n*The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was **94% accurate.**  Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram’s API.  If you want to see if Deepgram is right for your use case, [contact us](https://deepgram.com/contact-us/).*\n\n\\[Bradley Metrock:] Alright. We’re ready to go. Welcome to Project Voice x. Give yourselves a round of applause. So we’re looking forward to this. You know, we’re we’re so glad that y’all are here. You know, we we know that people will be coming in throughout the two and a half days, but we’re grateful for each and every one of you. I’ve got a brief presentation that I’ll be using to start the day off, and it’s just gonna be an exciting day full of a lot of different speaker for a lot of different points of view that that will add a lot to your business and and give you good perspective heading to the end of the year. My name is Bradley Metrock. I’m CEO of a company called Project Voice. We do a lot to accelerate the adoption of voice technology and conversational AI. I’ll go into that a little bit here.\n\nBut all in all, we just, you know… I I guess the one main point I wanna make is just how hard it is to do an in-person event. Every single thing that you look around here and see or the the Voiceflow social… make sure you sign up for that by the way. There’s an email prolly in your inbox from Eventbrite this afternoon. Deepgram’s got a movie screening of Dune Tuesday afternoon, so there’s some fun stuff. It looks like the weather’s gonna cooperate. But in-person events are real hard, and they’re hard for a lot of different reasons, but the fact is they’re real hard. So a lot of gratitude for y’all being here, and thank you for setting the time aside.\n\nSo just a little bit about us to bore you right off the bat. Project Voice Catalyst is a consulting program. We started amidst the pandemic to help companies that are born native to voice and AI. Now this program has represents over seven billion dollars in market value of companies working in the space. Rob Fletcher, who you met, runs this for us. We’ll be talking about this later on. Project Voice capital partners, this is a twenty million dollar venture capital fund. It’s just now getting stood up that Marc Ladin and I are running. Excited to to hear a bunch of great pitches Wednesday morning from a lot of interesting companies and excited to talk to them and and and help them every way we can. This Week In Voice VIP, this is a Substack newsletter that some people read for some reason that I write.\n\nSo in this talk, I’m gonna talk briefly about what I see with voice and AI coming down the pike, and I’m gonna spend much more time talking about a personal story that I think is very applicable to to where we all sit right now. And, like, all of the This Week In Voice VIPs, the newsletter I write, all of ’em have a musical theme. And and so the name of this talk, I just decided to call it Brand New Day. So Sting came out with this album and this song twenty two years ago this month, and he wrote it for separate reasons. But as he’s talked about amidst the pandemic, it’s become very pressing, you know, to to help, you know, help people think about how to possibly get through what we’ve all been through and get into a next chapter. And I thought it would be a good place… oh, I forgot about this. So Sting had an interview that was in conjunction with… he’s gonna do a Residency in Las Vegas, and he was talking about this song in particular. I thought it would be a good place to start this entire conference just playing this song.\n\nTypically, I wouldn’t probably spend four minutes to do this, but I challenge you to listen to the words of this. And I’m gonna share some stuff about voice and AI. Three things, I think, are gonna happen. We’ll talk some technology, but this is not a bad place to start. And this gets subscribers a chance to get it here too, so I’m gonna hit play if I can. Let’s see if this will… oh, let’s see if this will work.\n\n\\[SPEAKER 2:] — then I’d feel like it was worthwhile. It’s always a pleasure to have our next guest on the show. This is the new CD, Brand New Day. And next week, he begins touring the United States. Ladies and gentlemen, Sting.\n\n\\[nonspeech:music]\n\n\\[SPEAKER 3:] Thank you very much. Thank you.\n\n\\[SPEAKER 2:] Sting, good to see you again. Great song. It’s a Brand New Day, ladies and gentlemen. We’ll be right back…\n\n\\[Bradley Metrock:] I’m just a letterman, but that’s a separate subject. You know, it’s easy to lose sight of the fact that coming out of this pandemic, it…\n\n> it’s almost like everything that came before with voice and AI is gone. A lot of the players are gone. It’s… there’s there’s incredible new interest. The whole landscape is different.\n\nEvery time I see some sort of statistic from, you know, twenty eighteen or something like that, it it it feels dated and and almost useless. It’s exciting to come to something like this and see the beginnings, the ground swell, the the seedlings starting to to pop up for what’s to come. You know, COVID impacted everybody, and it’s just a deeply traumatic event. And, you know, the the the the few… I like this graphic just because of how much it represents out the whole thing, united everybody, but now it it becomes who can move the quickest. And that’s that’s what I’m gonna talk about here.\n\nWe had a couple of things happened during the pandemic that that sorta shaped the perspective of the space. This is one of ’em, Microsoft buying Nuance. I can’t tell you how many people just did I interact with that this affected. So just the numbers of the deal, the companies involved with the deal. There was people who didn’t hear anything about any of this stuff that we’re gonna hear all about today until this, and it’s important to sit here and know just to sort of reset the stage for this conference. But I’ll tell you a deal that I think is maybe a little bit more worth paying attention to, and I’ma talk about this a second. So Aiquodo. Does anybody in this room know who that is or was? Ok. So a couple. So Aiquodo was a company that we worked with and really liked them. John Foster, intense intense guy. But they had they had the distinction of being the only company that I have ever seen, in which the cofounders argued with themselves on stage in the middle of an event. It was our event, so I know. I saw it. And it was bitter, and it was it was weird. And, you know, they got a good… they had a good product and everything, but it was super weird. And and they had some challenges, but even despite all of that, earlier this year, they’re bought by Peloton. And I think that this… even more than the the Nuance and the Microsoft gets all the headlines, but for the people in this room, I would say take a good look at this deal here for a glimpse into the future. Because what I see when I look at that is that no matter what it is you’re doing in the space, how you’re almost… no matter how it is you’re doing it, there’s such a dearth of companies doing meaningful work in the space and such a lack of… such scarcity of talented people working in the space.\n\nThen one of the things I think we’re gonna see is, you know, companies that maybe in twenty seventeen or twenty eighteen would’ve never been acquired by anybody get snapped up as if they were hot commodities. So I think it’s a good news story, and that’s really where these three takeaways come into play. Let me hit that right there. So there’s three three voice and AI points I’m gonna make here before I tell a personal story, and then I’m gonna be done. So twenty seventeen, twenty eighteen, there was a lot of conversation about what’s the killer app for voice. You know, that was a common refrain. What’s a killer app for voice? And then you had people arguing like, what is that even supposed to mean? It was a good rhetorical question for for debate purposes.\n\nBut along came contact centers, and it’s gotten to the point now where if you have a contact center or call center operation of some sort, and you don’t have voice and AI deeply in… interwoven into how that thing work, sure, you’re ancient. You know, you’re you’re uncompetitive, and, you know, you’re you’re definitely behind. And it’s interesting to look at that and realize that and and then jump to the next conclusion, which is that every single other industry will ultimately end up that way. We’re gonna get to a point where if you don’t have a hotel with voice and AI integrated in different ways, you’re behind. You don’t have a hospital healthcare system with an integrated, you’re behind. If you… if your triple a video game title doesn’t have an integrated, somehow, you’re behind. I could go on and on. So that’s that’s one thing to realize sort of as we embark on a new chapter.\n\nSecond thing is voice payments. So, you know, voice payments and voice commerce is something that impacts everybody who’s here. The willingness of people to make payments with voice is really important, and this is another one of these things where there’s sort of a pre-pandemic way of looking at this, and then there’s a current way of looking at this. The pre-pandemic way of looking at this was, well, ok. You know, for repeat purchasing, I could see it working. Maybe under the right circumstances, but making origin… original purchases of things that you never bought before, I don’t know. Well, along came the pandemic, and there’s a couple of little data points I could hold up, but this is this is one I thought would do the best job. So I would never buy a car online. I would just never do it. I I don’t think I would ever do it. I I guess you never say never. But this article talks about… and this is just from a few days ago. It talks about how people are buying cars online. I could’ve chosen anything I wanted to choose. I could’ve chose… my wife buys a bunch of stuff online, clothing-wise, and then, like, returns it, you know, ’cause that’s just the way it needs to work if people aren’t comfortable with dressing rooms. And the pandemic has changed purchasing habits, such that the door is now open for voice commerce and voice payments in a way that I don’t think it was before. So that’s the second point I wanna make.\n\nAnd then the third is just touching on something I already said. I think that we’re looking at a big shortage of people who know how to add value in the space, and I think that, you know, there’s there’s corresponding opportunities related to that. I think that you were gonna see more education at the collegiate level. I think we’ll see more continuous learning and professional certifications and things like that to get people trained up, but the nearer term outcome of that is a lot of merger and acquisition activity to try to capture that labor where it exist. So those are sort of the three things I wanted to lay out. And, you know, I I wanted to tell a story.\n\nYou know, the technology you’re gonna hear a bunch about that today, but I wanted to tell a story that’s a little personal just to try to get you thinking in a certain way. So these two people, I… they… I don’t need to say their names. They’re they’re friends of mine. So they were married long ago, and, you know, they… this is this is them, their family here. And so then they had this child here, and his name is Leland. And he was born with a condition that basically causes you to have seizures all the time. So you have seizures all the time, and when I say all the time, I I mean it. And and so what happens when you have seizures all the time is that doctors go through a a see… a sequence of steps to figure out what works for you and what does not work for you. So there’s there’s a drug and then there’s another drug and then there’s another drug and then there’s another drug. And and what happens when none of them work is something called a hemispherectomy.\n\nSo I wish I had never heard of this word or that it didn’t need to exist, but it’s actually kind of a miracle. And what this means is that when your brain… so they… they’ve studied seizures, and they know seizures well enough to know you might have three fourths of a working brain, and one fourth is the problem causing the seizures. So depending on if it’s horizontal. You know, there’s two horizontal. You know, the left and right part of the brain, and then there’s a top and the bottom part. Not to go into a bunch of detail here, but depending on what quadrant it’s in depends on what they’ll do. And what they do is they cut your brain in half, and that’s where this term comes from. And the the the metaphor here is that you… you know, I’ve seen it with this event here. I’ve seen it with a bunch of stuff that we do that’ll… that… I think the story for this next chapter is… and and companies that go on to be successful will recognize the part of them that’s not working. And it’s hard to make a cut. The… there’s the type of cut that many of us have to make. You know, coming out of the pandemic, we had plenty of time to sit around and think about the people in our lives that are holding us back, the things in our lives that are holding us back, the people that our companies that, you know, don’t wanna be there, and and sort of reflect on what it is that we actually wanna be doing. But making the cut is hard.\n\nBut what happened in this case is they took Leland to to have this hemispherectomy, and not only did he recover, but he’s now seeing a lot of brain functionality that is not normally seen. He’s way above average, and he’s he’s gaining a lot of skills. And he’s gonna have a normal… you know, relatively normal life. And every day is joyful for this group of people. So the point I wanna make is you’re you’re gonna hear a lot about the technology, and and I like to see the dip and dust back there. Hopefully, you’ll get a lot of dip and dust today. We we added that as a permanent fixture in here. But there’s a lot of people who are having a lot of trouble getting over what has happened to them last twelve to eighteen months. And the fact that you’re sitting in here means that you’re multiple standard deviations further along than they are, and you should be thankful for that because there’s plenty of people who have gone through hard cuts and hard choices they’ve had to make. And so anything that happens here, hopefully, is easy by comparison. I talked earlier this year when we did Project Voice a hundred about the k.\n\nWhen the pandemic started, we thought that there would be a v-shaped recovery. That was wrong. The the premise of that was, you know, a little crater and then come right back… the world would just come right back to the way it was. That was completely wrong. The k is all about… half of half of us are moving up, and half of us are moving down. And even within that, we might be moving in a positive direction from a business point of view, but from a mental health point of view or spiritual point of view, we may be moving down.\n\nAnd so, ultimately, we’re just gonna end up at the bottom part of the cave. I challenge you as you’re hearing all the talks and enjoying some time to yourself over the next two and a half days to think about, you’re gonna win from a technological standpoint being here. There’s plenty of people to meet that are gonna help set you on the right course. You’re already on the right course. We know who’s here and who’s registered. But what are those other things that you can do from a personal standpoint to to make your life better and more joyful and to have the right perspective? If that’s if that’s some… you know, a way that I can get you to think heading into this conference, then this was worth the time. So that’s my email address there. We’re excited to have just what… what’ll be a fun, casual, but very valuable experience these next couple of days. Thank you for being here, and thank you for listening to this. So next up… and we’ll just get right into it. And I’ll be MC-ing throughout the day, by the way. We’re fortunate to have Amazon be part of what we’re doing, and the gentleman you’re about to hear is Jeff Blankenburg. He’s done a phenomenal job helping Amazon continue to grow their business and continue to maintain their positioning within voice. Let’s give him a round of applause.";
						}
						async function compiledContent$1C() {
							return load$1C().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1C() {
							return (await import('./chunks/index.5a6b6cd9.mjs'));
						}
						function Content$1C(...args) {
							return load$1C().then((m) => m.default(...args));
						}
						Content$1C.isAstroComponentFactory = true;
						function getHeadings$1C() {
							return load$1C().then((m) => m.metadata.headings);
						}
						function getHeaders$1C() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1C().then((m) => m.metadata.headings);
						}

const __vite_glob_0_174 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1C,
  file: file$1C,
  url: url$1C,
  rawContent: rawContent$1C,
  compiledContent: compiledContent$1C,
  default: load$1C,
  Content: Content$1C,
  getHeadings: getHeadings$1C,
  getHeaders: getHeaders$1C
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1B = {"title":"Opening Keynote - Jeff Blankenberg, Principal Technical Evangelist, Amazon Alexa - Project Voice X","description":"Opening Keynote presented by Jeff Blankenberg, Principal Technical Evangelist at Amazon Alexa, presented on day one of Project Voice X. ","date":"2021-12-09T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981394/blog/opening-keynote-jeff-blankenberg-principal-technical-evangelist-amazon-alexa-project-voice-x/proj-voice-x-session-jeff-blankenberg-blog-thumb-5.png","authors":["claudia-ring"],"category":"speech-trends","tags":["amazon","project-voice-x"],"seo":{"title":"Opening Keynote - Jeff Blankenberg, Principal Technical Evangelist, Amazon Alexa - Project Voice X","description":"Opening Keynote presented by Jeff Blankenberg, Principal Technical Evangelist at Amazon Alexa, presented on day one of Project Voice X. "},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981394/blog/opening-keynote-jeff-blankenberg-principal-technical-evangelist-amazon-alexa-project-voice-x/proj-voice-x-session-jeff-blankenberg-blog-thumb-5.png"},"shorturls":{"share":"https://dpgr.am/3f51f66","twitter":"https://dpgr.am/c9c956c","linkedin":"https://dpgr.am/81098ee","reddit":"https://dpgr.am/5c101a2","facebook":"https://dpgr.am/43755d3"}};
						const file$1B = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/opening-keynote-jeff-blankenberg-principal-technical-evangelist-amazon-alexa-project-voice-x/index.md";
						const url$1B = undefined;
						function rawContent$1B() {
							return "*This is the transcript for the opening keynote presented by Jeff Blankenberg, Principal Technical Evangelist at Amazon Alexa, presented on day one of Project Voice X.* \n\n*The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was **94% accurate.**  Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram’s API.  If you want to see if Deepgram is right for your use case, [contact us](https://deepgram.com/contact-us/).*\n\n\\[Jeff Blankenberg:] I just wanna make sure I’m really clear on that disclaimer because the last thing I need is my PR team saying, what did you say? That’s that’s the worst thing that could happen. Ok. So the future for us is still really fuzzy. We’re looking at it from a perspective like this. Right? We’re out in space, and we know there’s a thing there, but it’s really unclear. It’s it’s it’s quite fuzzy. But what we do know at the center of all of this is people. People are the reason for everything we do. We go to work to create products and services for people. We create free time to spend time with the people in our lives that are important to us.\n\nPeople are the reason for pretty much everything we do in our lives. I want you to imagine for a minute, if you’re skeptical about that point, about being the only person left on earth. For a few of us, this probably sounds awesome. I’m so tired of all the noise and nonsense that some people can bring into our lives. But this guy specifically, his life has changed fundamentally. His entire world is about survival, and how am I gonna get on to the next thing. Because there aren’t people there to create and generate all of the things that we just take for granted in our lives. But it’s not just about the people. Right? It’s also about all the technology that surrounds us.\n\nIn fact, I wanted to think of a world where we had people and technology combined, and I wanted to find a way in which that specifically created a ton of stress and anxiety for all of us. Like, the the mixture of people in technology, and I found one. And so I want you guys to be prepared here when you see this next image ’cause this brings a lot of stress to my life. The incoming call. Anyone else? This this just gives me a little bit of tension, a little anxiety. When you think about the idea that someone’s just calling me out of the blue, no. That’s not how it works anymore. When someone says, hey. You know what? I’ll just call you later. I know what my response is. Chill. Just text me, bro. Like, I don’t need a phone call right now. And I thought a lot about this. Why is it that the incoming phone call is so off putting? Aside from the fact that we’re spammed to death from, you know, extended warranties on our cars and stuff like that,\n\n> I think a lot of it has to do with the fact that we have come to appreciate completely asynchronous time. We, as humans, as people, appreciate the fact that no matter what we’re doing, we want to be able to do the thing we’re focused on, and a phone call, as a very good example, interrupts us. It it says I need to talk to you right now. I need your attention. And as we think about the technology that we use, there’s a lot of those.\n\nRight? How many notifications a day do you get? If you guys looked on your phones, it’ll tell you how many notifications you get. I’m embarrassed by the number. And I’m… I’ve gone through, and I… I’ve tried to mute and turn a lot of them off, and it’s still well over a hundred notifications a day that are just calling for my attention because something happened on Twitter or, you know, whatever it happens to be. Here’s another example of how technology has changed our lives. I don’t know if it’s good or bad, but it’s an interesting one. You’re sitting in a restaurant with your friends, and someone raises a question. Maybe it’s a trivia question. But someone raises a question that probably is unanswerable without a deep, deep set of knowledge. I’ll give you an example of one of those questions. Which actor or actress has won the most Academy Awards? Does anyone know the answer? Good guess? Meryl Streep is the guess. Anyone else has a guess? So nobody knows, but I guarantee someone right now is pulling their phone out there like, let’s who… see who the thing is. Right? Because that’s what we do. We sit at a restaurant, and twenty five years ago, we just had to agree to disagree. Right? There was no looking it up. Nobody was running to the library to look this stuff up. It was just, I guess we’ll never know. By the way, the answer is Catherine Hepburn.\n\nAnd this is the list of people that were probably in your heads also. They all have three. I had to do this because I know you guys can’t leave this stuff unresolved because we, as humans, can’t leave this stuff unresolved. I I need to know that now. That’s a technology problem. And we use all of this technology in a way, like I said, asynchronously to free up our time to do the things we want to do. Right? Some people, I’m not including myself in this, some people love to exercise and get stronger and fit. Some people are artists. Right? They love to… they… they’re optimizing their time around being able to create art, and that comes in a variety of forms. Right? This is a very visible painter artist, but there’s all sorts of different kinds of art. Right?\n\nI have some friends that have gotten incredibly deep into woodworking. They are master woodworkers at this point. I’ve seen the work they’re creating. Beautiful chess boards with gorgeous inlays in them. I would not be able to touch the stuff they’re creating. But they’re not a professional woodworker. This is just what they’re doing with the time that they have. They’re optimizing around what is the thing I really want to do. I’m a programmer. I spend a lot of my time outside of work building things that I care about, that I’m interested in. One of the things that I’m actually building software for is because I got back into the habit of collecting baseball cards. And I wanted a really awesome way to organize them and share them with other people, so I started writing some software to help me do that ’cause there isn’t something like that today.\n\nBut for a lot of us, and there’s nothing wrong with this at all, this is how we optimize our time. I wanna be able to do more of this. I’ve got Netflix and Disney plus and HBO and all the other stuff, and there’s so much I wanna watch. We are in a golden era of television, but we’re optimizing around being able to do these things. So much so in fact, this is from Nielsen’s last fall report, the average American spends over ten hours a day consuming content. We are optimizing around the things we want to do. Ten hours a day is mind blowing if you think about that. Now it’s spread across a bunch of different things, but still, ten hours a day, it’s no wonder we’re also tired. We gotta work. We gotta consume. Somewhere in there, we gotta sleep. But everybody has their own set of priorities, their own interests, their own optimizations that they’re building around. But the thing that all of us in this room have, regardless of our interest or passions, we all have stuff. We all have those things that we have to do, but we don’t want to do. Right?\n\nAnd in many cases, they float around in our heads as thoughts. They’re they’re taking up cycles in our brain’s computer. Sometimes they’re good. You know? I… my kid’s doing really great in school. I wonder if I should have conferences with her teacher. Or it might be something cool like, who won the most Academy Awards or when are we gonna finally land on Mars, and who’s gonna go? Right? You have all these things that are just floating around in your head, but you also have a bunch of these. Right? When’s the last time I went to the doctor? What’s that pain in my side? How are my kids doing in school? Are they doing well? Is my boss happy with my work? He did call me at four thirty on a Friday just to chat. I thought that was weird. Did I leave the oven on? Right? All of these things that are just floating in our heads. We all have those stressors that are just eating up time inside our brains, taking away from all those things we really wanna do.\n\nThis is where I want you to take a big jump forward. I want you to think about the idea that we all, in the future, have a personal artificial intelligence assistant. Now I don’t mean something like Google or Alexa or anything like that. I’m talking about something far more robust, far more impactful than what we see today. With this, I want you to think about what makes that possible. The number one thing that stands out to me is trust. If I were to have a human personal assistant, I would almost need to inherently trust them before I’m gonna start giving them access to portions of my life to run for me. Let’s talk about trust for a second. I want you to imagine for a moment that you’re standing in this field. The sunrise is coming up over the mountains. You can see the glue… the the the dew glistening on all the blades of grass. And out of that forest, all of a sudden, you see me charging at you just running out of the forest. My knees are covered all the way with dew, everything soaked. I look like a crazy person running at you, and I tell you, by the way, you better enjoy that sunrise because that’s the last one there will ever be. Now like all of you are doing now, you’re looking at me like I’m crazy ’cause that’s not likely something that you think is possible. If you think about a sunrise, you have enough evidence over your entire lifetime to say, I don’t believe you for a moment. There’s no way I believe you that the sun isn’t gonna rise tomorrow. You have full faith in the fact that that sun will rise tomorrow.\n\nSo let’s go back to that assistant. Let’s think about all of the things that it could do for us and all the trust that we need to make sure that that’s possible because that falls on us, the people in the technology, the ones that are building all of this stuff. Another good example? How many of you have a pair of Bluetooth headphones? How many of your Bluetooth headphones work every time you try to use them? Right? They’re amazing technology. The fact that I just have this wireless thing, I pop ’em on my head, they connect to my device in my pocket, unbelievable. Ninety five percent of the time. But that five percent doesn’t surprise you because you’ve had a history of experience where it falls down or fails or doesn’t work. That isn’t good enough when we’re talking about artificial intelligence. That isn’t good enough when we’re trusting it to manage parts of our lives, to solve all of these problems for us.\n\nBut if we do, oh my goodness, those Bluetooth headphones, when they work, they’re as good as that sunrise. Right? They’re as good as just standing there taking that fresh air. So let’s talk about some of the things that AI can handle for us literally today if we really wanted it to. The first one for me, this is a big one, nobody will pay their bills anymore. Everyone ok with that? We don’t pay our bills? I don’t physically mean we’re not paying our creditors. What I do mean is I don’t have to worry about it anymore. And I’ve had people challenge me on this, and they say, well, Jeff, you’re in a position where you probably make more money than the bills you have. That’s a nice luxury to have. And my argument is actually specifically for the opposite. I believe that the people that need this help the most, the people that don’t have enough money to even pay their bills are the ones that probably should most rely on artificial intelligence.\n\nPeople will argue all the time that I need to make very subjective decisions about my finances and my money and where I’m gonna save and where I’m gonna put all of these things. But, realistically, you make a lot of subjective decisions that aren’t always good. How many people have said, you know, I need to save more money, but I stopped at Wendy’s. Or how many times have you gone to the grocery store and spent two hundred dollars on your week’s worth of groceries only then swing through McDonald’s on the way home? And it’s like, you’re kinda defeating the purpose there, man. You’re you’re spending on both sides. So the argument is I’m suggesting maybe we could let artificial intelligence start to pay our bills for us, start to make some monetary decisions on our behalf. We, as the human, always have the power to override these things. But another great example of financial instruments is the stock… oh, I’m sorry. I jumped ahead.\n\nAI is a good thing as we think about all this stuff, but it does have its problems. Right? Think about the game of chess. The game of chess for the masters has mostly been ruined. They can still play, and they can still beat most humans all the time. But when it comes to beating AI, it’s really hard. Most of the time, they’re not successful. The stock market is another good example of this. If you think about how the stock market really works, most of the transactions that are happening are happening because of machine learning, because of artificial intelligence. The rent right around the New York Stock Exchange is astronomically high because the length of the wire from that building to the Stock Exchange servers is shorter, and that’s the only limitation to be able to make trades faster and more intelligently at a better pace to make more money. In fact, the stock exchange actually had to introduce miles and miles of cable inside the building just to slow everything down. This is happening today. This isn’t even something that we’re looking towards the future. Most of the trades that are happening in a stock market today are being made by a computer, not by a human.\n\nOn top of that, think about medicine. We’ve taken lots of situations where doctors are diving into cases and saying, oh, this person might have a certain kind of cancer, or they might be diagnosed with a certain disease. But what they found is that by feeding that same data, those same patient records into AI, that they’re finding that these systems can actually diagnose better than doctors. And doctors are doing a phenomenal job, but AI is often able to take the subjectivity out of things and say, no. You know what? Actually, this person looks like they are actually prone for Parkinson’s based on this factor, this factor, this factor across thousands of cases. Something a doctor may have overlooked or not had the ability to dive into the data the right way. That’s a little serious stock markets medicine.\n\nLet’s talk about baseball. Baseball is something that coaches and players have been asking for forever, to eliminate the subjectivity of an umpire. It’s good and bad. Right? There’s good things and bad things about having a subjective umpire. But when it comes to a strike zone, when you watch TV and they draw that box on the TV, and they whip the ball in there and it’s outside the box, but the umpire calls it a strike, that’s frustrating to the fans, to the players, to everybody. So they’re talking about this. They’re experimenting with it at the minor league levels. They’re trying to figure out how they can best do this, and the the solution they’ve come up with so far is an earpiece for the umpire. It beeps if it’s a strike, it doesn’t if it’s a ball. So to the average viewer, the game looks exactly the same. But to the umpire, he has more information than he had before.\n\nAnother good example is making phone calls. We talked about that earlier. But when was the last time you wanted to call your dentist? Anybody? Guessing probably no. My dad’s a dentist, so I have an excuse. But I think for most people, that’s probably not a legitimate thing they like to do. And the reason is is because it takes up our time. It’s doing this mundane task that I don’t really care about. Right? I have to schedule an appointment. If you’ve ever done this, right, you’re sitting in the chair, and they’re like, hey. We’re gonna schedule for your next appointment. It’s six months from now. You pull out your phone, and you look at your calendar six months from now. And if you’re anything like me, six months from now looks wide open.\n\nI have no idea what’s happening six months from now, so, yeah, let’s schedule that for Tuesday nine AM. Sounds great. Two weeks before that, they send me a text message or whatever. They’re like, hey. Just a reminder. Here’s your appointment coming up. I’m like, oh my gosh. Nine AM on a Tuesday. What was I thinking? That’s stupid. So now I’ve gotta call them, and I’ve gotta reschedule everything. So what was the point of the appointment in the first place? Now I’ve gotta call them. I’ll be like, how’s your next Wednesday after three o’clock? I’m sorry. The doctor won’t be in. Ok. How Thursday? Like… and we do this whole dance. Why couldn’t all of that be handled with AI? And instead, I just have an assistant pop up and say, hey. You have a dentist appointment next Tuesday at eight AM. I took into consideration your calendar and the dentist calendar. I even talked to your health care provider to make sure that you were within the six month window that’s appropriate. All of that’s handled for me automatically. And if I wanna reschedule it, I can still do that, but it can handle that stuff for me. And if a meeting pops up on my calendar or some other commitment, it recognizes that and does the process again and reschedules for me. I don’t have to do it.\n\nI also wanna apply that exact same concept to my social life. How amazing would it be if you saw on your calendar that this Friday, you were getting together with some friends you haven’t seen in, like, eight months. Oh my god. That’d be amazing. I didn’t have to call them and, like, schedule and like, oh, I got kids and marching band and what. It just figured all that out because it was able to talk to everybody’s stuff and say, oh, look. They’re both free. Let’s let’s make a reservation even and handle all of that for them. That would be the best. I would have the greatest social life ever because I don’t have to be the one actively calling everyone and trying to schedule things. My last quick example of this is refinancing your house. For those of you that have mortgages, you get a mortgage thirty years, fifteen years, whatever. And over time, interest rates change, and sometimes it’s in your best interest to refinance your house. You’ll save money. But we don’t do that until, like, interest rates drop low enough to make headlines or… we’re not actively paying attention to interest rates every day. Most of us aren’t, anyway.\n\nSo how cool would it be if your assistant just popped up and they’re like, hey. I refinanced your house for you. I saved you seventy bucks a month. Oh my god. That would be amazing. This is where we’re headed. This is the stuff that I’m talking about. But there’s one more thing that we wanna address, and that is human behavior. How many different ways are you aware of to manage your grocery shopping list? Some people have a piece of paper stuck to their fridge. Some people might use the shopping list that’s available on Alexa. Maybe you have one of the nine hundred different apps that are available on your phone.\n\nBut what do all of those require, every one of those things that I just mentioned? They require a behavior change. You actually have to do a thing that you weren’t doing before in order to make this all work. Now I want you to introduce a spouse and children and other people living in your house, and it just gets an order of complexity bigger every single time. So as we think about these things, all of them tell us, all… there’s so many apps out there that are super helpful as long as we’re willing to change our behavior. Right? You always see things like this. Right? Just get a new mindset, and you’ll have all these new results in your life. Right? It’s the same reason that so many New Year’s resolutions are always successful because we’re not gonna change our behaviors.\n\nWe are who we are. We’re going to do the things that we do, and we’re just gonna be frustrated every time we think about, man. I should be better about making a shopping list or doing meal planning or whatever it happens to be. All of these things are really hard. But when we start to put this AI stuff together with our human stuff, it becomes a lot more interesting. And I’ll just give you a couple more examples before I wrap up here. One, the garage door. There are lots and lots of controllers and garage doors and all sorts of things that you can do to set up a way to check and see if your garage door is open or closed. You can even, with your voice in many cases, say, you know, Alexa, close the garage. But those require you to remember to do that. So as I pull out of my driveway and I’m distracted by my phone or whatever, like getting directions to the place I’m going, I forgot to close the garage.\n\nWell, if I’m a person that forgets to close the garage, I’m probably also a person that forgets to check to see if I forgot to close the garage. Right? So why isn’t there already mechanisms in place? And we see this a little bit with our phones and services, like if this then that, where we can ring fence things. Hey. I I just noticed that you left your house, but the garage is still open. I closed it for you. Awesome. I didn’t have to change anything about my behavior, and the thing that I wanted to happen, happened. I’ll give you… actually, this is… isn’t fair. This is a picture of my garage. Right? No. It’s not true. But I want you to imagine for a moment that it is. My garage is proactive, and it uses context. I’ve closed that garage for you. Or I’ve noticed you’re on your way home, I’m going to open it as you’re pulling into your driveway. Now this uses a bunch of context and sensors. Right? Our cars have sensors, our phones have sensors, our house has cameras that could use AI and image recognition to recognize that your car is pulling in the driveway, all sorts of cool things we could do with that.\n\nThis is probably more of… this next example is probably a little more applicable to the ladies in the room. Have one of these at home? Ever had to come back home because you thought you left it plugged in? I know I have, I mean, for my wife. But I’ve come home dozens of times because we pulled out of the driveway and headed down the street and we’re like, did I unplug that? That’s a that’s a fire hazard. I probably should deal with that. These are the kinds of things we’re not gonna change our behavior. She will stand there, she will unplug it and say, I have unplugged the curling iron. And then she’ll go downstairs, get in the car, we drive five hundred feet down the street, did I unplugged that? We just can’t connect those things. Right? That’s not what our brains were designed to do. So I solved this temporarily with a smart plug. The smart plug’s name in our system is fire hazard. So I say turn off the fire hazard or, you know, whatever. That’s a… it’s a nice little simple thing. But I think we’re headed to to a world where the house is gonna be much, much smarter than this.\n\nWe’re gonna have smart fuse boxes and sensors. I even imagine a world potentially where our rooms don’t have light switches because the house is aware enough to know when a light should be on or off. But let’s head back to this shopping list. Think of all of the things that we could do if we didn’t have to change our behavior. If there were sensors in our fridge that could tell us that our seven year old just finished the milk and the Oreos, by the way, well, that tells me two things while I’m out. One, I don’t need to bring him dinner ’cause he ate all the Oreos and milk. But two, I need… I know I need more milk because tomorrow morning, they’re gonna need cereal for breakfast or whatever else it is. So I should stop on my way home. Imagine having that kind of knowledge and power in your life. Proactive notifications are the start of all this. Right? Being able to figure out where we are and what we need at the right time ’cause if I’m out, then I don’t know what’s happening at home. And if I’m at home, I don’t necessarily know what’s happening out in the world. But the first step, of course, is, hey. You’re out of milk. You probably should get some.\n\nThe second step, though, is, hey. I had some milk delivered to your house because you were out. Right? I’m just saving you time. I bought it from the place you wanted for the price you wanted, but milk is on the way. You see, some of this, the early, early days of this are in services like Amazon Dash, and this is not meant to be a commercial at all. I just think this is a really cool service. Printers and refrigerators and all sorts of other things that use consumables, washing machines, when they recognize they’re out of things, they have the smarts built into the device to make a call out to whatever the provider is and get more delivered to you so that you don’t run out. So that shopping… the the washing detergent shows up just at the time you need it. Printer ink shows up just at the time you need it.\n\nLet’s get more proactive. We get in our car… this happened to me just last weekend. I didn’t make a picture of it because I’m not calling any company out for being bad. But I was heading to a company called Costco. Maybe you’ve heard of them. And they have a late opening time. I’m an early riser. I’m usually up and, like, trying to get stuff done by, like, seven AM most days. I was like you know what? We’re out of this stuff. We have people coming over. I’m just gonna run over to Costco this morning. It was maybe eight fifteen. Costco doesn’t open ’til ten. So I got in my car, typed in the GPS, head my way to Costco, pull into the parking lot, and it’s a ghost town ’cause I didn’t think to check and see when they open. Why… at Saturday morning at eight thirty, why wouldn’t you be open? Well, ’cause they don’t open ’til ten. They take good they take good care of their employees. But if I had had a little bit more information, a little bit more artificial intelligence to help me understand this, I wouldn’t have been staring at this sign saying, sorry, man. You… you’re gonna have to wait two hours ’til you can get in here.\n\nThe other cool thing about this is as we talk with our cars and our phones and our echo devices or whatever else you have in your home, every one of them today has a different assistant. Right? BMW has their own assistant, Alexa has their own assistant, Google has their own assistant, Bixby, everybody has their own thing. That’s not a bad world. We want everybody to be able to build their own technology. We have an entire technology called Alexa Custom Assistant that let you build your own thing on top of the same technology that Alexa uses. But the thing that I’m most excited about, and I think that a lot of you who are here should consider, is the idea of the voice inter… interoperability initiative. If you don’t know about this, this is the simple idea that every assistant should work on every device. So if you need to talk to a specific assistant, your car, whatever it happens to be, you can talk to them. And when they don’t know the answer, they’ll hand off to the one that does know. But you’re not limited by talking to each individual device for a specific task. It’s ambient computing. It’s meant to be where you are, when you are. So let’s take a step back. We have all of those crazy thoughts going on in our heads.\n\nLet’s go back to a peaceful place. Let’s think about a world where we have artificial intelligence solving all of these problems for us. Our bills are paid. Our kids are where they need to be. Our our fridge is stocked. Our appointments are made. This is the future that I’m the most excited about because problems are being solved without my direct input every single time. It frees me up to do the things that I want to do. And the reason that I’m so excited about this is because we see the beginnings of this. Right? Today, as we talk about voice, this is the beginning. It’s not always just gonna be about voice, but it’s about the technology that powers all of it. We always say at Amazon that it’s day one. Every single day is day one, and that’s not necessarily wrong. If you think about building skills for Alexa, it is day one. We’re still really thinking about where is all this stuff gonna go and how is it gonna work.\n\nBradley was talking about voice payments, and how is that… how that problem is gonna be growing and growing and growing over time. But it all comes back to that story about trust. Do people trust to make purchases with their voice? We’re starting to see a change. It’s good. But I want that whole world in my hands. I want access to all of that technology. I wanna set it up once and forget it so that I can go do the things that I wanna do. So I can go walk on the beach later this afternoon without having to worry about a bunch of other stuff. That’s the future that I’m excited about, and I hope that I get to see all of you there too. So with that, thank you. As as Bradley is making his way up, I am literally Jeff Blankenberg everywhere. I mean, everywhere including, like, TikTok. So if you guys wanna find me, if you wanna connect with me, I would love to meet every single one of you while we’re here, and I wanna thank you guys for all of your attention. One last thing I’ll mention as you come up, we do have a community of people that are thinking about technology just like this. And if you’d like to join us, it is very Alexa-centric for the most part, but people are talking about the big issues. You can join us at alexa dot design slash slack. This is a Slack community for people just like all of us to talk about what’s going on, how to build things, how to interact, and I think it’s a place that all of you would be welcome as well. So thank you. Certainly.";
						}
						async function compiledContent$1B() {
							return load$1B().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1B() {
							return (await import('./chunks/index.f9cfd764.mjs'));
						}
						function Content$1B(...args) {
							return load$1B().then((m) => m.default(...args));
						}
						Content$1B.isAstroComponentFactory = true;
						function getHeadings$1B() {
							return load$1B().then((m) => m.metadata.headings);
						}
						function getHeaders$1B() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1B().then((m) => m.metadata.headings);
						}

const __vite_glob_0_175 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1B,
  file: file$1B,
  url: url$1B,
  rawContent: rawContent$1B,
  compiledContent: compiledContent$1B,
  default: load$1B,
  Content: Content$1B,
  getHeadings: getHeadings$1B,
  getHeaders: getHeaders$1B
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1A = {"title":"Technical Writing: Optimize Your Content Creation","description":"We spend a lot of time working on developer content. Optimizing your writing by creating a content creation life cycle can help to ensure a larger audience and greater value for the time you put into your writing.","date":"2022-04-04T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1654610025/blog/2022/04/optimizing-your-content/sharing-your-technical-blog-posts%402x.jpg","authors":["bekah-hawrot-weigel"],"category":"best-practice","tags":["technical-writing"],"seo":{"title":"Technical Writing: Optimize Your Content Creation","description":"We spend a lot of time working on developer content. Optimizing your writing by creating a content creation life cycle can help to ensure a larger audience and greater value for the time you put into your writing."},"shorturls":{"share":"https://dpgr.am/8336533","twitter":"https://dpgr.am/87b6ca7","linkedin":"https://dpgr.am/f22293e","reddit":"https://dpgr.am/4ed5a67","facebook":"https://dpgr.am/b318ee1"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454067/blog/optimizing-your-content/ograph.png"}};
						const file$1A = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/optimizing-your-content/index.md";
						const url$1A = undefined;
						function rawContent$1A() {
							return "\r\nWe spend a lot of time writing technically as developers. With the trends of [learning](https://swyx.io/learn-in-public) and [building](https://www.ceros.com/inspire/originals/building-in-public/) in public, we’re creating more content than ever. All of this technical content creation requires both knowledge and time to create the content. With different platforms, you also have to know how to navigate through various processes and genres. As technical writers and developers, this can be a challenge. Optimizing how we package our technical writing can help to ensure a larger audience and greater value for the time we put into our writing.\r\n\r\n## Content Creation Lifecycle\r\n\r\nEnter in the Content Creation lifecycle. What does that mean? Well, let’s start with a brainstorming session. There are three questions we’re going to answer:\r\n\r\n*   What’s your process?\r\n*   Where do you post?\r\n*   What content are you comfortable creating?\r\n\r\n## Your Process\r\n\r\nWhen you’re writing your post, how do you get started? Do you tweet about it? Ask others what they want to talk about? That’s a good way to gauge interest and let others know you’re working on it.\r\n\r\nDo you research the topic? How do you research it? Do you talk to others about the topic? Do you start drafting with an outline or just start writing?\r\n\r\nHow can you be more public with your writing process? This is the first step in optimizing your content.\r\n\r\n## Posting Your Content\r\n\r\nOnce you've completed your writing, take advantage of the many places to gain visibility. If you’re posting your blog and not sharing anywhere else, you’re losing your reach. For example, I post on [my own site](https://bekahhw.github.io/) and [Dev.to](https://dev.to/bekahhw) or on [the Deepgram Developers blog](https://blog.deepgram.com/) and Dev.to links on Twitter and LinkedIn and sometimes Instagram and Polywork. I’m familiar with those platforms, audiences, and how to post. You might also post on reddit, Hashnode, Facebook--there’s no short list of where to post.\r\n\r\n## Types of Content\r\n\r\nUp above I mentioned the type of content you’re comfortable creating. I say that but I really mean “what content can you tolerate creating?” For example, I’m super uncomfortable livestreaming, but I can tolerate doing it. Blogging is my happy place, so this is where I start. You might be more comfortable creating a YouTube video, so that video could be your starting point.\r\n\r\nMy suggestion is to list what you’re comfortable creating and then set some reach goals.\r\n\r\n## Putting it into action\r\n\r\nNow, we’re going to create your content creation lifecycle. Let’s venture back to your process. How can you utilize content creation in your discovery process? For example, if I’m writing a post on “Storytelling in Writing,” I’ll do research to ensure I grasp the topic. Here’s how I’d break down the content creation strategy:\r\n\r\n### Exploration Phase\r\n\r\nThe first step of my process is doing some research. That research includes reading articles and maybe watching some videos.\r\n\r\nMy next step will involve reaching out to my network. To do this, I could create a strategy to build my resources:\r\n\r\n1.  Create a message in my most active social media space. In this case, it’s Twitter:\r\n\r\n![image of tweet asking about storytelling in writing](https://res.cloudinary.com/deepgram/image/upload/v1654610029/blog/2022/04/optimizing-your-content/tweet.png)\r\n\r\n2.  Hold a Twitter Space: You’ll need to do some research to help create productive questions, but by inviting other experts in the field, you’ll be actively doing research and creating a content creation path for your technical writing. For example, you might build on the previous tweet with a [Twitter Space on Storytelling in Writing](https://twitter.com/DeepgramDevs/status/1508871335971278852?s=20\\&t=J4iC2JD-azMXNDk1rWr4ZA) like I did to help you develop your understanding of the topic.\r\n\r\n### Writing Phase\r\n\r\nAfter the exploration phase, I have a pretty good sense of the direction I want to take things. This is the perfect time to start drafting my blog post. Within that post, I can link to previous posts I’ve written like [Technical Writing: A Beginner's Guide](https://blog.deepgram.com/technical-writing-a-beginners-guide/) because it’s relevant to the topic of Storytelling-it’s useful to know writing basics before telling the story.\r\n\r\n### Repurpose Phase\r\n\r\nContent doesn’t have to stop when you hit that publish button. In fact, you have momentum you can continue to build on. Some options for repurposing your content:\r\n\r\n*   Livestream: You can do a project walk through or a Q\\&A.\r\n*   YouTube Video: Turn your blog post into a lightning talk or [short video like this one I created](https://www.youtube.com/shorts/aqtX-xAL4gc).\r\n*   Podcast: Turn your Twitter Space into a podcast\r\n*   TikTok/Instagram Reel: Create a fun or informative short video highlighting the main point of your writing.\r\n*   Give a talk: You’re already prepared. Depending on your audience, the material may need updated, but you’re close to being done if the talk is short.It can be a lunch and learn, a conference talk, or something you put together yourself.\r\n*   Wiki: Can your post be grouped with other posts? Is there a place they can live together to provide greater context to an overall subject? Grouping it with other posts can help you to maximize your visibility for your other posts as well.\r\n\r\n### Content Creation Tools\r\n\r\nIt’s worth noting that using tools that help you to repost content can make this whole process seem a lot easier and can provide inspiration for how to repurpose your content. For example, I use [Canva](https://www.canva.com/) a lot. There’s a free version and a paid version, but both give you tools to edit images and video in a variety of ways.\r\n\r\nAny audio or video you create for your projects can be repurposed through their transcripts too. For example, if you’re publishing a video you can [use Deepgram to get the transcripts of your pre-recorded audio](https://developers.deepgram.com/documentation/getting-started/prerecorded/) and use those transcripts to create an image of a quote, to create the rough draft of your blog post, or as part of the description for an Instagram post.\r\n\r\nWith so many social media options out there, there are many ways to optimize your technical writing. Where you start is up to you, but it’s often easiest to start where you’re most comfortable and then maximize your content by creating different paths to the work you want to highlight.\r\n\r\nWhat does your content creation lifecycle looks like? Let's us know on our [@DeepgramDevs](https://twitter.com/DeepgramDevs) Twitter.\r\n\r\n        ";
						}
						async function compiledContent$1A() {
							return load$1A().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1A() {
							return (await import('./chunks/index.63a21ba8.mjs'));
						}
						function Content$1A(...args) {
							return load$1A().then((m) => m.default(...args));
						}
						Content$1A.isAstroComponentFactory = true;
						function getHeadings$1A() {
							return load$1A().then((m) => m.metadata.headings);
						}
						function getHeaders$1A() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1A().then((m) => m.metadata.headings);
						}

const __vite_glob_0_176 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1A,
  file: file$1A,
  url: url$1A,
  rawContent: rawContent$1A,
  compiledContent: compiledContent$1A,
  default: load$1A,
  Content: Content$1A,
  getHeadings: getHeadings$1A,
  getHeaders: getHeaders$1A
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1z = {"title":"Playing With P5.js: Creating a Voice-Controlled Game","description":"In this post, we'll build a voice-controlled game with P5.js and Deepgram's Speech Recognition API. Learn how now.","date":"2022-03-22T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1646925972/blog/2022/03/p5js-deepgram-game/series-cover.jpg","authors":["kevin-lewis"],"category":"tutorial","tags":["javascript","p5js","beginner"],"seo":{"title":"Playing With P5.js: Creating a Voice-Controlled Game","description":"In this post, we'll build a voice-controlled game with P5.js and Deepgram's Speech Recognition API. Learn how now."},"shorturls":{"share":"https://dpgr.am/c11f55a","twitter":"https://dpgr.am/a0f09ef","linkedin":"https://dpgr.am/ebf80d1","reddit":"https://dpgr.am/7e24086","facebook":"https://dpgr.am/7c4f65d"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454040/blog/p5js-deepgram-game/ograph.png"}};
						const file$1z = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/p5js-deepgram-game/index.md";
						const url$1z = undefined;
						function rawContent$1z() {
							return "\nThis is the final part in a series on P5.js (from here 'P5') - a creative coding library that makes working with the Canvas API much easier. [In part one](https://blog.deepgram.com/p5js-getting-started/), we covered how to draw elements on the screen and react to keyboard and mouse input. We learned how to create common game features in [part two](https://blog.deepgram.com/blog/2022/03/p5js-game-logic/) - collision detection, entity management, and state management.\n\nIn today's tutorial, we'll bring together everything we know to create a voice-controlled game - [try the game out now](https://deepgram-p5-game.glitch.me/). A new enemy appears coming from one of four directions and begins moving towards you every few seconds. Each direction has a random word associated with it, and if said correctly, a bullet will fly in that direction. If an enemy reaches you, the game is over.\n\nThe final code for today's project can be found on [GitHub](https://github.com/deepgram-devs/playing-with-p5).\n\n## Before We Start\n\nYou will need a Deepgram API Key - [get one here](https://console.deepgram.com/signup?jump=keys).\n\n## Setting Up State\n\nOn your computer, create a new directory and open it in your code editor. Create an `index.html` file and add the following to it:\n\n```html\n<!DOCTYPE html>\n<html>\n  <head></head>\n  <body>\n    <script src=\"https://cdn.jsdelivr.net/npm/p5@1.4.1/lib/p5.js\"></script>\n    <script>\n      // Global Variable Section Starts\n      let playerSize = 50\n      let score = 0\n      let gameOver = false\n      // Global Variable Section Ends\n\n      function setup() {\n        createCanvas(1000, 1000)\n        frameRate(30)\n      }\n\n      function draw() {\n        background('black')\n        translate(width / 2, height / 2)\n\n        fill('white')\n        textSize(24)\n        textAlign(RIGHT)\n        text(`Score: ${score}`, width / 2 - 20, height / 2 - 20)\n\n        if (!gameOver) {\n          fill('white')\n          circle(0, 0, playerSize)\n\n          // Game logic goes here\n        } else {\n          fill('white')\n          textSize(36)\n          textAlign(CENTER)\n          text(`Game over! Score: ${score}`, 0, 0)\n        }\n      }\n    </script>\n  </body>\n</html>\n```\n\nIn the second post in this series, you learned how to [keep score and show a game over screen](https://blog.deepgram.com/blog/2022/03/p5js-game-logic/) - we are using both approaches here.\n\nThe only new thing here is `translate(width/2, height/2)`, which moves the origin (0, 0) to the center of the canvas. This means the top-left is now (-500, -500), and the bottom-right is (500, 500). It makes sense to do this when entities often need to refer to the center position.\n\n## Create Enemies\n\nAt the bottom of your `<script>`, create a new `Enemy` class:\n\n```js\nclass Enemy {\n  constructor(direction, distance) {\n    this.direction = direction\n    this.size = 25\n    this.x = 0\n    this.y = 0\n\n    if (this.direction == 'UP') this.y = -Math.abs(distance)\n    if (this.direction == 'RIGHT') this.x = distance\n    if (this.direction == 'DOWN') this.y = distance\n    if (this.direction == 'LEFT') this.x = -Math.abs(distance)\n  }\n\n  move() {\n    if (this.direction == 'UP') this.y++\n    if (this.direction == 'RIGHT') this.x--\n    if (this.direction == 'DOWN') this.y--\n    if (this.direction == 'LEFT') this.x++\n  }\n\n  touchedPlayer() {\n    const d = dist(this.x, this.y, 0, 0)\n    if (d < playerSize / 2 + this.size / 2) gameOver = true\n  }\n\n  display() {\n    fill('gray')\n    ellipse(this.x, this.y, this.size)\n  }\n}\n```\n\nWhen an instance is created, you must provide two arguments - `direction` - one of `'UP'`, `'DOWN'`, `'LEFT'`, or `'RIGHT'`, and `distance` - which dictates how far away from the center point the enemy should spawn.\n\nIn the `constructor`, the enemies are initially placed, and in `move()` they move one pixel closer to the center. `touchedPlayer()` uses collision detection -- we [learned about that last week](https://blog.deepgram.com/blog/2022/03/p5js-game-logic/) -- to set `gameOver` to `true` if an enemy touches the player in the center of the canvas. Finally, the enemy is drawn at its new (x, y) position.\n\nIn your global variable section, add these line:\n\n```js\nlet directions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\nlet enemies = []\n```\n\nAt the bottom of your `setup()` function, begin spawning enemies randomly every 2-5 seconds:\n\n```js\nsetInterval(() => {\n  enemies.push(new Enemy(random(directions), width / 4, width / 2))\n}, random(2000, 5000))\n```\n\nThe first argument will be randomly chosen from the `directions` array you just created. The final step is to loop through all existing enemies and run their methods in `draw()`. In your game logic section, add this code:\n\n```js\nfor (let enemy of enemies) {\n  enemy.move()\n  enemy.touchedPlayer()\n  enemy.display()\n}\n```\n\nOpen `index.html` in your browser, and it should look like this:\n\n![A black square with a small white circle in the middle. The bottom-right reads 'Score: 0'. Small gray circles representing enemies appear either above, below, left, or right, and move towards the center. An enemy touches the center circle and the screens ays \"Game over\"](https://res.cloudinary.com/deepgram/image/upload/v1646925990/blog/2022/03/p5js-deepgram-game/enemy-spawn.gif)\n\n## Create Bullets\n\nCurrently, there's no way to defend yourself. When a player presses their arrow keys, a new bullet will be created in that direction.\n\nAt the bottom of your `<script>`, create a new `Bullet` class. It should look familiar as it works largely the same as the `Enemy` class:\n\n```js\nclass Bullet {\n  constructor(direction) {\n    this.direction = direction\n    this.size = 5\n    this.speed = 6\n    this.x = 0\n    this.y = 0\n    this.spent = false\n  }\n\n  move() {\n    if (this.direction == 'UP') this.y -= this.speed\n    if (this.direction == 'RIGHT') this.x += this.speed\n    if (this.direction == 'DOWN') this.y += this.speed\n    if (this.direction == 'LEFT') this.x -= this.speed\n  }\n\n  touchedEnemy() {\n    for (let enemy of enemies) {\n      const d = dist(enemy.x, enemy.y, this.x, this.y)\n      if (d < this.size / 2 + enemy.size / 2) {\n        enemies = enemies.filter((e) => e != enemy)\n        this.spent = true\n        score++\n      }\n    }\n  }\n\n  display() {\n    fill('red')\n    ellipse(this.x, this.y, this.size)\n  }\n}\n```\n\nIf an enemy is hit, it is removed from the `enemies` array, and the bullet's `this.spent` value becomes `true`. In the global variable section, add a new array for bullets:\n\n```js\nlet bullets = []\n```\n\nUnderneath our `enemies` loop in `draw()`, add a loop for `bullets`:\n\n```js\nfor (let bullet of bullets) {\n  if (!bullet.spent) {\n    bullet.move()\n    bullet.touchedEnemy()\n    bullet.display()\n  }\n}\n```\n\nIf the bullet has been spent, it won't be shown or run its collision detection logic. This means a bullet can only successfully hit an enemy once.\n\nSo far, you have used the P5 `preload()`, `setup()`, and `draw()` functions, but there are a host more that are triggered based on user input.\n\nUnlike the `keyIsPressed` variable which is true every frame that a key is pressed, the built-in `keyPressed()` function is triggered only once when a user presses a key on their keyboard. In order to trigger the function twice, two distinct presses need to be made - much better for bullet firing. After you end the `draw()` function, add this:\n\n```js\nfunction keyPressed() {\n  if (key == 'ArrowLeft') bullets.push(new Bullet('LEFT'))\n  if (key == 'ArrowRight') bullets.push(new Bullet('RIGHT'))\n  if (key == 'ArrowUp') bullets.push(new Bullet('UP'))\n  if (key == 'ArrowDown') bullets.push(new Bullet('DOWN'))\n}\n```\n\nThat's the core game finished. Here's how it looks (recording is sped up):\n\n![As enemies appraoch the enemy, tiny red dots are fired out of the center player and towards enemies. When they hit an enemy, the bullet and the enemy disappear, and the score goes up by one.](https://res.cloudinary.com/deepgram/image/upload/v1646925990/blog/2022/03/p5js-deepgram-game/bullet-firing.gif)\n\n## Add Word Prompts\n\nCreate a new file called `words.js`, and copy and paste the content from [this file on GitHub](https://github.com/deepgram-devs/playing-with-p5/blob/main/words.js). This is a slight reformatting of the [repository](https://github.com/adamjgrant/Random-English-Word-Generator-42k-Words-) of over 42,000 English words.\n\nAs a note, this is a pretty long word list and includes some pretty long and complex words. You may want to experiment with the word selection you use to alter the difficulty.\n\nJust before the `<script>` tag with our P5 logic, include the `words.js` file:\n\n```html\n<script src=\"words.js\"></script>\n```\n\nThen, in your main `<script>` tag with our P5 logic, add the following:\n\n```js\nfunction getRandomWord() {\n  return words[Math.floor(Math.random() * 42812)]\n}\n```\n\nThis function gets one word at random and returns the string. You can add it anywhere, but I tend to add these utility functions to the very bottom of my `<script>`.\n\nIn your global variable section, store four random words:\n\n```js\nlet currentWords = {\n  UP: getRandomWord(),\n  DOWN: getRandomWord(),\n  LEFT: getRandomWord(),\n  RIGHT: getRandomWord(),\n}\n```\n\nJust after your `bullet` loop in the game logic section, draw the four random words to the canvas:\n\n```js\nfill('white')\ntextSize(24)\ntextAlign(CENTER)\ntext(currentWords.UP, 0, -height / 2 + 48)\ntext(currentWords.DOWN, 0, height / 2 - 48)\ntextAlign(RIGHT)\ntext(currentWords.RIGHT, width / 2 - 48, 0)\ntextAlign(LEFT)\ntext(currentWords.LEFT, -width / 2 + 48, 0)\n```\n\nFinally, in the `Bullet.touchedEnemy()` function, where we increment the score, replace a word when an enemy is hit:\n\n```js\ncurrentWords[enemy.direction] = getRandomWord()\n```\n\n![As enemies are hit, the word in their direction changes.](https://res.cloudinary.com/deepgram/image/upload/v1646925989/blog/2022/03/p5js-deepgram-game/words-change.gif)\n\n## Shoot Bullets With Your Voice\n\nIt's time to create bullets with your voice! A persistent WebSocket connection will be made with Deepgram, allowing Deepgram to constantly listen to your mic to hear what you say.\n\nThis part of the tutorial will assume you know how to do live browser transcription with Deepgram. If not, we have a [written and video tutorial available](https://blog.deepgram.com/live-transcription-mic-browser/) that explains every step in more detail.\n\nIn your global variable section, create one final value so we can display to the user what was heard:\n\n```js\nlet heard = ''\n```\n\nAt the very bottom of your `<script>`, add this:\n\n```js\nnavigator.mediaDevices.getUserMedia({ audio: true }).then((stream) => {\n  const mediaRecorder = new MediaRecorder(stream)\n  const socket = new WebSocket('wss://api.deepgram.com/v1/listen', [\n    'token',\n    'YOUR-DEEPGRAM-API-KEY',\n  ])\n\n  socket.onopen = () => {\n    mediaRecorder.addEventListener('dataavailable', async (event) => {\n      if (event.data.size > 0 && socket.readyState == 1) socket.send(event.data)\n    })\n    mediaRecorder.start(1000)\n  }\n\n  socket.onmessage = (message) => {\n    const received = JSON.parse(message.data)\n    const transcript = received.channel.alternatives[0].transcript\n    if (transcript && received.is_final) {\n      heard = transcript\n      for (let direction in currentWords) {\n        if (transcript.includes(currentWords[direction])) {\n          bullets.push(new Bullet(direction))\n        }\n      }\n    }\n  }\n})\n```\n\nRemember to provide your Deepgram API Key when creating the `socket`. At the bottom of this code, a check determines whether any of the directional words were heard and, if so, creates a bullet in that direction.\n\nFinally, show the user what was heard just under all of the `text()` statements in `draw()`:\n\n```js\nfill('green')\nif(`heard) text(`We heard \"${heard}\"`, -width/2+20, height/2-20)`\n```\n\n## In Summary\n\nThe fact it was so little code to integrate voice control into this game should be a testament to how easy [Deepgram's Speech Recognition API](https://developers.deepgram.com/documentation/) is to use.\n\nOnce again, a live version of the game can be found [here](https://deepgram-p5-game.glitch.me/) and the final codebase on [GitHub](https://github.com/deepgram-devs/playing-with-p5).\n\nIf you want to deploy your own, I encourage you to also read how to [protect your API Key](https://blog.deepgram.com/protecting-api-key/) when doing live transcription directly in your browser.\n\nIf you have any questions, please feel free to reach out to us on Twitter at [@DeepgramDevs](https://twitter.com/DeepgramDevs).\n\n        ";
						}
						async function compiledContent$1z() {
							return load$1z().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1z() {
							return (await import('./chunks/index.54265ca5.mjs'));
						}
						function Content$1z(...args) {
							return load$1z().then((m) => m.default(...args));
						}
						Content$1z.isAstroComponentFactory = true;
						function getHeadings$1z() {
							return load$1z().then((m) => m.metadata.headings);
						}
						function getHeaders$1z() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1z().then((m) => m.metadata.headings);
						}

const __vite_glob_0_177 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1z,
  file: file$1z,
  url: url$1z,
  rawContent: rawContent$1z,
  compiledContent: compiledContent$1z,
  default: load$1z,
  Content: Content$1z,
  getHeadings: getHeadings$1z,
  getHeaders: getHeaders$1z
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1y = {"title":"Playing With P5.js: Implementing Game Logic","description":"In this post, we'll use P5.js to implement collision detection, manage entities with classes, and handle game state. Learn more now.","date":"2022-03-15T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1646780523/blog/2022/03/p5js-game-logic/Playing-with-p5js%402x.jpg","authors":["kevin-lewis"],"category":"tutorial","tags":["javascript","p5js","beginner"],"seo":{"title":"Playing With P5.js: Implementing Game Logic","description":"In this post, we'll use P5.js to implement collision detection, manage entities with classes, and handle game state. Learn more now."},"shorturls":{"share":"https://dpgr.am/5e8b30b","twitter":"https://dpgr.am/f15c93a","linkedin":"https://dpgr.am/13a6df7","reddit":"https://dpgr.am/060133c","facebook":"https://dpgr.am/9b5bbd1"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454042/blog/p5js-game-logic/ograph.png"}};
						const file$1y = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/p5js-game-logic/index.md";
						const url$1y = undefined;
						function rawContent$1y() {
							return "\r\nThis is the second in a three-part series on P5.js (from here 'P5') - a creative coding library that makes working with the Canvas API much easier. [In part one](https://blog.deepgram.com/p5js-getting-started/), we covered how to draw elements on the screen and react to keyboard and mouse input.\r\n\r\nToday, we're taking that theoretical knowledge and building some features you may need when creating a game. Then, in next week's final part, we will add voice functionality to our game using Deepgram.\r\n\r\n## Collision Detection\r\n\r\nEvery element you draw in a P5 sketch has a specific placement and size. Collision detection in games lets you know when one element overlaps with another or touches a location such as a wall. This is often used to avoid users going through walls or floors or to 'pick up' items such as food or hearts.\r\n\r\nAssuming a collision check between you (the 'player') and another entity (a 'pick up'), a collision detection relies on four conditional checks:\r\n\r\n1.  Is your x position greater than the leftmost x position of the pickup?\r\n2.  Is your x position less than the rightmost x position of the pickup?\r\n3.  Is your y position greater than the topmost y position of the pickup?\r\n4.  Is your y position less than the bottommost y position of the pickup?\r\n\r\nLet's start putting this into practice. Create an `index.html` file, open it in your code editor, and add the following to it:\r\n\r\n```html\r\n<!DOCTYPE html>\r\n<html>\r\n<head></head>\r\n<body>\r\n    <script src=\"https://cdn.jsdelivr.net/npm/p5@1.4.1/lib/p5.js\"></script>\r\n    <script>\r\n        const pickupX = 200\r\n        const pickupY = 50\r\n        const pickupSize = 100\r\n\r\n        function setup() {\r\n            createCanvas(500, 200)\r\n        }\r\n\r\n        function draw() {\r\n            background(100)\r\n\r\n            const collisionX = mouseX>pickupX && mouseX<pickupX+pickupSize\r\n            const collisionY = mouseY>pickupY && mouseY<pickupY+pickupSize\r\n            if(collisionX && collisionY) fill('green')\r\n            else fill('red')\r\n\r\n            square(pickupX, pickupY, pickupSize)\r\n        }\r\n    </script>\r\n</body>\r\n</html>\r\n```\r\n\r\nTo see your sketch running, just double click the `index.html` file in your file explorer and it will open in your default browser. To see new changes once you save your code, refresh the browser.\r\n\r\n![A gray canvas has a red square in the middle. When the mouse cursor touches the box, the square goes green.](https://res.cloudinary.com/deepgram/image/upload/v1646780525/blog/2022/03/p5js-game-logic/mouse-collision.gif)\r\n\r\nIf the player is bigger than a single pixel point, you need to offset the conditionals by the size of the player. Try this:\r\n\r\n```js\r\nconst pickupX = 225\r\nconst pickupY = 75\r\nconst pickupSize = 50\r\nconst playerSize = 50\r\n\r\nfunction setup() {\r\n    createCanvas(500, 200)\r\n}\r\n\r\nfunction draw() {\r\n    background(100)\r\n\r\n    fill('black')\r\n    square(pickupX, pickupY, pickupSize)\r\n\r\n    const collisionX = mouseX>pickupX-pickupSize && mouseX<pickupX+pickupSize\r\n    const collisionY = mouseY>pickupY-pickupSize && mouseY<pickupY+pickupSize\r\n    if(collisionX && collisionY) fill('green')\r\n    else fill('white')\r\n\r\n    square(mouseX, mouseY, playerSize)\r\n}\r\n```\r\n\r\n![On the bottom-right of the cursor is a white box which moves with the cursor. When it touches the box in the middle, the box goes green.](https://res.cloudinary.com/deepgram/image/upload/v1646780526/blog/2022/03/p5js-game-logic/square-collision.gif)\r\n\r\nIf you want to learn more about collision detection, check out [this lovely video](https://www.youtube.com/watch?v=uAfw-ko3kB8) by Dan Shiffman.\r\n\r\n### Example: Blocking Walls\r\n\r\nThe P5-provided `width` and `height` variables are always set to the canvas values provided in `createCanvas()`. You can use these along with the collision detection conditionals above to ensure a user cannot navigate outside of the canvas.\r\n\r\nExpanding on our keyboard user input introduction in [last week's post](https://blog.deepgram.com/p5js-getting-started/), try this:\r\n\r\n```js\r\nlet playerX = 20\r\nlet playerY = 20\r\nconst playerSize = 10\r\n\r\nfunction setup() {\r\n    createCanvas(500, 200)\r\n}\r\n\r\nfunction draw() {\r\n    background(100)\r\n\r\n    if(keyIsPressed) {\r\n        if(key == 'ArrowLeft') playerX -= 1\r\n        if(key == 'ArrowRight') playerX += 1\r\n        if(key == 'ArrowUp') playerY -= 1\r\n        if(key == 'ArrowDown') playerY += 1\r\n    }\r\n\r\n    // Not allowing out-of-bounds values\r\n    if(playerX < 0) playerX = 0\r\n    if(playerX > width - playerSize) playerX = width - playerSize\r\n    if(playerY < 0) playerY = 0\r\n    if(playerY > height - playerSize) playerY = height - playerSize\r\n\r\n    square(playerX, playerY, playerSize)\r\n}\r\n```\r\n\r\nIf a player attempts to set `playerX` or `playerY` outside of the allowed bounds, they are set at the bounds. This means a player will see their square stop moving.\r\n\r\n## Entity Management\r\n\r\nGames often have many entities: players, enemies, and items. Entities of the same category likely have similar logic but need to maintain their own state. In P5 sketches, it's common to use JavaScript classes for game entity management. Classes provide a blueprint for an object. They have their own properties, including data and functions (called 'methods' in a class). Try this code, and then we'll walk through it:\r\n\r\n```js\r\nconst bubbles = []\r\n\r\nfunction setup() {\r\n    createCanvas(500, 200)\r\n    for(let i = 0; i < 100; i++) {\r\n        bubbles.push(new Bubble(250, 100))\r\n    }\r\n}\r\n\r\nfunction draw() {\r\n    background(100)\r\n    for(let bubble of bubbles) {\r\n        bubble.move()\r\n        bubble.display()\r\n    }\r\n}\r\n\r\nclass Bubble {\r\n    constructor(x, y) {\r\n        this.x = x\r\n        this.y = y\r\n        this.xOff = random(0, 1000)\r\n        this.yOff = random(0, 1000)\r\n    }\r\n\r\n    move() {\r\n        this.xOff += 0.01\r\n        this.yOff += 0.01\r\n\r\n        this.x = noise(this.xOff) * width\r\n        this.y = noise(this.yOff) * height\r\n    }\r\n\r\n    display() {\r\n        circle(this.x, this.y, 5)\r\n    }\r\n}\r\n```\r\n\r\n![On a gray canvas, 100 small white circles move around.](https://res.cloudinary.com/deepgram/image/upload/v1646780529/blog/2022/03/p5js-game-logic/perlin.gif)\r\n\r\nStarting at the bottom with the `Bubble` class. When a new class instance is created, it expects a starting x and y value, which is made available inside of the class as member properties called `this.x` and `this.y`. Two other member properties are also created - `xOff` (x offset) and `yOff` (y offset). More on these later.\r\n\r\nThis class has two methods - you can name methods whatever you want, but `move` and `display` are common in P5 sketches.\r\n\r\nThe `move()` method uses the P5-provided `noise()` function to return a value in a Perlin noise sequence. Perlin noise generates a random value that exists in a more natural-looking sequence - by very slightly modifying the value passed into `noise()`, the bubbles look to follow a 'path.' The small changes in `xOff` and `yOff` are used to move the bubbles smoothly. Perlin noise is fascinating, and I encourage you to [read more about `noise()`](https://p5js.org/reference/#/p5/noise).\r\n\r\nThe `display()` method draws a circle at the new values stored in `this.x` and `this.y`.\r\n\r\nDuring `setup()`, 100 `Bubble` instances are created with a starting position of `(250, 100)` and stored in the `bubbles` array. Every `draw()`, each `bubble` has it's `move()` and `display()` methods run.\r\n\r\nThe next example combines collision detection and entity management:\r\n\r\n```js\r\nconst bubbles = []\r\n\r\nfunction setup() {\r\n    createCanvas(500, 200)\r\n    frameRate(10)\r\n    for(let i = 0; i < 10; i++) {\r\n        bubbles.push(new Bubble(250, 100))\r\n    }\r\n}\r\n\r\nfunction draw() {\r\n    background(100)\r\n    for(let bubble of bubbles) {\r\n        bubble.move()\r\n        bubble.checkIfTouched()\r\n        bubble.display()\r\n    }\r\n}\r\n\r\nclass Bubble {\r\n    constructor(x, y) {\r\n        this.x = x\r\n        this.y = y\r\n        this.xOff = random(0, 1000)\r\n        this.yOff = random(0, 1000)\r\n\r\n        this.radius = 10\r\n        this.touched = false\r\n    }\r\n\r\n    move() {\r\n        this.xOff += 0.01\r\n        this.yOff += 0.01\r\n\r\n        this.x = noise(this.xOff) * width\r\n        this.y = noise(this.yOff) * height\r\n    }\r\n\r\n    checkIfTouched() {\r\n        const d = dist(mouseX, mouseY, this.x, this.y)\r\n        if(d < this.radius) {\r\n            this.touched = true\r\n        }\r\n    }\r\n\r\n    display() {\r\n        if(this.touched) fill('green')\r\n        else fill('white')\r\n        circle(this.x, this.y, this.radius * 2)\r\n    }\r\n}\r\n```\r\n\r\nWhat's changed?\r\n\r\n1.  The `frameRate(10)` function in `setup()` drastically slows down the rate at which `draw()` is run from about 60 times per second to 10. This is only done to make this game playable.\r\n2.  There are only ten instances of `Bubble` created instead of 100.\r\n3.  Two new properties are now included in `Bubble` - `radius` and `touched`. The `radius` is used in the collision detection and when drawing the bubble.\r\n4.  A new `checkifTouched()` method is included in `Bubble`. This method determines the distance (`dist()`) between the mouse position and the bubble center (x, y). If it is less than the radius, you know a collision has taken place and set `this.touched` to `true`.\r\n5.  The color of the bubble changed once touched.\r\n6.  The `checkIfTouched()` method is called for every bubble in `draw()`.\r\n\r\n![10 larger white circles move around the canvas. When the cursor touches them, they turn green permanently.](https://res.cloudinary.com/deepgram/image/upload/v1646780526/blog/2022/03/p5js-game-logic/dist.gif)\r\n\r\n## Keeping Score\r\n\r\nCurrently, every bubble currently tracks its own state, but there is no global indication of how a player has scored. This can be implemented with a global variable. Follow these steps:\r\n\r\n1.  Add a global variable called `score` with a value of `0`.\r\n2.  Inside of the `Bubble.checkIfTouched()` method, before `this.touched` is set to `true`, check if `this.touched` is still false, and then  also increment `score`.\r\n3.  In the `draw()` function, set the color to white using `fill('white')`, and then display the `score` by using `text()`.\r\n\r\nIn case you don't remember the parameters for `text()` that we went over in the previous post, `text()` takes three arguments - the text to display, and the (x,y) coordinates.\r\n\r\nFor step 2, the additional check is required to stop `score` incrementing more than once. If successful, your sketch should function like this:\r\n\r\n![When circles are hovered over, they go green and a number in the top-left goes up by 1.](https://res.cloudinary.com/deepgram/image/upload/v1646780526/blog/2022/03/p5js-game-logic/score.gif)\r\n\r\n## Starting, Winning, and Losing\r\n\r\nMost games have a number of states - a landing page on load, the game itself, and an endgame. This state can often be held in global scope, and code that runs in `draw()` can be altered as a result. Leaving your `Bubble` class unchanged, try this to implement game state management:\r\n\r\n```js\r\nconst bubbles = []\r\nlet score = 0\r\nlet win = false\r\n\r\nfunction setup() {\r\n    createCanvas(500, 200)\r\n    frameRate(10)\r\n    for(let i = 0; i < 3; i++) {\r\n        bubbles.push(new Bubble(250, 100))\r\n    }\r\n}\r\n\r\nfunction draw() {\r\n    background(100)\r\n\r\n    if(score >= 3) win = true\r\n\r\n    if(!win) {\r\n        for(let bubble of bubbles) {\r\n            bubble.move()\r\n            bubble.checkIfTouched()\r\n            bubble.display()\r\n        }\r\n        fill('white')\r\n        text(score, 10, 20)\r\n    } else {\r\n        textSize(36)\r\n        textAlign(CENTER)\r\n        text('You Win!', width/2, height/2-16)\r\n    }\r\n}\r\n```\r\n\r\nThe `win` variable starts as false, and when the `score` reaches three or more, the game logic stops running, and the text 'You Win!' will be shown instead.\r\n\r\nThis is a simplistic example, but the same approach can be taken to implement more game states.\r\n\r\n![3 white circles move on the canvas. When the mouse cursor touches them they go green and the score goes up by one. When all 3 are green, the game ends and the text \"You win\" is displayed on the screen.](https://res.cloudinary.com/deepgram/image/upload/v1646780525/blog/2022/03/p5js-game-logic/win.gif)\r\n\r\n## In Summary\r\n\r\nTogether with the first post in this series, I hope you have the tools you need to build a fun game with P5.js with these game logic implementations. For further inspiration, here are some of my favorite P5 examples:\r\n\r\n*   [Particles in a flow field](https://openprocessing.org/sketch/1245844) - this example uses perlin noise in a way which may help further illustrate how it works.\r\n*   [A game of snake](https://p5js.org/examples/interaction-snake-game.html)\r\n*   [Full 2D platformer game](https://editor.p5js.org/L0808866/sketches/lvURvk4QN)\r\n*   [A kaleidoscope drawing program](https://p5js.org/examples/interaction-kaleidoscope.html)\r\n*   [Interactive artwork with animated interactive stars](https://openprocessing.org/sketch/570102)\r\n*   [A generative painting program](https://p5js.org/examples/hello-p5-drawing.html)\r\n*   [John Conway's Game of Life cellular automata](https://p5js.org/examples/simulate-game-of-life.html)\r\n*   [L-Systems generative art](https://p5js.org/examples/simulate-l-systems.html)\r\n*   [Applying realistic forces like gravity](https://p5js.org/examples/simulate-forces.html)\r\n*   [Rainbow meatballs shader](https://openprocessing.org/sketch/838276) - this involved writing a shader which is an advanced topic but it looks extremely cool.\r\n\r\nNext week in the third and final part of this series, we'll cover how to integrate voice into your P5 sketches. Until then, please feel free to reach out to us on Twitter at [@DeepgramDevs](https://twitter.com/DeepgramDevs) if you have any questions or thoughts.\r\n\r\n        ";
						}
						async function compiledContent$1y() {
							return load$1y().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1y() {
							return (await import('./chunks/index.141effbf.mjs'));
						}
						function Content$1y(...args) {
							return load$1y().then((m) => m.default(...args));
						}
						Content$1y.isAstroComponentFactory = true;
						function getHeadings$1y() {
							return load$1y().then((m) => m.metadata.headings);
						}
						function getHeaders$1y() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1y().then((m) => m.metadata.headings);
						}

const __vite_glob_0_178 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1y,
  file: file$1y,
  url: url$1y,
  rawContent: rawContent$1y,
  compiledContent: compiledContent$1y,
  default: load$1y,
  Content: Content$1y,
  getHeadings: getHeadings$1y,
  getHeaders: getHeaders$1y
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1x = {"title":"Playing With P5.js: Getting Started","description":"P5.js is a JavaScript library for creative coding. In this series, we'll get you up and running with everything you need to build a basic game. Get started now.","date":"2022-03-08T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1646414380/blog/2022/03/p5js-getting-started/series-cover.jpg","authors":["kevin-lewis"],"category":"tutorial","tags":["javascript","p5js","beginner"],"seo":{"title":"Playing With P5.js: Getting Started","description":"P5.js is a JavaScript library for creative coding. In this series, we'll get you up and running with everything you need to build a basic game. Get started now."},"shorturls":{"share":"https://dpgr.am/1cf2b50","twitter":"https://dpgr.am/861d800","linkedin":"https://dpgr.am/38c6234","reddit":"https://dpgr.am/6877e1a","facebook":"https://dpgr.am/2e2f7af"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454044/blog/p5js-getting-started/ograph.png"}};
						const file$1x = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/p5js-getting-started/index.md";
						const url$1x = undefined;
						function rawContent$1x() {
							return "\nThe Canvas API allows developers to draw 2D and 3D graphics in the browser using a `<canvas>` element. As it uses JavaScript, you can create interactive and animated graphics which any other logic in your application can also impact. The Canvas API is supported in [nearly 98% of browsers](https://caniuse.com/canvas) but is quite verbose, which is where today's focus lies.\n\n[P5.js](https://p5js.org/) (from here 'P5') is a JavaScript library that makes working with the Canvas API much easier. Once included in a project, you are automatically given access to a set of global functions, variables, and lifecycle hooks. Instead of several long lines of code, most operations can be completed in a single function call.\n\nBelow is an example of a P5 sketch with only 20 lines of code. Try moving your mouse inside of it for some interactivity. [The code for this example can be found here](https://p5js.org/examples/interaction-wavemaker.html).\n\n<iframe src=\"https://hwtc8x.csb.app/\" height=\"320\" width=\"320\"></iframe>\n\nThis is the first in a three-part series on learning P5, where we will cover the basics of drawing and interacting with a P5 'sketch.' Next week, we will cover many approaches used in creating games, and in the final part, we will integrate Deepgram into a sketch.\n\nThe [P5 Reference](https://p5js.org/reference/) is your friend and documents all of the variables and functions provided to your sketches.\n\n## Getting Started\n\nOn your computer, create a new directory and open it in your code editor. Create an `index.html` file and add the following to it:\n\n```html\n<!DOCTYPE html>\r\n<html>\r\n<head></head>\r\n<body>\r\n    <script src=\"https://cdn.jsdelivr.net/npm/p5@1.4.1/lib/p5.js\"></script>\r\n    <script>\r\n        function setup() {\r\n        }\r\n\r\n        function draw() {\r\n        }\r\n    </script>\r\n</body>\r\n</html>\n```\n\n## `setup()` and `draw()`\n\nP5 relies on two main functions to be written - `setup()` and `draw()`.\n\nCode in the `setup()` is run once when the program begins - initial and fixed settings are often declared here.\n\nThe `draw()` function is continuously run from top to bottom, defaulting to 60 times a second (this frequency is known as the 'frame rate'). Most of your logic will be written here.\n\nThe frame rate is a perfect example of a value you would set during `setup()` as you are likely to only do it once in an application.\n\nTo begin, create a new canvas on your page by adding the following lines to your `setup()` function:\n\n```js\nfunction setup() {\r\n    createCanvas(500, 500)\r\n    background('red')\r\n}\n```\n\nLoad your `index.html` file in a browser, and you should see a large red square. This is your canvas which you can draw on - exciting! The `createCanvas()` function takes two arguments - a width and a height, which you have set to 500.\n\n## Colors In P5\n\nIn the current example, the `background()` function sets the entire canvas' background to `red`. Note that `red` is in quotes, as this is a CSS named color. There are plenty of other ways to define colors in P5 - try changing `background('red')` to any of the following:\n\n```js\n// Red, Green, Blue - range of 0 to 255\r\nbackground(233, 61, 69)\r\n\r\n// Equivalent to 150, 150, 150 - will always be a gray\r\nbackground(150)\r\n\r\n// Hex codes work too\r\nbackground('#38edac')\n```\n\nThere are other ways to define colors - but these represent most of what you are likely to use. There are other contexts where colors are used beyond the background that will be covered later in this tutorial.\n\nReset the background to `0`, which will result in black.\n\n## Drawing Shapes\n\nThis canvas is rather lovely, and now is the time to draw elements on it. Before you draw anything, it's worth noting that the coordinate system sets the origin (0, 0) in the top-left. The first number always represents the x-axis (left to right), and the second number represents the y-axis (top to bottom).\n\nEven though we are currently only drawing shapes once, it is recommended that you draw to the canvas in `draw()`:\n\n```js\nfunction draw() {\r\n    circle(20, 40, 10)\r\n}\n```\n\nRefresh your browser, and you should see a small white circle on the canvas. The three arguments for `circle()` indicate the x position, y position, and diameter. For `circle()`, the x and y values indicate circle's center.\n\n![A back square marked zero zero in the top-left. A white ball is 20 points from the left (x) and 40 from the top (y). It is 10 points wide.](https://res.cloudinary.com/deepgram/image/upload/v1646435886/blog/2022/03/p5js-getting-started/coordinate.png)\n\nAdd a new square to your canvas and refresh your browser:\n\n```js\nfunction draw() {\r\n    circle(20, 40, 10)\r\n    square(100, 100, 25)\r\n}\n```\n\nThe arguments for `square()` are the same as circle - x, y, and size. The only difference is that the (x,y) values are for the top-left corner of the square and not the center of the shape.\n\nAdd a rectangle to your canvas and refresh your browser:\n\n```js\nfunction draw() {\r\n    circle(20, 40, 10)\r\n    square(100, 100, 40)\r\n    rect(120, 50, 40, 70)\r\n}\n```\n\nThe `rect()` function's arguments specify the (x,y) of the top-left corner, the size of the shape on the x-axis (length), and the size on the y-axis (height).\n\nThese values cause the square and rectangle to overlap, and for the first time, you'll see that all of the shapes so far have a black stroke (border) around them. Change the `background()` argument in `setup()` to 100 to see this more clearly.\n\n![A gray canvas with three white chapes - a circle near the top-right, a square, and a rectangle half overlapping the square.](https://res.cloudinary.com/deepgram/image/upload/v1646436612/blog/2022/03/p5js-getting-started/overlapping-shapes.png)\n\nThere are a range of other shapes to use, including `triangle()`, `ellipse()`, `line()`, and `quad()`. All work similarly, though the exact number of arguments may be different. Take a look at the [P5 Reference](https://p5js.org/reference/) for more information.\n\n## Setting Fills & Strokes\n\nStatements in P5 run in the order they are written, and elements 'drawn' are done in that order. If elements overlap, ones drawn afterward will appear 'on top,' as the other element has already been placed. If you want to see this in action, temporarily swap the `square()` and `rect()` statements to see the difference.\n\nYou need to understand that the order of statements is important to control the colors of elements. Colors aren't set when drawing an element, but instead, use their own set of functions provided by P5.\n\nUpdate `draw()` to the following and refresh your browser:\n\n```js\nfunction draw() {\r\n    fill('red')\r\n    stroke('blue')\r\n    circle(20, 40, 10)\r\n    square(100, 100, 40)\r\n    rect(120, 50, 40, 70)\r\n}\n```\n\nAll of the shapes are now red with a stroke of blue as the `fill()` and `stroke()` values are applied until it is unset or set to something else. Try this:\n\n```js\nfunction draw() {\r\n    fill('red')\r\n    circle(20, 40, 10)\r\n    square(100, 100, 40)\r\n    fill('green')\r\n    rect(120, 50, 40, 70)\r\n}\n```\n\nNow the first two shapes are red, but the third is green. Finally, try this:\n\n```js\nfunction draw() {\r\n    circle(20, 40, 10)\r\n    fill('red')\r\n    square(100, 100, 40)\r\n    fill('green')\r\n    rect(120, 50, 40, 70)\r\n}\n```\n\nYou may have expected the circle to be its initial white color, but instead, it's green. Why is this?\n\n## Persistence Between Draws\n\nThe `draw()` function executes statements from beginning to end, and once completed, it starts again and repeats endlessly. The steps of 'draw a circle, then a square, then a rectangle' are happening thousands of times a second, but you can't see it because the steps are happening in the same order and in the same positions.\n\nFunctions that apply settings to the canvas are not reset between draws. Because of this, the `fill('green')` run is still the most recent `fill()` every time after the first draw. We'll get to see this more clearly later in this post.\n\n## Moving Elements\n\nBecause of variable scoping in JavaScript, any variables created in `draw()` are recreated with their initial value every frame:\n\n```js\nfunction draw() {\r\n    let frame = 1\r\n    frame += 1\r\n    frame // is always 2, regardless of how many times draw() runs\r\n}\n```\n\nInstead, persistent variables should be defined in global scope. Try this:\n\n```js\nlet frame = 1\r\nfunction draw() {\r\n    frame += 1\r\n    circle(frame, 40, 10)\r\n}\n```\n\nThe first argument in `circle()` is now the value of `frame`. Here's what it looks like:\n\n![An animation of a white circle moving across the canvas. A black trail of where it has been is visible.](https://res.cloudinary.com/deepgram/image/upload/v1646676858/blog/2022/03/p5js-getting-started/500-circles.gif)\n\nThe circle is not being moved every frame, but a new circle is being drawn on the existing canvas. When the circle goes off-screen, there are just over 500 circles visible. It is common to redraw the whole canvas background at the beginning of `draw()` to 'wipe' the canvas:\n\n```js\nlet frame = 1\r\nfunction draw() {\r\n    background(100)\r\n    frame += 1\r\n    circle(frame, 40, 10)\r\n}\n```\n\n![An animation of a white circle moving across the canvas.](https://res.cloudinary.com/deepgram/image/upload/v1646676858/blog/2022/03/p5js-getting-started/1-circle.gif)\n\n## User Input\n\n### Mouse Input\n\nAll of the P5 functionality we've used so far is in the form of global functions, but there are also many global variables provided for use in your sketches. Try this:\n\n```js\nfunction draw() {\r\n    background(100)\r\n    if(mouseIsPressed) {\r\n        fill('red')\r\n    } else {\r\n        fill('white')\r\n    }\r\n    circle(mouseX, mouseY, 10)\r\n}\n```\n\nThis small snippet effectively shows off three variables:\n\n1.  `mouseIsPressed` is `true` if a mouse button is pressed.\n2.  `mouseX` is the position of the mouse on the x-axis.\n3.  `mouseY` is the position of the mouse on the y-axis.\n\nIt's also worth knowing that the `mouseButton` variable will hold the last pressed button - either `LEFT`, `RIGHT`, or `CENTER`.\n\n### Keyboard Input\n\nJust like `mouseIsPressed`, there is a `keyIsPressed` variable. Like `mouseButton`, the `key` variable will hold the last-pressed key. We can combine this to control the position of elements:\n\n```js\nlet circleX = 250\r\nlet circleY = 250\r\n\r\nfunction draw() {\r\n    background(100)\r\n\r\n    if(keyIsPressed) {\r\n        if(key == 'ArrowLeft') circleX -= 1\r\n        if(key == 'ArrowRight') circleX += 1\r\n        if(key == 'ArrowUp') circleY -= 1\r\n        if(key == 'ArrowDown') circleY += 1\r\n    }\r\n\r\n    circle(circleX, circleY, 10)\r\n}\n```\n\n## Drawing Text\n\nThere are a bunch of typography-related functions provided by P5 which you can read more about in the [P5 Reference](https://p5js.org/reference/), but to focus on the two most important, try this:\n\n```js\nfunction draw() {\r\n    background(100)\r\n    fill('white')\r\n    textSize(24)\r\n    text('Current frame is ' + frameCount, 100, 100)\r\n}\n```\n\n`textSize()` changes the font size - the default is 12. `text()` takes three arguments - the text to display, and the (x,y)  `frameCount` is a built-in variable which goes up by 1 every time `draw()` is run.\n\n## Drawing Images\n\nThere are two sets to drawing images - loading them, and displaying them. We also want to make sure it is fully loaded before showing it.\n\nUp until now, we have used `setup()` and `draw()`, but one of the other lifecycle functions with P5 is `preload()`. `preload()` loads in external files fully before `setup()` is run, and this is where we will load images. Try this:\n\n```js\nlet catImage\r\nfunction preload() {\r\n    catImage = loadImage('https://placekitten.com/200/100')\r\n}\r\n\r\nfunction setup() {\r\n    createCanvas(500, 500)\r\n    background(100)\r\n}\r\n\r\nfunction draw() {\r\n    background(100)\r\n    image(catImage, 10, 20)\r\n}\n```\n\n![A gray canvas with a picture of a cat displayed on it near the top-left. It's slightly further down than it is across.](https://res.cloudinary.com/deepgram/image/upload/v1646676857/blog/2022/03/p5js-getting-started/cat.png)\n\nThe image will load in at its full size, so if the image is 300 pixels wide, it would use up 300 pixels on the canvas. You can optionally provide two more to set the width and height explicitly:\n\n```js\nimage(catImage, 10, 20, 100, 100)\n```\n\n## In Summary\n\nThat was a lot, and we're just warming up. We covered lifecycle functions (`preload()`, `setup()`, and `draw()`), setting colors, the P5 coordinate system, drawing shapes and text, basic animation, interacting with elements through a keyboard and mouse, and finally loading and showing images.\n\nIn next week's post, we'll cover how to build several games-related features in your P5 sketch, and in the third and final post, we'll integrate Deepgram into a P5 sketch.\n\nIf you have any questions, please feel free to reach out to us on Twitter at [@DeepgramDevs](https://twitter.com/DeepgramDevs).\n\n        ";
						}
						async function compiledContent$1x() {
							return load$1x().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1x() {
							return (await import('./chunks/index.bb385126.mjs'));
						}
						function Content$1x(...args) {
							return load$1x().then((m) => m.default(...args));
						}
						Content$1x.isAstroComponentFactory = true;
						function getHeadings$1x() {
							return load$1x().then((m) => m.metadata.headings);
						}
						function getHeaders$1x() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1x().then((m) => m.metadata.headings);
						}

const __vite_glob_0_179 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1x,
  file: file$1x,
  url: url$1x,
  rawContent: rawContent$1x,
  compiledContent: compiledContent$1x,
  default: load$1x,
  Content: Content$1x,
  getHeadings: getHeadings$1x,
  getHeaders: getHeaders$1x
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1w = {"title":"How to Build the World's Ugliest Podcast Search Engine with Python","description":"In this post, you'll learn how to search podcast episode transcripts to find words and phrases that were discussed.","date":"2022-08-26T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1660863776/blog/2022/08/podcast-search-engine/2208-Build-a-Podcast-Search-Engine-blog%402x.jpg","authors":["michael-jolley"],"category":"tutorial","tags":["python","podcast"],"seo":{"title":"How to Build the World's Ugliest Podcast Search Engine with Python","description":"In this post, you'll learn how to search podcast episode transcripts to find words and phrases that were discussed."},"shorturls":{"share":"https://dpgr.am/281ef0e","twitter":"https://dpgr.am/dbc3a70","linkedin":"https://dpgr.am/2cee49b","reddit":"https://dpgr.am/c1ffde6","facebook":"https://dpgr.am/b65c2cb"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661541401/blog/podcast-search-engine/ograph.png"}};
						const file$1w = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/podcast-search-engine/index.md";
						const url$1w = undefined;
						function rawContent$1w() {
							return "\r\nSimone Giertz has a great [TED Talk](https://www.youtube.com/watch?v=c0bsKc4tiuY)\r\nwhere she extols the virtues of building useless things. I often find myself\r\nbuilding useless things to teach others about new technologies and development\r\npractices. So when I started picking up Python, building another useless thing\r\nseemed like the best way to start.\r\n\r\nSince Python is an object-oriented language, I expected to pick it up quickly.\r\nAfter decades of .NET and JavaScript, OOP languages are my safe space. But\r\nbeyond the syntax, what type of things do I need to know? I made a list:\r\n\r\n*   Loops and conditions\r\n*   File access\r\n*   HTTP requests\r\n\r\nThen there were questions like \"could I build an API?\" and \"what do Python\r\ndevelopers do for front-ends?\" Of course, Deepgram has a Python SDK so gaining\r\nexperience using it would be beneficial and I could even provide feedback to the\r\nfolks that are building it. That meant I needed to do something with audio. HTTP\r\nrequests, audio, files, loops, and conditions... clearly, I needed to build a\r\nsearch engine for podcasts.\r\n\r\nSince I'm still learning Python, I leaned on our team at Deepgram to help me\r\nspeed up the process. First up, accessing a podcast's RSS feed and identifying\r\nthe URL to the mp3 files.\r\n\r\n## Pulling Podcast Episodes from an RSS Feed\r\n\r\nNeil deGrasse Tyson has a great podcast called\r\n[StarTalk Radio](https://startalkmedia.com/) that would provide tons of\r\nsearchable words. Its RSS feed is located at\r\nhttps://feeds.simplecast.com/4T39_jAj, so I needed to read that and pull in\r\nindividual episodes. Originally, I planned to save the data from the feed into\r\na PostgreSQL or MySQL database but decided to keep it simple by just saving the\r\ninfo from one episode to a Python file.\r\n\r\nI created a file named `load.py` to get the episode information and transcribe\r\nthe audio. You can see the code below, but the TL;DR is that it:\r\n\r\n*   Downloads the RSS feed locally to a file called `theRSSfeed.xml`\r\n*   Parses the XML in that file to find all the mp3 URLs\r\n*   Takes the first episode and feed its mp3 to Deepgram for transcription\r\n*   Saves the transcript that Deepgram generates to a file named `podcast_data.py`\r\n\r\n```python\r\n# use python3\r\n\r\nimport requests\r\nfrom os.path import exists\r\nimport xml.etree.ElementTree as ET\r\nfrom urllib.parse import scheme_chars, urlparse, parse_qs\r\nimport json\r\n\r\n# testing to see if we already downloaded an xml file\r\n# if not, download it and save to file\r\n\r\nthe_rss_file = \"theRSSfeed.xml\"\r\nfile_exists = False\r\nfile_exists = exists(the_rss_file)\r\nif not file_exists:\r\n    print(\"no file found:\",  the_rss_file, \"... downloading ...\")\r\n    url = \"https://feeds.simplecast.com/4T39_jAj\"  # startalk podcast rss feed\r\n    response = requests.get(url)\r\n    the_content = response.content\r\n    with open(the_rss_file, \"wb\") as outfile:\r\n        outfile.write(the_content)\r\nelse:\r\n    print(\"found file:\",  the_rss_file, \"... using it ...\")\r\n    with open(the_rss_file, \"rb\") as infile:\r\n        the_content = infile.read()\r\n\r\n# grab the tree object from the parsed xml file\r\n\r\nroot = ET.fromstring(the_content)\r\n\r\n# loop through it and find all mp3s in the document\r\n# (the way I found which query to make [the \"enclosure\" query])\r\n# is by using my eyeballs to find the relevant location of mp3s\r\n# in the rss feed, other rss feeds might be different\r\n\r\nlist_of_mp3s = []\r\nfor thing in root.iter('enclosure'):\r\n    full_url_mp3 = thing.attrib[\"url\"]\r\n    parsed_url = urlparse(full_url_mp3)  # stripping off query params\r\n    the_mp3_url = parsed_url.scheme + \"://\" + parsed_url.hostname + parsed_url.path\r\n    list_of_mp3s.append(the_mp3_url)\r\n\r\nfirst_mp3 = list_of_mp3s[:1][0]  # take the first\r\n\r\n# now feed this url to DGs transcription API. You would keep track of the\r\n# transcription object you get back so you can use it later for\r\n# searching capabilities.\r\n\r\nDG_API_KEY = \"YOUR_DEEPGRAM_API_KEY_HERE\"\r\n\r\nheaders = {\r\n    'content-type': \"application/json\",\r\n    'Authorization': \"Token \" + DG_API_KEY\r\n}\r\n\r\nresponse = requests.post(\r\n    \"https://api.deepgram.com/v1/listen?punctuate=true&utterances=true\", headers=headers, json={\"url\": first_mp3})\r\n\r\nprint(first_mp3)\r\nopen('/workspace/podcast_data.py',\r\n     'wb').write(response.content)\r\n```\r\n\r\nThen I ran `python load.py` and BAM!, I've got a `podcast_data.py` with the\r\ntranscript of the episode. Now to start building an API that I can send search\r\nterms to.\r\n\r\n## Building the Podcast Search Engine\r\n\r\nI spent some time reading\r\n[Tonya](https://blog.deepgram.com/authors/tonya-sims/)'s blog posts on\r\n[FastAPI](https://blog.deepgram.com/live-transcription-fastapi/) and\r\n[Django](https://blog.deepgram.com/live-transcription-django/),\r\nbut eventually decided on [Flask](https://blog.deepgram.com/live-transcription-flask/)\r\nto build the back-end API.\r\n\r\n### Receiving and Responding to Requests\r\n\r\nBefore I could receive search terms and respond, I had to figure out how to\r\nreceive *any* request and return a response. Luckily, the Flask documentation\r\nprovides several good examples of doing that. I started by installing Flask with\r\npip.\r\n\r\n```bash\r\npip install Flask\r\n```\r\n\r\nFlask's documentation told me that if I name my file `app.py` I can default to\r\nstarting the server using `flask run` in the terminal. I started with a very\r\nbasic `app.py` to see if I could return anything.\r\n\r\n```python\r\nfrom flask import Flask\r\n\r\napp = Flask(__name__)\r\n\r\n@app.get(\"/\")\r\ndef index():\r\n    return {\r\n      \"name\": \"Hello World\"\r\n    }\r\n\r\n```\r\n\r\nThat little bit of Python returns a JSON object. Visiting `http://127.0.0.1:5000`,\r\nconfirmed that it responded with the JSON I expected. Now I can receive a\r\nrequest and respond to it.\r\n\r\nNext, I needed to be able to receive data that is sent via an HTTP POST request.\r\nAgain, I was saved by the Flask documentation. I knew that I would eventually be\r\nsending a form field named `search`, so I added a new method to the `app.py`\r\nfile:\r\n\r\n```python\r\n@app.post(\"/\")\r\ndef index_post():\r\n    search = request.form['search']\r\n    return {\r\n        \"name\": \"You searched for \" + search\r\n    }\r\n```\r\n\r\n> Note on the above: It also requires you to modify the `from flask import Flask`\r\n> to `from flask import Flask, request`.\r\n\r\nA quick test confirmed that I could pass in form values and respond with them.\r\nWith those wins under my belt, I was ready to tackle the job of searching\r\nthrough the transcript.\r\n\r\n### Searching the Podcast Transcript\r\n\r\nTo make sure I'm comparing apples to apples, I needed some basic text\r\nnormalization. The `text_normalize` function lowercases everything, removes\r\ncommon punctuation, removes unnecessary whitespace, and flattens the string to\r\nASCII.\r\n\r\n```python\r\ndef deep_replace(inString, inQuery, inReplacement):\r\n    text = inString\r\n    query = inQuery\r\n    replacement = inReplacement\r\n    text = text.replace(query, replacement)\r\n    if query in text:\r\n        return deep_replace(text, query, replacement)\r\n    else:\r\n        return text\r\n\r\ndef text_normalize(inString):\r\n    text = inString\r\n    text = normalize('NFKD', text).encode(\r\n        'ascii', 'ignore').decode('ascii').lower()\r\n    text = deep_replace(text, \"?\", \" \")\r\n    text = deep_replace(text, \".\", \" \")\r\n    text = deep_replace(text, \"!\", \" \")\r\n    text = deep_replace(text, \",\", \" \")\r\n    text = deep_replace(text, \"-\", \" \")\r\n    text = \" \".join(text.split())\r\n    return text\r\n```\r\n\r\nOnce I knew I could compare strings relatively well, it was time to look through\r\nthe transcript to find a search phrase. All the magic of the search engine takes\r\nplace in the `search` function. First, it normalizes the phrase I'm searching\r\nfor and then looks through all the words in the transcript for a match.\r\n\r\nFor any matches, it creates a string containing the matching word and the five\r\nwords that precede and follow the matching word. The match and its corresponding\r\nphrase are loaded to an array called `query_results` that is finally returned.\r\n\r\n```python\r\ndef search(phrase):\r\n    the_query = phrase\r\n    the_query = text_normalize(the_query)\r\n    print(the_query)\r\n\r\n    # word array search\r\n    the_body_array = podcast_data.data[\"results\"][\"channels\"][0][\"alternatives\"][0][\"words\"]\r\n    the_body_list = []\r\n    for thing in the_body_array:\r\n        the_body_list.append(text_normalize(thing[\"word\"]))\r\n    query_index_list = [idx for idx, s in enumerate(\r\n        the_body_list) if the_query in s]\r\n\r\n    query_results = []\r\n    for i in query_index_list:\r\n        backforwardcount = 5\r\n        quick_text = \" \".join(the_body_list[max(\r\n            0, i-backforwardcount):min(i+backforwardcount, len(the_body_list))])\r\n        query_results.append([the_body_array[i], quick_text])\r\n    return query_results\r\n```\r\n\r\nWith the search function ready, it was time to update the POST route of my API.\r\nI passed the search phrase submitted in the POST request to my `search` function\r\nand then returned the result.\r\n\r\n```python\r\n@app.post(\"/\")\r\ndef index_post():\r\n    phrase = request.form['search']\r\n    query_results = search(phrase=phrase)\r\n    return query_results\r\n```\r\n\r\nJust like magic, I could send requests to my API and have it return matches in\r\nthe podcast. But no one wants to send cURL requests all day. It was time to\r\nbuild the worst user interface for a search engine ever.\r\n\r\n## Building the Ugliest User Interface\r\n\r\nThe last step was to build a user interface. Fortunately, since I was building\r\nthe ugliest search engine, the bar was low on how it looked. In fact, it was a\r\nbit of a challenge to not try and improve the interface. 😁\r\n\r\n### The Search Interface\r\n\r\nOne of the reasons I chose to use Flask on the back-end was the fact that it\r\nsupported [Jinja2](https://pypi.org/project/Jinja2/) out of the box. I had never\r\nused Jinja2, but when someone mentioned it in our Slack, I noticed how similar\r\nit was to [Handlebars](https://handlebarsjs.com/) for JavaScript developers.\r\n\r\nMy goal was to create one HTML file that could display the search box and\r\nresults. To separate it from my Python code, I created a new HTML file\r\nat `templates/index.html`. It was very basic with an H1 tag and a form that\r\nwould send a post back to its route.\r\n\r\n```html\r\n<html>\r\n    <head>\r\n        <title>Podcast Search</title>\r\n    </head>\r\n\r\n    <body style=\"text-align:center;\">\r\n        <h1>World's Ugliest Podcast Search</h1>\r\n        <form action='#' method='POST'>\r\n            <label for='search'>Search for a word</label><br />\r\n            <input type='text' name='search' value='{{ search }}' /><br />\r\n            <button type=\"submit\">Search</button>\r\n        </form>\r\n    </body>\r\n</html>\r\n```\r\n\r\nOnce the HTML file was in place, I updated the original HTTP GET request to\r\nserve it. Because I'm injecting the `search` parameter, I needed to supply it\r\nwith an empty string.\r\n\r\n```python\r\n@app.get(\"/\")\r\ndef index():\r\n    return render_template('index.html', search=\"\")\r\n```\r\n\r\nA quick `flask run` in the terminal served up my ugly podcast search engine. To\r\nmy surprise, it was technically already working. When I entered a search phrase\r\nand pressed the 'Search' button, it sent the search phrase to the API, which\r\nreturned the results as JSON. Of course, that's not what I want it to do in the\r\nend, but it was a great feeling to know I was close to the end.\r\n\r\n![Interface of an ugly podcast search engine](https://res.cloudinary.com/deepgram/image/upload/v1661132558/blog/2022/08/podcast-search-engine/original-podcast-search-engine.png)\r\n\r\n### Displaying the Search Results\r\n\r\nWhile a JSON response would be pretty ugly, I was enjoying Jinja2 too much to\r\nnot build an interface to display the results of the search. After the form in\r\nmy `templates/index.html` file, I added an H2 and UL to list the results. If\r\nthere was a search phrase, it shows any results in a list.\r\n\r\n```html\r\n{% if search %}\r\n<h2>Search results for {{ search }}</h2>\r\n\r\n<ul style=\"list-style: none;\">\r\n    {% for result in results %}\r\n    <li style=\"padding:1rem;\">\r\n        {{ result[1] }}\r\n    </li>\r\n    {% endfor %}\r\n</ul>\r\n\r\n{% endif %}\r\n```\r\n\r\nOnce the template was ready, I needed to update my API to return the HTML.\r\nRather than returning the results as JSON, I return `render_template` passing\r\nthe search phrase and the query results.\r\n\r\n```python\r\n@app.post(\"/\")\r\ndef index_post():\r\n    phrase = request.form['search']\r\n    query_results = search(phrase=phrase)\r\n    return render_template('index.html', search=phrase, results=query_results)\r\n```\r\n\r\n![Results interface of the podcast search engine](https://res.cloudinary.com/deepgram/image/upload/v1661136247/blog/2022/08/podcast-search-engine/podcast-search-engine-results.png)\r\n\r\nThere you have it. Searching works and shows all places where a word was spoken.\r\nThe phrases are a nice touch because they give context to what is being said at\r\nthat moment. That should be the end right? Oh no. I'm nothing if not a little\r\nextra. It was time to add a little pizzazz.\r\n\r\n### Getting a Little Fancy\r\n\r\nWe're searching through podcasts. By their nature, they are meant for audio.\r\nWhile I could have stopped by showing the phrase the user was looking for, I\r\nthought it would be cooler if we could play that section of audio. I started by\r\nadding an audio player to the HTML file with the podcast episode I'm searching\r\nthrough. Users can press play and listen to the podcast if they like, but the\r\nreal fun will happen once they search.\r\n\r\n```html\r\n    <audio controls>\r\n        <source\r\n            src=\"https://stitcher.simplecastaudio.com/8b62332a-56b8-4d25-b175-1e588b078323/episodes/774634ab-c3f5-4100-b6a0-8554c63002c0/audio/128/default.mp3\"\r\n            type=\"audio/mpeg\">\r\n        Your browser does not support the audio element.\r\n    </audio>\r\n```\r\n\r\nNext, I updated the result LI elements to include an anchor tag that will call\r\na JavaScript function. (You know I wouldn't get through all this work without\r\nusing a touch of JavaScript.) When it calls the upcoming `seek` function, it\r\nsupplies it with the timestamp of the start of the found word.\r\n\r\n```html\r\n<a href=\"javascript:void(0);\" onclick=\"seek({{result[0].start}})\">{{ result[1] }}</a>\r\n```\r\n\r\nFinally, I added a JavaScript function to the head of the page called `seek`. It\r\nexpects a timestamp parameter. It then grabs the audio player, pauses its\r\nplayback, seeks to timestamp location minus eight-tenths of a second, and plays.\r\nWhy eight-tenths? I found it started the audio a few words before the searched\r\nphrase so you can better hear the word in context.\r\n\r\n```html\r\n<script type=\"text/javascript\">\r\n    function seek(timestamp) {\r\n        const audio = document.getElementsByTagName('audio')[0];\r\n        audio.pause();\r\n        audio.currentTime = timestamp - .8;\r\n        audio.play();\r\n    }\r\n</script>\r\n```\r\n\r\n![Results interface with an audio player](https://res.cloudinary.com/deepgram/image/upload/v1661136247/blog/2022/08/podcast-search-engine/podcast-search-results-with-audio-player.png)\r\n\r\n## Final Results\r\n\r\nOverall, I really enjoyed dipping my toes into the Python world. I learned several\r\nthings that are universal to all languages and I'm excited to learn more. If you\r\nwant to build this fun, but completely useless project, the full Python\r\nand HTML files are below. Enjoy!\r\n\r\n```python\r\n# app.py\r\n\r\nfrom flask import Flask, render_template, request\r\n\r\nfrom unicodedata import normalize\r\nimport podcast_data\r\n\r\napp = Flask(__name__)\r\n\r\n\r\n@app.get(\"/\")\r\ndef index():\r\n    return render_template('index.html', search=\"\")\r\n\r\n\r\n@app.post(\"/\")\r\ndef index_post():\r\n    phrase = request.form['search']\r\n    query_results = search(phrase=phrase)\r\n    return render_template('index.html', search=phrase, results=query_results)\r\n\r\n\r\ndef search(phrase):\r\n    the_query = phrase\r\n    the_query = text_normalize(the_query)\r\n    \r\n    the_body_array = podcast_data.data[\"results\"][\"channels\"][0][\"alternatives\"][0][\"words\"]\r\n    the_body_list = []\r\n    for thing in the_body_array:\r\n        the_body_list.append(text_normalize(thing[\"word\"]))\r\n    query_index_list = [idx for idx, s in enumerate(\r\n        the_body_list) if the_query in s]\r\n\r\n    query_results = []\r\n    for i in query_index_list:\r\n        backforwardcount = 5\r\n        quick_text = \" \".join(the_body_list[max(\r\n            0, i-backforwardcount):min(i+backforwardcount, len(the_body_list))])\r\n        query_results.append([the_body_array[i], quick_text])\r\n    return query_results\r\n\r\n\r\ndef deep_replace(inString, inQuery, inReplacement):\r\n    text = inString\r\n    query = inQuery\r\n    replacement = inReplacement\r\n    text = text.replace(query, replacement)\r\n    if query in text:\r\n        return deep_replace(text, query, replacement)\r\n    else:\r\n        return text\r\n\r\n\r\ndef text_normalize(inString):\r\n    text = inString\r\n    text = normalize('NFKD', text).encode(\r\n        'ascii', 'ignore').decode('ascii').lower()\r\n    text = deep_replace(text, \"?\", \" \")\r\n    text = deep_replace(text, \".\", \" \")\r\n    text = deep_replace(text, \"!\", \" \")\r\n    text = deep_replace(text, \",\", \" \")\r\n    text = deep_replace(text, \"-\", \" \")\r\n    text = \" \".join(text.split())\r\n    return text\r\n```\r\n\r\n```html\r\n<!-- templates/index.html -->\r\n<html>\r\n    <head>\r\n        <title>Podcast Search</title>\r\n        <script type=\"text/javascript\">\r\n            function seek(timestamp) {\r\n                const audio = document.getElementsByTagName('audio')[0];\r\n                audio.pause();\r\n                audio.currentTime = timestamp - .8;\r\n                audio.play();\r\n            }\r\n        </script>\r\n    </head>\r\n    <body style=\"text-align:center;\">\r\n        <h1>World's Ugliest Podcast Search</h1>\r\n        <form action='#' method='POST'>\r\n            <label for='search'>Search for a word</label><br />\r\n            <input type='text' name='search' value='{{ search }}' /><br />\r\n            <button type=\"submit\">Search</button>\r\n        </form>\r\n\r\n        <audio controls>\r\n            <source\r\n                src=\"https://stitcher.simplecastaudio.com/8b62332a-56b8-4d25-b175-1e588b078323/episodes/774634ab-c3f5-4100-b6a0-8554c63002c0/audio/128/default.mp3\"\r\n                type=\"audio/mpeg\">\r\n            Your browser does not support the audio element.\r\n        </audio>\r\n\r\n        {% if search %}\r\n        <h2>Search results for {{ search }}</h2>\r\n\r\n        <ul style=\"list-style: none;\">\r\n            {% for result in results %}\r\n            <li style=\"padding:1rem;\">\r\n                <a href=\"javascript:void(0);\" onclick=\"seek({{result[0].start}})\">{{ result[1] }}</a>\r\n            </li>\r\n            {% endfor %}\r\n        </ul>\r\n\r\n        {% endif %}\r\n    </body>\r\n</html>\r\n```\r\n\r\n        ";
						}
						async function compiledContent$1w() {
							return load$1w().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1w() {
							return (await import('./chunks/index.9c6e7678.mjs'));
						}
						function Content$1w(...args) {
							return load$1w().then((m) => m.default(...args));
						}
						Content$1w.isAstroComponentFactory = true;
						function getHeadings$1w() {
							return load$1w().then((m) => m.metadata.headings);
						}
						function getHeaders$1w() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1w().then((m) => m.metadata.headings);
						}

const __vite_glob_0_180 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1w,
  file: file$1w,
  url: url$1w,
  rawContent: rawContent$1w,
  compiledContent: compiledContent$1w,
  default: load$1w,
  Content: Content$1w,
  getHeadings: getHeadings$1w,
  getHeaders: getHeaders$1w
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1v = {"title":"Practice Spelling Bees with Spelling Hero","description":"We speak to the developers behind Spelling Hero - a spelling bee simulator complete with contextual usage and multi-difficulty. Learn more here.","date":"2022-04-05T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1646255609/blog/2022/04/practice-spelling-bees-hero/cover.jpg","authors":["kevin-lewis"],"category":"project-showcase","tags":["hackathon","javascript"],"seo":{"title":"Practice Spelling Bees with Spelling Hero","description":"We speak to the developers behind Spelling Hero - a spelling bee simulator complete with contextual usage and multi-difficulty. Learn more here."},"shorturls":{"share":"https://dpgr.am/57b1035","twitter":"https://dpgr.am/19800df","linkedin":"https://dpgr.am/c19e631","reddit":"https://dpgr.am/e61da07","facebook":"https://dpgr.am/0fb0ea5"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454069/blog/practice-spelling-bees-hero/ograph.png"}};
						const file$1v = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/practice-spelling-bees-hero/index.md";
						const url$1v = undefined;
						function rawContent$1v() {
							return "\r\nThe Scripps National Spelling Bee is America's longest-running educational competition - its primary goal being to help young people increase the size of their vocabulary through practice and a friendly competitive atmosphere. The team behind Spelling Hero wanted to simulate this great educational experience in the browser. I sat down with [Judah Daniels](https://www.linkedin.com/in/judah-daniels/), [Leon Zhang](https://www.linkedin.com/in/leon-bz/), [Saksham Shah](https://www.linkedin.com/in/saksham-shah-9803581b9/), and [Saomiyan Mathetharan](https://linkedin.com/in/saomiyan-mathetharan) to ask them about their project.\r\n\r\nSpelling Hero is a spelling bee simulator to emulate the experience of participating in a spelling bee competition. Users select a difficulty level, and the browser speaks a word aloud. Players then have to spell out the word to complete the level.\r\n\r\n![Easy mode. Word 1 of 5 with a score of 1. The word 'answer' is in green and displayed as correct. Several buttons read start spelling, repeat word, definition, language of origin, type of word, and example of sentence.](https://res.cloudinary.com/deepgram/image/upload/v1646255611/blog/2022/04/practice-spelling-bees-hero/screenshot.png)\r\n\r\nSpelling Hero's data was manually curated in this first version, and each word also includes a definition, origin, type, and a sample sentence - all options given to spelling bee participants.\r\n\r\nThe team used the Deepgram Speech Recognition API to understand a user's voice input, basing their code on our [Browser Live Transcription](https://blog.deepgram.com/live-transcription-mic-browser/) tutorial. The user interface was built with P5.js ([see our P5.js tutorial here](https://blog.deepgram.com/p5js-getting-started/)), and you can check out the [code on GitHub](https://github.com/saksham-shah/deepgram-game).\r\n\r\n        ";
						}
						async function compiledContent$1v() {
							return load$1v().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1v() {
							return (await import('./chunks/index.b34990f6.mjs'));
						}
						function Content$1v(...args) {
							return load$1v().then((m) => m.default(...args));
						}
						Content$1v.isAstroComponentFactory = true;
						function getHeadings$1v() {
							return load$1v().then((m) => m.metadata.headings);
						}
						function getHeaders$1v() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1v().then((m) => m.metadata.headings);
						}

const __vite_glob_0_181 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1v,
  file: file$1v,
  url: url$1v,
  rawContent: rawContent$1v,
  compiledContent: compiledContent$1v,
  default: load$1v,
  Content: Content$1v,
  getHeadings: getHeadings$1v,
  getHeaders: getHeaders$1v
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1u = {"title":"Propelled by Product, Customer and Industry Momentum, Deepgram Continues to Build the Future of Speech Recognition","description":"Deepgram, the leading automatic speech recognition vendor, today announces continued momentum following impressive company and customer growth as the demand for automatic speech recognition (ASR) continues to rise. Deepgram will mark the end of Q2 2021 with 3.2x YoY revenue growth and 2.7x YoY employee growth.","date":"2021-06-29T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981377/blog/propelled-by-product-customer-and-industry-momentum-deepgram-continues-to-build-the-future-of-speech-recognition/propelled-by-product%402x.jpg","authors":["katie-byrne"],"category":"dg-insider","tags":["machine-learning","deep-learning"],"seo":{"title":"Propelled by Product, Customer and Industry Momentum, Deepgram Continues to Build the Future of Speech Recognition","description":"Deepgram, the leading automatic speech recognition vendor, today announces continued momentum following impressive company and customer growth as the demand for automatic speech recognition (ASR) continues to rise. Deepgram will mark the end of Q2 2021 with 3.2x YoY revenue growth and 2.7x YoY employee growth."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981377/blog/propelled-by-product-customer-and-industry-momentum-deepgram-continues-to-build-the-future-of-speech-recognition/propelled-by-product%402x.jpg"},"shorturls":{"share":"https://dpgr.am/afd1751","twitter":"https://dpgr.am/48398ba","linkedin":"https://dpgr.am/fd312e3","reddit":"https://dpgr.am/b720800","facebook":"https://dpgr.am/7af8826"}};
						const file$1u = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/propelled-by-product-customer-and-industry-momentum-deepgram-continues-to-build-the-future-of-speech-recognition/index.md";
						const url$1u = undefined;
						function rawContent$1u() {
							return "## Growth fueled by increased ASR adoption, expanding software customer base and key executive hires\n\n**SAN FRANCISCO, Calif.,** June 29, 2021 - Deepgram, the leading automatic speech recognition vendor, today announces continued momentum following impressive company and customer growth as the demand for automatic speech recognition (ASR) continues to rise. Deepgram will mark the end of Q2 2021 with 3.2x YoY revenue growth and 2.7x YoY employee growth. Voice is proving to be a killer application for companies across industries seeking to build better relationships with their customers and empower their employees. The global market for speech and voice recognition technology is poised to reach $31.82 billion by 2025, according to a [report](https://www.grandviewresearch.com/press-release/global-voice-recognition-industry) by Grand View Research. Deepgram's recent [data report](https://deepgram.com/state-of-asr-report/) with Opus Research also showed that 2020 proved to be a banner year for ASR providers, revealing that the COVID-19 pandemic accelerated ASR adoption and that a large majority of respondents plan to significantly increase their investment in the technology in 2021. Off the heels of a [$25 million Series B](https://blog.deepgram.com/we-raised-25-million/) funding round earlier this year, Deepgram appointed four new executive and senior hires and expanded its base of software customers in the Contact Center as a Service (CCaaS) and Call Analytics verticals, including RingDNA, Regalix, Authenticx, Red Box, and Podsights. Deepgram was also recently recognized in [Forbes' AI 50](https://www.forbes.com/sites/alanohnsman/2021/04/26/ai-50-americas-most-promising-artificial-intelligence-companies/) and [Inc.'s Best Workplaces](https://www.inc.com/best-workplaces/2021).\n\n> \"With the ASR industry rapidly expanding, more and more companies are recognizing speech as a powerful tool that unlocks invaluable insights and helps shape state-of-the-art voice experiences,\" said Mehul Patel, Vice President of Products at Deepgram. \"At Deepgram, we are constantly working to ensure that we're providing customers with the highest quality speech recognition platform on the market. We're poised for strong, continued growth as a result of our recent momentum and look forward to expanding our offerings and customer base in 2021 and beyond.\"\n\nOver the past year, Deepgram met growing customer demand by investing heavily into its product roadmap. The company announced [AutoML](https://blog.deepgram.com/deepgram-pioneers-novel-training-approach-setting-new-standard-for-ai-companies-2/)-a novel training capability that streamlines AI model development-as well as conversational AI, sales and support enablement, and real-time streaming [features](https://blog.deepgram.com/deepgram-pioneers-novel-training-approach-setting-new-standard-for-ai-companies-2/) to help enable the next generation of human-like virtual agents.\n\n> \"For Conversational AI voicebots, it all starts off with speech recognition, if you don't understand what the person said and transcribe it to text accurately, you are not in the game. Unfortunately, the general ASR models standardize around 70% accuracy, and it is just not good enough to respond to a caller with real-time accuracy and relevance. Our partnership with Deepgram and their models in conjunction with our internal models that are trained on case-specific data get well over 90% accuracy.\" Dion Millson, CEO Elerian AI\n>\n> \"Voice technology is such a fast-moving market that we have to be everywhere at the same time and products change constantly,\" said Pete Ellis, Chief Product Officer of Red Box. \"With Deepgram, we can innovate and demonstrate to customers that we are ahead of the competition.\"\n\n**Deepgram's recent highlights and milestones include:** **Corporate Momentum** **New executive and senior hires:**\n\n* **Mehul Patel**, **VP of Product**: Mehul joins Deepgram from SoundHound, where he led product management for Houndify, the AI platform. He previously co-founded Synapse.ai as part of Stanford's StartX accelerator class in 2015, and worked at VMware and Adobe.\n* **Jason Rubinstein**, **VP of Finance**: Jason joins Deepgram from Yext, where he was vice president of finance. He previously held additional finance leadership roles at Marketo and Google, as well as served in the U.S. Navy as a nuclear submarine officer.\n* **Ralphette English,** **Head of Customer Success**: Ralphette was previously the VP of Customer Success at Deep 6 AI. Prior to that, she held various leadership roles in customer service, technical and implementation support for TeleSign, OpenX and Logics Solutions, LLC.\n* **Michael Jolley, Head of Developer Experience**: Michael was previously the senior developer advocate at Vonage. Prior to that, he spent more than 17 years leading software and tech at software companies Advanced Systems Unlimited and Success Products Inc.\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-state-of-voice-2022\"></WhitepaperPromo>\n\n**Product Leadership**\n\n* **Expanded base speech models**: Deepgram expanded its base language models from one language (English) to eight (English, French, Hindi, Korean, Portuguese, Russian, Spanish, and Turkish), and introduced a domain specific model for Conversational AI.\n* **10x streaming volume at 3x lower latencies**: Deepgram is now the fastest speech recognition platform on the market, delivering transcriptions with less than a 300 millisecond lag.\n* **Better hardware performance:** The Deepgram team improved the platform's hardware performance by 3x, and added a new data center dedicated to research.\n\n**Customer Excellence**\n\n* **New partner alliances**: Deepgram became a [founding member](https://blog.deepgram.com/deepgram-is-a-founding-member-of-callminers-open-voice-transcription-standard-ovts/) of CallMiner's Open Voice Transcription Standard (OVTS), and a [strategic partner](https://www.businesswire.com/news/home/20201203005228/en/Red-Box-and-Deepgram-Partner-to-Deliver-Highly-Scalable-Real-Time-Audio-Capture-and-Automatic-Speech-Recognition-for-Enterprises) of Red Box's Enterprise Voice Platform.\n* **Customer success:** Deepgram added more than 3.2x in customer revenue over the last year and brought on a Head of Customer Success, Ralphette English. Organizations such as Valyant AI, Plivo, Khoros, and Marchex shared their success on panels and in [case studies](https://deepgram.com/company/resources/).\n\n**About Deepgram** Deepgram is the leader in enterprise automatic speech recognition (ASR) for call centers and software providers. With our patented end-to-end deep learning approach, data scientists get access to the industry's fastest, most accurate and highly scalable AI technology. We take the heavy lifting out of noisy, multi-speaker, hard to understand audio transcription, so you can focus on what you do best. To learn more visit deepgram.com or [contact us](https://deepgram.com/contact-us/) to get started. **Press Contact:** [deepgram@inkhouse.com](mailto:deepgram@inkhouse.com)";
						}
						async function compiledContent$1u() {
							return load$1u().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1u() {
							return (await import('./chunks/index.873b851f.mjs'));
						}
						function Content$1u(...args) {
							return load$1u().then((m) => m.default(...args));
						}
						Content$1u.isAstroComponentFactory = true;
						function getHeadings$1u() {
							return load$1u().then((m) => m.metadata.headings);
						}
						function getHeaders$1u() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1u().then((m) => m.metadata.headings);
						}

const __vite_glob_0_182 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1u,
  file: file$1u,
  url: url$1u,
  rawContent: rawContent$1u,
  compiledContent: compiledContent$1u,
  default: load$1u,
  Content: Content$1u,
  getHeadings: getHeadings$1u,
  getHeaders: getHeaders$1u
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1t = {"title":"Browser Live Transcription - Protecting Your API Key","description":"Protect your API Key from unauthorized use with these tips.","date":"2022-01-17T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1642183098/blog/2022/01/protecting-api-key/Browser-Live-Transcription-Protecting-Your-API-Key%402x.jpg","authors":["kevin-lewis"],"category":"best-practice","tags":["browser"],"seo":{"title":"Browser Live Transcription - Protecting Your API Key","description":"Protect your API Key from unauthorized use with these tips."},"shorturls":{"share":"https://dpgr.am/c14514d","twitter":"https://dpgr.am/ceb2d19","linkedin":"https://dpgr.am/86668f1","reddit":"https://dpgr.am/801f139","facebook":"https://dpgr.am/5a7bbeb"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453850/blog/protecting-api-key/ograph.png"}};
						const file$1t = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/protecting-api-key/index.md";
						const url$1t = undefined;
						function rawContent$1t() {
							return "\r\nWe love how little code is needed to get live transcripts directly from a web browser with Deepgram, but doing so may leave your API Key vulnerable in a user-accessible client. Any user with access to your key can access the Deepgram APIs, which, in turn, may provide full account access.\r\n\r\nThis post will cover three approaches to live transcription from the browser while protecting your account. Before reading this guide, you should understand how to open a WebSocket connection and send data to Deepgram in the browser - if not, we cover it in this [blog post](https://blog.deepgram.com/live-transcription-mic-browser/).\r\n\r\n## Scopes and Roles\r\n\r\nEach project in your Deepgram console can have multiple API Keys. Each key has several 'scopes' that describe the key's permissions. For example, one key may provide access to manage a project's team members, while others may not.\r\n\r\nTo make working with scopes easier, we also provide some 'roles' which provide a defined list of scopes for you. Instead of providing six common scopes, you can use `member`, and it will apply them all for you when creating a key. We'll be creating keys with specific scopes in this guide, and if you want to learn more about roles, we have a [Working with Roles](https://developers.deepgram.com/documentation/getting-started/roles/) guide in our documentation.\r\n\r\n![One project can have multiple keys. The first key in this diagram has permissions to create new keys, alter project settings, and use transcription APIs. The second key only has access to transcription APIs.](https://res.cloudinary.com/deepgram/image/upload/v1639592303/blog/2022/01/protecting-api-key/project-keys-scopes.png)\r\n\r\n## Approach 1: Create and Delete Keys On-Demand\r\n\r\nDeepgram provides a set of API endpoints to manage project keys. In this approach, we will create a key when required and then delete it when finished.\r\n\r\nTo create and delete additional keys with the API, the key used for this operation must have the `keys:write` scope. This key will not be sent to the browser - its whole purpose is to manage sharable keys on our behalf. If you created the initial key in our web console, you would have assigned a role that will include the `keys:write` permission.\r\n\r\n![Clicking the start transcription button in the browser sends a request to the server which creates a usage-only key with Deepgram. Clicking the end transcription button deletes it.](https://res.cloudinary.com/deepgram/image/upload/v1639592303/blog/2022/01/protecting-api-key/create-destroy-key.png)\r\n\r\nAn abbreviated version of this with code with the [Deepgram Node.js SDK](https://developers.deepgram.com/sdks-tools/sdks/node-sdk/) may look like this:\r\n\r\n```html\r\n<!-- public/index.html -->\r\n<!DOCTYPE html>\r\n<html>\r\n  <body>\r\n    <button id=\"start\">Start transcription</button>\r\n    <button id=\"end\">End transcription</button>\r\n    <script>\r\n      let key, api_key_id, mediaRecorder, socket\r\n\r\n      document.querySelector('#start').addEventListener('click', async () => {\r\n        // Access key and key id from server\r\n        const result = await fetch('/key', { method: 'POST' }).then((r) =>\r\n          r.json()\r\n        )\r\n        key = result.key\r\n        api_key_id = result.api_key_id\r\n\r\n        // Standard logic utilizing fetched key\r\n        navigator.mediaDevices.getUserMedia({ audio: true }).then((stream) => {\r\n          mediaRecorder = new MediaRecorder(stream)\r\n          socket = new WebSocket('wss://api.deepgram.com/v1/listen', [ 'token', key ])\r\n          socket.onopen = () => {\r\n            mediaRecorder.addEventListener('dataavailable', async (event) =>\r\n              socket.send(event.data)\r\n            )\r\n            mediaRecorder.start(250)\r\n          }\r\n          socket.onmessage = (message) => console.log(JSON.parse(message))\r\n        })\r\n      })\r\n\r\n      document.querySelector('#end').addEventListener('click', async () => {\r\n        // Delete key\r\n        const result = await fetch('/key/' + api_key_id, {\r\n          method: 'DELETE',\r\n        }).then((r) => r.json())\r\n        console.log(result)\r\n\r\n        // Client logic for closing connection\r\n        socket.close()\r\n        mediaRecorder.stop()\r\n      })\r\n    </script>\r\n  </body>\r\n</html>\r\n```\r\n\r\n```js\r\n// index.js\r\nconst express = require('express')\r\nconst { Deepgram } = require('@deepgram/sdk')\r\nconst app = express()\r\nconst deepgram = new Deepgram('YOUR_DEEPGRAM_API_KEY')\r\napp.use(express.static('public'))\r\n\r\napp.post('/key', (req, res) => {\r\n  const { key, api_key_id } = await deepgram.keys.create(\r\n    'PROJECT_ID',\r\n    'Temporary user key',\r\n    ['usage:write']\r\n  )\r\n  res.json({ key, api_key_id })\r\n})\r\n\r\napp.delete('/key/:keyId', (req, res) => {\r\n  const result = await deepgram.keys.delete('PROJECT_ID', req.params.keyId)\r\n  res.json(result)\r\n})\r\n\r\napp.listen(3000)\r\n```\r\n\r\nIn this example, clicking the **start** button sends a request to our server, creating a brand-new Deepgram key with the only scope required to use transcription - `usage:write`. It then sends the API Key and key ID to the browser - we require the key ID to refer to this key when deleting it.\r\n\r\nWhen the user clicks the **end** button, a request is sent to our server, which, in turn, deletes the key so it is no longer usable.\r\n\r\n## Approach 2: Automatically-Expiring Keys\r\n\r\nWe recently released some additional properties you can provide when creating project keys via the API, which set an expiry time. After the provided time, the key is invalidated automatically. You may either provide `expiration_date` or `time_to_live_in_seconds`, so pick whatever is best for your use case.\r\n\r\nKeys are validated by Deepgram when a new live transcription session is started, so you can set a short `time_to_live_in_seconds` as it is only needed when initially connecting.\r\n\r\nYou can also do this with the Node.js SDK with an object containing either `expirationDate` or `timeToLive`:\r\n\r\n```js\r\napp.get('/deepgram-token', async (req, res) => {\r\n  const newKey = await deepgram.keys.create(\r\n    process.env.DG_PROJECT_ID,\r\n    'Temporary key - works for 10 secs',\r\n    ['usage:write'],\r\n    { timeToLive: 10 }\r\n  )\r\n\r\n  res.send(newKey)\r\n})\r\n```\r\n\r\n## Approach 3: Create a Server Proxy to Deepgram\r\n\r\nThe first two approaches in this guide are a good stop-gap, but you should avoid sending keys to the client wherever possible. The most common and recommended approach is to set up a signaling server that will proxy requests to and from your browser and Deepgram. This approach means that your server communicates with Deepgram, avoiding the need for a Deepgram API Key to be present in the browser.\r\n\r\nAn illustration for how this might work in code looks like this:\r\n\r\n```js\r\n// index.js\r\nconst express = require('express')\r\nconst app = express()\r\napp.use(express.static('public'))\r\n\r\nconst WebSocket = require('ws')\r\nconst wss = new WebSocket.Server({ port: 3001 })\r\n\r\nconst { Deepgram } = require('@deepgram/sdk')\r\nconst deepgram = new Deepgram(process.env.DG_KEY)\r\nconst deepgramLive = deepgram.transcription.live({ utterances: true })\r\n\r\ndeepgramLive.onopen = () => console.log('dg onopen')\r\n\r\nwss.on('connection', (ws) => {\r\n  ws.onmessage = (event) => deepgramLive.send(event.data)\r\n  deepgramLive.addListener('transcriptReceived', (data) => ws.send(data))\r\n})\r\n\r\napp.listen(3000)\r\n```\r\n\r\n```html\r\n<!-- public/index.html -->\r\n<!DOCTYPE html>\r\n<html>\r\n  <body>\r\n    <button id=\"start\">Start transcription</button>\r\n    <script>\r\n      document.querySelector('#start').addEventListener('click', async () => {\r\n        navigator.mediaDevices.getUserMedia({ audio: true }).then((stream) => {\r\n          const mediaRecorder = new MediaRecorder(stream)\r\n          const socket = new WebSocket(`ws://localhost:3001`)\r\n          socket.onopen = () => {\r\n            mediaRecorder.addEventListener('dataavailable', (event) =>\r\n              socket.send(event.data)\r\n            )\r\n            mediaRecorder.start(250)\r\n          }\r\n          socket.onmessage = (message) => console.log(JSON.parse(message.data))\r\n        })\r\n      })\r\n    </script>\r\n  </body>\r\n</html>\r\n```\r\n\r\nThis approach is very similar to opening a connection directly to Deepgram. In this example, however, we create a Node.js application that starts an express web application on port 3000 and a WebSocket server on port 3001, then connect to the WebSocket endpoint from our client. The server's main job is pushing data to and from the browser and Deepgram. These lines handle all of that logic:\r\n\r\n```js\r\nws.onmessage = (event) => deepgramLive.send(event.data)\r\ndeepgramLive.addListener('transcriptReceived', (data) => ws.send(data))\r\n```\r\n\r\n## In Summary\r\n\r\nAPI Keys created in the console with roles have more permissions than you would want users to have access to. Hopefully, this guide provides some useful strategies which allow you to protect your Deepgram account with minimal additional code in your project.\r\n\r\nIf you have any questions, please feel free to reach out on Twitter - we're [@DeepgramDevs](https://twitter.com/DeepgramDevs).\r\n\r\n        ";
						}
						async function compiledContent$1t() {
							return load$1t().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1t() {
							return (await import('./chunks/index.64ea3fd1.mjs'));
						}
						function Content$1t(...args) {
							return load$1t().then((m) => m.default(...args));
						}
						Content$1t.isAstroComponentFactory = true;
						function getHeadings$1t() {
							return load$1t().then((m) => m.metadata.headings);
						}
						function getHeaders$1t() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1t().then((m) => m.metadata.headings);
						}

const __vite_glob_0_183 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1t,
  file: file$1t,
  url: url$1t,
  rawContent: rawContent$1t,
  compiledContent: compiledContent$1t,
  default: load$1t,
  Content: Content$1t,
  getHeadings: getHeadings$1t,
  getHeaders: getHeaders$1t
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1s = {"title":"Deepgram Speech-to-Text Use Cases That Will Transform Your Life - (Special PyCon Edition)","description":"Let's look at compelling use cases for Python and Deepgram's speech recognition technology in preparation for PyCon US 2022.","date":"2022-04-14T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1649792989/blog/2022/04/pycon-deepgram-usecases/pycon-posts%402x.jpg","authors":["tonya-sims"],"category":"announcement","tags":["python"],"seo":{"title":"Deepgram Speech-to-Text Use Cases That Will Transform Your Life - (Special PyCon Edition)","description":"Let's look at compelling use cases for Python and Deepgram's speech recognition technology in preparation for PyCon US 2022."},"shorturls":{"share":"https://dpgr.am/24f2c59","twitter":"https://dpgr.am/c8eb3bc","linkedin":"https://dpgr.am/60878d1","reddit":"https://dpgr.am/69f0ab8","facebook":"https://dpgr.am/d51135b"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454071/blog/pycon-deepgram-usecases/ograph.png"}};
						const file$1s = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/pycon-deepgram-usecases/index.md";
						const url$1s = undefined;
						function rawContent$1s() {
							return "\nPop quiz. As a Python developer or enthusiast, why might you attend PyCon?\n\nA. Hear some of the most amazing talks and presentations on Python\nB. Expand your network with other Python professionals from around the world\nC. Land your dream job as a Pythonista\nD. Learn about the most innovative trends in Python land\nE. Participate in tutorials and workshops delivered by the top experts\nF. All of the above\n\nIf you chose F, All of the above, you are correct!\n\n## What is PyCon?\n\n[PyCon](https://us.pycon.org/2022/) is the largest Python conference for developers worldwide, and it’s back in person this year in Salt Lake City, Utah. The members of the Python community organize PyCon and welcome people of all developer experience levels. We at Deepgram are incredibly excited about PyCon US 2022 because we are one of the sponsors this year and have a booth. You might be wondering what’s so special about having one? The best part about it is that we get to meet people like yourself, and you can interact with our technology!\n\n## What is Deepgram?\n\nDeepgram is an automated voice-to-text company that allows you to build applications that transcribe speech-to-text, where you’ll receive an actual transcript of the person speaking or a conversation between multiple people. One of the many reasons to choose us over other providers is that we build better voice applications with faster, more accurate transcription through AI Speech Recognition.\n\nWe offer real-time transcription and pre-recorded speech-to-text, allowing you to upload a file with voice. We recently published a blog post on using our Python SDK to do live transcription with some of the most popular Python web frameworks, including [FastAPI](https://blog.deepgram.com/live-transcription-fastapi/), [Flask](https://blog.deepgram.com/live-transcription-flask/), [Django](https://blog.deepgram.com/live-transcription-django/), and [Quart](https://blog.deepgram.com/live-transcription-quart/).\n\nNow that you have a better understanding of Deepgram let’s see how you can use us to change the world.\n\n## Deepgram Speech-to-Text Use Cases\n\nAt Deepgram, we live by our mission of “We believe every voice should be heard – and understood.” We do this by making it easy for innovators to build scalable, more humanized voice experiences that could be game-changers or save lives.\n\nHere are some speech recognition use cases for Deepgram.\n\n### Medical Transcription Speech-to-Text\n\nImagine you have a loved one who needs medical attention. The physician usually takes notes by memory or types them into the computer when visiting the doctor. What if they could use the patient’s voice to capture what’s happening with them medically? The application can transcribe the real-time conversation between the doctor and patient. This feature will save lots of time which is crucial for saving a patient. You could also provide valuable insights from the speech recognition transcript that the doctor may have missed or look for important words or phrases the patient spoke using Deepgram’s search feature. You can check out more information about this feature on our [API Reference page](https://developers.deepgram.com/api-reference/).\n\n### Police BodyCam Analysis Speech-to-Text\n\nIn today’s world, there are police departments that have officers wearing body cameras. It records their interactions and behaviors using it as a training and safety tool. Using Deepgram’s Python SDK speech-to-text features, you can capture the audio into a transcript that can provide insights into how a police officer interacts with the public. To de-escalate situations, they can receive feedback from their superiors.\n\nAnother scenario is a police captain can analyze a Deepgram transcript using speech recognition to determine the effectiveness of something, like a traffic stop. Essential phrases can be verified, decreasing the number of lawsuits for police departments.\n\n### Accessibility Speech-to-Text\n\nImagine you have a friend who has a disability who cannot use their hands to type and needs the help of customer service at their favorite store. With Deepgram, you can build an application that allows for Conversational AI, to hold a conversation with a chatbot instead of typing. Chatbots are increasing in popularity and sometimes the only way to reach customer service but limit a whole population segment.\n\nUsing Deepgram Python speech recognition can also increase educational opportunities for those who may have learning disabilities. Instead of writing an essay or paper, you can build an editor that allows people to write with their voice.\n\n## Visit Deepgram at PyCon\n\nYou’ve now learned a handful of use cases for Deepgram voice-to-text transcription and Python. There are many more, and the opportunities are endless. Can you think of other uses cases for Deepgram and our Python SDK? To let us know, you can Tweet us at [@deepgramdevs](https://twitter.com/DeepgramDevs). We would love to see you at PyCon US 2022, so stop by and say hello!\n\n        ";
						}
						async function compiledContent$1s() {
							return load$1s().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1s() {
							return (await import('./chunks/index.59ec67f8.mjs'));
						}
						function Content$1s(...args) {
							return load$1s().then((m) => m.default(...args));
						}
						Content$1s.isAstroComponentFactory = true;
						function getHeadings$1s() {
							return load$1s().then((m) => m.metadata.headings);
						}
						function getHeaders$1s() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1s().then((m) => m.metadata.headings);
						}

const __vite_glob_0_184 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1s,
  file: file$1s,
  url: url$1s,
  rawContent: rawContent$1s,
  compiledContent: compiledContent$1s,
  default: load$1s,
  Content: Content$1s,
  getHeadings: getHeadings$1s,
  getHeaders: getHeaders$1s
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1r = {"title":"Using Python With Speech-to-Text (Special PyCon Edition)","description":"Let's look at how we can use Python and Deepgram's speech recognition technology together in preparation for PyCon US 2022.","date":"2022-04-21T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1650386018/blog/2022/04/pycon-python-speech-to-text/pycon-posts%402x.jpg","authors":["tonya-sims"],"category":"announcement","tags":["python"],"seo":{"title":"Using Python With Speech-to-Text (Special PyCon Edition)","description":"Let's look at how we can use Python and Deepgram's speech recognition technology together in preparation for PyCon US 2022."},"shorturls":{"share":"https://dpgr.am/6a4d67a","twitter":"https://dpgr.am/1d02f94","linkedin":"https://dpgr.am/f232fb0","reddit":"https://dpgr.am/a7dc3a9","facebook":"https://dpgr.am/76602c4"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454073/blog/pycon-python-speech-to-text/ograph.png"}};
						const file$1r = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/pycon-python-speech-to-text/index.md";
						const url$1r = undefined;
						function rawContent$1r() {
							return "\nDid you know that 90% of the world’s data is not being used for business insights? Most of this data is unstructured, making it difficult to search and organize. Unlike structured data, unstructured data is harder to store in a traditional database where it's mapped to different fields. You can think of it as user-created data like voice and video, which accounts for most unstructured data.\n\nLet’s discover how a voice-to-text AI provider like Deepgram, combined with converting speech-to-text using Python, can alleviate headaches for you and help you tap into missed opportunities that could help you solve some of the world’s biggest problems.\n\n## What is Deepgram?\n\nDeepgram is a speech recognition company that transcribes real-time or recorded audio from voice-to-text. You can see how this is super helpful with unstructured data by gathering from voice and changing it into machine-readable data for making decisions. You can even use Deepgram for video by overlaying on top, collecting subtitles, and running insights or analytics. Here’s an [example of speech recognition analytics in Python you can create](https://blog.deepgram.com/python-talk-time-analytics/).\n\nThis year, we also have a booth at PyCon in Salt Lake City, Utah, and we can’t wait to meet you! You’ll be able to try our technology and chat with people who work for Deepgram.\n\n## Why Deepgram?\n\nDeepgram has a remarkable architecture that makes it stand out from the rest of the other speech recognition providers in the voice-to-text market. At Deepgram, we use End-to-End Deep Learning Speech Recognition, the most cutting-edge and newest technology currently being used. With this innovative speech-to-text architecture, you’ll receive more accurate transcripts and increase the number of trained models for different languages, use cases, dialects, accents, and industry jargon. With End-to-End Deep Learning, it’s continuously “learning” and improving to provide a better experience.\n\nDeepgram has tons of powerful features as well, including but not limited to:\n\nReal-time or Streaming Transcription\nRecorded or Batch Transcription\nNoise Reduction\nDiarization\nAnd More!\n\nLet’s take a look at these features in more detail.\n\n### Real-time or Streaming Transcription Feature in Speech-To-Text\n\nThe real-time feature allows you to receive a live voice-to-text transcription as the audio is streaming. This feature is crucial because it enables you to receive transcripts faster, makes them more inclusive and accessible, and increases transparency.\n\n### Recorded or Batch Audio Feature in Speech-To-Text\n\nWith the Recorded speech-to-text feature, you can upload a file in many formats and receive a transcription. The recorded transcriptions would be a great place to incorporate storytelling or visualization with Python.\n\n### Noise Reduction Feature in Speech-To-Text\n\nOur Noise Reduction feature helps identify and reduces background noise. This feature is critical as it dramatically improves the accuracy of a transcript.\n\n### Diarization Feature in Speech-To-Text\n\nDeepgram’s Diarization feature recognizes the voice of multiple speakers and assigns a speaker to each work in the transcript. As you can imagine, this is wonderful when there is more than one speaker, and you can analyze a whole conversation.\n\nCheck out [this article](https://deepgram.com/the-definitive-guide-to-speech-recognition/) for our complete list of Deepgram features.\n\n## How to use Deepgram?\n\nTo get started with our Deepgram Python SDK, you can visit our open-sourced [Github repository here](https://github.com/deepgram/python-sdk). The samples in the `README.md` will help you get up and running with real-time streaming and recorded audio. We also created special Python documentation for developers, which you can access [here](https://developers.deepgram.com/sdks-tools/sdks/python-sdk/).\n\nIf you’re interested in learning about different use cases for Deepgram, we just published an article about how [speech-to-text with Deepgram can transform lives around the world](https://blog.deepgram.com/pycon-deepgram-usecases/).\n\n## Will We See You at PyCon?\n\nWe are very excited to help sponsor PyCon this year. When you’re free, you can stop by our booth and try out our Deepgram application to see how our voice-to-text works. In the meantime, please feel free to message us on Twitter at [@DeepgramDevs](https://twitter.com/DeepgramDevs).\n\nWe’re looking forward to connecting with you!\n\n        ";
						}
						async function compiledContent$1r() {
							return load$1r().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1r() {
							return (await import('./chunks/index.95377e09.mjs'));
						}
						function Content$1r(...args) {
							return load$1r().then((m) => m.default(...args));
						}
						Content$1r.isAstroComponentFactory = true;
						function getHeadings$1r() {
							return load$1r().then((m) => m.metadata.headings);
						}
						function getHeaders$1r() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1r().then((m) => m.metadata.headings);
						}

const __vite_glob_0_185 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1r,
  file: file$1r,
  url: url$1r,
  rawContent: rawContent$1r,
  compiledContent: compiledContent$1r,
  default: load$1r,
  Content: Content$1r,
  getHeadings: getHeadings$1r,
  getHeaders: getHeaders$1r
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1q = {"title":"Starting Out with Python and Deepgram Live Streaming Audio","description":"Learn how to perform real-time Automatic Speech Recognition using various Python web frameworks and Deepgram's Speech-to-Text API with this roundup post.","date":"2022-06-23T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1655914351/blog/2022/06/python-deepgram-roundup/Transcribing-Real-Time-Audio-Using-Python-in-5-Minutes%402x.jpg","authors":["tonya-sims"],"category":"tutorial","tags":["python","fastapi"],"seo":{"title":"Starting Out with Python and Deepgram Live Streaming Audio","description":"Learn how to perform real-time Automatic Speech Recognition using various Python web frameworks and Deepgram's Speech-to-Text API with this roundup post."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454100/blog/python-deepgram-roundup/ograph.png"},"shorturls":{"share":"https://dpgr.am/bf4bebf","twitter":"https://dpgr.am/d4a669d","linkedin":"https://dpgr.am/f0f7a33","reddit":"https://dpgr.am/039d224","facebook":"https://dpgr.am/02c041e"}};
						const file$1q = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/python-deepgram-roundup/index.md";
						const url$1q = undefined;
						function rawContent$1q() {
							return "## Python Web Frameworks for Live Audio Transcription\n\nThis blog post will summarize how to transcribe speech-to-text streaming audio in real-time using Deepgram with four different Python web frameworks. At Deepgram, we have a Python SDK that handles pre-recorded and live streaming speech recognition transcription, which can be used with your framework of choice.\n\n### FastAPI Live Streaming Audio\n\nFastAPI is a new, innovative Python web framework gaining popularity because of its modern features, such as concurrency and asynchronous code support.\n\nWorking with WebSockets in FastAPI is a breeze because it uses the [WebSocket API](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API), making it easier to establish two-way communication between the browser and server. There’s a section about working with WebSockets in the [FastAPI documentation](https://fastapi.tiangolo.com/advanced/websockets/).\n\nFastAPI is very easy to use because of its thorough documentation, so even beginners can get started. Remember that supporting community resources, as a newer Python web framework, may not be as robust as other options. It didn’t take long to get FastAPI up and running with Deepgram’s live streaming audio speech-to-text transcription in Python. We wrote a [step-by-step tutorial](https://blog.deepgram.com/live-transcription-fastapi/) on using FastAPI with Deepgram for real-time audio transcription in Python.\n\n### Flask 2.0 Live Streaming Audio\n\nFlask 2.0 is a familiar, lightweight, micro web framework that is very flexible. It doesn't make decisions for you, meaning you are free to choose which database, templating engine, etc., to use without lacking functionality. Check out the [tutorial we wrote on using Flask](https://blog.deepgram.com/live-transcription-flask/) to get up and running with a live-streamed audio speech-to-text transcript in Python.\n\nFlask does not have WebSocket support built-in, but there is a workaround. You use [aiohttp](https://docs.aiohttp.org/en/v3.8.1/faq.html), an Async HTTP client/server for asyncio and Python. It also supports server and client WebSockets out of the box.\n\nOnce you get aiohttp configured for WebSockets, getting Flask 2.0 working with Deepgram is pretty straightforward. If you'd like to work with a Python framework similar to Flask with WebSocket support built-in, you can use Quart.\n\n### Quart Live Streaming Audio\n\nQuart is a Python web microframework that is asynchronous, making it easier to serve WebSockets. Quart is an asyncio reimplementation of Flask. If you're familiar with Flask, you'll be able to ramp up on Quart quickly. We have a tutorial on using [Quart with Deepgram](https://blog.deepgram.com/live-transcription-quart/) live streaming audio speech-to-text.\n\nGetting started with Quart was very simple. They have a short tutorial on WebSockets on their website that covers the basics. Since Quart is very similar to Flask, there wasn’t as much ramp-up time, which is nice. Quart also has support for WebSockets, so there was no need for extra configuration, and it worked perfectly with Deepgram’s live streaming audio.\n\n### Django Live Streaming Audio\n\nDjango is a familiar Python web framework for rapid development. It provides a lot of things you need \"out of the box\" and everything is included with the framework, following a “Batteries included” philosophy.\n\nDjango uses [Channels](https://channels.readthedocs.io/en/stable/introduction.html) to handle WebSockets. It allows for real-time communication to happen between a browser and a server. The Django Channels setup was different than the other three Python web frameworks but was easy to follow because of their documentation. It might be good to have a little experience with Django, but if you want to use it with Deepgram, check out the [blog post](https://blog.deepgram.com/live-transcription-django/) we wrote on using Django to handle real-time speech-to-text transcription.\n\n## Final Words\n\nHopefully, you can see that regardless of your application's Python web framework choice, you can use Deepgram speech-to-text live streaming transcription. As a next step, you can go to the [Deepgram console](https://console.deepgram.com/) and grab an API Key. You'll need this key to do speech-to-text transcription with Deepgram and Python. We also have missions to try in the console to get up and running quickly with real-time or pre-recorded audio-to-text transcription.\n\nPlease feel free to Tweet us at [@deepgramdevs](https://twitter.com/DeepgramDevs). We would love to hear from you!";
						}
						async function compiledContent$1q() {
							return load$1q().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1q() {
							return (await import('./chunks/index.c8c62373.mjs'));
						}
						function Content$1q(...args) {
							return load$1q().then((m) => m.default(...args));
						}
						Content$1q.isAstroComponentFactory = true;
						function getHeadings$1q() {
							return load$1q().then((m) => m.metadata.headings);
						}
						function getHeaders$1q() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1q().then((m) => m.metadata.headings);
						}

const __vite_glob_0_186 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1q,
  file: file$1q,
  url: url$1q,
  rawContent: rawContent$1q,
  compiledContent: compiledContent$1q,
  default: load$1q,
  Content: Content$1q,
  getHeadings: getHeadings$1q,
  getHeaders: getHeaders$1q
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1p = {"title":"Speech Recognition with Twilio and Python","description":"Use Deepgram's speech-to-text features with Python and Twilio to transcribe audio such as incoming phone calls.","date":"2022-04-13T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1649274528/blog/2022/04/python-deepgram-twilio/Speech-Analytics-Real-Time-Audio-w-Twilio-Python%402x.jpg","authors":["tonya-sims"],"category":"tutorial","tags":["python","twilio"],"seo":{"title":"Speech Recognition with Twilio and Python","description":"Use Deepgram's speech-to-text features with Python and Twilio to transcribe audio such as incoming phone calls."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454075/blog/python-deepgram-twilio/ograph.png"},"shorturls":{"share":"https://dpgr.am/8ff4f6b","twitter":"https://dpgr.am/0abeb3e","linkedin":"https://dpgr.am/4674338","reddit":"https://dpgr.am/25a704d","facebook":"https://dpgr.am/a3fa9d8"}};
						const file$1p = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/python-deepgram-twilio/index.md";
						const url$1p = undefined;
						function rawContent$1p() {
							return "Imagine having the ability to transcribe your voice calls. Look no further because we’ll learn how to do that in this article by combining Twilio with Deepgram.\n\nWith Twilio, we can use one of their phone numbers to receive and record incoming calls and get a transcript using the Deepgram Speech Recognition API. We’ll use the Deepgram Python SDK in this example.\n\nHere’s a snapshot of what we’ll see in the browser after making the phone call and using Deepgram voice-to-text.\n\n![Deepgram voice-to-text with Twilio](https://res.cloudinary.com/deepgram/image/upload/v1649274530/blog/2022/04/python-deepgram-twilio/deepgram_twilio_transcribe.png)\n\n## Getting Started\n\nBefore we start, it’s essential to generate a Deepgram API key to use in our project. We can go to our [Deepgram console](https://console.deepgram.com/signup?jump=keys). Make sure to copy it and keep it in a safe place, as you won’t be able to retrieve it again and will have to create a new one. In this tutorial, we’ll use Python 3.10, but Deepgram supports some earlier versions of Python.\n\nMake sure to go to [Twilio](https://www.twilio.com/login?g=%2Fconsole-zen%3F&t=9de6cbac864dd16dddf0f56899857674d172ed98651d03476c82bc96f0bf39e0) and sign up for an account. We’ll need to purchase a phone number with voice capabilities.\n\nWe’ll also need two phones to make the outgoing call and another to receive a call.\n\nIn our project, we’ll use Ngrok, which provides a temporary URL that will act as the webhook in our application. Ngrok will forward requests to our application that is running locally. You can download it [here](https://ngrok.com/).\n\nNext, let’s make a directory anywhere we’d like.\n\n```bash\nmkdir deepgram-twilio\n```\n\nThen change into that directory so we can start adding things to it.\n\n```bash\ncd deepgram-twilio\n```\n\nWe’ll also need to set up a virtual environment to hold our project and its dependencies. We can read more about those [here](https://blog.deepgram.com/python-virtual-environments/) and how to create one.\n\n<Panel> Panel with important note\nIt’s recommended in Python to use a virtual environment so our project can be installed inside a container rather than installing it system-wide. </Panel>\n\nEnsure our virtual environment is activated because we’ll install dependencies inside. If our virtual environment is named `venv`, then activate it.\n\n```bash\nsource venv/bin/activate\n```\n\nLet’s install our dependencies for our project by running the below `pip` installs from our terminal inside our virtual environment.\n\n```bash\n pip install deepgram-sdk\n pip install twilio\n pip install python-dotenv\n pip install Flask\n pip install 'flask[async]'\n pip install pysondb\n```\n\nNow we can open up our favorite editor and create a file called `deepgram-twilio-call.py`. If you’d like to make it from the command line, do this:\n\n```\ntouch deepgram-twilio-call.py\n```\n\n## The Code\n\nNow to the fun part! Open our script called `deepgram-twilio-call.py` and add the following code to make sure our Flask application runs without errors:\n\n```python\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.get(\"/\")\ndef hello():\n    return \"Hello World!\"\n\nif __name__ == \"__main__\":\n    app.run()\n```\n\nRun our Flask application by typing this into the terminal `python deepgram-twilio-call.py`.\n\nThen pull up the browser window by going to `http://127.0.0.1:5000/` and we should see the text `Hello World`.\n\nAt the same time our application is running, open a new terminal window and type ![ngrok terminal with python flask](https://res.cloudinary.com/deepgram/image/upload/v1649274531/blog/2022/04/python-deepgram-twilio/ngrok-terminal-with-python-flask.png):\n\n```\nngrok http 127.0.0.1:5000\n```\n\nCopy the ngrok url and add it to Twilio by navigating to ‘Phone Numbers -> Manage -> Active Numbers’, then click on your Twilio phone number.\n\n![manage Twilio phone number](https://res.cloudinary.com/deepgram/image/upload/v1649274531/blog/2022/04/python-deepgram-twilio/active-twilio-numbers.png)\n\nScroll down to the ‘Voice’ section and add the webhook, our ngrok URL with the recordings endpoint and save. Like this `https://6d71-104-6-9-133.ngrok.io/recordings`\n\n![twilio webhook ngrok](https://res.cloudinary.com/deepgram/image/upload/v1649274530/blog/2022/04/python-deepgram-twilio/twilio-webhook-ngrok.png)\n\nWe’ll implement the `/recordings` endpoint in a few.\n\nLeave both terminals running as we’ll need these to run our application and receive the phone call.\n\nLet’s store our environment variables in a `.env` file with the following:\n\n```\nDEEPGRAM_API_KEY=[‘YOUR_API_KEY’]\nRECEIVER_NUMBER=[‘PHONE_NUMBER_TO_RECEIVE_CALL’]\n```\n\nWe can replace `YOUR_API_KEY` with the API key we received from signing up in the Deepgram console, and the `PHONE_NUMBER_TO_RECEIVE_CALL` is the phone number we would like to receive the call.\n\nLet’s replace the code in our `deepgram-twilio-call.py` with the following:\n\n```python\nimport asyncio\nimport json\nimport os\n\nfrom flask import Flask, request, render_template\nfrom deepgram import Deepgram\nfrom twilio.twiml.voice_response import Dial, VoiceResponse\nfrom twilio.rest import Client\nfrom pysondb import db\nfrom dotenv import load_dotenv\n\napp = Flask(__name__)\n\ncalls_db=db.getDb('calls')\n\nload_dotenv()\n\n@app.post(\"/inbound\")\ndef inbound_call():\n  response = VoiceResponse()\n  dial = Dial(\n      record='record-from-answer-dual',\n      recording_status_callback='https://6d71-104-6-9-133.ngrok.io/recordings'\n      )\n\n  dial.number(os.getenv(\"RECEIVER_NUMBER\"))\n  response.append(dial)\n\n  return str(response)\n```\n\nHere we are importing our libraries and creating a new instance of a Flask application. Then we create a new database named `calls`. We are using a lightweight JSON database called [PysonDB](https://dev.to/fredysomy/pysondb-a-json-based-lightweight-database-for-python-ija).\n\nWe create the `/inbound` endpoint, which allows us to make a voice call. The parameter `record='record-from-answer-dual'` will help us make a dual call or a phone that can call another.\n\nNext, in our `/recordings` route below, we tap into Deepgram’s speech-to-text feature by getting the recording of our call and using speech recognition to transcribe the audio. We check if `results` is in the response and format it by using a list comprehension and storing the results in `utterances`. We then add the `utterances` to the `calls` database.\n\n```python\n@app.route(\"/recordings\", methods=['GET', 'POST'])\nasync def get_recordings():\n   deepgram = Deepgram(os.getenv(\"DEEPGRAM_API_KEY\"))\n\n   recording_url = request.form['RecordingUrl']\n   source = {'url': recording_url}\n   transcript_data = await deepgram.transcription.prerecorded(source, {'punctuate': True,\n    'utterances': True,\n    'model': 'phonecall',\n    'multichannel': True\n  })\n\n   if 'results' in transcript_data:\n       utterances = [\n           {\n               'channel': utterance['channel'],\n               'transcript': utterance['transcript']\n           } for utterance in transcript_data['results']['utterances']\n       ]\n\n       calls_db.addMany(utterances)\n\n       return json.dumps(utterances, indent=4)\n```\n\nWe can see how the utterances will look after they’re formatted:\n\n```\n[{'channel': 0, 'transcript': 'Hello?', 'id': 288397603074461838},\n{'channel': 1, 'transcript': 'Hello?', 'id': 109089630999017748},\n{'channel': 0, 'transcript': \"Hey. How's it going? It's good to hear from you.\", 'id': 124620676610936565},\n{'channel': 0, 'transcript': 'Thanks. You too.', 'id': 182036969834868158},\n{'channel': 1, 'transcript': 'Thanks. You too.', 'id': 817052835121297399}]\n```\n\nLastly, let’s add our `/transcribe` route and a templates folder with an `index.html` file that will display our phone speech-to-text transcript.\n\nIn our Python file, add the following code, which will get the voice-to-text transcript from the database and renders them in the HTML template.\n\n```python\n@app.route(\"/transcribe\", methods=['GET', 'POST'])\ndef transcribe_call():\n   context = calls_db.getAll()\n   return render_template(\"index.html\", context=context )\n\n\nif __name__ == \"__main__\":\n   app.run(debug=True)\n```\n\nCreate a folder in our project directory called `templates` and add an `index.html` file. In that file, add the following HTML and Jinja code:\n\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n  <head>\n    <meta charset=\"UTF-8\" />\n    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\" />\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n    <title>Document</title>\n  </head>\n  <body>\n    {% for c in context %} {{ c.transcript }} <br />\n    {% endfor %}\n  </body>\n</html>\n```\n\nHere we loop through every transcript and display it on the screen.\n\nFinally, let’s try making a phone call and using your non-Twilio phone to initiate a phone conversation with the phone number you provided in the environment variable `RECEIVER_NUMBER`. We should be able to receive a call and engage in a conversation. After we hang up, the transcript will appear in our browser.\n\nCongratulations on building a speech-to-text Python project with Twilio and Deepgram! If you have any questions, please feel free to reach out to us on Twitter at [@DeepgramDevs](https://twitter.com/DeepgramDevs).";
						}
						async function compiledContent$1p() {
							return load$1p().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1p() {
							return (await import('./chunks/index.a2ce9b44.mjs'));
						}
						function Content$1p(...args) {
							return load$1p().then((m) => m.default(...args));
						}
						Content$1p.isAstroComponentFactory = true;
						function getHeadings$1p() {
							return load$1p().then((m) => m.metadata.headings);
						}
						function getHeaders$1p() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1p().then((m) => m.metadata.headings);
						}

const __vite_glob_0_187 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1p,
  file: file$1p,
  url: url$1p,
  rawContent: rawContent$1p,
  compiledContent: compiledContent$1p,
  default: load$1p,
  Content: Content$1p,
  getHeadings: getHeadings$1p,
  getHeaders: getHeaders$1p
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1o = {"title":"How to Turn Transcripts into Data Visualizations with Python","description":"Use Deepgram's speech-to-text features with Python to transcribe audio and graphing library Matplotlib to create a data visualization dashboard.","date":"2022-05-12T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1652286544/blog/2022/05/python-graphing-transcripts/Build-Dashboard-Visualize-Real-Time-Speech-Python%402x.jpg","authors":["tonya-sims"],"category":"tutorial","tags":["python","data-visualization"],"seo":{"title":"How to Turn Transcripts into Data Visualizations with Python","description":"Use Deepgram's speech-to-text features with Python to transcribe audio and graphing library Matplotlib to create a data visualization dashboard."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454089/blog/python-graphing-transcripts/ograph.png"},"shorturls":{"share":"https://dpgr.am/19fc3de","twitter":"https://dpgr.am/cae50e6","linkedin":"https://dpgr.am/a9a4fcf","reddit":"https://dpgr.am/983a9bf","facebook":"https://dpgr.am/3e7c681"}};
						const file$1o = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/python-graphing-transcripts/index.md";
						const url$1o = undefined;
						function rawContent$1o() {
							return "240 million emergency 911 calls are made in the United States per year. That averages out to roughly 600,000 calls per day. However, many of those calls are not emergencies. First responders often respond to barking dog complaints when people in need could use those resources.\n\nIt’s estimated that nearly 10,000 lives could be saved every year if emergency response times were reduced by one minute. Is there a way to visualize emergency calls by their type? Can we analyze the result and measure how to limit wasting resources on non-emergencies? Can we help increase the well-being of others when they’re having an emergency?\n\nThe answers are Yes, Yes, and Yes! We can combine speech-to-text using Deepgram and turn transcripts into data visualizations using a Python package like Matplotlib. Let's see why these two technologies are a perfect match.\n\n## What is Deepgram?\n\n​​Deepgram is an automated speech recognition voice-to-text company that allows you to build applications that transcribe speech-to-text. You’ll receive an actual transcript of the person speaking or a conversation between multiple people. One of the many reasons to choose Deepgram over other providers is that we build better voice applications with faster, more accurate transcription through AI Speech Recognition.\n\nWe offer real-time transcription and pre-recorded speech-to-text. The latter allows uploading of a file that contains audio voice data to be transcribed. We recently published a few blog posts on using our Python SDK to do live transcription with some of the most popular Python web frameworks, including [FastAPI](https://blog.deepgram.com/live-transcription-fastapi/), [Flask](https://blog.deepgram.com/live-transcription-flask/), [Django](https://blog.deepgram.com/live-transcription-django/), and [Quart](https://blog.deepgram.com/live-transcription-quart/).\n\n## The Deepgram Python SDK Project With Matplotlib Visualization\n\nNow that you have a better understanding of Deepgram, let’s see how we can use the Deepgram speech-to-text Python SDK to turn transcripts into data visualizations with a package like Matplotlib. In the following project, let’s transcribe pre-recorded audio with Deepgram and use a bar graph to analyze the types of emergency calls and how many of those calls are received.\n\n## Setting Up the Deepgram Speech-to-Text Python Project\n\nBefore we start, it’s essential to generate a Deepgram API key to use in our project. We can go to the [Deepgram Console](https://console.deepgram.com/signup?jump=keys). We'll make sure to copy it and keep it in a safe place, as we won’t be able to retrieve it again and will have to create a new one. In this tutorial, we’ll use Python 3.10, but Deepgram supports some earlier versions of Python.\n\nNext, we'll make a directory anywhere we’d like.\n\n```\nmkdir deepgram-dashboard\n```\n\nThen we'll change into that directory to start adding things to it.\n\n```\ncd deepgram-dashboard\n```\n\nWe’ll also need to set up a virtual environment to hold the project and its dependencies. We can read more about those [here](https://blog.deepgram.com/python-virtual-environments/) and how to create one. It’s recommended in Python to use a virtual environment so the project can be installed inside a container rather than installing it system-wide.\nWe need to ensure the virtual environment is activated because we’ll install dependencies inside. If the virtual environment is named `venv`, we'll need to activate it.\n\n```\nsource venv/bin/activate\n```\n\nWe'll install the dependencies for the project by running the below `pip` installs from the terminal inside the virtual environment.\n\n```\npip install deepgram-sdk\npip install python-dotenv\npip install matplotlib\n```\n\nWe now can open up an editor and create an environment variable file to store the Deepgram API Key from the [Deepgram Console](https://console.deepgram.com/). Create a new file called `.env` at the project level and add the following Python environment variable, replacing `[YOUR_API_KEY]` with the API Key from the console:\n\n```\nDEEPGRAM_API_KEY=”[YOUR_API_KEY]”\n```\n\nLastly, files with audio need to be added to the project so Deepgram can transcribe them. This project uses small audio-created samples using the PCM recorder lite for [Apple](https://apps.apple.com/us/app/pcm-recorder-lite/id439572045) or [Android](https://play.google.com/store/apps/details?id=com.kohei.android.pcmrecorder&hl=en_US&gl=US). This app will create `.wav`audio files but please note that Deepgram supports over [100+ audio formats and encodings](https://developers.deepgram.com/documentation/getting-started/audio-formats/).\n\n## The Code for the Deepgram Speech-to-Text Python Project with Matplotlib Graphing\n\nNow to the fun part! Let’s create a file called `transcribe-with-deepgram.py`, which holds all of the code in this project.\n\nThe project structure looks like this:\n\n![Deepgram speech-to-text project structure](https://res.cloudinary.com/deepgram/image/upload/v1652286550/blog/2022/05/python-graphing-transcripts/deepgram-project.png)\n\n### The Python Imports\n\nLet’s open the file `transcribe-with-deepgram.py` and add the following imports:\n\n```python\nimport asyncio\nimport os\nfrom collections import Counter\nfrom deepgram import Deepgram\nfrom dotenv import load_dotenv\nfrom matplotlib import pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\n```\n\n* `import asyncio` helps with writing asynchronous code in Python with the `async` and `await` keywords.\n* `import os` helps working with files and directories.\n* `from collections import Counter` helps to count key/value pairs in an object which is needed to track the words from the transcript and how many times they were spoken.\n* `from deepgram import Deepgram` allows access to the Deepgram Python SDK and its types like pre-recorded and live streaming transcription.\n* `from dotenv import load_dotenv` reads the key/value pairs from the `.env` file and sets them as environment variables.\n* `from matplotlib import pyplot as plt` creates a figure, a plotting area in a figure, plots some lines in a plotting area and decorates the plot with labels.\n* `from matplotlib.ticker import MaxNLocator` helps provide the graph with friendly integer tick values.\n\n### The Python Globals\n\nLet’s add this code underneath the imports:\n\n```python\nload_dotenv()\n\nDEEPGRAM_API_KEY = os.getenv('DEEPGRAM_API_KEY')\n\nfiles = [filename for filename in os.listdir() if filename.endswith('.wav')]\n\nwords_list = []\n```\n\nThe first line `load_dotenv()` loads the environment variables from the `.env` file and makes them available in the project.\n\nThis line `DEEPGRAM_API_KEY = os.getenv('DEEPGRAM_API_KEY')` uses `os.getenv()` to return the value of the environment variable key, if it exists, and sets it to a variable.\n\nThe `files` variable holds all of the files in our directory that end in `wav` as we loop through each, indicated by the list comprehension `[filename for filename in os.listdir() if filename.endswith('.wav')]`.\n\nFinally, an empty list called `words_list` is created, storing the words extracted from the JSON response Deepgram returns.\n\n### Get the Deepgram Speech-to-Text Transcript\n\nLet’s add our first function to the `transcribe-with-deepgram.py` file.\n\n```python\nasync def get_transcript():\n    deepgram = Deepgram(DEEPGRAM_API_KEY)\n\n    words_count = Counter()\n\n    for file in files:\n        with open(file, 'rb') as audio:\n            source = {'buffer': audio, 'mimetype': 'audio/wav'}\n            response = await deepgram.transcription.prerecorded(source, {'punctuate': True})\n\n            if 'results' in response:\n                get_words = response['results']['channels'][0]['alternatives'][0]['words']\n                for words in get_words:\n                    word = words['word']\n                    words_list.append(word)\n\n\n        words_count += Counter([w.lower() for w in words_list if w.lower() not in ['a', 'the', 'is', 'this', 'i', 'to', 'and']])\n\n    return words_count\n```\n\nHere `deepgram = Deepgram(DEEPGRAM_API_KEY)` Deepgram is initialized by providing the API Key from variable `DEEPGRAM_API_KEY` below the imports.\n\n`words_count = Counter()` creates a `Counter` object that holds key/value pairs of the words spoken in the transcript and how many times they appear.\n\nIn the below code snippet, we iterate through the `.wav` audio files in our directory and open each one. The source is set to a dictionary with the `buffer` value as `audio` and `mimetype` as `audio/wav`. If we were using `.mp3` files the `mimetype` would be `audio/mp3`. The next line is where the actual Deepgram transcription happens with the pre-recorded audio `await deepgram.transcription.prerecorded(source, {'punctuate': True})`. Notice the `source` is passed in along with a dictionary `{'punctuate': True}`, which is a Deepgram feature that adds punctuation and capitalization to the transcript.\n\n```python\nfor file in files:\n    with open(file, 'rb') as audio:\n        source = {'buffer': audio, 'mimetype': 'audio/wav'}\n        response = await deepgram.transcription.prerecorded(source, {'punctuate': True)\n```\n\nTo get the words from the transcript, let’s check the JSON response object for `results`. Then we loop through the response and parse it to find each word in the transcript and append it to our list called `words_list` that was defined earlier.\n\n```python\nif 'results' in response:\n    get_words = response['results']['channels'][0]['alternatives'][0]['words']\n    for words in get_words:\n        word = words['word']\n        words_list.append(word)\n```\n\nIn the last part of the function, we take our `words_count` Counter and create a list comprehension that appends all the words in the list `words_list` with counts. For example, it will have key/value pairs with each word from the transcript and how many times they appeared. The last line, `return words_count` returns it, so it’s accessible outside our function when we need it.\n\n```python\nwords_count += Counter([w.lower() for w in words_list if w.lower() not in ['a', 'the', 'is', 'this', 'i', 'to', 'and']])\n\nreturn words_count\n```\n\n### Data Visualization with Matplotlib\n\nLet’s look at turning transcripts into data visualizations by creating a function called `get_graph()`.\n\n```python\nasync def get_graph():\n    words = await get_transcript()\n\n    x = range(len(words.keys()))\n    width = 0.35\n\n    fig, ax = plt.subplots()\n\n    ax.set_ylabel('Word Count')\n    ax.set_xlabel('Emergency Call Types')\n    ax.set_title('Deepgram Transcript')\n    ax.set_xticks(x)\n    ax.set_xticklabels(words.keys())\n    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n\n    pps = ax.bar([i - width/2 for i in x], words.values(), width, label='words')\n    for p in pps:\n        height = p.get_height()\n        ax.annotate('{}'.format(height),\n            xy=(p.get_x() + p.get_width() / 2, height),\n            xytext=(0, 3),\n            textcoords='offset points',\n            ha='center', va='bottom')\n\n    plt.show()\n```\n\nA lot is going on in this function, so let’s simplify it by looking at the code in bigger chunks.\n\nLet’s get the returned value of `words_count` from the previous function by creating a new object `words = await get_transcript()`.\n\nThe code below sets the labels on the x and y-axis, sets the title of the bar graph, and grabs the keys. The keys are the words in the transcript from the `word` object. Then it places each in the chart.\n\n```python\nax.set_ylabel('Word Count')\nax.set_xlabel('Emergency Call Types')\nax.set_title('Deepgram Transcript')\nax.set_xticks(x)\nax.set_xticklabels(words.keys())\nax.yaxis.set_major_locator(MaxNLocator(integer=True))\n```\n\nLastly, we get the exact word count above each bar in the graph, loop through the graph, and create the height and width of the bars. `plt.show()` will display the bar graph.\n\n```python\npps = ax.bar([i - width/2 for i in x], words.values(), width, label='words')\n\n    for p in pps:\n        height = p.get_height()\n        ax.annotate('{}'.format(height),\n            xy=(p.get_x() + p.get_width() / 2, height),\n            xytext=(0, 3),\n            textcoords='offset points',\n            ha='center', va='bottom')\n\n    plt.show()\n```\n\nNow, run the project by going to a command line prompt in the terminal and type:\n\n```\npython3 transcribe-with-deepgram.py\n```\n\nA beautiful bar graph with Deepgram Python speech-to-text transcription and Matplotlib data visualization will get generated and look something like this (depending on the audio files used):\n\n![Deepgram speech-to-text transcript with matplotlib data visualization dashboard](https://res.cloudinary.com/deepgram/image/upload/v1652286552/blog/2022/05/python-graphing-transcripts/deepgram-transcript-with-matplotlib.png)\n\n## Conclusion of Deepgram Speech-to-Text with Python and Matplotlib\n\nThere are many other use cases for why one might want to use Python with Deepgram for voice-to-text transcription and data visualization. This project is just an example, and it’s encouraged to continue brainstorming innovative and game-changing ideas for speech-to-text and graphing. Can you think of other use cases for Deepgram and our Python SDK? To let us know, you can Tweet us at [@deepgramdevs](https://twitter.com/DeepgramDevs). We would love to hear from you!";
						}
						async function compiledContent$1o() {
							return load$1o().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1o() {
							return (await import('./chunks/index.eed0826c.mjs'));
						}
						function Content$1o(...args) {
							return load$1o().then((m) => m.default(...args));
						}
						Content$1o.isAstroComponentFactory = true;
						function getHeadings$1o() {
							return load$1o().then((m) => m.metadata.headings);
						}
						function getHeaders$1o() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1o().then((m) => m.metadata.headings);
						}

const __vite_glob_0_188 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1o,
  file: file$1o,
  url: url$1o,
  rawContent: rawContent$1o,
  compiledContent: compiledContent$1o,
  default: load$1o,
  Content: Content$1o,
  getHeadings: getHeadings$1o,
  getHeaders: getHeaders$1o
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1n = {"title":"Speech Recognition to Monitor Script Compliance in Python","description":"Use Deepgram with Python for script compliance, monitoring real-time audio such as phone calls for specific words and phrases that are legally required.","date":"2022-03-30T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1648578346/blog/2022/03/python-script-compliance/Speech-Recognition-Monitor-Script-Compliance%402x.jpg","authors":["tonya-sims"],"category":"tutorial","tags":["python"],"seo":{"title":"Speech Recognition to Monitor Script Compliance in Python","description":"Use Deepgram with Python for script compliance, monitoring real-time audio such as phone calls for specific words and phrases that are legally required."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454046/blog/python-script-compliance/ograph.png"},"shorturls":{"share":"https://dpgr.am/cbb4071","twitter":"https://dpgr.am/b461e6a","linkedin":"https://dpgr.am/ba726b8","reddit":"https://dpgr.am/6a4598c","facebook":"https://dpgr.am/33fe5fc"}};
						const file$1n = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/python-script-compliance/index.md";
						const url$1n = undefined;
						function rawContent$1n() {
							return "Imagine having the ability to monitor certain words or phrases during phone conversations by doing voice transcription with Python. This capability is a typical use case for a call center where calls are monitored between a customer service agent and a customer. Most of these conversations start with the agent saying, “This call is recorded for quality assurance purposes”. This phrase is usually legally required to inform the person on the other end that they are being recorded, and this scenario is a common use case for ASR technology known as script compliance.\n\nCombining voice transcription with Python using Deepgram, there are many innovative ways to monitor script compliance without manually listening to each recorded call. Deepgram provides a speech-to-text solution that transcribes audio to text with Python, real-time and pre-recorded calls. This solution is ideal to:\n\n* Provide insights into how the agent handled the call by running analytics in Python (you can read more about analytics with Deepgram and Python [here](https://blog.deepgram.com/python-talk-time-analytics/).\n* Keep customers happy by improving their experience and satisfaction, increasing sales and revenue.\n* Reduce costs and save time by identifying non-compliance immediately.\n\nIn the next section, let’s review the project we’ll build together. If you’d like to jump ahead and grab the code for this project, you can do so [here in our Github repo](https://github.com/deepgram-devs/python-script-compliance).\n\n# What We’ll Build\n\nThis tutorial will use the Deepgram Python SDK to build a simple script that transcribes audio to text with Python and simulates monitoring script compliance by searching words and phrases. Although Deepgram has a [diarize feature](https://developers.deepgram.com/documentation/features/diarize/) to help us recognize speakers when multiple people are talking (which is very useful for script compliance examples with an agent and a customer), we’ll use a script with one person speaking to keep things simple. In a [previous article](https://blog.deepgram.com/python-talk-time-analytics/) we built a project using the `diarize` feature to gather transcripts for multiple speakers if you'd like to learn more.\n\nWe’ll focus on monitoring script compliance by doing the following:\n\n* Read our audio and receive our transcript\n* Write a function that searches for flagged words and phrases\n* Use the same function to search for wanted keywords and phrases\n* Create a scorecard of how many flagged words and keywords are spoken\n\nNow that we’re clear on what we’re building let’s get started!\n\n# Getting Started\n\nBefore we start, it’s essential to generate a Deepgram API key to use in our project. We can go to our [Deepgram console](https://console.deepgram.com/signup?jump=keys). Make sure to copy it and keep it in a safe place, as you won’t be able to retrieve it again and will have to create a new one. In this tutorial, we’ll use Python 3.10, but Deepgram supports some earlier versions of Python.\n\nNext, let’s make a directory anywhere we’d like.\n\n```\nmkdir deepgram_script_compliance\n```\n\nThen change into that directory so we can start adding things to it.\n\n```\ncd deepgram_script_compliance\n```\n\nWe’ll also need to set up a virtual environment to hold our project and its dependencies. We can read more about those [here](https://blog.deepgram.com/python-virtual-environments/) and how to create one.\n\n<Panel type=\"info\" title=\"Important Note\">\n\nIt’s recommended in Python to use a virtual environment so our project can be installed inside a container rather than installing it system-wide.\n\n</Panel>\n\nNow we can open up our favorite editor and create a file called `script_compliance.py`. If you’d like to make it from the command line, do this:\n\n```\ntouch script_compliance.py\n```\n\nFinally, let’s install our dependencies for our project. Ensure our virtual environment is activated because we’ll install those dependencies inside. If your virtual environment is named `venv`, then activate it.\n\n```\nsource venv/bin/activate\n```\n\nAfter activation, we install the dependencies, including:\n\n* The Deepgram Python SDK\n* The dotenv library helps us work with our environment variables\n* The library Tabulate to pretty-print our tables\n* The Colorama library to color-code our terminal\n\n<!---->\n\n```\npip install deepgram-sdk\npip install python-dotenv\npip install tabulate\npip install colorama\n```\n\nThe following section will show the ease of using Python with Deepgram to monitor search terms with a newbie-friendly script.\n\n# The Code\n\nLet’s open our `script_compliance.py` file and include the following code at the top:\n\n```python\nimport asyncio\nfrom deepgram import Deepgram\nfrom dotenv import load_dotenv\nfrom typing import Dict\nfrom tabulate import tabulate\nfrom colorama import init\nfrom colorama import Fore\nimport os\n\ninit()\n\nload_dotenv()\n\nPATH_TO_FILE = 'gettysburg.wav'\n\nflagged_words = {\n   \"and\": \"This is a flagged word!\",\n   \"are\": \"This is another flagged word!\",\n   \"um\": \"This is a filler word!\"\n}\n\nsearch_words = [\"engaged in a great civil war\", \"new nation\", \"Ok I see\"]\n\nscore_card = []\n```\n\nThe first part is Python imports. We need to access the modules and libraries for our script to work correctly.\n\nThe `load_dotenv()` will help us load our `api_key` from an `env` file, which holds our environment variables.\n\nThe `PATH_TO_FILE = 'gettysburg.wav'` is a path to the audio file we’ll use to do the speech-to-text transcription.\n\nThe `flagged_words` dictionary is where we’ll keep the words in a `key` which we monitor what we don’t want our speaker to say. The `values` in the dictionary contain a warning for each flagged word.\n\nThe `search_words` list are words or phrases that we monitor and want our speaker to say during the audio.\n\nThe `score_card` will keep track of how many flagged words and search words our speaker says in the transcript.\n\nCreate an `env` file at the same level as our `script_compliance.py`. Put the following inside of it:\n\n```\nDEEPGRAM_API_KEY = “YOUR_API_KEY”\n```\n\nWe replace `YOUR_API_KEY` with our api_key, which we got from Deepgram.\n\nNext, let’s add the audio file to our project by [downloading it here](https://developers.deepgram.com/data/audio/gettysburg.wav) and adding it to our project directory.\n\n<Panel type=\"info\" title=\"Important Note\">\n\nTo follow along, we’ll need to download this .wav file. If you’d like to use another file, please note you’ll have to change the `flagged_words` and `search_words` for the project to work correctly.\n\n</Panel>\n\nOur project directory structure should look like this:\n\n![project structure for Deepgram script compliance with Python](https://res.cloudinary.com/deepgram/image/upload/v1648578364/blog/2022/03/python-script-compliance/project-structure.png)\n\nBack in our `script_compliance.py`, let’s add this code to our `main` function:\n\n```python\n​​async def main():\n   deepgram = Deepgram(os.getenv(\"DEEPGRAM_API_KEY\"))\n\n   with open(PATH_TO_FILE, 'rb') as audio:\n       source = {'buffer': audio, 'mimetype': 'audio/wav'}\n       transcription = await deepgram.transcription.prerecorded(source, {'punctuate': True })\n\n       speakers = await script_compliance(transcription)\n\n\nasyncio.run(main())\n```\n\nHere we are initializing Deepgram and pulling in our `DEEPGRAM_API_KEY`. We open our audio file and set the `source` to recognize it’s an `audio/wav`. Then we get the transcription and pass in the `source` and a Python dictionary `{'punctuate': True}`. The Deepgram `punctuate` option adds punctuation and capitalization to our transcript. Read more on [how to use](https://developers.deepgram.com/documentation/features/punctuate/) `punctuate`.\n\nLastly, let’s add our `script_compliance` function to the `script_compliance.py` file, just above our `main` function.\n\n```python\nasync def script_compliance(transcript_data: Dict) -> None:\n   if 'results' in transcript_data:\n       transcript = transcript_data['results']['channels'][0]['alternatives'][0]['transcript']\n\n       data = []\n\n       for key,value in flagged_words.items():\n           score_flagged_words = transcript.count(key)\n           if score_flagged_words:\n               data.append([key, value])\n\n           score_card.append(score_flagged_words)\n\n       print(Fore.RED, tabulate(data, headers=[\"Flagged Word\", \"Warning Message\"]))\n\n       print()\n\n       words = []\n       for item in search_words:\n           if item in transcript:\n               words.append([\"Yes\", item])\n           else:\n               words.append([\"No\", item])\n\n       print(Fore.GREEN, tabulate(words, headers=[\"Word(s) Found\", \"item\"]))\n\n       print()\n\n       print(Fore.YELLOW, tabulate([[sum(score_card), len([w for w in words if w[0] == \"Yes\"])]], headers=[\"Flagged Word Count\", \"Search Word Count\"]))\n```\n\nLet’s break the code down.\n\n```python\n   if 'results' in transcript_data:\n       transcript = transcript_data['results']['channels'][0]['alternatives'][0]['transcript']\n```\n\nThe lines above get the transcript as a String type from the JSON response and store it in a variable called `transcript`.\n\n```python\ndata = []\n\nfor key,value in flagged_words.items():\n     score_flagged_words = transcript.count(key)\n      if score_flagged_words:\n             data.append([key, value])\n\n       score_card.append(score_flagged_words)\n\n print(Fore.RED, tabulate(data, headers=[\"Flagged Word\", \"Warning Message\"]))\n```\n\nWe create an empty list called `data` that will hold the flagged words and their warnings that we find in the transcript.\n\nWe then loop over the dictionary to search for `flagged_words` in the transcript and append those to our `data` list.\n\nThis line `score_flagged_words = transcript.count(key)` counts the number of occurrences of each key or `flagged_words` in our transcript and appends it to the `score_card` list.\n\nLastly, we print out the flagged words we find in red by passing in our `data` list and table headers.\n\nThe second part of the code works similarly.\n\n```python\nwords = []\nfor item in search_words:\n    if item in transcript:\n        words.append([\"Yes\", item])\n    else:\n        words.append([\"No\", item])\n\nprint(Fore.GREEN, tabulate(words, headers=[\"Word(s) Found\", \"item\"]))\n```\n\nWe define a `words` list that will hold all of the search words we find in the transcript.\n\nThen we loop through all the `search_words` that we defined at the beginning of the code example. If a search word is in the transcript, we append it to the `words` list with another value, `Yes`. Otherwise, we append it with a value `No`, which means we did not find the word in the transcript.\n\nLastly, we print the `words` list and table headers to the terminal in green.\n\nOur last line of code prints out the scorecard if yellow of how many flagged words and search words we found. We get the sum of the scorecard, which holds our flagged words and uses a list comprehension to get how many search words we find in the transcript.\n\n```python\n  print(Fore.YELLOW, tabulate([[sum(score_card), len([w for w in words if w[0] == \"Yes\"])]], headers=[\"Flagged Word Count\", \"Search Word Count\"]))\n```\n\nType `python script_compliance.py` or `python3 script_compliance.py` from your terminal to run our script.\n\nHere’s an example of what our output would look like:\n\n![terminal output for Deepgram script compliance with Python](https://res.cloudinary.com/deepgram/image/upload/v1648586684/blog/2022/03/python-script-compliance/terminal-output.png)\n\nCongratulations on building a Python application with Deepgram to monitor script compliance! You can find the [code here](https://github.com/deepgram-devs/python-script-compliance) with instructions on how to run the project. If you have any questions, please feel free to reach out to us on Twitter at [@DeepgramDevs](https://twitter.com/DeepgramDevs).";
						}
						async function compiledContent$1n() {
							return load$1n().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1n() {
							return (await import('./chunks/index.7c1e3278.mjs'));
						}
						function Content$1n(...args) {
							return load$1n().then((m) => m.default(...args));
						}
						Content$1n.isAstroComponentFactory = true;
						function getHeadings$1n() {
							return load$1n().then((m) => m.metadata.headings);
						}
						function getHeaders$1n() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1n().then((m) => m.metadata.headings);
						}

const __vite_glob_0_189 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1n,
  file: file$1n,
  url: url$1n,
  rawContent: rawContent$1n,
  compiledContent: compiledContent$1n,
  default: load$1n,
  Content: Content$1n,
  getHeadings: getHeadings$1n,
  getHeaders: getHeaders$1n
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1m = {"title":"Python Speech Recognition Locally with TorchAudio","description":"Learn how to use the Python TorchAudio library and its Emformer Model for local speech recognition","date":"2022-07-14T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1657708926/blog/2022/07/python-speech-recognition-locally-torchaudio/cov.jpg","authors":["yujian-tang"],"category":"tutorial","tags":["python"],"seo":{"title":"Python Speech Recognition Locally with TorchAudio","description":"Learn how to use the Python TorchAudio library and its Emformer Model for local speech recognition"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454110/blog/python-speech-recognition-locally-torchaudio/ograph.png"},"shorturls":{"share":"https://dpgr.am/6f6eb4e","twitter":"https://dpgr.am/7fdfd20","linkedin":"https://dpgr.am/d2e2258","reddit":"https://dpgr.am/1385e37","facebook":"https://dpgr.am/b02940e"}};
						const file$1m = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/python-speech-recognition-locally-torchaudio/index.md";
						const url$1m = undefined;
						function rawContent$1m() {
							return "\"Your call may be recorded for quality assurance purposes.\"\r\n\r\nWe’ve all heard this when calling customer service. What are they doing with that call? How are they transcribing it? Back in the early 2000s and late 1990s, companies were using people to transcribe these. Now, [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing) has come far along enough that we can use Python and machine learning to do automatic speech recognition.\r\n\r\nIn this post, we’ll focus on how to do speech recognition locally on your device using TorchAudio’s pre-built Emformer RNN-T model. We will cover:\r\n\r\n* [Understanding PyTorch TorchAudio](#understanding-pytorch-torchaudio)\r\n* [Setting Up TorchAudio for Speech Recognition](#setting-up-torchaudio-for-speech-recognition)\r\n* [Building a Python Audio Data Streaming Function for Speech Recognition](#building-a-python-audio-data-streaming-function-for-speech-recognition)\r\n* [Setting up Python Speech Recognition Inference Pipeline](#setting-up-python-speech-recognition-inference-pipeline)\r\n* [Creating a Context Cache to Store Audio Data for Speech Recognition](#creating-a-context-cache-to-store-audio-data-for-speech-recognition)\r\n* [Using TorchAudio’s Emformer Model for Local Speech Recognition in Python](#using-torchaudios-emformer-model-for-local-speech-recognition-in-python)\r\n* [In Summary](#in-summary)\r\n\r\n## Understanding PyTorch TorchAudio\r\n\r\nPyTorch is a versatile and powerful Python library for quickly developing machine learning models. Is it the open-source Python version of the Torch library (built on Lua) and primarily developed by Meta/Facebook. TorchAudio is an additional library to PyTorch that handles dealing with audio data for machine learning models.\r\n\r\nTorchAudio isn’t just for creating machine learning models, you can also use it to do some audio data manipulation. We previously covered how to [use TorchAudio to manipulate audio data in Python](https://blog.deepgram.com/pytorch-intro-with-torchaudio/). In this piece, we’re going to use it to build an inference pipeline to do speech recognition in real time.\r\n\r\n## Setting Up TorchAudio for Speech Recognition\r\n\r\nBefore we can dive into the program for doing speech recognition with TorchAudio, we need to set up our system. We need to install the `pytorch`, `torchaudio`, `sentencepiece`, and `ffmpeg-python` libraries. We can install all of these with the command `pip install pytorch torchaudio sentencepiece ffmpeg-python`. If you encounter errors, you may need to upgrade `pip`, which you can do using `pip install -U pip`.\r\n\r\nIf you already have PyTorch and TorchAudio installed and you encounter an error importing `torchaudio` as I did. To get around this, force update both libraries with `pip install -U torch torchaudio --no-cache-dir`. Next, we’re going to build our script. It’s going to consist of an audio data function, the machine learning model pipeline, a context cache, and a main function to run.\r\n\r\n## Building a Python Audio Data Streaming Function for Speech Recognition\r\n\r\nNow that our system is set up to work with PyTorch and Torchaudio for speech recognition, let’s build our script. The first thing we need is an audio data streaming function, we’ll call ours `stream`. We also declare a constant number of iterations for the streaming function to stream.\r\n\r\nOur `stream` function needs five parameters. It needs a queue, a format or device (different for Mac vs Windows), a source, a segment length, and a sample rate. There are some print statements in our function that I’ve commented out. You can uncomment these to show more information about the program as it runs.\r\n\r\nThe first thing we’re going to do in our `stream` function is create a `StreamReader` instance from `torchaudio.io`. Next, we’ll populate that stream reader with an audio stream of the passed in segment length and sample rate. I would leave the `\"Streaming\"` and blank `print` statements uncommented so you know when you can start talking into the mic.\r\n\r\nNext, we’ll create an iterator to iterate across our stream. Finally, we’ll loop through the number of predefined iterations, in our case 100, and get the next chunk of audio data from the stream iterator and put it in the queue.\r\n\r\n```py\r\nimport torch\r\nimport torch.multiprocessing as mp\r\nimport torchaudio\r\nfrom torchaudio.io import StreamReader\r\n \r\nITERATIONS = 100\r\ndef stream(queue: mp.Queue(),\r\n   format: str,\r\n   src: str,\r\n   frames_per_chunk: int,\r\n   sample_rate: int):\r\n   '''Streams audio data\r\n  \r\n   Parameters:\r\n       queue: Queue of data chunks\r\n       format: Format\r\n       src: Source\r\n       frames_per_chunk: How many frames are in each data chunk\r\n       sample_rate: Sample rate\r\n \r\n   Returns:\r\n       None'''\r\n   print(\"Initializing Audio Stream\")\r\n   streamer = StreamReader(src, format=format)\r\n   streamer.add_basic_audio_stream(frames_per_chunk=frames_per_chunk,\r\n       sample_rate=sample_rate)\r\n   print(\"Streaming\\n\")\r\n   stream_iterator = streamer.stream(timeout=-1, backoff=1.0)\r\n   for _ in range(ITERATIONS):\r\n       (chunk,) = next(stream_iterator)\r\n       queue.put(chunk)\r\n```\r\n\r\n## Setting up Python Speech Recognition Inference Pipeline\r\n\r\nNow that we have a way to smoothly stream the audio data from a microphone, we can run predictions on it. Let’s build a pipeline to do speech recognition with. Unlike the part above, this time we will be creating a `Class` and not a function.\r\n\r\nWe require our class to be initialized with one variable, an RNN-T model from TorchAudio’s Pipelines library. We also allow a second, optional variable to define beam width, which is set at 10 by default. Our `__init__` function sets the instance’s `bundle` value to the model we passed in. Then, it uses that model to get the feature extractor, decoder, and token processor. Next, it sets the beam width, the state of the pipeline, and the predictions.\r\n\r\nThe other function that our `Pipeline` object has is an inference function. This function takes a PyTorch `Tensor` object and returns a string, the predicted text. This function uses the feature extractor to get the features and the length of the input tensor. Next, call the inference function from the decoder to get the hypothesis and set the state of the Pipeline. We set the instance’s `hypothesis` attribute to the value in the first index of the returned hypothesis. Finally, we use the token processor on the first entry in the instance’s hypothesis attribute and return that value.\r\n\r\n```py\r\nclass InferencePipeline:\r\n   '''Creates an inference pipeline for streaming audio data'''\r\n   def __init__(self,\r\n       pipeline: torchaudio.pipelines.RNNTBundle,\r\n       beam_width: int=10):\r\n       '''Initializes TorchAudio RNNT Pipeline\r\n      \r\n       Parameters:\r\n           pipeline: TorchAudio Pipeline to use\r\n           beam_width: Beam width\r\n \r\n       Returns:\r\n           None'''\r\n \r\n       self.pipeline = pipeline\r\n       self.feature_extractor = pipeline.get_streaming_feature_extractor()\r\n       self.decoder = pipeline.get_decoder()\r\n       self.token_processor = pipeline.get_token_processor()\r\n       self.beam_width = beam_width\r\n       self.state = None\r\n       self.hypothesis = None\r\n  \r\n   def infer(self, segment: torch.Tensor) -> str:\r\n       '''Runs inference using the initialized pipeline\r\n      \r\n       Parameters:\r\n           segment: Torch tensor with features to extract\r\n      \r\n       Returns:\r\n           Transcript as string type'''\r\n \r\n       features, length = self.feature_extractor(segment)\r\n       predictions, self.state = self.decoder.infer(\r\n           features, length, self.beam_width, state=self.state,\r\n           hypothesis=self.hypothesis\r\n       )\r\n       self.hypothesis = predictions[0]\r\n       transcript = self.token_processor(self.hypothesis[0], lstrip=False)\r\n       return transcript\r\n```\r\n\r\n## Creating a Context Cache to Store Audio Data for Speech Recognition\r\n\r\nWe need a context cache to batch store the audio data frames as we read them. The context cache requires two parameters to initialize. First, a segment length, then a context length. The initialization function sets the instance’s attributes to the passed in values and initializes a tensor of zeros of the context length as the instance’s current context.\r\n\r\nThe second function we’ll define is the `call` function. This is another automatic function that we’re overwriting. Whenever you see a class function preceded and followed by two underscores, that’s a default function. In this case, we require the call function to intake a PyTorch tensor object.\r\n\r\nIf the size of the first object in the tensor is less than the set segment length for the cache, we’ll pad that chunk with 0s. Next, we use the concatenate function from `torch` to add that chunk to the current context. Then, we set the instance’s context attribute to the last entries in the chunk equivalent to the context length. Finally, we return the concatenate context.\r\n\r\n```py\r\nclass ContextCacher:\r\n   def __init__(self, segment_length: int, context_length: int):\r\n       '''Creates initial context cache\r\n      \r\n       Parameters:\r\n           segment_length: length of one audio segment\r\n           context_length: length of the context\r\n \r\n       Returns:\r\n           None'''\r\n       self.segment_length = segment_length\r\n       self.context_length = context_length\r\n       self.context = torch.zeros([context_length])\r\n  \r\n   def __call__(self, chunk: torch.Tensor):\r\n       '''Adds chunk to context and returns it\r\n      \r\n       Parameters:\r\n           chunk: chunk of audio data to process\r\n      \r\n       Returns:\r\n           Tensor'''\r\n       if chunk.size(0) < self.segment_length:\r\n           chunk = torch.nn.functional.pad(chunk,\r\n               (0, self.segment_length - chunk.size(0)))\r\n       chunk_with_context = torch.cat((self.context, chunk))\r\n       self.context = chunk[-self.context_length :]\r\n       return chunk_with_context\r\n```\r\n\r\n## Using TorchAudio's Emformer Model for Local Speech Recognition in Python\r\n\r\nFinally, all our helper functions have been made. It is now time to create the main function to orchestrate the audio data streamer, the machine learning pipeline, and the cache. Our main function will take three inputs: the input device, the source, and the RNN-T model. As before, I’ve left some print statements in here commented out, uncomment them if you’d like.\r\n\r\nFirst, we create an instance of our Pipeline. This is what turns the audio data into text. Next, we use the RNN-T model to set the sample rate, the segment length, and the context length. Then, we instantiate the context with the newly gotten segment and context lengths.\r\n\r\nNow things get funky. We use an annotation on the `infer` function we’re creating that signals to `torch` to turn on inference mode while this function runs. The infer function does not need any parameters. It loops through the preset number of iterations. For each iteration, it gets a chunk of data from the queue, calls the cache to return the concatenated first column entry of that chunk of data and the context, calls the pipeline to do speech recognition on that cached segment of data, and finally prints the transcript and flushes the I/O buffer.\r\n\r\nWe won’t actually call that `infer` function just yet. We’re just defining it. Next, we import PyTorch’s multiprocessing capability to spawn subprocesses. We need to spawn the audio data capture as a subprocess for correct functionality. Otherwise, it will block I/O.\r\n\r\nThe first thing we do with the multiprocessing function is to signal we are spawning a new process. Second, we create a queue object in the context. Next, we create a process that runs the `stream` function from above in the context with the created queue, the passed in device, source, and model, the segment length, and the sample rate. Once we start that subprocess, we run our infer function until we end the subprocess.\r\n\r\nWhen the script actually runs (in the `if __name__ == \"__main__\"` section), we call the main function. For Mac’s we pass the \"avfoundation\" for the device and the second entry for the source. The first entry in a Mac’s setup is the video.\r\n\r\nFor Windows, see [the original notebook](https://pytorch.org/audio/main/tutorials/device_asr.html).\r\n\r\n```py\r\ndef main(device: str, src: str, bundle: torchaudio.pipelines):\r\n   '''Transcribed audio data from the mic\r\n  \r\n   Parameters:\r\n       device: Input device name\r\n       src: Source from input\r\n       bundle: TorchAudio pipeline\r\n  \r\n   Returns:\r\n       None'''\r\n   pipeline = InferencePipeline(bundle)\r\n  \r\n   sample_rate = bundle.sample_rate\r\n   segment_length = bundle.segment_length * bundle.hop_length\r\n   context_length = bundle.right_context_length * bundle.hop_length\r\n  \r\n   cacher = ContextCacher(segment_length, context_length)\r\n  \r\n   @torch.inference_mode()\r\n   def infer():\r\n       for _ in range(ITERATIONS):\r\n           chunk = q.get()\r\n           segment = cacher(chunk[:,0])\r\n           transcript = pipeline.infer(segment)\r\n           print(transcript, end=\"\", flush=True)\r\n  \r\n   ctx = mp.get_context(\"spawn\")\r\n   q = ctx.Queue()\r\n   p = ctx.Process(target=stream, args=(q, device, src, segment_length, sample_rate))\r\n   p.start()\r\n   infer()\r\n   p.join()\r\nif __name__ == \"__main__\":\r\n   main(\r\n       device=\"avfoundation\",\r\n       src=\":1\",\r\n       bundle=torchaudio.pipelines.EMFORMER_RNNT_BASE_LIBRISPEECH\r\n   )\r\n```\r\n\r\n## Full Code for Building a Local Streaming Audio Transcription Tool with PyTorch TorchAudio\r\n\r\nThat was a lot of code. Let’s see how it looks all together in one file.\r\n\r\n```py\r\nimport torch\r\nimport torch.multiprocessing as mp\r\nimport torchaudio\r\nfrom torchaudio.io import StreamReader\r\n \r\nITERATIONS = 100\r\ndef stream(queue: mp.Queue(),\r\n   format: str,\r\n   src: str,\r\n   frames_per_chunk: int,\r\n   sample_rate: int):\r\n   '''Streams audio data\r\n  \r\n   Parameters:\r\n       queue: Queue of data chunks\r\n       format: Format\r\n       src: Source\r\n       frames_per_chunk: How many frames are in each data chunk\r\n       sample_rate: Sample rate\r\n \r\n   Returns:\r\n       None'''\r\n   print(\"Initializing Audio Stream\")\r\n   streamer = StreamReader(src, format=format)\r\n   streamer.add_basic_audio_stream(frames_per_chunk=frames_per_chunk,\r\n       sample_rate=sample_rate)\r\n   print(\"Streaming\\n\")\r\n   stream_iterator = streamer.stream(timeout=-1, backoff=1.0)\r\n   for _ in range(ITERATIONS):\r\n       (chunk,) = next(stream_iterator)\r\n       queue.put(chunk)\r\n \r\nclass InferencePipeline:\r\n   '''Creates an inference pipeline for streaming audio data'''\r\n   def __init__(self,\r\n       pipeline: torchaudio.pipelines.RNNTBundle,\r\n       beam_width: int=10):\r\n       '''Initializes TorchAudio RNNT Pipeline\r\n      \r\n       Parameters:\r\n           pipeline: TorchAudio Pipeline to use\r\n           beam_width: Beam width\r\n \r\n       Returns:\r\n           None'''\r\n \r\n       self.pipeline = pipeline\r\n       self.feature_extractor = pipeline.get_streaming_feature_extractor()\r\n       self.decoder = pipeline.get_decoder()\r\n       self.token_processor = pipeline.get_token_processor()\r\n       self.beam_width = beam_width\r\n       self.state = None\r\n       self.hypothesis = None\r\n  \r\n   def infer(self, segment: torch.Tensor) -> str:\r\n       '''Runs inference using the initialized pipeline\r\n      \r\n       Parameters:\r\n           segment: Torch tensor with features to extract\r\n      \r\n       Returns:\r\n           Transcript as string type'''\r\n \r\n       features, length = self.feature_extractor(segment)\r\n       predictions, self.state = self.decoder.infer(\r\n           features, length, self.beam_width, state=self.state,\r\n           hypothesis=self.hypothesis\r\n       )\r\n       self.hypothesis = predictions[0]\r\n       transcript = self.token_processor(self.hypothesis[0], lstrip=False)\r\n       return transcript\r\n \r\nclass ContextCacher:\r\n   def __init__(self, segment_length: int, context_length: int):\r\n       '''Creates initial context cache\r\n      \r\n       Parameters:\r\n           segment_length: length of one audio segment\r\n           context_length: length of the context\r\n \r\n       Returns:\r\n           None'''\r\n       self.segment_length = segment_length\r\n       self.context_length = context_length\r\n       self.context = torch.zeros([context_length])\r\n  \r\n   def __call__(self, chunk: torch.Tensor):\r\n       '''Adds chunk to context and returns it\r\n      \r\n       Parameters:\r\n           chunk: chunk of audio data to process\r\n      \r\n       Returns:\r\n           Tensor'''\r\n       if chunk.size(0) < self.segment_length:\r\n           chunk = torch.nn.functional.pad(chunk,\r\n               (0, self.segment_length - chunk.size(0)))\r\n       chunk_with_context = torch.cat((self.context, chunk))\r\n       self.context = chunk[-self.context_length :]\r\n       return chunk_with_context\r\n \r\ndef main(device: str, src: str, bundle: torchaudio.pipelines):\r\n   '''Transcribed audio data from the mic\r\n  \r\n   Parameters:\r\n       device: Input device name\r\n       src: Source from input\r\n       bundle: TorchAudio pipeline\r\n  \r\n   Returns:\r\n       None'''\r\n   pipeline = InferencePipeline(bundle)\r\n  \r\n   sample_rate = bundle.sample_rate\r\n   segment_length = bundle.segment_length * bundle.hop_length\r\n   context_length = bundle.right_context_length * bundle.hop_length\r\n  \r\n   cacher = ContextCacher(segment_length, context_length)\r\n  \r\n   @torch.inference_mode()\r\n   def infer():\r\n       for _ in range(ITERATIONS):\r\n           chunk = q.get()\r\n           segment = cacher(chunk[:,0])\r\n           transcript = pipeline.infer(segment)\r\n           print(transcript, end=\"\", flush=True)\r\n  \r\n   ctx = mp.get_context(\"spawn\")\r\n   q = ctx.Queue()\r\n   p = ctx.Process(target=stream, args=(q, device, src, segment_length, sample_rate))\r\n   p.start()\r\n   infer()\r\n   p.join()\r\nif __name__ == \"__main__\":\r\n   main(\r\n       device=\"avfoundation\",\r\n       src=\":1\",\r\n       bundle=torchaudio.pipelines.EMFORMER_RNNT_BASE_LIBRISPEECH\r\n   )\r\n```\r\n\r\n## Summary of Python Speech Recognition Locally with PyTorch\r\n\r\nSpeech recognition has come a long way from its first inception. We can now use Python to do speech recognition in many ways, including with the TorchAudio library from PyTorch. The TorchAudio library comes with many pre-built models we can use for automatic speech recognition as well as many audio data manipulation tools.\r\n\r\nIn this post, we covered how to run speech recognition locally with their Emformer RNN-T. First, we created an audio data streaming function. Next, we defined a pipeline object which can run speech recognition on tensors and turn them into text. Then, we created a cache to cache the audio data as it came in. We also created a main function to handle running the audio data stream as a subprocess and run inference on it while it ran. Finally, we ran that main function with varying parameters to let it know we are streaming data in from the mic and using an Emformer RNN-T model.\r\n\r\nWant to do speech recognition without all that code? [Sign up for Deepgram](https://console.deepgram.com) today and be up and running in just a few minutes.";
						}
						async function compiledContent$1m() {
							return load$1m().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1m() {
							return (await import('./chunks/index.d5d61744.mjs'));
						}
						function Content$1m(...args) {
							return load$1m().then((m) => m.default(...args));
						}
						Content$1m.isAstroComponentFactory = true;
						function getHeadings$1m() {
							return load$1m().then((m) => m.metadata.headings);
						}
						function getHeaders$1m() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1m().then((m) => m.metadata.headings);
						}

const __vite_glob_0_190 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1m,
  file: file$1m,
  url: url$1m,
  rawContent: rawContent$1m,
  compiledContent: compiledContent$1m,
  default: load$1m,
  Content: Content$1m,
  getHeadings: getHeadings$1m,
  getHeaders: getHeaders$1m
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1l = {"title":"Speech Recognition Analytics for Audio with Python","description":"Learn how to combine speech recognition on real-time audio with analytics by utilizing Python and Deepgram's Speech-to-Text API.","date":"2022-03-23T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1647974816/blog/2022/03/python-talk-time-analytics/Speech-Analytics-Real-Time-Audio-w-Python%402x.jpg","authors":["tonya-sims"],"category":"tutorial","tags":["python"],"seo":{"title":"Speech Recognition Analytics for Audio with Python","description":"Learn how to combine speech recognition on real-time audio with analytics by utilizing Python and Deepgram's Speech-to-Text API."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454048/blog/python-talk-time-analytics/ograph.png"},"shorturls":{"share":"https://dpgr.am/ad5301e","twitter":"https://dpgr.am/035d8b3","linkedin":"https://dpgr.am/6994e98","reddit":"https://dpgr.am/4d26964","facebook":"https://dpgr.am/3828283"}};
						const file$1l = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/python-talk-time-analytics/index.md";
						const url$1l = undefined;
						function rawContent$1l() {
							return "Have you ever wondered what you could build using voice-to-text and analytics? This article will discover how we can combine a speech recognition provider that transcribes audio to text with Python using Deepgram and speech-to-text analytics.\n\nAnalytics is all about measuring patterns in data to discover insights that help us make better decisions. These decisions could improve business capacity, raise sales, enhance communication between a customer service agent and customer, and much more.\n\nIf you’d like to jump ahead and grab the code for this project, please do so on our [Deepgram Devs Github](https://github.com/deepgram-devs/python-talk-time-analytics).\n\n# What We'll Build Together\n\nThis tutorial will use the Deepgram Python SDK to build a simple script that does voice transcription with Python. One of the many beauties of Deepgram is our [diarize feature](https://developers.deepgram.com/documentation/features/diarize/). We’ll use this feature to help us recognize which speaker is talking and assigns a transcript to that speaker. The `diarize` feature will help us recognize multiple speakers. We’ll see how to get the transcript from the audio and assign it to each speaker.\n\nThen we’ll focus on analytics by measuring the following:\n\n* The amount of time each speaker spoke per phrase\n* The average amount of time they spoke\n* The total time of conversation for each speaker\n\n# Getting Started\n\nBefore we start, it’s essential to generate a Deepgram API key to use in our project. To grab one, we can go to our [Deepgram console](https://console.deepgram.com/signup?jump=keys). Make sure to copy it and keep it in a safe place, as you won’t be able to retrieve it again and will have to create a new one. In this tutorial, we’ll use Python 3.10, but Deepgram supports some earlier versions of Python.\n\nNext, let’s make a directory anywhere we’d like.\n\n```\nmkdir deepgram_analytics_project\n```\n\nThen change into that directory so we can start adding things to it.\n\n```\ncd deepgram_analytics_project\n```\n\nWe’ll also need to set up a virtual environment to hold our project and its dependencies. We can read more about those [here](https://blog.deepgram.com/python-virtual-environments/) and how to create one.\n\n<Panel type=\"info\" title=\"Important Note\">\n\nIt’s recommended in Python to use a virtual environment so our project can be installed inside a container rather than installing it system-wide.\n\n</Panel>\n\nNow we can open up our favorite editor and create a file called `deepgram_analytics.py`. If you’d like to make it from the command line, do this:\n\n```\ntouch deepgram_analytics.py\n```\n\nFinally, let’s install our dependencies for our project. Ensure our virtual environment is activated because we’ll install those dependencies inside. If your virtual environment is named `venv` then activate it.\n\n```\nsource venv/bin/activate\n```\n\nAfter activation, we install the dependencies, including:\n\n* The Deepgram Python SDK\n* The dotenv library, which helps us work with our environment variables\n\n<!---->\n\n```\npip install deepgram-sdk\npip install python-dotenv\n```\n\n# The Code\n\nLet’s open our `deepgram_analytics.py` file and include the following code at the top:\n\n```python\nimport asyncio\nfrom deepgram import Deepgram\nfrom dotenv import load_dotenv\nfrom typing import Dict\nimport os\n\nload_dotenv()\n\nPATH_TO_FILE = 'premier_broken-phone.mp3'\n```\n\nThe first part is Python imports. We need to access the modules and libraries for our script to work correctly.\n\nThe `load_dotenv()` will help us load our `api_key` from an `env` file, which holds our environment variables.\n\nThe `PATH_TO_FILE = 'premier_broken-phone.mp3'` is a path to our audio file we’ll use to do the speech-to-text transcription.\n\nCreate an `env` file at the same level as our `deepgram_analytics.py`. Put the following inside of it:\n\n```\nDEEPGRAM_API_KEY = “YOUR_API_KEY”\n```\n\nWhere you’d replace `YOUR_API_KEY` with your api_key you got from Deepgram.\n\nNext, let’s add the audio file to our project by [downloading it here](https://developers.deepgram.com/data/audio/premier_broken-phone.mp3), and adding it to our project directory.\n\n<Panel type=\"info\" title=\"Important Note\">\n\nThis audio file is a sample phone call from Premier Phone Services. To follow along, we’ll need to download this .mp3 file.\n\n</Panel>\n\nOur project directory structure should look like this:\n\n![project structure for Deepgram talk time analytics](https://res.cloudinary.com/deepgram/image/upload/v1647974833/blog/2022/03/python-talk-time-analytics/project_structure.png)\n\nBack in our `deepgram_analytics.py` let’s add this code to our `main` function:\n\n```python\n​​async def main():\n   deepgram = Deepgram(os.getenv(\"DEEPGRAM_API_KEY\"))\n\n   with open(PATH_TO_FILE, 'rb') as audio:\n       source = {'buffer': audio, 'mimetype': 'audio/mp3'}\n       transcription = await deepgram.transcription.prerecorded(source, {'punctuate': True, 'diarize': True})\n\n       speakers = await compute_speaking_time(transcription)\n\n\nasyncio.run(main())\n```\n\nHere we are initializing Deepgram and pulling in our `DEEPGRAM_API_KEY`. We open our audio file set the `source` to recognize it’s an `audio/mp3`. Then we get the transcription and pass in the `source` and a Python dictionary `{'punctuate': True, 'diarize': True}`. The `diarize` option helps us assign the transcript to the speaker. More on [how to use](https://developers.deepgram.com/api-reference/#diarize-pr) `diarize` and the other options.\n\nLastly, let’s add our `compute_speaking_time` function to the `deepgram_analytics.py` file, just above our `main` function.\n\n```python\nasync def compute_speaking_time(transcript_data: Dict) -> None:\n   if 'results' in transcript_data:\n       transcript = transcript_data['results']['channels'][0]['alternatives'][0]['words']\n\n       total_speaker_time = {}\n       speaker_words = []\n       current_speaker = -1\n\n       for speaker in transcript:\n           speaker_number = speaker[\"speaker\"]\n\n           if speaker_number is not current_speaker:\n               current_speaker = speaker_number\n               speaker_words.append([speaker_number, [], 0])\n\n               try:\n                   total_speaker_time[speaker_number][1] += 1\n               except KeyError:\n                   total_speaker_time[speaker_number] = [0,1]\n\n\n           get_word = speaker[\"word\"]\n           speaker_words[-1][1].append(get_word)\n\n           total_speaker_time[speaker_number][0] += speaker[\"end\"] - speaker[\"start\"]\n           speaker_words[-1][2] += speaker[\"end\"] - speaker[\"start\"]\n\n       for speaker, words, time_amount in speaker_words:\n           print(f\"Speaker {speaker}: {' '.join(words)}\")\n           print(f\"Speaker {speaker}: {time_amount}\")\n\n       for speaker, (total_time, amount) in total_speaker_time.items():\n           print(f\"Speaker {speaker} avg time per phrase: {total_time/amount} \")\n           print(f\"Total time of conversation: {total_time}\")\n\n   return transcript\n```\n\nLet’s break the code down.\n\n```python\n   if 'results' in transcript_data:\n       transcript = transcript_data['results']['channels'][0]['alternatives'][0]['words']\n```\n\nThese lines get the transcript as a String type from the JSON response and store it in a variable called `transcript`.\n\n```python\n   total_speaker_time = {}\n   speaker_words = []\n   current_speaker = -1\n```\n\nWe define an empty dictionary called `total_speaker_time` and empty list `speaker_words`. We also need to keep track of the current speaker as each person talks. The `current_speaker` variable is set to `-1` because a speaker will never have that value, and we can update it whenever someone new is speaking.\n\n```python\n       for speaker in transcript:\n           speaker_number = speaker[\"speaker\"]\n\n           if speaker_number is not current_speaker:\n               current_speaker = speaker_number\n               speaker_words.append([speaker_number, [], 0])\n               try:\n                   total_speaker_time[speaker_number][1] += 1\n               except KeyError:\n                   total_speaker_time[speaker_number] = [0,1]\n\n\n           get_word = speaker[\"word\"]\n           speaker_words[-1][1].append(get_word)\n\n           total_speaker_time[speaker_number][0] += speaker[\"end\"] - speaker[\"start\"]\n           speaker_words[-1][2] += speaker[\"end\"] - speaker[\"start\"]\n```\n\nNext, we loop through the transcript and find which speaker is talking. We append their `speaker_number`, an empty list `[]` to add their transcript, and `0`, the total time per phrase for each speaker.\n\nWe use a `try/except` block to add to our `total_speaker_time` dictionary. We check if the key `speaker_number` is already in the dictionary. If so, then we just add how many times the speaker speaks `total_speaker_time[speaker_number][1] += 1`. If not in the dictionary, we add the key and its values `total_speaker_time[speaker_number] = [0,1]`, with `0` as the time spoken in seconds and `1` is how many times they speak.\n\nThe below lines of code get the transcript from each speaker `get_word = speaker[\"word\"]`. We then appended those to our `speaker_words` list. Finally, we get the `total_speaker_time` for each speaker by subtracting their `end` and `start` speaking times and adding them together.\n\n```python\nget_word = speaker[\"word\"]\nspeaker_words[-1][1].append(get_word)\n\ntotal_speaker_time[speaker_number][0] += speaker[\"end\"] - speaker[\"start\"]\n```\n\nLastly, we do our analytics:\n\n```python\n       for speaker, words, time_amount in speaker_words:\n           print(f\"Speaker {speaker}: {' '.join(words)}\")\n           print(f\"Speaker {speaker}: {time_amount}\")\n\n       for speaker, (total_time, amount) in total_speaker_time.items():\n           print(f\"Speaker {speaker} avg time per phrase: {total_time/amount} \")\n           print(f\"Total time of conversation: {total_time}\")\n\n   return transcript\n```\n\nIn the first `for` loop, we print out each speaker with their speaker number and their transcript. After each person talks, we calculate how long they spoke in that sentence.\n\nIn the second `for` loop, we calculate on average how long each person spoke and the total time of the conversation for each speaker.\n\nTo run our script type `python deepgram_analytics.py` or `python3 deepgram_analytics.py` from your terminal.\n\nHere’s an example of what our output would look like:\n\n![terminal output for Deepgram talk time analytics](https://res.cloudinary.com/deepgram/image/upload/v1647974833/blog/2022/03/python-talk-time-analytics/terminal_output.png)\n\nCongratulations on transcribing audio to text with Python using Deepgram with speech-to-text analytics! You can find the [code here](https://github.com/deepgram-devs/python-talk-time-analytics) with instructions on how to run the project. If you have any questions, please feel free to reach out to us on Twitter at [@DeepgramDevs](https://twitter.com/DeepgramDevs)";
						}
						async function compiledContent$1l() {
							return load$1l().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1l() {
							return (await import('./chunks/index.9a369e8e.mjs'));
						}
						function Content$1l(...args) {
							return load$1l().then((m) => m.default(...args));
						}
						Content$1l.isAstroComponentFactory = true;
						function getHeadings$1l() {
							return load$1l().then((m) => m.metadata.headings);
						}
						function getHeaders$1l() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1l().then((m) => m.metadata.headings);
						}

const __vite_glob_0_191 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1l,
  file: file$1l,
  url: url$1l,
  rawContent: rawContent$1l,
  compiledContent: compiledContent$1l,
  default: load$1l,
  Content: Content$1l,
  getHeadings: getHeadings$1l,
  getHeaders: getHeaders$1l
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1k = {"title":"Virtual Environments in Python","description":"Learn about why it's important to use virtual environments in Python, how to create one and use them in your next project.","date":"2022-02-14T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1644600894/blog/2022/02/python-virtual-environments/Setting-Up-Your-Python-Developer-Environment%402x.jpg","authors":["tonya-sims"],"category":"tutorial","tags":["python"],"seo":{"title":"Virtual Environments in Python","description":"Learn about why it's important to use virtual environments in Python, how to create one and use them in your next project."},"shorturls":{"share":"https://dpgr.am/0c37b14","twitter":"https://dpgr.am/6627d16","linkedin":"https://dpgr.am/2777e69","reddit":"https://dpgr.am/91fcf1a","facebook":"https://dpgr.am/63f791a"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453996/blog/python-virtual-environments/ograph.png"}};
						const file$1k = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/python-virtual-environments/index.md";
						const url$1k = undefined;
						function rawContent$1k() {
							return "\nImagine developing two Flask applications, one for your customer and the other for a personal project. Let’s say your customer uses Flask 1.1, and your other application uses Flask 2.0.\n\nWhen you create your customer’s project directory and install Flask, you do the usual global systemwide installation from the terminal using `pip install Flask==1.1.4`. You start developing that application but let’s say you’d also like to spend some time working on your project. You create another project directory and run `pip install Flask==2.0.3`, which installs a later version of Flask.\n\nYou receive a message saying you have to fix a bug in your client’s code. You go into your customer’s project directory, make the bug fix, and run your tests. You receive an error message indicating that “Flask 2.0.3 is not installed.”\n\nThe problem with Pip is that you can only have one version of the package installed. It uninstalls the previous version and overwrites it with the newer version when installing a package.\n\nYou may be wondering how we solve this problem?\n\nThe answer is using virtual environments.\n\nHeads up, if you want faster access to set up your virtual environment, scroll down to the **Quick Commands** section, otherwise keep reading to understand how virtual environments work.\n\n## What are virtual environments?\n\nWhen you install a package with Pip, it installs every package in the same folder. The best way to handle this is to temporarily install them in a different folder and let Python use that folder instead.\n\nThis example is a perfect use case for a virtual environment.\n\nWhen you create a virtual environment, it makes a particular folder that includes Python binaries. You then must activate a virtual environment, which does two things. It directs Pip to install packages to the specific folder and lets Python use packages from that folder.\n\nLet’s walk through how to create virtual environments in the next section.\n\n## Create a Virtual Environment\n\nThere’s a built-in module in Python called **venv** that handles virtual environments.\n\nTo create a virtual environment, in your terminal, navigate to where you want your folder structure and make a directory like so:\n\n    mkdir client_flask_app\n\nOnce you have your folder, change directories inside of that folder:\n\n    cd client_flask_app\n\nThen create the virtual environment (this assumes you’re using a Python 3 installation):\n\n    python3 -m venv my_virtual_environment\n\nLet’s break this down.\n\nThe `-m` says, “Hey, call that built-in Python venv module.” This `venv` takes one parameter, the name of the virtual environment, which is **my\\_virtual\\_environment**.\n\nNow list the directories and files in your folder:\n\n    ls\n\nYou should see the virtual environment you created, which is that special folder with many Python things inside!\n\n![creating a virtual environment in Python](https://res.cloudinary.com/deepgram/image/upload/v1644610609/blog/2022/02/python-virtual-environments/python_virtual_environment_create.png)\n\n*Tip! A more common convention is to name your virtual environment the same as the module venv. For example:*\n\n    python3 -m venv venv\n\n*The benefits to this approach are that it’s more obvious this is a folder for a virtual environment, and some of the code editors like VSCode and Pycharm will automatically recognize the folder and start using it in your project. You can give your virtual environment name an alias by passing in the - - prompt flag, so it’s easier to switch in between projects. This flag will change the name of the virtual environment without changing the folder name. An example would be:*\n\n    python3 -m venv --prompt myclientapp venv\n\nLet’s take a look inside your newly created virtual environment folder:\n\n    cd my_virtual_environment\n\n<!---->\n\n    ls\n\nYou’ll see a bin folder inside of your virtual environment.\n\n![Python virtual environment bin folder](https://res.cloudinary.com/deepgram/image/upload/v1644610726/blog/2022/02/python-virtual-environments/python_virtual_environment_bin.png)\n\nLet’s change directories so we can look to see what it contains.\n\n    cd bin\n\n<!---->\n\n    ls\n\nYou should see an **activate** script inside of the bin directory.\n\n![Python virtual environment activate script](https://res.cloudinary.com/deepgram/image/upload/v1644610838/blog/2022/02/python-virtual-environments/python_virtual_environment_activate.png)\n\nSince this is a bash script, you’ll have to use the `source` command to run it.\n\nFrom this directory type:\n\n    source activate\n\nAfter the virtual environment is activated, your command-line prompt will change to the name of the virtual environment. This activation lets you know that this environment is now active and ready to be used as a virtual one.\n\n![Python virtual environment activated](https://res.cloudinary.com/deepgram/image/upload/v1644610838/blog/2022/02/python-virtual-environments/python_virtual_environment_activated.png)\n\nNow you can start installing your Python packages with Pip, and they’ll all be installed inside of that virtual environment as long as it’s activated.\n\n## Using a Virtual Environment\n\nNow that you’ve activated your virtual environment and it’s ready for use let’s quickly walk through how to work with it.\n\nThe first thing you want to do is make sure your virtual environment is activated. So navigate to your directory with the virtual folder and ensure it’s activated.\n\nIf you’re already in the client\\_flask\\_app directory and see your virtual environment folder, activate it by typing:\n\n    source my_virtual_environment/bin/activate\n\nNow you can install Python packages that will be installed inside this folder and won’t collide globally with any other installations.\n\nLet’s check to see if we have anything installed in our environment by running this command:\n\n    pip freeze\n\nYou shouldn’t see any output because we have no packages installed yet.\n\nNow let’s install Flask by doing the following inside your virtual environment:\n\n    pip install Flask\n\nFlask will get installed, and if we list out our packages again, we should see something this time.\n\n    pip freeze\n\n![Python virtual environment pip freeze](https://res.cloudinary.com/deepgram/image/upload/v1644610897/blog/2022/02/python-virtual-environments/python_virtual_environment_pip_freeze.png)\n\nThe above is a list of all of our packages inside our virtual environment.\n\nTo turn off your virtual environment or deactivate it, you’ll need to type:\n\n    deactivate\n\n![Python virtual environment deactivated](https://res.cloudinary.com/deepgram/image/upload/v1644610897/blog/2022/02/python-virtual-environments/python_virtual_environment_deactivated.png)\n\nThat wraps up this article about virtual environments in Python. You can start using them in all of your projects and switching in between them.\n\nThere are a couple of things to remember. Make sure the environment is activated before you install your packages. You can name your virtual environment anything you’d like, but the recommended naming convention is using the name of the **venv** module like `python3 -m venv venv`. If this gets confusing, remember to use the `--prompt` flag to change the name of the virtual environment.\n\nIf you ever have any questions about this post, please feel free to reach out to us on Twitter [@deepgramdevs](https://twitter.com/DeepgramDevs). We’re happy to help!\n\n## Quick Commands Creating and Activating a Virtual Environment\n\n    mkdir [% NAME_OF_YOUR_DIRECTORY %]\r\n    cd [% NAME_OF_YOUR_DIRECTORY %]\r\n    python3 -m venv venv\r\n    source venv/bin/activate\n\n        ";
						}
						async function compiledContent$1k() {
							return load$1k().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1k() {
							return (await import('./chunks/index.f08c2069.mjs'));
						}
						function Content$1k(...args) {
							return load$1k().then((m) => m.default(...args));
						}
						Content$1k.isAstroComponentFactory = true;
						function getHeadings$1k() {
							return load$1k().then((m) => m.metadata.headings);
						}
						function getHeaders$1k() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1k().then((m) => m.metadata.headings);
						}

const __vite_glob_0_192 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1k,
  file: file$1k,
  url: url$1k,
  rawContent: rawContent$1k,
  compiledContent: compiledContent$1k,
  default: load$1k,
  Content: Content$1k,
  getHeadings: getHeadings$1k,
  getHeaders: getHeaders$1k
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1j = {"title":"Introduction to PyTorch Audio Data via TorchAudio","description":"Learn how to use TorchAudio to transform, augment, and extract features from audio data.","date":"2022-06-27T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1656347792/blog/2022/06/pytorch-intro-with-torchaudio/Introduction-to-PyTorch-Audio-Data-via-TorchAudio%402x.jpg","authors":["yujian-tang"],"category":"tutorial","tags":["python","pytorch","torchaudio"],"seo":{"title":"Introduction to PyTorch Audio Data via TorchAudio","description":"Learn how to use TorchAudio to transform, augment, and extract features from audio data."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454102/blog/pytorch-intro-with-torchaudio/ograph.png"},"shorturls":{"share":"https://dpgr.am/08d1281","twitter":"https://dpgr.am/b0e6457","linkedin":"https://dpgr.am/22478f3","reddit":"https://dpgr.am/33e3a34","facebook":"https://dpgr.am/7f0d64d"}};
						const file$1j = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/pytorch-intro-with-torchaudio/index.md";
						const url$1j = undefined;
						function rawContent$1j() {
							return "PyTorch is one of the leading machine learning frameworks in Python. Recently, PyTorch released an updated version of their framework for working with audio data, [TorchAudio](https://github.com/pytorch/audio). TorchAudio supports more than just using audio data for machine learning. It also supports the data transformations, augmentations, and feature extractions needed to use audio data for your machine learning models.\n\nIn this post, we'll cover:\n\n* [Setting up PyTorch TorchAudio for Audio Data Augmentation](#setting-up-pytorch-torchaudio-for-audio-data-augmentation)\n* [Adding Effects for Audio Data Augmentation with PyTorch TorchAudio](#adding-effects-for-audio-data-augmentation-with-pytorch-torchaudio)\n* [Using Sound Effects in Torchaudio](#using-sound-effects-in-torchaudio)\n* [Adding Background Noise](#adding-background-noise)\n* [Adding Room Reverberation](#adding-room-reverberation)\n* [Advanced Resampling of Audio Data with TorchAudio](#advanced-resampling-of-audio-data-with-torchaudio)\n* [Audio Feature Extraction with PyTorch TorchAudio](#audio-feature-extraction-with-pytorch-torchaudio)\n* [In Summary](#in-summary)\n\n## Setting up PyTorch TorchAudio for Audio Data Augmentation\n\nAt the time of writing, `torchaudio` is on version `0.11.0` and only works with Python versions 3.6 to 3.9. For this example, we’ll be using Python 3.9. We’ll also need to install some libraries before we dive in. The first libraries we’ll need are `torch` and `torchaudio` from PyTorch. We’ll be using `matplotlib` to plot our visual representations, `requests` to get the data, and `librosa` to do some more visual manipulations for spectrograms.\n\nTo get started we’ll pip install all of these into a new virtual environment. [To start a virtual environment](https://blog.deepgram.com/python-virtual-environments/) run `python3 -m venv <new environment name>`. Then run `pip install torch torchaudio matplotlib requests librosa` and let `pip` install all the libraries necessary for this tutorial.\n\n## Adding Effects for Audio Data Augmentation with PyTorch TorchAudio\n\nRecently, we covered the basics of [how to manipulate audio data in Python](https://blog.deepgram.com/best-python-audio-manipulation-tools/). In this section we’re going to cover the basics of how to pass sound effect options to TorchAudio. Then, we’ll go into specifics about how to add background noise at different sound levels and how to add room reverb.\n\nBefore we get into that, we have to set some stuff up. This section of code is entirely auxiliary code that you can [skip](#using-sound-effects-in-torchaudio). It would be good to understand this code if you’d like to continue testing on the provided data.\n\nIn the code block below, we first import all the libraries we need. Then, we define the URLs where the audio data is stored and the local paths we’ll store the audio at. Next, we fetch the data and define some helper functions.\n\nFor this example, we’ll define functions to get a noise, speech, and reverb sample. We will also define functions to plot the waveform, spectrogram, and `numpy` representations of the sounds that we are working with.\n\n```py\nimport math\nimport os\n\nimport matplotlib.pyplot as plt\nimport requests\nimport torchaudio\nimport torch\n\n_SAMPLE_DIR = \"_assets\"\nSAMPLE_WAV_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/steam-train-whistle-daniel_simon.wav\"\nSAMPLE_WAV_PATH = os.path.join(_SAMPLE_DIR, \"steam.wav\")\n\nSAMPLE_RIR_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/distant-16k/room-response/rm1/impulse/Lab41-SRI-VOiCES-rm1-impulse-mc01-stu-clo.wav\"  # noqa: E501\nSAMPLE_RIR_PATH = os.path.join(_SAMPLE_DIR, \"rir.wav\")\n\nSAMPLE_WAV_SPEECH_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"  # noqa: E501\nSAMPLE_WAV_SPEECH_PATH = os.path.join(_SAMPLE_DIR, \"speech.wav\")\n\nSAMPLE_NOISE_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/distant-16k/distractors/rm1/babb/Lab41-SRI-VOiCES-rm1-babb-mc01-stu-clo.wav\"  # noqa: E501\nSAMPLE_NOISE_PATH = os.path.join(_SAMPLE_DIR, \"bg.wav\")\n\nos.makedirs(_SAMPLE_DIR, exist_ok=True)\n\ndef _fetch_data():\n   uri = [(SAMPLE_WAV_URL, SAMPLE_WAV_PATH),\n           (SAMPLE_RIR_URL, SAMPLE_RIR_PATH),\n           (SAMPLE_WAV_SPEECH_URL, SAMPLE_WAV_SPEECH_PATH),\n           (SAMPLE_NOISE_URL, SAMPLE_NOISE_PATH),]\n   for url, path in uri:\n       with open(path, \"wb\") as file_:\n           file_.write(requests.get(url).content)\n\n_fetch_data()\n\ndef _get_sample(path, resample=None):\n   effects = [[\"remix\",\"1\"]]\n   if resample:\n       effects.extend([\n           [\"lowpass\", f\"{resample // 2}\"],\n           [\"rate\", f\"{resample}\"]\n       ])\n   return torchaudio.sox_effects.apply_effects_file(path, effects=effects)\n\ndef get_sample(*, resample=None):\n   return _get_sample(SAMPLE_WAV_PATH, resample=resample)\n\ndef get_speech_sample(*, resample=None):\n   return _get_sample(SAMPLE_WAV_SPEECH_PATH, resample=resample)\n\ndef plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n   waveform = waveform.numpy()\n   num_channels, num_frames = waveform.shape\n   time_axis = torch.arange(0, num_frames) / sample_rate\n\n   figure, axes = plt.subplots(num_channels, 1)\n   if num_channels == 1:\n       axes = [axes]\n   for c in range(num_channels):\n       axes[c].plot(time_axis, waveform[c], linewidth=1)\n       axes[c].grid(True)\n       if num_channels > 1:\n           axes[c].set_ylabel(f\"Channel {c+1}\")\n       if xlim:\n           axes[c].set_xlim(xlim)\n       if ylim:\n           axes[c].set_ylim(ylim)\n   figure.suptitle(title)\n   plt.show(block=False)\n\ndef print_stats(waveform, sample_rate=None, src=None):\n   if src:\n       print(\"-\"*10)\n       print(f\"Source: {src}\")\n       print(\"-\"*10)\n   if sample_rate:\n       print(f\"Sample Rate: {sample_rate}\")\n   print(\"Dtype:\", waveform.dtype)\n   print(f\" - Max:     {waveform.max().item():6.3f}\")\n   print(f\" - Min:     {waveform.min().item():6.3f}\")\n   print(f\" - Mean:    {waveform.mean().item():6.3f}\")\n   print(f\" - Std Dev: {waveform.std().item():6.3f}\")\n   print()\n   print(waveform)\n   print()\n\ndef plot_specgram(waveform, sample_rate, title=\"Spectrogram\", xlim=None):\n   waveform = waveform.numpy()\n   num_channels, num_frames = waveform.shape\n   figure, axes = plt.subplots(num_channels, 1)\n   if num_channels == 1:\n       axes = [axes]\n   for c in range(num_channels):\n       axes[c].specgram(waveform[c], Fs=sample_rate)\n       if num_channels > 1:\n           axes[c].set_ylabel(f\"Channel {c+1}\")\n       if xlim:\n           axes[c].set_xlim(xlim)\n   figure.suptitle(title)\n   plt.show(block=False)\n\ndef get_rir_sample(*, resample=None, processed=False):\n   rir_raw, sample_rate = _get_sample(SAMPLE_RIR_PATH, resample=resample)\n   if not processed:\n       return rir_raw, sample_rate\n   rir = rir_raw[:, int(sample_rate*1.01) : int(sample_rate * 1.3)]\n   rir = rir / torch.norm(rir, p=2)\n   rir = torch.flip(rir, [1])\n   return rir, sample_rate\n\ndef get_noise_sample(*, resample=None):\n   return _get_sample(SAMPLE_NOISE_PATH, resample=resample)\n```\n\n## Using Sound Effects in Torchaudio\n\nNow that we’ve set everything up, let’s take a look at how to use PyTorch’s `torchaudio` library to add sound effects. We’re going to pass a list of list of strings (`List[List[Str]])` object to the `sox_effects.apply_effects_tensor` function from `torchaudio`.\n\nEach of the internal lists in our list of lists contains a set of strings defining an effect. The first string in the sequence indicates the effect and the next entries indicate the parameters around how to apply that effect. In the example below we show how to add a lowpass filter, augment the speed, and add some reverb. For a full list of sound effect options available, check out the [sox documentation](http://sox.sourceforge.net/sox.html). Note: this function returns two return values, the waveform and the new sample rate.\n\n```py\n# Load the data\nwaveform1, sample_rate1 = get_sample(resample=16000)\n\n# Define effects\neffects = [\n   [\"lowpass\", \"-1\", \"300\"],  # apply single-pole lowpass filter\n   [\"speed\", \"0.8\"],  # reduce the speed\n   # This only changes sample rate, so it is necessary to\n   # add `rate` effect with original sample rate after this.\n   [\"rate\", f\"{sample_rate1}\"],\n   [\"reverb\", \"-w\"],  # Reverbration gives some dramatic feeling\n]\n# Apply effects\nwaveform2, sample_rate2 = torchaudio.sox_effects.apply_effects_tensor(waveform1, sample_rate1, effects)\nprint_stats(waveform1, sample_rate=sample_rate1, src=\"Original\")\nprint_stats(waveform2, sample_rate=sample_rate2, src=\"Effects Applied\")\nplot_waveform(waveform1, sample_rate1, title=\"Original\", xlim=(-0.1, 3.2))\nplot_specgram(waveform1, sample_rate1, title=\"Original\", xlim=(0, 3.04))\nplot_waveform(waveform2, sample_rate2, title=\"Effects Applied\", xlim=(-0.1, 3.2))\nplot_specgram(waveform2, sample_rate2, title=\"Effects Applied\", xlim=(0, 3.04))\n```\n\nThe printout from plotting the waveforms and spectrograms are below. Notice that adding the reverb necessitates a multichannel waveform to produce that effect. You can see the difference in the waveform and spectrogram from the effects. Lowering the speed lengthened the sound. Adding a filter compresses some of the sound (visible in the spectrogram). Finally, the reverb adds noise we can see reflected mainly in the “skinnier” or quieter sections of the waveform.\n\n![Waveform and Spectrogram of Original and Augmented Audio Data](https://res.cloudinary.com/deepgram/image/upload/v1655980723/blog/2022/06/pytorch-intro-with-torchaudio/1.png)\n*Above: Original Waveform and Spectrogram + Added Effects from TorchAudio*\n\n## Adding Background Noise\n\nNow that we know how to add effects to audio using `torchaudio`, let’s dive into some more specific use cases. If your model needs to be able to detect audio even when there’s background noise, it’s a good idea to add some background noise to your training data.\n\nIn the example below, we will start by declaring a sample rate (8000 is a pretty typical rate). Next, we’ll call our helper functions to get the speech and background noise and reshape the noise. After that, we’ll use the `norm` function to normalize both the speech and the text to the [second order](https://pytorch.org/docs/stable/generated/torch.norm.html). Next, we’ll define a list of decibels that we want to play the background noise at over the speech and create a “background noise” version at each level.\n\n```py\nsample_rate = 8000\nspeech, _ = get_speech_sample(resample=sample_rate)\nnoise, _ = get_noise_sample(resample=sample_rate)\nnoise = noise[:, : speech.shape[1]]\n\nspeech_power = speech.norm(p=2)\nnoise_power = noise.norm(p=2)\n\nsnr_dbs = [20, 10, 3]\nnoisy_speeches = []\nfor snr_db in snr_dbs:\n   snr = math.exp(snr_db / 10)\n   scale = snr * noise_power / speech_power\n   noisy_speeches.append((scale * speech + noise) / 2)\n\nplot_waveform(noise, sample_rate, title=\"Background noise\")\nplot_specgram(noise, sample_rate, title=\"Background noise\")\n```\n\n![Waveform and Spectrogram of Noise Audio Data created by TorchAudio](https://res.cloudinary.com/deepgram/image/upload/v1655982113/blog/2022/06/pytorch-intro-with-torchaudio/2.png)\n\nThe above pictures show the waveform and the spectrogram of the background noise. We have already created all the noise speech audio data clips in the code above. The code below prints all of them out so we can see what the data looks like at different levels of audio. Note that the 20dB `snr` means that the signal (speech) to noise (background noise) ratio is at 20 dB, not that the noise is being played at 20 db.\n\n```py\n# background noise at certain levels\nsnr_db20, noisy_speech20 = snr_dbs[0], noisy_speeches[0]\nplot_waveform(noisy_speech20, sample_rate, title=f\"SNR: {snr_db20} [dB]\")\nplot_specgram(noisy_speech20, sample_rate, title=f\"SNR: {snr_db20} [dB]\")\n\nsnr_db10, noisy_speech10 = snr_dbs[1], noisy_speeches[1]\nplot_waveform(noisy_speech10, sample_rate, title=f\"SNR: {snr_db10} [dB]\")\nplot_specgram(noisy_speech10, sample_rate, title=f\"SNR: {snr_db10} [dB]\")\n\nsnr_db3, noisy_speech3 = snr_dbs[2], noisy_speeches[2]\nplot_waveform(noisy_speech3, sample_rate, title=f\"SNR: {snr_db3} [dB]\")\nplot_specgram(noisy_speech3, sample_rate, title=f\"SNR: {snr_db3} [dB]\")\n```\n\n![20 and 10 dB SNR added background audio data waveforms and spectrograms](https://res.cloudinary.com/deepgram/image/upload/v1655982113/blog/2022/06/pytorch-intro-with-torchaudio/3.png)\n*Above: 20 and 10 dB SNR added background noise visualizations via PyTorch TorchAudio*\n\n![PyTorch generated waveform and spectrogram for 3 dB SNR background noise](https://res.cloudinary.com/deepgram/image/upload/v1655982112/blog/2022/06/pytorch-intro-with-torchaudio/4.png)\n*Above: 3 dB signal to noise ratio waveform and spectrogram for added background noise*\n\n## Adding Room Reverberation\n\nSo far we’ve applied audio effects and background noise at different noise levels. Let’s also take a look at how to add a reverb. Adding reverb to an audio clip gives the impression that the audio has been recorded in an echo-y room. You can do this to make it seem like a presentation you gave to your computer was actually given to an audience in a theater.\n\nTo add a room reverb, we’re going to start by making a request for the audio from where it lives online using one of the functions we made above (`get_rir_sample`). We’ll take a look at the waveform before we clip it to get the “reverb” of the sound, normalize it, and then flip the sound so that the reverb works correctly.\n\n```py\nsample_rate = 8000\n\nrir_raw, _ = get_rir_sample(resample=sample_rate)\n\nplot_waveform(rir_raw, sample_rate, title=\"Room Impulse Response (raw)\", ylim=None)\nplot_specgram(rir_raw, sample_rate, title=\"Room Impulse Response (raw)\")\n\nrir = rir_raw[:, int(sample_rate * 1.01) : int(sample_rate * 1.3)]\nrir = rir / torch.norm(rir, p=2)\nrir = torch.flip(rir, [1])\n\nprint_stats(rir)\nplot_waveform(rir, sample_rate, title=\"Room Impulse Response\", ylim=None)\nplot_specgram(rir_raw, sample_rate, title=\"Room Impulse Response (raw)\")\n```\n\n![Reverb audio data waveform and spectrogram with PyTorch](https://res.cloudinary.com/deepgram/image/upload/v1655982114/blog/2022/06/pytorch-intro-with-torchaudio/5.png)\n*Above: Original and augmented reverb sound visualizations from PyTorch TorchAudio*\n\nOnce we have the sound normalized and flipped, we’re ready to use it to augment the existing audio. We will first use PyTorch to create a “padding” that uses the speech and the augmented sound. Then, we’ll use PyTorch to apply the sound with a 1 dimensional convolution.\n\n```py\nspeech, _ = get_speech_sample(resample=sample_rate)\n\nspeech_ = torch.nn.functional.pad(speech, (rir.shape[1] - 1, 0))\naugmented = torch.nn.functional.conv1d(speech_[None, ...], rir[None, ...])[0]\n\nplot_waveform(speech, sample_rate, title=\"Original\", ylim=None)\nplot_specgram(speech, sample_rate, title=\"Original\")\nplay_audio(speech, sample_rate)\n\nplot_waveform(augmented, sample_rate, title=\"RIR Applied\", ylim=None)\nplot_specgram(augmented, sample_rate, title=\"RIR Applied\")\nplay_audio(augmented, sample_rate)\n```\n\n![Waveform and spectrogram for original and reverb’d sound with PyTorch TorchAudio](https://res.cloudinary.com/deepgram/image/upload/v1655982114/blog/2022/06/pytorch-intro-with-torchaudio/6.png)\n*Above: Visualizations for audio with reverb applied by TorchAudio*\n\nFrom the printout above we can see that adding the room reverb adds echo like sounds to the waveform. We can also see that the spectrogram is less defined than it would be for a crisp, next-to-the-mic sound.\n\n## Advanced Resampling of Audio Data with TorchAudio\n\nWe briefly mentioned how to resample data before using the `pydub` and the `sklearn` libraries. TorchAudio also lets you easily resample audio data using multiple methods. In this section, we’ll cover how to resample data using low-pass, rolloff, and window filters.\n\nAs we have done above, we need to set up a bunch of helper functions before we get into actually resampling the data. Many of these setup functions serve the same functions as the ones above. The one here to pay attention to is `get_sine_sweep` which is what we’ll be using instead of an existing audio file. All the other functions like getting ticks and reverse log frequencies are for plotting the data.\n\n```py\nimport math\nimport torch\n\nimport matplotlib.pyplot as plt\nfrom IPython.display import Audio, display\n\n\nDEFAULT_OFFSET = 201\nSWEEP_MAX_SAMPLE_RATE = 48000\nDEFAULT_LOWPASS_FILTER_WIDTH = 6\nDEFAULT_ROLLOFF = 0.99\nDEFAULT_RESAMPLING_METHOD = \"sinc_interpolation\"\n\ndef _get_log_freq(sample_rate, max_sweep_rate, offset):\n   \"\"\"Get freqs evenly spaced out in log-scale, between [0, max_sweep_rate // 2]\n\n   offset is used to avoid negative infinity `log(offset + x)`.\n\n   \"\"\"\n   start, stop = math.log(offset), math.log(offset + max_sweep_rate // 2)\n   return torch.exp(torch.linspace(start, stop, sample_rate, dtype=torch.double)) - offset\n\ndef _get_inverse_log_freq(freq, sample_rate, offset):\n   \"\"\"Find the time where the given frequency is given by _get_log_freq\"\"\"\n   half = sample_rate // 2\n   return sample_rate * (math.log(1 + freq / offset) / math.log(1 + half / offset))\n\n\ndef _get_freq_ticks(sample_rate, offset, f_max):\n   # Given the original sample rate used for generating the sweep,\n   # find the x-axis value where the log-scale major frequency values fall in\n   time, freq = [], []\n   for exp in range(2, 5):\n       for v in range(1, 10):\n           f = v * 10 ** exp\n           if f < sample_rate // 2:\n               t = _get_inverse_log_freq(f, sample_rate, offset) / sample_rate\n               time.append(t)\n               freq.append(f)\n   t_max = _get_inverse_log_freq(f_max, sample_rate, offset) / sample_rate\n   time.append(t_max)\n   freq.append(f_max)\n   return time, freq\n\ndef get_sine_sweep(sample_rate, offset=DEFAULT_OFFSET):\n   max_sweep_rate = sample_rate\n   freq = _get_log_freq(sample_rate, max_sweep_rate, offset)\n   delta = 2 * math.pi * freq / sample_rate\n   cummulative = torch.cumsum(delta, dim=0)\n   signal = torch.sin(cummulative).unsqueeze(dim=0)\n   return signal\n\ndef plot_sweep(\n   waveform,\n   sample_rate,\n   title,\n   max_sweep_rate=SWEEP_MAX_SAMPLE_RATE,\n   offset=DEFAULT_OFFSET,\n):\n   x_ticks = [100, 500, 1000, 5000, 10000, 20000, max_sweep_rate // 2]\n   y_ticks = [1000, 5000, 10000, 20000, sample_rate // 2]\n\n   time, freq = _get_freq_ticks(max_sweep_rate, offset, sample_rate // 2)\n   freq_x = [f if f in x_ticks and f <= max_sweep_rate // 2 else None for f in freq]\n   freq_y = [f for f in freq if f >= 1000 and f in y_ticks and f <= sample_rate // 2]\n\n   figure, axis = plt.subplots(1, 1)\n   axis.specgram(waveform[0].numpy(), Fs=sample_rate)\n   plt.xticks(time, freq_x)\n   plt.yticks(freq_y, freq_y)\n   axis.set_xlabel(\"Original Signal Frequency (Hz, log scale)\")\n   axis.set_ylabel(\"Waveform Frequency (Hz)\")\n   axis.xaxis.grid(True, alpha=0.67)\n   axis.yaxis.grid(True, alpha=0.67)\n   figure.suptitle(f\"{title} (sample rate: {sample_rate} Hz)\")\n   plt.show(block=True)\n\ndef plot_specgram(waveform, sample_rate, title=\"Spectrogram\", xlim=None):\n   waveform = waveform.numpy()\n\n   num_channels, num_frames = waveform.shape\n\n   figure, axes = plt.subplots(num_channels, 1)\n   if num_channels == 1:\n       axes = [axes]\n   for c in range(num_channels):\n       axes[c].specgram(waveform[c], Fs=sample_rate)\n       if num_channels > 1:\n           axes[c].set_ylabel(f\"Channel {c+1}\")\n       if xlim:\n           axes[c].set_xlim(xlim)\n   figure.suptitle(title)\n   plt.show(block=False)\n```\n\nI put the two `torchaudio` imports here to clarify that these are the `T` and `F` letters we’ll be using to pull functions from (as opposed to true and false!). We’ll declare a sample rate and a resample rate, it doesn’t really matter what these are, feel free to change these as it suits you.\n\nThe first thing we’ll do is create a waveform using the `get_sine_sweep` function. Then, we’ll do a resampling without passing any parameters. Next, we’ll take a look at what the sweeps look like when we use a low pass filter width parameter. For this, we’ll need the functional `torchaudio` package.\n\nTechnically, there are infinite frequencies, so a low pass filter cuts off sound below a certain frequency. The low pass filter width determines the window size of this filter. Torchaudio’s default is 6 so our first and second resampling are the same. Larger values here result in “sharper” noise.\n\n```py\nimport torchaudio.functional as F\nimport torchaudio.transforms as T\nsample_rate = 48000\nresample_rate = 32000\n\nwaveform = get_sine_sweep(sample_rate)\nplot_sweep(waveform, sample_rate, title=\"Original Waveform\")\n\nprint(\"basic resampling\")\nresampler = T.Resample(sample_rate, resample_rate, dtype=waveform.dtype)\nresampled_waveform = resampler(waveform)\nplot_sweep(resampled_waveform, resample_rate, title=\"Resampled Waveform\")\n\nprint(\"lowpass resampling\")\nlp6_resampled_waveform = F.resample(waveform, sample_rate, resample_rate, lowpass_filter_width=6)\nplot_sweep(resampled_waveform, resample_rate, title=\"lowpass_filter_width=6\")\n\nlp128_resampled_waveform = F.resample(waveform, sample_rate, resample_rate, lowpass_filter_width=128)\nplot_sweep(resampled_waveform, resample_rate, title=\"lowpass_filter_width=128\")\n```\n\n![Basic and Low-Pass Filter Spectrograms from PyTorch TorchAudio](https://res.cloudinary.com/deepgram/image/upload/v1655982113/blog/2022/06/pytorch-intro-with-torchaudio/7.png)\n*Above: Basic and Low Pass Filter Example Spectrogram from TorchAudio*\n\nFilters are not the only thing we can use for resampling. In the example code below, we’ll be using both the default Hann window and the Kaiser window. Both windows serve as ways to automatically filter. Using rolloff for resampling achieves the same goals. In our examples, we’ll take a rolloff of 0.99 and 0.8. A rolloff represents what proportion of the audio will be attenuated.\n\n```py\nprint(\"using a window to resample\")\nhann_window_resampled_waveform = F.resample(waveform, sample_rate, resample_rate, resampling_method=\"sinc_interpolation\")\nplot_sweep(resampled_waveform, resample_rate, title=\"Hann Window Default\")\n\nkaiser_window_resampled_waveform = F.resample(waveform, sample_rate, resample_rate, resampling_method=\"kaiser_window\")\nplot_sweep(resampled_waveform, resample_rate, title=\"Kaiser Window Default\")\n\nprint(\"user rollof to determine window\")\nrolloff_resampled_waveform = F.resample(waveform, sample_rate, resample_rate, rolloff=0.99)\nplot_sweep(resampled_waveform, resample_rate, title=\"rolloff=0.99\")\n\nrolloff_resampled_waveform = F.resample(waveform, sample_rate, resample_rate, rolloff=0.8)\nplot_sweep(resampled_waveform, resample_rate, title=\"rolloff=0.8\")\n```\n\n![Resampling Audio Data with Windows and Rolloff filters](https://res.cloudinary.com/deepgram/image/upload/v1655982112/blog/2022/06/pytorch-intro-with-torchaudio/8.png)\n*Above: Windowed and Rolloff parameter resampling visualizations from TorchAudio*\n\n## Audio Feature Extraction with PyTorch TorchAudio\n\nSo far we’ve taken a look at how to use `torchaudio` in many ways to manipulate our audio data. Now let’s take a look at how to do feature extraction with `torchaudio`. As we have in the two sections above, we’ll start by setting up.\n\nOur setup functions will include functions to fetch the data as well as visualize it like the “effects” section above. We also add some functions for doing Mel scale buckets. We will use [Mel scale](https://en.wikipedia.org/wiki/Mel_scale) buckets to make Mel-frequency cepstral coefficients (MFCC), these coefficients represent audio timbre.\n\n```py\nimport os\n\nimport torch\nimport torchaudio\nimport torchaudio.functional as F\nimport torchaudio.transforms as T\nimport librosa\nimport matplotlib.pyplot as plt\nimport requests\n\n\n_SAMPLE_DIR = \"_assets\"\n\nSAMPLE_WAV_SPEECH_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"  # noqa: E501\nSAMPLE_WAV_SPEECH_PATH = os.path.join(_SAMPLE_DIR, \"speech.wav\")\n\nos.makedirs(_SAMPLE_DIR, exist_ok=True)\n\n\ndef _fetch_data():\n   uri = [\n       (SAMPLE_WAV_SPEECH_URL, SAMPLE_WAV_SPEECH_PATH),\n   ]\n   for url, path in uri:\n       with open(path, \"wb\") as file_:\n           file_.write(requests.get(url).content)\n\n\n_fetch_data()\n\n\ndef _get_sample(path, resample=None):\n   effects = [[\"remix\", \"1\"]]\n   if resample:\n       effects.extend(\n           [\n               [\"lowpass\", f\"{resample // 2}\"],\n               [\"rate\", f\"{resample}\"],\n           ]\n       )\n   return torchaudio.sox_effects.apply_effects_file(path, effects=effects)\n\n\ndef get_speech_sample(*, resample=None):\n   return _get_sample(SAMPLE_WAV_SPEECH_PATH, resample=resample)\n\n\ndef plot_spectrogram(spec, title=None, ylabel=\"freq_bin\", aspect=\"auto\", xmax=None):\n   fig, axs = plt.subplots(1, 1)\n   axs.set_title(title or \"Spectrogram (db)\")\n   axs.set_ylabel(ylabel)\n   axs.set_xlabel(\"frame\")\n   im = axs.imshow(librosa.power_to_db(spec), origin=\"lower\", aspect=aspect)\n   if xmax:\n       axs.set_xlim((0, xmax))\n   fig.colorbar(im, ax=axs)\n   plt.show(block=False)\n\n\ndef plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n   waveform = waveform.numpy()\n\n   num_channels, num_frames = waveform.shape\n   time_axis = torch.arange(0, num_frames) / sample_rate\n\n   figure, axes = plt.subplots(num_channels, 1)\n   if num_channels == 1:\n       axes = [axes]\n   for c in range(num_channels):\n       axes[c].plot(time_axis, waveform[c], linewidth=1)\n       axes[c].grid(True)\n       if num_channels > 1:\n           axes[c].set_ylabel(f\"Channel {c+1}\")\n       if xlim:\n           axes[c].set_xlim(xlim)\n       if ylim:\n           axes[c].set_ylim(ylim)\n   figure.suptitle(title)\n   plt.show(block=False)\n\n\ndef plot_mel_fbank(fbank, title=None):\n   fig, axs = plt.subplots(1, 1)\n   axs.set_title(title or \"Filter bank\")\n   axs.imshow(fbank, aspect=\"auto\")\n   axs.set_ylabel(\"frequency bin\")\n   axs.set_xlabel(\"mel bin\")\n   plt.show(block=False)\n```\n\nThe first thing we’re going to do here is plot the spectrogram and reverse it. The waveform to spectrogram and then back again. Why is converting a waveform to a spectrogram useful for feature extraction? This representation is helpful for extracting spectral features like frequency, timbre, density, rolloff, and more.\n\nWe’ll define some constants before we create our spectrogram and reverse it. First, we want to define `n_fft`, the size of the fast fourier transform, then the window length (the size of the window) and the hop length (the distance between short-time fourier transforms). Then, we’ll call `torchaudio` to transform our waveform into a spectrogram. To turn a spectrogram back into a waveform, we’ll use the `GriffinLim` function from `torchaudio` with the same parameters we used above to turn the waveform into a spectrogram.\n\n```py\n# plot spectrogram\nwaveform, sample_rate = get_speech_sample()\n\nn_fft = 1024\nwin_length = None\nhop_length = 512\n\n# create spectrogram\ntorch.random.manual_seed(0)\nplot_waveform(waveform, sample_rate, title=\"Original\")\n\nspec = T.Spectrogram(\n   n_fft=n_fft,\n   win_length=win_length,\n   hop_length=hop_length,\n)(waveform)\nplot_spectrogram(spec[0], title=\"torchaudio spec\")\n\n# reverse spectrogram to waveform with griffinlim\ngriffin_lim = T.GriffinLim(\n   n_fft=n_fft,\n   win_length=win_length,\n   hop_length=hop_length,\n)\nwaveform = griffin_lim(spec)\nplot_waveform(waveform, sample_rate, title=\"Reconstructed\")\n```\n\n![Waveform to Spectrogram and back with PyTorch](https://res.cloudinary.com/deepgram/image/upload/v1655982113/blog/2022/06/pytorch-intro-with-torchaudio/9.png)\n*Above: Creating and reversing a spectrogram in PyTorch*\n\nLet’s take a look at one of the more interesting things we can do with spectral features, [mel-frequency cepstrum](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum). The mel-frequency ceptrsal coefficients (MFCC) represent the timbre of the audio. Before we get started getting these feature coefficients, we’ll define a number of mel filterbanks (256), and a new sample rate to play with.\n\nThe first thing we need for MFCC is getting the mel filterbanks. Once we get mel filter banks, we’ll use that to get the mel spectrogram. Now, we’re ready to get the coefficients. First we need to define how many coefficients we want, then we’ll use the mel filterbanks and the mel spectrogram to create an MFCC diagram. This is what our mel spectrogram looks like when reduced to the number of coefficients we specified above.\n\n```py\n# mel spectrogram\n# mel scale waveforms\n# mel scale bins\nn_mels = 256\nsample_rate = 6000\n\nmel_filters = F.melscale_fbanks(\n   int(n_fft // 2 + 1),\n   n_mels=n_mels,\n   f_min=0.0,\n   f_max=sample_rate / 2.0,\n   sample_rate=sample_rate,\n   norm=\"slaney\",\n)\nplot_mel_fbank(mel_filters, \"Mel Filter Bank - torchaudio\")\n\n# mel spectrogram\nmel_spectrogram = T.MelSpectrogram(\n   sample_rate=sample_rate,\n   n_fft=n_fft,\n   win_length=win_length,\n   hop_length=hop_length,\n   center=True,\n   pad_mode=\"reflect\",\n   power=2.0,\n   norm=\"slaney\",\n   onesided=True,\n   n_mels=n_mels,\n   mel_scale=\"htk\",\n)\n\nmelspec = mel_spectrogram(waveform)\nplot_spectrogram(melspec[0], title=\"MelSpectrogram - torchaudio\", ylabel=\"mel freq\")\n\nn_mfcc = 256\n\nmfcc_transform = T.MFCC(\n   sample_rate=sample_rate,\n   n_mfcc=n_mfcc,\n   melkwargs={\n       \"n_fft\": n_fft,\n       \"n_mels\": n_mels,\n       \"hop_length\": hop_length,\n       \"mel_scale\": \"htk\",\n   },\n)\n\nmfcc = mfcc_transform(waveform)\n\nplot_spectrogram(mfcc[0])\n```\n\n![Mel-scale buckets and mel-frequency cepstrum coefficient plots from TorchAudio](https://res.cloudinary.com/deepgram/image/upload/v1655982114/blog/2022/06/pytorch-intro-with-torchaudio/10.png)\n*Above: MFCC Feature Extraction of Audio Data with PyTorch TorchAudio*\n\n## In Summary\n\nIn this epic post, we covered the basics of how to use the `torchaudio` library from PyTorch. We saw that we can use `torchaudio` to do detailed and sophisticated audio manipulation. The specific examples we went over are adding sound effects, background noise, and room reverb.\n\nTorchAudio also provides other audio manipulation methods as well, such as advanced resampling. In our resampling examples, we showed how to use multiple functions and parameters from TorchAudio’s `functional` and `transform` libraries to resample with different filters. We used low-pass filters, roll off filters, and window filters.\n\nFinally, we covered how to use TorchAudio for feature extraction. We showed how to create a spectrogram to get spectral features, reverse that spectrogram with the Griffin-Lim formula, and how to create and use mel-scale bins to get mel-frequency cepstral coefficients (MFCC) features.";
						}
						async function compiledContent$1j() {
							return load$1j().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1j() {
							return (await import('./chunks/index.00273b50.mjs'));
						}
						function Content$1j(...args) {
							return load$1j().then((m) => m.default(...args));
						}
						Content$1j.isAstroComponentFactory = true;
						function getHeadings$1j() {
							return load$1j().then((m) => m.metadata.headings);
						}
						function getHeaders$1j() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1j().then((m) => m.metadata.headings);
						}

const __vite_glob_0_193 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1j,
  file: file$1j,
  url: url$1j,
  rawContent: rawContent$1j,
  compiledContent: compiledContent$1j,
  default: load$1j,
  Content: Content$1j,
  getHeadings: getHeadings$1j,
  getHeaders: getHeaders$1j
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1i = {"title":"Real-time routing of conversational data is table stakes for enterprises","description":"Fuel your companys strategy, insights, and growth with capturing and using the valuable voice data from all your meetings, phone calls, and video conferences.","date":"2021-07-08T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981377/blog/real-time-routing-of-conversational-data-is-table-stakes-for-enterprises/real-time-routing-convo-data%402x.jpg","authors":["richard-stevenson"],"category":"speech-trends","tags":["voice-strategy"],"seo":{"title":"Real-time routing of conversational data is table stakes for enterprises","description":"Fuel your companys strategy, insights, and growth with capturing and using the valuable voice data from all your meetings, phone calls, and video conferences."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981377/blog/real-time-routing-of-conversational-data-is-table-stakes-for-enterprises/real-time-routing-convo-data%402x.jpg"},"shorturls":{"share":"https://dpgr.am/6e39a74","twitter":"https://dpgr.am/3dcbc65","linkedin":"https://dpgr.am/58e1a21","reddit":"https://dpgr.am/3ea450b","facebook":"https://dpgr.am/6781440"}};
						const file$1i = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/real-time-routing-of-conversational-data-is-table-stakes-for-enterprises/index.md";
						const url$1i = undefined;
						function rawContent$1i() {
							return "***With voice data increasingly seen as a strategic asset awash with rich insights, timely access to real-time high-quality audio and transcripts for AI engines to reason over is critical.***  In fact, the advanced capabilities of speech analytics and associated use-cases are making enterprises wake up to the value held in call recordings, with almost 76%\\* of C-level executives regarding voice data as \"valuable\" or \"very valuable\" to their organization.  Add to this the 85%\\** of organizations regarding ASR as \"important\" to their future AI strategies, it is evident that the value of the spoken word is rising.\n\n## **Barely scratching the surface of voice and ASR's potential** \n\nHowever, despite the potentially game-changing ability to extract rich insights from the analysis of audio data at scale, around 66% of organizations\\*\\* are unable to capture and fully utilize the vast amount of voice and unstructured/ structured data that flows throughout their business daily, preventing them from tapping into valuable AI-driven insights that could help optimize customer experience (CX), improve sales performance, enhance compliance, and drive process efficiencies through automation.\n\nMost often, companies are hindered by data that is siloed and locked away in proprietary systems, with traditional incumbents often charging to export call recordings in batch. Add to this the absence of real-time recording streams and an inability to access 'AI ready' uncompressed voice data, and the result is untimely access to low-quality recordings that impact the accuracy of transcripts which in turn drives sub-par analytics. The ultimate impact is failure to fully optimize ROI from conversational AI investments and the stifling of agile innovation.\n\n## **Overcoming barriers to deliver voice and video analytics at scale**\n\nThe good news is that there are ways to overcome issues with the consumption of media across the enterprise to satisfy the demand for accurate, real-time data that fuels customer analysis and enhances competitive advantage. To get the most out of data, organizations should look for a call recording vendor that is able to capture every conversation taking place and not just within siloed departments. They should ensure they have free access to their aggregated voice data sets and the ability to leverage these in transcription engines of their choice whether through an extensive ecosystem of leading AI and ML vendors or their own in-house applications. This methodology will become increasingly important as the capture and real-time routing of data with context becomes table stakes in many leading enterprise operations.\n\n## **Maximizing your voice and AI strategy** \n\nAs data grows exponentially across enterprises and as the capture of conversational data-driven by AI - becomes more broadly focused, organizations should ensure they have the correct foundations in place to maximize the benefits of their voice and AI strategy.  [TIPS TO MAXIMIZE YOU VOICE AND AI STRATEGY](https://www.redboxvoice.com/campaigns/the-secrets-to-maximizing-your-voice-and-ai-strategy-whitepaper) \n\n\\*Survey conducted by SAPIO Research for Red Box, 2020\n\n\\*\\* 2021 Deepgram State of Automatic Speech Recognition Report";
						}
						async function compiledContent$1i() {
							return load$1i().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1i() {
							return (await import('./chunks/index.7615ed3b.mjs'));
						}
						function Content$1i(...args) {
							return load$1i().then((m) => m.default(...args));
						}
						Content$1i.isAstroComponentFactory = true;
						function getHeadings$1i() {
							return load$1i().then((m) => m.metadata.headings);
						}
						function getHeaders$1i() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1i().then((m) => m.metadata.headings);
						}

const __vite_glob_0_194 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1i,
  file: file$1i,
  url: url$1i,
  rawContent: rawContent$1i,
  compiledContent: compiledContent$1i,
  default: load$1i,
  Content: Content$1i,
  getHeadings: getHeadings$1i,
  getHeaders: getHeaders$1i
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1h = {"title":"Retail, Restaurants and Travel - Shilp Agarwal, CEO, Blutag - Project Voice X","description":"Retail, Restaurants and Travel, presented by Shilp Agarwal of Blutag, presented on day one of Project Voice X. ","date":"2021-12-09T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981400/blog/retail-restaurants-and-travel-shilp-agarwal-ceo-blutag-project-voice-x/proj-voice-x-session-shilp-agarwal-blog-thumb-554x.png","authors":["claudia-ring"],"category":"speech-trends","tags":["amazon","ecommerce","project-voice-x"],"seo":{"title":"Retail, Restaurants and Travel - Shilp Agarwal, CEO, Blutag - Project Voice X","description":"Retail, Restaurants and Travel, presented by Shilp Agarwal of Blutag, presented on day one of Project Voice X. "},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981400/blog/retail-restaurants-and-travel-shilp-agarwal-ceo-blutag-project-voice-x/proj-voice-x-session-shilp-agarwal-blog-thumb-554x.png"},"shorturls":{"share":"https://dpgr.am/d4a7805","twitter":"https://dpgr.am/d14996d","linkedin":"https://dpgr.am/6c5e0ed","reddit":"https://dpgr.am/48e087e","facebook":"https://dpgr.am/aa648d1"}};
						const file$1h = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/retail-restaurants-and-travel-shilp-agarwal-ceo-blutag-project-voice-x/index.md";
						const url$1h = undefined;
						function rawContent$1h() {
							return "*This is the transcript for “NLP on the Edge: Voice, AI and Hardware,” presented by Robert Daigle and Andi Huels of Lenovo, presented on day one of Project Voice X.*\n\n*The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was **94% accurate.**  Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram’s API.  If you want to see if Deepgram is right for your use case, [contact us](https://deepgram.com/contact-us/).*\n\n\\[Shilp Agarwal:] Oh, hi, everyone. I’m Shilp Agarwal. I’m the CEO of Blutag. The title of my talk is retail restaurants and travel. I don’t know why travel ’cause I don’t know anything about travel. I don’t even like to travel. I think the only thing that happened was that I complained to Bradley about the fact that there was no nonstop flights from New York, so maybe that’s why he added that. So we’re not gonna talk about travel. We are a voice commerce platform focusing on retail experiences primarily for shopping of consumable items for for consumers.\n\nSo just a little background on the company, we’ve been around for over three years. Oh, we’re not gonna go there right now. We’ve been around for over three years. We work with… we have hundreds of retailers on our platform right now, some really prominent brands. And our focus has been primarily on using Alexa and Google Assistant. And I’ll go into a little bit more about, you know, how we’re venturing into other voice assistance as well. But it’s a really quick talk. I’m just gonna talk a little bit about the current state of where things are with the voice commerce, everything that we’ve seen working with all these brands over the last few years as to what has not worked, which is a lot of things, and and what’s working now. So just starting with kind of things, voice commerce related and smart speaker related, so there’s about hundred and fifty million smart speakers right now in the US.\n\nIf you just look at the overall voice commerce and and and the shopping usage, almost thirty five million people have made at least one purchase using a smart speaker last year. So that’s a, you know, significant number of people that are that are doing that. That accounted for almost twenty two billion dollars in voice commerce again last year. So those numbers are there. When we started this thing in twenty eighteen, the numbers that came out of twenty seventeen were about two billion dollars. So it has grown significantly since then. And now it’s on track to get two hundred and sixty four billion by twenty twenty five. So when you look at what’s happened, it has beaten expectations and it continues to grow really fast. At the same time, you know, about a year and a half ago… two years ago, there were a couple of articles out, you know, including this one which said that voice commerce has not taken off and it’s not working.\n\nAnd, you know, and we were a part of the voice commerce journey at that time as well. So, you know, I wanna kind of shed a little light on kind of what we saw during that phase. You know? And and I think it’s important to see that if the numbers are there, if the voice comma… transactions are there and they’re growing faster than they were supposed to, but still people are complaining about it, where did we go wrong, or where were things going wrong? So I think one of the biggest things that happened with retail was a lot of people tried to do too much when it came to voice base commerce. We said that… you know, we have this website that does all these things. You know? You can browse products. You can look at products, look at reviews. You can make a product selection and buy them. Let’s bring all of that functionality to smart speakers or voice assistants. And that’s just not… that just didn’t work ’cause, you know, there’s a lot of elements of that that don’t make sense today and more of them that didn’t make sense even back then when the only smart speakers you had were primarily the ones that did not have screens. So when you did that, people started using those experiences and didn’t didn’t like ’em. You know? And and there was all sorts of issues with, you know, whether you couldn’t find the right product or the conversation was too long.\n\nIt just didn’t work. You know? And when you look at the journey of voice commerce, you know, it’s kinda the same thing when we went with early days of ecommerce, and and there’s some similarities and there’s some this… differences. This is what it looked like twenty years ago. Right? This this used to be a website where you would go and you would buy things. It wasn’t ideal, but it still kinda worked. You still ended up doing a purchase. I mean, the credit card validation did not work. You had to do it multiple times. Finally, you got a transaction and you were super happy that you placed this order. Because at that point, you know, we had a couple of options. We either have an option to try to figure out how to buy a product from this website or we had to go out there and to try to find that product in the real world outside. Right?\n\nSo because that was our only option, we figured out how to do it and we made it work. Now things are different right now with with voice commerce. Everything that you’re trying to do with voice, people can do that in another method. They have their computer. You know? They have this all the time. They’re able to perform all these things. The only way it would make sense for them is if you actually made it a lot more convenient and if it actually made better sense for them to perform that particular action through voice as opposed to going on their phone. And, especially, now with the introduction of, you know, all these ecommerce platforms, that problem with ecommerce has been solved. So it’s a lot easier to shop using web or mobile, but voice continues to be a challenge. So so this is kinda like what hasn’t worked. Right? What hasn’t worked is trying to do too much. What hasn’t worked is we’re trying to replace every functionality that that we have on the web and try to bring all of that through voice. So what is working? What what are all those large numbers of voice commerce that we see? And, you know, before we look into what’s working, it’s it’s important to understand. You know?\n\nWe had a few people talk about, you know, how the user behavior changes, so important to understand kind of how user behavior works. And and one of the biggest examples, I think, of… in recent times of user behavior is is is QR code. They’ve been around for over twenty five years. They came out in nineteen ninety four, the same year as Amazon came out. We had no idea what they were for twenty five years, and then all of a sudden we look at them and we feel hungry. Right? ‘Cause we figured out that that’s how you order food. So so that behavior changed even though things happen around. And and and and, similarly, when you start thinking about shopping behaviors, there’s a certain phase that we’ve gone through over the last few decades that has changed that as well. And I think the biggest force for digital behavior change has been Amazon. And and what’s happened is that Amazon has created certain shopping behaviors whether it’s about, you know, the speed of delivery, whether it’s about the ease of return, two days shipping. And all retailers have to kind of follow that just because that behavior has already been formed. The user has that expectation.\n\nSo same thing is with with voice. There are certain elements that people have been used to with voice. And and, you know, I’ll I’ll show a couple of examples of, you know, basic demos of kind of things that we’ve seen working. And, you know, a big one is being able to quickly reorder something from Amazon. You know, that’s something… was the first thing… you know, they used to have these dash buttons. They got rid of that because a lot of that functionality was replaced by by voice. People were able to quickly reorder something. We provide the same functionality, you know, with with somebody else who does not have products on Amazon and have their own brand with loyal customers and selling something like this, see if voice works, “Alexa, reorder my coffee from Black Rifle Coffee.”\n\n\\[Amazon Alexa:] Last time, you ordered Coffee Or Die roast whole bean twelve ounces bag for thirteen dollars and ninety nine cents. Would you like to add it to your cart?\n\n\\[Shilp Agarwal:] Yes. So this is a very basic example. It only does one thing. It lets you reorder your coffee. Now, you know, how many times does it happen that you’re running out of a particular product? You know, it could be your dog food or it could be a moisturizer. You know that you are gonna run out of it and you wait for it ’til it’s fully out and then you realize that it’s to order… too late to order it online and you go out to the store. With with smart speakers all around, you know, ambient computing, that, again, was talked earlier as well, is is something that, you know, captures that moment when you’re… you are actually thinking about that.\n\nSo if you were able to capture the moment when somebody knew that they were gonna run out of a product and give them the ability to order it, you would be able to save that sale. The people, you know… our our categories have been the replenishable ones. Our top categories are, you know, groceries, baby, pet, beauty for the same exact reason that that we mentioned. And we have seen people who have their customers using voice assistance have seen an increase of up thirty five percent in reorder frequency, simply for the same reason that they’re able to capture that moment when they’re when they’re running out of a product. Now the the other aspect when it when it comes to things like groceries is, you know, being able to add things. And I think Jeff had talked earlier about being able to quickly add things to your shopping list, whether it’s a voice assistant. We have customers that we’ve given them the ability to add items directly to their shopping cart. You know, again, a lot of these devices sit in the kitchens. You’re running out of milk. You’re running out of eggs. You can just quickly… simply ask your voice assistant to do that. So that experience looks something like this.\n\n\\[Commercial Speaker:] Alexa, order milk from FreshDirect.\n\n\\[Amazon Alexa:] Added. Anything else?\n\n\\[Commercial Speaker:] Strawberries.\n\n\\[Amazon Alexa:] Added. Anything else?\n\n\\[Commercial Speaker:] Bananas.\n\n\\[Amazon Alexa:] Added. Anything else?\n\n\\[Commercial Speaker:] No.\n\n\\[Amazon Alexa:] You’re now leaving FreshDirect.\n\n\\[Commercial Speaker:] Alexa?\n\n\\[Shilp Agarwal:] So what this has done is… do you guys hear me? Hello? Yeah. Did I turn it off, Bradley? I don’t even know how to turn it off. I think the battery died.\n\n\\[Bradley Metrock:] Testing. Yeah. It might have. \n\n\\[Shilp Agarwal:] I can just use this.\n\n\\[Bradley Metrock:] Yeah.\n\n\\[Shilp Agarwal:] Or I can just speak loud. \n\n\\[Bradley Metrock:] This mic.\n\n\\[Shilp Agarwal:] Can you guys hear me now? Alright. So, essentially, what we’ve done is that we’ve taken the same exact experience of being able to add things to your shopping list, which people have been used to and, given those customers, the ability to add it directly to their shopping cart. And in these cases, what we’ve seen is the same thing. It’s slightly different. It’s not the reorder frequency, but we’ve seen the in… increase in shopping cart sizes. So our average grocery customer sees an increase of eleven to twelve percent in their shopping cart sizes. If their shopping cart was about thirty, thirty five items, they’re seeing an additional three to four items getting added to the cart. And this is huge, especially when you are at a stage where you’re doing deliveries. You’re letting people do curbside pickup.\n\nHaving those three, four extra items in the cart makes all the difference ’cause that’s… that covers your cost of of a lot of different things that are not there yet. So, again, these are things… and and when you add items like the way we showed you, this is eight to ten times faster than any existing method of adding items to your shopping cart, and you’re capturing that moment. So now, because people have the ability to use their computer or their mobile app to perform these actions, the only way that behavior is gonna change is if you provide them something that is a lot better, and I think that’s the key. So what it comes down to is when we started doing this a couple of years ago, we tried designing a lot of conversations. Let them talk to this device. People don’t wanna talk. People don’t wanna have conversations. They just wanna perform tasks.\n\n> So when you start thinking about shopping in ecommerce, think about what kind of tasks you can let them provide. They wanna say something, and that’s what they wanna get done. And, you know, it goes back to the same thing as to what are the other things that people have been able to do really well. Right? They’ve been able to check the weather, set the timer, listen to music, these are all tasks. They’re not trying to talk with you. And and I think having that mindset makes it really important to to see that, ok, these are the different tasks that you can let your customers provide or the end consumer provide, and that’s where it’s gonna add value.\n\nAnd, essentially, being able to use that to to add value is something that we’ve been able to do effectively. Again, you know, started with really long conversations, decided that a lot of those were ending up in, you know, very early drop-offs ’cause people don’t wanna have that conversation and and, you know, finally, saying that, ok. There’s few elements of this and there’s other other things. Right? You can check your order status. You can check for any sales, coupons, check for product that’s in stock, again, performing these tasks, and then directly relating that to to ROI for a customer. And and once you can figure out the ROI, you know, in… which in in our case is increasing the reorder frequency and increasing shopping cart sizes, then you actually have a customer that is willing to buy your product as well, ’cause at that point, they’re more… going more from saying that, ok. It’s a really cool technology. You should use it because everybody’s gonna use it in the future to saying that I can show you value today, and that’s where it starts making a lot of sense for them. So, yeah, I mean, that’s pretty much what I have more for the retail. You know?\n\nThe other things, you know, we know are gonna be big spaces are gonna be voice in in in auto and and while people are driving. That function is slightly different ’cause I think those are things where people are not gonna be able to do on a screen ’cause they’re driving. I mean, right now, you can still technically do that, but in a few years, we know that it’s gonna go away. You actually will not be able to use your phone when you’re driving. And those experiences, I think, are gonna be, you know, somewhere in between a task and a conversation because those are areas where people are driving and helping them do their grocery shopping when they’re driving back from work is something that’s gonna save them a lot of time, and and it’s gonna make a lot of sense. And that, combined with, you know, I think, a few other things, like restaurant and ordering and drive-throughs, are are are some areas that we see, you know, a lot of big opportunity. But, yeah, that’s that’s all I have and it’s it’s quick, and we’d love to talk to anybody else if they have any ideas. But hopefully, we all have a good show. Thank you.";
						}
						async function compiledContent$1h() {
							return load$1h().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1h() {
							return (await import('./chunks/index.e8ecc739.mjs'));
						}
						function Content$1h(...args) {
							return load$1h().then((m) => m.default(...args));
						}
						Content$1h.isAstroComponentFactory = true;
						function getHeadings$1h() {
							return load$1h().then((m) => m.metadata.headings);
						}
						function getHeaders$1h() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1h().then((m) => m.metadata.headings);
						}

const __vite_glob_0_195 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1h,
  file: file$1h,
  url: url$1h,
  rawContent: rawContent$1h,
  compiledContent: compiledContent$1h,
  default: load$1h,
  Content: Content$1h,
  getHeadings: getHeadings$1h,
  getHeaders: getHeaders$1h
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1g = {"title":"Saving Transcripts From Your Terminal","description":"Learn how to use cURL to generate and save Deepgram transcripts directly from your terminal. Start now!","date":"2022-08-15T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1660569910/blog/2022/08/saving-transcripts-from-terminal/cover.jpg","authors":["kevin-lewis"],"category":"tutorial","tags":["terminal"],"seo":{"title":"Saving Transcripts From Your Terminal","description":"Learn how to use cURL to generate and save Deepgram transcripts directly from your terminal. Start now!"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454119/blog/saving-transcripts-from-terminal/ograph.png"},"shorturls":{"share":"https://dpgr.am/8c8743f","twitter":"https://dpgr.am/4db85ee","linkedin":"https://dpgr.am/76ca356","reddit":"https://dpgr.am/c6681df","facebook":"https://dpgr.am/282a2d3"}};
						const file$1g = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/saving-transcripts-from-terminal/index.md";
						const url$1g = undefined;
						function rawContent$1g() {
							return "I've recently started doing a lot more work directly in my terminal - and I've learned that writing Bash scripts doesn't have to be scary. Today, we'll write a set of commands and scripts to execute directly in our terminal.\n\n## Before You Start\n\nYou will need a Deepgram API Key - [get one here](https://console.deepgram.com/signup?jump=keys). You will also need to install [jq](https://stedolan.github.io/jq/).\n\n## Making a cURL Request\n\nOpen your terminal and type (or copy and paste) the following, not forgetting to change `YOUR_DEEPGRAM_API_KEY` with a real API Key, and then press enter:\n\n```bash\ncurl \\\n  --request POST \\\n  --header 'Authorization: Token YOUR_DEEPGRAM_API_KEY' \\\n  --header 'Content-Type: application/json' \\\n  --data '{\"url\":\"https://static.deepgram.com/examples/nasa-spacewalk-interview.wav\"}' \\\n  --url 'https://api.deepgram.com/v1/listen?punctuate=true'\n```\n\n![A terminal showing a huge block of JSON data returned from Deepgram](https://res.cloudinary.com/deepgram/image/upload/v1658401448/blog/2022/08/saving-transcripts-from-terminal/full-output.png)\n\nLet's break down each part of this request:\n\n* `--request POST`: is a HTTP request with the POST method.\n* `--header 'Authorization: Token YOUR_DEEPGRAM_API_KEY'` - include details to link this request with our account/project.\n* `--header 'Content-Type: application/json'` - JSON data will be sent with this request.\n* `--data '{\"url\":\"https://static.deepgram.com/examples/nasa-spacewalk-interview.wav\"}'`. - is the JSON data sent to Deepgram (an object containing one url parameter).\n* `--url 'https://api.deepgram.com/v1/listen?punctuate=true'` - the URL to make the request to (Deepgram's endpoint). `punctuate=true` enables the [punctuation](https://developers.deepgram.com/documentation/features/punctuate/) feature.\n* `\\` allows us to break one command over several lines for readability.\n\n## Shortening Your Request\n\nThe first example is handy for understanding all of the required parameters. Here is a more concise way to make the same request:\n\n```bash\ncurl https://api.deepgram.com/v1/listen?punctuate=true \\\n     -H \"Authorization: Token YOUR_DEEPGRAM_API_KEY\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"url\":\"https://static.deepgram.com/examples/nasa-spacewalk-interview.wav\"}'\n```\n\nThe first thing you'll notice is that the URL comes immediately after the `curl` command. You may have also noted the absence of an HTTP method - this would normally default to a GET request, but as this request has a body, a POST request is inferred. `--header` is shortened to `-H`, and `--data` to `-d`.\n\n## Adding jq\n\n`jq` is an excellent command-line utility that allows you to display and manipulate JSON data. On the terminal, a pipe (`|`) is often used to send the output of one command as an input for a second command. `jq` expects some JSON as input and an expression to describe how to display it.\n\nThis jq expression will extract just the transcript from the returned data object:\n\n`| jq '.results.channels[0].alternatives[0].transcript'`\n\nYou can add it to the end of your cURL request like so:\n\n```bash\ncurl https://api.deepgram.com/v1/listen?punctuate=true \\\n  -H \"Authorization: Token YOUR_DEEPGRAM_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"url\":\"https://static.deepgram.com/examples/nasa-spacewalk-interview.wav\"}' \\\n  | jq '.results.channels[0].alternatives[0].transcript'\n```\n\n![A terminal showing just the transcript text](https://res.cloudinary.com/deepgram/image/upload/v1658401443/blog/2022/08/saving-transcripts-from-terminal/jq.png)\n\n## Saving Output to File\n\nOnce you have the correct data extracted and formatted from `jq`, you can redirect the output to a new file by appending `> output.txt` to any command that prints to the terminal. Here it is in practice:\n\n```bash\ncurl https://api.deepgram.com/v1/listen?punctuate=true \\\n  -H \"Authorization: Token YOUR_DEEPGRAM_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"url\":\"https://static.deepgram.com/examples/nasa-spacewalk-interview.wav\"}' \\\n  | jq '.results.channels[0].alternatives[0].transcript'\n  > output.txt\n```\n\n## Processing Multiple Files\n\nYou can create `.sh` files to execute from your terminal, which contain multiple lines of bash script. Create a new file called `transcripts.sh` and open it in a code editor. Copy and paste the following:\n\n```bash\n#!/bin/bash\n\nurls=(\"https://static.deepgram.com/examples/TrumpDemocratsMeeting.nancyshort.wav\" \"https://static.deepgram.com/examples/nasa-spacewalk-interview.wav\" \"https://static.deepgram.com/examples/deep-learning-podcast-clip.wav\")\n\ndg_features=\"punctuate=true&utterances=true&diarize=true&tier=enhanced\"\ndg_key=\"YOUR_DEEPGRAM_API_KEY\"\n\nfor url in ${urls[@]}; do\n  filename=${url##*/}\n\n  RESPONSE=$(\n    curl -X POST https://api.deepgram.com/v1/listen?$dg_features \\\n         -H \"Authorization: Token $dg_key\" \\\n         -H \"Content-Type: application/json\" \\\n         -d \"{\\\"url\\\":\\\"$url\\\"}\"\n  )\n\n  echo $RESPONSE | jq '.results.channels[0].alternatives[0].transcript' > $filename.txt\ndone\n```\n\nLet's break down each part of this file:\n\n* The first line - `#!/bin/bash` - is a shebang, and specifies which program should be called to run the script. In this case, bash.\n* `urls` is a variable containing an array with three URLs. Notice that arrays use parentheses, and items are separated by a space only.\n* `dg_features` and `dg_key` are variables you should alter for your exact use case.\n* Inside of the `for` loop:\n\n  * `filename` extracts the last part of the URL (the filename), which will later be used to name the output file.\n  * The `curl` command is the same as before, with variables interpolated. The output is stored in a new variable called `RESPONSE`.\n  * `RESPONSE` is sent to `jq`, and then redirected into a new text file.\n\nRun the file in your terminal with `./transcripts.sh`. As a note, this request uses Deepgram's punctuation, utterances, diarize, and tier features.\n\n## Playing with jq\n\n`jq` is a remarkably powerful tool. The following expression will loop through the `results.utterances` array, and format a string for each item interpolating the speaker identifier and transcript text:\n\n```bash\necho $RESPONSE | jq -r '.results.utterances[] | \"[Speaker:\\(.speaker)] \\(.transcript)\"' > $filename.txt\n```\n\nThe output looks like this:\n\n```\n[Speaker:0] agreement on other things that are really good. Nancy, would you like to say something?\n[Speaker:1] Well, thank you, mister president for the opportunity to meet with you so that we can work together in a bipartisan way\n[Speaker:1] to meet the needs of the American people. I think the American people recognize\n[Speaker:1] that we must keep government open, that a shutdown is not worth anything.\n[Speaker:1] And that you should\n```\n\nI hope you found this valuable and interesting. If you have any questions, please feel free to get in touch - we love to help!";
						}
						async function compiledContent$1g() {
							return load$1g().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1g() {
							return (await import('./chunks/index.dcd3325e.mjs'));
						}
						function Content$1g(...args) {
							return load$1g().then((m) => m.default(...args));
						}
						Content$1g.isAstroComponentFactory = true;
						function getHeadings$1g() {
							return load$1g().then((m) => m.metadata.headings);
						}
						function getHeaders$1g() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1g().then((m) => m.metadata.headings);
						}

const __vite_glob_0_196 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1g,
  file: file$1g,
  url: url$1g,
  rawContent: rawContent$1g,
  compiledContent: compiledContent$1g,
  default: load$1g,
  Content: Content$1g,
  getHeadings: getHeadings$1g,
  getHeaders: getHeaders$1g
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1f = {"title":"Say What You Mean: Navigating Critical Conversations - Scott Sandland, CEO, Cyrano.ai - Project Voice X","description":"Say What You Mean: Navigating Critical Conversations  presented by Scott Sandland, CEO of Cyrano.ai, presented on day one of Project Voice X. ","date":"2021-12-09T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981394/blog/say-what-you-mean-navigating-critical-conversations-scott-sandland-ceo-cyrano-ai-project-voice-x/proj-voice-x-session-scott-sandland-blog-thumb-554.png","authors":["claudia-ring"],"category":"speech-trends","tags":["cyrano-ai","project-voice-x"],"seo":{"title":"Say What You Mean: Navigating Critical Conversations - Scott Sandland, CEO, Cyrano.ai - Project Voice X","description":"Say What You Mean: Navigating Critical Conversations  presented by Scott Sandland, CEO of Cyrano.ai, presented on day one of Project Voice X. "},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981394/blog/say-what-you-mean-navigating-critical-conversations-scott-sandland-ceo-cyrano-ai-project-voice-x/proj-voice-x-session-scott-sandland-blog-thumb-554.png"},"shorturls":{"share":"https://dpgr.am/bd2d6be","twitter":"https://dpgr.am/e770ed4","linkedin":"https://dpgr.am/9f42cf8","reddit":"https://dpgr.am/9a89cdc","facebook":"https://dpgr.am/931b6d2"}};
						const file$1f = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/say-what-you-mean-navigating-critical-conversations-scott-sandland-ceo-cyrano-ai-project-voice-x/index.md";
						const url$1f = undefined;
						function rawContent$1f() {
							return "*This is the transcript for the opening keynote presented by Scott Sandland, CEO of [Cyrano.ai](https://www.cyrano.ai/), presented on day one of Project Voice X.* \n\n*The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was **94% accurate.**  Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram’s API.  If you want to see if Deepgram is right for your use case, [contact us](https://deepgram.com/contact-us/).*\n\n\\[Scott Sandland:] Thank you to Bradley, Rob, the entire Project Voice team. My name is Scott Sandland. I’m the CEO of a company called Cyrano.AI, and what we do is we measure words and language in contextual ways to have subjective understanding about your audience. And just to back up and kinda set the stage, my background is in clinical work. So I had a private practice, and then I was the CEO of a mental health clinic.\n\nIn the private practice, I did a lot of high-functioning executive types, a lot of good to great, that kind of work. And then in the clinic that I ran, we worked with a lot of at-risk teams, like drug-addicted adolescents, a lot of people who were in real trouble. And so I went back and forth between those two populations for about twenty years, which is, you know, a considerable amount of time, and I watched the teen crisis unfold in America from a front-row seat. And I’m not gonna make this entire talk about it because I could talk about this for days, but the second leading cause of death in America for people under the age of twenty four is suicide. To the the tune of about three thousand attempts per day just in high schools, and I was watching this get worse, you know, increasing by twenty percent in given years. I mean, just crazy numbers, and then the kids who were getting over medicated or under medicated or dropping out of school, or we’re getting them… to them too late. And I said, we need to do something they can get on the prevention side of this at scale. And so the easy answer is, ok. That’s some form of tech, but what could that be?\n\nSo I said, alright. I’m just going to build a chatbot. That’s an easy idea. I’ve trained a staff. I’ll just train a robot, and then that scales better. And so I tried, and I tried really hard for quite a while. And I found out that the systems that were available to me couldn’t do it because those systems were optimized for things that weren’t what I optimize for. See, chat systems, you know, natural language processing systems tend to be optimized for a couple of things.\n\nThe first one is efficiency. You know, they want you to get resolved in the contact center fast. It makes sense. Second one is accuracy. Also makes sense. And the third one is complete responses. So complete, accurate responses is what these chat systems are optimized for. And as soon as I really understood that, I went, oh, this is why chat systems are so frustrating because they’re not doing what I do, which is active listening. My peers and my colleagues have a lot of practice with soft skills, with feeling and reading the room, and customizing the message for the intended audience. And everyone in this room knows how to do that, and it’s just about how to break that down into the fundamentals and show the work.\n\nSo I went to a friend of mine who’s a sociologist and a neurolinguist, and I said, this is the thing we gotta do. And he’s been in the… that world for about twenty years as well. So between the two of us, we have about forty years experience of subjective assessment of words. So we started building out a system. We did some prototypes that started working well. I’ll show you some examples. We ended up getting a patent in this system that I’m gonna show you today and how it can be meaningful for the people that you guys interact with. Let’s see which button. There we go.\n\nSo I love words and communication deeply. I always have.\n\n> I believe words and act and effective communication strategies are the thing that defines humanity. In the way that peacocks have feathers and the way that deer have antlers, humans have language, and that language allows us to run this planet. It is not thumbs. It is our idea that we can communicate complex ideas to each other. We can pass on complex ideas to the next generation so we can stand on the shoulders of those who came before us, but also we can support each other. We can help each other. We can empathize. We can crowdsource thought like nothing else we’ve seen in the universe.\n\nThe first most valuable thing in the known universe is oceans made out of water. That’s what we’re looking for with all those telescopes and stuff. Oceans made of water. The second most exciting thing is communication. So we’re talking about the thing that defines us is the second most important thing, and we are in a really interesting spot. And and Bradley’s talk half an hour ago is exactly right, that we’re at this really great point in history where the defining technology on the planet is aligned with communication, and that rarely happens. You’re looking at cave drawings. You’re looking at the printing press, the radio, TV, and now as we’re getting into Web three-point-o, communication. And this is only the second time, and this is my hypothesis that as we’re getting into Web three-point-o, what we have is the second time that is not just about broadcasting. It’s not just about how many retweets or how many views on TikTok. It’s second only to the telephone in how we receive that information is just as important. And so I’m genuinely grateful to be on this stage and in this room because some of the people in this room get to be having some of the conversations that make some of the decisions that will define these categories and more for the next couple generations. And that’s a really exciting window for us to just acknowledge.\n\nSo if we’re gonna do that, we have to think about how we organize things. We can’t we can’t process all the details. So we have to segment. And and Jeff use a a wonderful example of ’em, but let me back up. First, it started off with demographic segmentation, and that’s a very primal easy thing. People who look like me and live near me should probably think and act like me, and people who look like you and live near you, I will extrapolate probably think and act like you. A very simple and useful thought process, but we all know that’s not good enough, and so we did better than that. And we went to psychographics.\n\nEasy example of psychographics is what Netflix does so well. If you like that, you’re gonna love this because people who enjoy and think about these things, enjoy and think about those things. Also nice. And then you get into behavioral, which is what Amazon does incredibly well. Amazon does, hey. People who buy these things end up needing those things. Or it tells me when I’m out of dog food. It’s… it knows how much my dog eats. It knows the behaviors of my dog and segments the messaging appropriately. Wonderful. But now what if we could add contextual? And the reason I love language is because I believe words are a behavior. I am a behavioralist. I think of words that way as an activity. But the great thing about words is that they are absolutely in the moment contextual because we all say the best indicator of future actions is past actions. But that’s not exactly true because we also have to say to the SCC, past results are not a guarantee of future results. But the present, the right now, that’s accurate. I get to find out how this person’s showing up in front of me today. Not the… their demographics, not if they’re a forty-ish year old white male, but what mood they’re in, what they’re prioritizing.\n\nSo we need to move beyond sentiment analysis is the first step of this. No one in this room thinks sentiment analysis is good enough. No one’s looking at sentiment analysis and going, a fifth grade reading level? Yeah. Let’s call it a day. We’ll have billion dollar companies making decisions based on thumbs up, thumbs down assessments. I like the book. I hated the movie. That’s what we got. And there’s some gradients in there, but we need to do more. And so what I’d like to show you is a a small piece of our patent actually is based on this, which is social science established your peer review ideas of motivational interviewing. And we look at the underpinnings of how a person actually makes a decision, and it is this, desire, ability, reason, need, and commitment. I’ll say it again. And, by the way, everything on this presentation is gonna be on the TV at our table back there. So if you wanna see all this, don’t worry about writing anything down or taking good notes. We’ll have it over there slowly, but desire, ability, reason, need, commitment, and then it goes into activation and achievement of a goal.\n\nThis is what I would do with my clients. I would pay attention to which one of those were in play and which one of those we needed to augment. So let’s use some simple examples. Dippin’ Dots. Do I want Dippin’ Dots? Yes. High yes. This is not binary. This gets a high score. Do I want Dippin’ Dots? High yes. Can I have Dippin’ Dots? Yes, Bradley, and the good people at Lenovo have made this very easy for me. They’ve made it accessible close and free. So it requires very little agency on my part to have that ability to be very high. Logic, should I have a Dippin’ Dots? It’s kind of a weak score here. If then, if I have Dippin’ Dots, then I will enjoy it, and my blood sugar will go up a little bit. Not really a compelling thing either way. Do I need Dippin’ Dots? No. Do not need them. Am I gonna have Dippin’ Dots? Yeah. Yeah. I absolutely am. You you know your whole heart, I’m gonna have Dippin’ Dots in like an hour. K?\n\nI almost brought it on stage to eat it now. So in this example, high desire, high ability, no reason, and no need high commitment. What’s that come to? Activation. That’s enough. It crosses that threshold, but let’s give you a couple other examples. Because I told you I worked with high-functioning executives and drug-addicted teenagers, and they actually have a similar thought process, which is interesting. High desire, high commitment. That’s it. High desire, high commitment. That is a language pattern that they use. I want to get this done, get it done. High-functioning executive has a team of people who can show their work in the middle, and they trust the people that report to them and their teams to deliver on those middle steps. And so that high functioning, high level, I want us to do this. Let’s get it done. Good enough.\n\nAlso, drug addicts talk that way. I wanna get some pills, I go get them. That’s gonna be a great clip out of context. But they have this ability, so I got to watch drug addicts and high-functioning executives have the same language patterns and have the same thought processes. Interestingly, that’s how Donald Trump speaks. It really is. And he’s a high-functioning executive, so it makes sense. But he says we’re gonna do this, we’re gonna make it happen. We will do this. I want this. We will do this. And the high-functioning executives and the people who are hourly employees who are used to being told what to do, both are satisfied by that speech pattern. And this is not a political statement on if it’s a good or a bad thing, it’s just really interesting that the people who wanted and to show his work were unsatisfied by that speech pattern, and the people who were used to that speech pattern resonated with it.\n\nSo it wasn’t just the content, it was actually the structure of his message that was congruent with his base, which is why he won an election with a bunch of controversial statements because it was congruent. So the next level of how we can personalize this… and, by the way, what if you had an AI system that knew all that about you and every other person. What if it new desire, ability, reason, need, commitment in real time and changed how it was talking to each person? What if in your HubSpot or in your CRM, you knew where a person’s commitment level was, and you sent them different campaigns based on their word choice? The next way we could analyze a person is personality tests, which is a dramatically flawed science.\n\nThis book, Personality Isn’t Permanent in New York Times’ best-selling book, if you haven’t read it, you absolutely should. But the idea is, Jeff, great. You just finished your third interview. We really like you. Your resume is good. Your credentials are good. You got a couple good letters of rec from your previous employer. Now before we give you an offer letter, what I’d like to do is sit you alone in a room, and I’m gonna give you a hundred question multiple-choice test. And you’re going to fill out hypothetical responses to hypothetical global scenarios out of context that we will then apply to you at the identity level for your entire time at the company. And we may even put a thing on the door to your office that labels you as a c or a INTP or an ESTJ, and that’s who you are. But that’s not who you are. You’re not one person. I know that context matters in terms of my priorities. Because if I go to Disneyland with my kid and my wife, I care about relationships, their feelings, experiences, and happy moments. And when I go to the car dealership, I care about bottom-line price out the door. I care about monthly payment. I care about resale value, and my priorities are different.\n\nAnd I could give you a dozen other examples of that, but I don’t need to because you know how you talk to some people is different than how you talk to others. And how you listen is also different, and you’re paying attention to different things. We’ve all heard that thing. If you want to get money, ask for advice. If you wanna get advice, ask for money. And when you go to a person you say, hey. I need some money. They’re ready to tell you what’s wrong with your idea. Anybody who’s been looking at fundraising with some VCs has had this experience. They put on a different, you know, headspace in a different filter. What if the system was paying attention to your priorities? What if it was segmenting and truly respecting what matters to you? What moves the needle for you? This is what my colleagues and I do all day. This is not a bold idea. That just the idea of how to label the parts becomes interesting. And then learning styles. Quick demo. I’d love for all of you to do this with me. It’s very easy. Everyone just take your right hand and put it straight up like this, please. Now just make it your index finger point at the ceiling. Great. Point at your neighbor. Cool. Now make an ok sign. Everyone take a look at your ok sign. Now look at mine. Go ahead and take and put it right on your chin. k. Real quick. This is your chin. Ok?\n\nSo here’s what we noticed. k? This is not an IQ test ’cause a lot of you failed. Right? So here’s what this is. If you did this, you followed my visual suggestion, and you’re maybe a visual learner. If you did this, you heard me say the word chin and did the right thing there. So you’re an auditory learner. And if you did this, you’re a cheater. And I saw you. I saw… you know, you know, you did it, but that’s ok. But we all know there’s visual learners, there’s auditory learners, and there’s kinesthetic sense of feel or touch, and they’re different learning styles. And the way a person processes information, it interacts with the world internally and externally is broken down with this.\n\nOk. So if you could say, you know, this is a visual person. Make sure the thing is beautiful. Or this is an auditory person, send them a link to a podcast if they… if you want them to learn about this. Or this is a person who cares about feelings, which includes their emotions, so hit them with the heartstrings or give them a sample that they can touch. That’s what matters to each type of person. So this is what our company does. We measure all those things in rank to order and a bunch of other stuff. We do that based on the person’s words and phrases and context so that we know how to change their mind, so we can talk them into and out of decisions, which is a great thing when you’re talking a person into a sale. We’re talking a kid into a treatment plan, or you talk the kid out of self-harm, or you talk a frustrated customer out of churning and going to the competition. You talk to them into and out of these things based on who they are contextually right now, and I should probably show some stats to defend to this.\n\nWithout going too far into the details, I just wanna say all of these numbers on this screen right now we’re all done with the same model. The set… we’re not retraining and refactoring for different deployments. One tool gave us all these scores. That top one being in retail automotive where we found a really interesting thing. There were times where a person’s need was so high that nothing else mattered. The desire, ability, reason, need, and commitment chart, they cared about if they had enough need, that was it. We worked with a car dealership ship where there had been a natural disaster, and we were looking at the transcripts. And these people just needed trucks, and so they were calling a Ford dealership. They say, do you have an f one fifty? I need an f one fifty. And they would say, yes. We have one. That’s the ability of the dealership. So we hear about their need and my ability as the dealer. And they go, great. I’ll be down in an hour to pick up a… the f one fifty. They don’t care what color it is. They don’t care at trim level it is. They don’t care what it can… they just… they don’t care. That car dealership could have said, yes. We have an f one fifty, and, by the way, we hate you, and your mom’s ugly. And notice then, fine. That’s great. I’ll be down in an hour to pick up the truck. Their need was so high. But, also, there’s a lot of times where that’s not the case, where you actually have to pay attention to which of these leverage you’re going to pull. And and those numbers in that hundred and twenty day split test was the first deployment of our system. We were hoping for, like, maybe a five percent lift, and the numbers were incredible. So let’s play a game. We all love live demos, so what if we could work at the speed of transcription? Well, then the transcription really matters.\n\nSo we partner with Deepgram, who analyzed Bradley about half an hour ago. So what we did… you might know… that’s a picture of him from today. So we really did this. You might have noticed that’s kind of working frantically in the back a few moments ago because we took Bradley speaking, and Deepgram did an amazing job. You can see Claudia right here, making sure everything was working. We took everything he said, ran it through their system. She passed it over to Cyrano. Cyrano ran it through our systems, and this is what Cyrano noticed.\n\n![](https://1p70r33dscm81ov8jv3f36b5-wpengine.netdna-ssl.com/wp-content/uploads/2021/12/Bradleys-Analysis--1024x576.jpg)\n\nSo, again, this will be on the back table, so don’t bother trying to read it right now. But this is genuinely the speech he gave, and anybody who does NLP work knows what this is like. All the colored word represents words within different dimensions within a taxonomy that we pay attention to, and then those words are scaled and weighted in their appropriate dimensions. Ok. So then what do we do with this? Oh, we analyze it. So this is Bradley’s speech. You’ll notice that three and a half minute window where the song was playing is blank, and you’ll notice there’s quotes from those moments and then those two blue points, one where he’s impulsive, maybe a high-functioning executive, maybe drug-addicted adolescent. We’ll see. And then also when he was talking about his family friend and the the little boy, right where he got to resolved, was right at the end of that story.\n\nAll of this was created by Cyrano’s analysis, and then it tells about him, his profile. Bradley is a caring and confident individual who cares about relationships and auditory processing. The guy who created a podcast about voice tech is auditory, so shock to us all, but you get some really interesting… he values storytelling, connecting people and ideas, and fairness. Next step is based on this graph, what I need to do next with Bradley. The system gives us bullet point advice so that you don’t need to do any interpretation. It just makes it actionable. Next step, ask them to tailor this for other end users, and they’ll dedicate more energy to the outcome. This tells you exactly what you need to do to be successful. And we know a lot about Bradley based on… we need about one minute of talking is what we need. We can do it with less. We can do it with a couple text messages. By the way, we do this on Twitter. So I’ll show you a couple examples at our table if you want, where we can just take a person’s Twitter feed and create a profile on the person based on what they’re publicly saying ’cause we know a lot about Bradley now. We know how to target our presentations to him. We know how to upsell him. We know how to resolve conflict and deescalate problems, and we know how to negotiate against him without losing rapport. We also know how to show ’em open houses. We also know how to mentor him. We know a lot of other things.\n\nI’m just giving you some high level. But what if this was deployed in more systems and it was able to… and I’ll use an example. We’re working… I’m really excited. I don’t know if Mark’s here yet from Constant Companion. Great organization. They’ve been a speaker a bunch of Project Voice stuff. We’re gonna do some work with them next year that I’m really looking forward to because what if this tool could help grandma. What if when grandma is upset? This tool tells us how to calm her down and deescalate things. What if this helps talk her into taking the medication she doesn’t wanna take or we just give this information to the occupational therapist? And just really simple, hey. Before you go, see grandma. You should know, here are the things to say to help her stay motivated and get her better faster.\n\nWhat if this system was in your call center so that when you’re still dealing with the IVR chatbot side, this is getting enough data so that if the person breaks containment and goes to a rep, you can route them to a rep who matches well. Or you just give the bullet points to the rep of, hey. This person just need bottom line to get them out of here, or this person is more emotional and needs a moment of connection with you. Make this a connection or don’t. What if that same information could then go to the sales department? And now your remarketing efforts with that customer forever are contextually connected to your product. You’re not talking about their search engine history, you’re not talking about their browsing history, you’re talking about what they care about when they’re using your stuff. That’d be pretty cool.\n\nIt’d be pretty exciting if we could say, I know my customer, and I know what matters to them deeply, individually, and personally. We can break people down into a hundred and forty four categories before you start talking about real-time commitment, but no human can track that. When you talk about all the different measurements that we do in real time, my cofounder Dan and I can do that because we had twenty years experience in practice doing it. And we weren’t good at it at first and asking your sales team to remember all the things that I just talked about. And then add, you know, the actual sales pitch and paying attention to the content of the conversation, it’s too much. But the AI can do that, and also the AI can work in multiple languages.\n\nSo when you’re having a conversation, you only speak French, and they only speak Chinese, and you’re working through an interpreter, there’s a lot… that’s literally lost in translation. And while you’re both paying attention to the interpreter, whether it be a human or a robot, you missed the soft skills moment, and so this can fill in that layer. So we’ve decided to build this as an API so that we can plug it into literally everything. And we’re gonna be a very narrow, narrow sliver, and we’re just gonna nail this. And there’s a lot of other companies that are looking at the waveform and tonality and pitch rate and all those things. That’s great. Those other companies are looking at pupil dilation and microgesture. Also great. And what we’re looking at is the words, and we’re gonna find the partnerships that allow us to help kids at risk.\n\nWe’re gonna find the partnerships that allow us to commercially deploy this, and there’s a lot of logistical mathematical and ethical reasons to start in commercial applications that allow us to do a lot more innovation, and we’re doing that today. And there are some companies here on this list and a few others that I’m really excited to be partnering with because it’s going to allow all of us to get more comfortable with a personal assistant that knows that when I say, hey. Put this in my calendar, that she should put it in pencil. What if Alexa heard me say, yeah. Add that party to my list, and she says, yeah. But, you know, this one could be moved if something else was a bigger priority in their command.\n\nThis is where we’re going, and we can get this to the point where we could put these tools on every phone and give every at-risk teenager an entry-level therapist before there’s a problem. At the call center sort of scope of practice, an easy place where we could help a lot of kids. Of course, we could do that with, you know, senior citizens, veterans, and a lot of marginalized populations. And in doing so, create better commercial applications for all of us. So that’s what we’re doing. If you guys want to know how to influence Bradley, you can come to our booth, and you can read that. And we can do this on any of you who like. If you want Cyrano to analyze you and see what it says, happy to do that as well. And either way, thank you guys for your time, and, again, thanks to Bradley for letting me analyze him on the stage.";
						}
						async function compiledContent$1f() {
							return load$1f().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1f() {
							return (await import('./chunks/index.e193a275.mjs'));
						}
						function Content$1f(...args) {
							return load$1f().then((m) => m.default(...args));
						}
						Content$1f.isAstroComponentFactory = true;
						function getHeadings$1f() {
							return load$1f().then((m) => m.metadata.headings);
						}
						function getHeaders$1f() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1f().then((m) => m.metadata.headings);
						}

const __vite_glob_0_197 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1f,
  file: file$1f,
  url: url$1f,
  rawContent: rawContent$1f,
  compiledContent: compiledContent$1f,
  default: load$1f,
  Content: Content$1f,
  getHeadings: getHeadings$1f,
  getHeaders: getHeaders$1f
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1e = {"title":"Build a Web Scraper With Your Voice Using Python","description":"This tutorial will use Python, Beautiful Soup and Deepgram speech-to-text Python API to scrape a website with your voice.","date":"2022-09-15T15:37:24.138Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1663256104/blog/python-scrape-with-voice/2209-Scrape-a-website-with-your-voice-using-Python-blog_2x_r7cpk9.jpg","authors":["tonya-sims"],"category":"tutorial","tags":["python","scraping","beautifulsoup"],"shorturls":{"share":"https://dpgr.am/6af9220","twitter":"https://dpgr.am/17235b9","linkedin":"https://dpgr.am/b8a45f8","reddit":"https://dpgr.am/b0903c3","facebook":"https://dpgr.am/cc11378"}};
						const file$1e = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/scrape-a-website-with-your-voice-using-python/index.md";
						const url$1e = undefined;
						function rawContent$1e() {
							return "\nVoice commands are intriguing, especially with a speech recognition API. After getting exposure to Deepgram’s real-time transcription, and speech-to-text Python SDK, I thought it’d be cool to scrape a website with my voice.\n\nThe way the project works is simple:\n\n1.  Speak the command **scrape** into my computer’s microphone.\n2.  That will kick off the Python scraper, which extracts links from a webpage.\n\nLet’s take a closer look at how I built this project using Python, FastAPI, and Deepgram speech-to-text.\n\n## Python Code Web Scraper Using a Voice Command With Speech-to-Text\n\nFor this voice command scraper, I used one of Python’s newest web frameworks, FastAPI. I’ve already written a blog post about how to get up and running with [FastAPI and Deepgram’s live transcription](https://developers.deepgram.com/blog/2022/03/live-transcription-fastapi/) using the Python SDK.\n\nSince there’s already a tutorial about FastAPI written on Deepgram’s blog, I won’t go into tremendous detail as my [original post](https://developers.deepgram.com/blog/2022/03/live-transcription-fastapi/) covers most of the Python code.\n\nLet’s start with the installation.\n\nI installed two additional Python libraries from my terminal inside of a virtual environment:\n\n```python\npip install beautifulsoup4\n\npip install requests\n```\n\nThen, I added the import statements to the **main.py** file:\n\n```python\nfrom bs4 import BeautifulSoup\nimport requests\nimport re\n```\n\n*   `BeautifuSoup` is for web scraping.\n*   The `requests` library is to get the text from the page source.\n*   The `re` import is to get the links in a specific format.\n\nThe only new function in this file is `scrape_links`. I also defined a new list called `hold_links` which will hold all the links extracted from the webpage. I pass in a URL to scrape to `requests.get` and loop through a BeautifulSoup object. A link from the webpage gets appended to the list each time through the loop.\n\n```python\nhold_links = []\n\ndef scrape_links():\n  url = \"https://xkcd.com/\"\n  r = requests.get(url)\n  \n  soup = BeautifulSoup(r.text, \"html.parser\")\n\n  for link in soup.find_all(\"a\", attrs={'href': re.compile(\"^https://\")}):\n    hold_links.append(link.get('href'))\n\n  return hold_links\n```\n\nNext, is the `get_transcript` inner function.\n\n```python\nasync def process_audio(fast_socket: WebSocket):\n  async def get_transcript(data: Dict) -> None:\n    if 'channel' in data:\n      transcript = data['channel']['alternatives'][0]['transcript']\n      if transcript and transcript == 'scrape':\n        scrape_links()\n        await fast_socket.send_text(transcript)\n\n  deepgram_socket = await connect_to_deepgram(get_transcript)\n\n  return deepgram_socket\n```\n\nThe only change here are these lines to check if there’s a transcript and if the transcript or voice command is **scrape**, then call the `scrape_links` function:\n\n```python\nif transcript and transcript == 'scrape':\n  scrape_links()\n```\n\nLast but not least, when rendering the template, I passed in the `hold_links` list as a context object so the HTML page could display the links using Jinja.\n\n```python\n@app.get(\"/\", response_class=HTMLResponse)\ndef get(request: Request):\n  return templates.TemplateResponse(\"index.html\", {\"request\": request, \"hold_links\": hold_links})\n```\n\nIn the **index.html** file, I added the following line to the `<head></head>` section to refresh the page every five seconds:\n\n```html\n<meta http-equiv=\"refresh\" content=\"5\" />\n```\n\nThe page needs to be refreshed after speaking the voice command **scrape** to display the extracted links.\n\nLastly, in the `<body></body>`, add these lines which loop over the extracted links from the webpage and render them to the HTML page, `index.html`:\n\n```html\n<body>\n  <p>\n    {% for link in hold_links %}\n      {{ link }}</br>\n    {% endfor %}\n  </p>\n</body>\n```\n\nFinally, to run the FastAPI Python voice-to-text web scraper, type `uvicorn main:app --reload` from the terminal and navigate to `http://127.0.0.1:8000/`.\n\nAfter speaking the word **scrape** into my computer’s microphone, a list of extracted links for the specified URL appeared on the webpage.\n\n![Scrape a website using voice commands with Python](https://res.cloudinary.com/deepgram/image/upload/v1663256081/blog/python-scrape-with-voice/python-scrape-with-voice_ijgh01.png \"Scrape a website using voice commands with Python\")\n\n![Scrape and extract links using Beautiful Soup with Python](https://res.cloudinary.com/deepgram/image/upload/v1663256081/blog/python-scrape-with-voice/python-extract-links-with-voice_sc8lid.png \"Scrape and extract links using Beautiful Soup with Python\")\n\nIf you found my project exciting or have questions, please feel free to [Tweet me](https://twitter.com/DeepgramAI)! I’m happy to help!\n\n";
						}
						async function compiledContent$1e() {
							return load$1e().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1e() {
							return (await import('./chunks/index.1f45620b.mjs'));
						}
						function Content$1e(...args) {
							return load$1e().then((m) => m.default(...args));
						}
						Content$1e.isAstroComponentFactory = true;
						function getHeadings$1e() {
							return load$1e().then((m) => m.metadata.headings);
						}
						function getHeaders$1e() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1e().then((m) => m.metadata.headings);
						}

const __vite_glob_0_198 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1e,
  file: file$1e,
  url: url$1e,
  rawContent: rawContent$1e,
  compiledContent: compiledContent$1e,
  default: load$1e,
  Content: Content$1e,
  getHeadings: getHeadings$1e,
  getHeaders: getHeaders$1e
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1d = {"title":"Search Through Sound: Finding Phrases in Audio","description":"Searching through audio is now a reality with Deepgram's ability to find specific phrases in audio.","date":"2016-01-07T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981169/blog/search-through-sound-finding-phrases-in-audio/search-through-sound%402x.jpg","authors":["scott-stephenson"],"category":"dg-insider","tags":["search"],"seo":{"title":"Search Through Sound: Finding Phrases in Audio","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981169/blog/search-through-sound-finding-phrases-in-audio/search-through-sound%402x.jpg"},"shorturls":{"share":"https://dpgr.am/933768f","twitter":"https://dpgr.am/23223ba","linkedin":"https://dpgr.am/15d0c34","reddit":"https://dpgr.am/b782bb0","facebook":"https://dpgr.am/47817dc"}};
						const file$1d = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/search-through-sound-finding-phrases-in-audio/index.md";
						const url$1d = undefined;
						function rawContent$1d() {
							return "![](https://res.cloudinary.com/deepgram/image/upload/v1661721060/blog/search-through-sound-finding-phrases-in-audio/Screen-Shot-2016-01-25-at-7-42-58-PM.png)\n\nMy Co-Founder and I were kicking around the idea of a search engine that would let a person find phrases in a block of audio. We were looking for something that could peer into interviews, podcasts, video lectures - things like that. And if it was done right, you would be able to search through many seasons of a certain TV show and find all the crucial moments like, \"You're fired!\". We thought, *'This has to exist, right?'*. Surprisingly, no. There wasn't a company out there that really provided the functionality. Certainly not in a way that was useful to us, at least. So we started hacking together a Google-based transcription to see if we can get a barebones prototype going. In a couple days it was running - *search for something,* and *most of the time* you got it. Huge pat on the back, right?\n\n### Speech recognition is hard.\n\nReality hit us when we noticed a problem. Sometimes the phrase was definitely spoken-you could hear it plain as day in the audio stream-but the search missed it. It turns out this is due to the inaccuracy of automatic speech transcription software. We went on a quest to get our hands on some top quality speech recognition bad-assery. What we were met with was another dose of reality; *speech recognition is hard*. More evidence emerges when you dig into the current audio research scene and notice that this topic is still a very active topic. The big tech companies (Google, Microsoft, Apple, etc.) put forth large efforts to get this sort of thing right. Even after that, you generally only get 90% word accuracy. That's on very clean, well recorded speech. With input sources containing conversational speech of questionable quality-say, YouTube videos-the word error rate get pretty bad (more than half is wrong sometimes!).\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-how-deepgram-works\"></WhitepaperPromo>\n\n### Can audio search work well?\n\nThis got us wondering, *'can we improve the audio search situation?'*. We landed on something we think is pretty good- search based on how a phrase sounds, not on the precise spelling in text. We were sure this would provide better results but we weren't sure just how much better it would be. We dug into research to see if this technique had been tried in a production form. We turned up quite a few papers-most were not totally relevant-but a Google academic paper on searching through political speeches from 2008 was striking. *'What was their method?'*, you might wonder. They used just regular old text transcription with no additional incorporation of the way the audio actually sounded. Bummer, right?\n\n### Use the way words sound\n\nWhat we were stumbling across was what speech researchers call **keyword search**. There is an existing method for doing this called acoustic keyword spotting, but that requires reprocessing the data every time for each and every search - that's totally impractical. So, yeah, applying this idea is a fairly difficult problem. We didn't really know just how hard at the time, but we know now (eight months of coding our first search engine and starting a company along the way helps beat that into you). [Our API](https://developers.deepgram.com/) allows you to upload audio and have the server process that audio into a giant searchable lattice. With a lattice like this, you can fuzzily go through the entire audio file for your search phrase in a fraction of a second. There is a huge improvement using this method when compared to the text-based approach - search recall goes from a tepid 45% to a grin-inducing 90%+. Now we have our secret sauce.\n\n- - -\n\n***A year and a half after writing this post, NVIDIA's Jensen Huang demonstrated the power of our search on stage at GTC China:***\n\n<iframe src=\"https://www.youtube.com/embed/1SxygN_MODg\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>";
						}
						async function compiledContent$1d() {
							return load$1d().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1d() {
							return (await import('./chunks/index.d2eb5e44.mjs'));
						}
						function Content$1d(...args) {
							return load$1d().then((m) => m.default(...args));
						}
						Content$1d.isAstroComponentFactory = true;
						function getHeadings$1d() {
							return load$1d().then((m) => m.metadata.headings);
						}
						function getHeaders$1d() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1d().then((m) => m.metadata.headings);
						}

const __vite_glob_0_199 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1d,
  file: file$1d,
  url: url$1d,
  rawContent: rawContent$1d,
  compiledContent: compiledContent$1d,
  default: load$1d,
  Content: Content$1d,
  getHeadings: getHeadings$1d,
  getHeaders: getHeaders$1d
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1c = {"title":"Sending Audio Files to Your Express.js Server","description":"Learn how to easily send files through an HTML form to an Express.js server using the middleware package Multer.","date":"2021-11-18T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1641852664/blog/2021/11/sending-audio-files-to-expressjs-server/Posting-Audio-File-to-Express-js-App%402x.jpg","authors":["sandra-rodgers"],"category":"tutorial","tags":["nodejs","express"],"seo":{"title":"Sending Audio Files to your Express.js server","description":"Learn how to easily send files through an HTML form to an Express.js server using the middleware package Multer."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453820/blog/sending-audio-files-to-expressjs-server/ograph.png"},"shorturls":{"share":"https://dpgr.am/64d6c5c","twitter":"https://dpgr.am/c511a95","linkedin":"https://dpgr.am/5c6e6df","reddit":"https://dpgr.am/b7f7b05","facebook":"https://dpgr.am/d38a267"}};
						const file$1c = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/sending-audio-files-to-expressjs-server/index.md";
						const url$1c = undefined;
						function rawContent$1c() {
							return "Here at Deepgram, we work with audio files all the time. We have SDKs that make it easy for developers to send audio files to our API. But it's also really useful to know how to post an audio file to a backend server in case you ever find yourself needing to do that. So I thought I would introduce a simple way for Node developers to take an audio file that is uploaded through the browser and send it to a backend Express.js server.\n\n## Express Server\n\nLet's start on the backend with our Express.js server. Express.js is a Node.js framework that provides us with so many useful methods and middleware, making working in Node that much easier.\n\nWe'll set up the server and implement a middleware package called Multer that helps us handle different file types that we receive to our post endpoint.\n\n### Get the Server Running\n\nWe can start our project by going into the terminal and making a new node project. Here's a reminder of how you would get that started:\n\n```bash\nmkdir project-name\ncd project-name\nnpm init\n```\n\nIf you want to set up a basic Express server, I go over step-by-step how to do that in [this post](https://dev.to/sandrarodgers/basic-express-server-using-replitcom-2ba9). We'll use that basic server code to get us started. In your project folder, create a file where you can place this code. The standard is to use `app.js` as your file where you put your server.\n\n```js\nconst express = require('express')\nconst cors = require('cors')\nconst app = express()\nconst port = 8080\n\napp.use(cors())\n\napp.get('/', (req, res) => {\n  res.json('Hello World')\n})\n\napp.listen(port, () => {\n  console.log(`Example app listening at http://localhost:${port}`)\n})\n```\n\nYou see that we already have a GET endpoint where it says `app.get`. However, we don't want to just send out data from our server to the front end, we want to be able to *receive* an audio file to our server on the backend. So we will set up a POST endpoint to receive a data object sent from the client. Your most basic POST endpoint might look something like this:\n\n```js\napp.post('/test', (req, res, err) => {\n  console.log(req.body)\n  res.json('Successful post')\n})\n```\n\nA post endpoint expects a **request body** sent to it from the client-side. (When that data comes through to the server, you can see the data object in your terminal if you `console.log` the req.body like I did in the example). The format of that data sent in the request body (which is a set of key value pairs) will determine how you deal with the data you have been sent. **A post request using fetch on the client-side might send you a string, a Form Data object, a Blob (for binary data), or URL Search Parameters.**\n\nYou're probably more familiar with data being sent as a JSON encoded string, but since we want to send an audio file, we have to prepare for a different kind of data than JSON. On the front end (which we'll build in the next section), we intend to send the audio file by way of an HTML form, so **we can expect that the data we will be receiving at our server will come in as Form Data** (with a content-type of form/multipart). This [tutorial](https://javascript.info/fetch#post-requests) has all the info you need if you want to read more about fetch post requests and the different data types.\n\nNow, we could parse the raw data of the sound file ourselves (which would require a bunch of code and would probably make us feel very accomplished when we finally manage to get it working), or we could take advantage of the beauty of Node and use a tool that has already been made to help us with this immense task. There are [plenty of packages](https://npm.io/search/keyword:form-data) to choose from to help us handle Form Data, but since we are using Express, a great choice is [Multer](http://expressjs.com/en/resources/middleware/multer.html).\n\n### Multer\n\nMulter is a node.js middleware for handling multipart/form-data. If you're familiar with [body-parser](https://www.npmjs.com/package/body-parser), Multer is similar, except it is built only to deal with multipart bodies.\n\nTo use middleware in Express, we must bring in the package using `require`. We will also want to configure Multer for our needs, and we want to make sure that the audio file we are receiving actually gets written to the disk rather than just stored in memory. So we will include an options object like so `const upload = multer({opts})`, with 'opts' being the specific options for our configuration. **If you do not use the options object, multer will write the file to memory, so make sure you use the options object if you want your file stored on your disk.**\n\n```js\nconst multer = require('multer')\nconst upload = multer({ storage })\n```\n\nRight now, that storage property doesn't have anything behind it. It's an empty variable. But I'm going to show you how I configure that storage option.\n\n```js\nconst multer = require('multer')\n\nconst storage = multer.diskStorage({\n  filename: function (req, file, cb) {\n    console.log('filename')\n    cb(null, file.originalname)\n  },\n  destination: function (req, file, cb) {\n    console.log('storage')\n    cb(null, './uploads')\n  },\n})\n\nconst upload = multer({ storage })\n```\n\nAs you can see, I want to configure *how* we store the file. Using this `storage` option lets us use the Multer [disk storage engine](https://github.com/expressjs/multer#storage), which basically lets us program **how we want the file to be named** (using the disk storage `filename` method) and **where we wanted it to be stored** (using the disk storage `destination` method) . You can see in my code that I'm choosing to name the file exactly what it was named originally when it was sent to us from the client-side, and I am choosing to store it in a folder called `/uploads`.\n\nNow I will write out my post endpoint. It will include the Multer upload middleware. Since we are only uploading one file, I am using `upload.single()` but if you want to upload multiple files, Multer also has the `upload.array()` method. Multer adds a **request file** object which contains the file, and a **request body** object which contains the values of the text fields of the form.\n\n```js\napp.post('/upload_files', upload.single('file'), (req, res) => {\n  console.log(req.body)\n  console.log(req.file)\n  res.send({ message: 'Successfully uploaded files' })\n})\n```\n\nOne crucial thing to be sure of is that the parameter you include in the method (in this case, \"file\" in `upload.single(\"file\")` must correspond to the name field in your HTML form file input. According to the [Multer](https://github.com/expressjs/multer#usage) docs:\n\n> It is important that you use the `name` field value from the form in your upload function. This tells multer which field on the request it should look for the files in. If these fields aren't the same in the HTML form and on your server, your upload will fail.\n\nWe will make sure those values correspond when we build our HTML form in the next section.\n\n## HTML\n\nOver to the front-end now. In your project, you can make an `index.html` file, or you can test this out in something like CodePen. I'll link to both my examples in CodePen so you have access to the working front-end code.\n\n### Pure HTML (No Javascript)\n\nThe first example I want to show you is an HTML form that uses no Javascript.\n\n```html\n<form enctype=\"multipart/form-data\" action=\"http://localhost:8080/upload_files\" method=\"POST\">\n    <label for=\"file-upload\">Select file:</label>\n    <input id=\"file-upload\" type=\"file\" name=\"file\"/>\n <input type=\"submit\" value=\"POST to server\"></input>\n  </form>\n```\n\nThe form tag must include the `enctype` attribute to identify the media, or MIME type. For Multer, you must use `enctype=\"multipart/form-data\"`.\n\nWe also include the `action` attribute, which tells the form the url of the server and the endpoint. Since my server is running locally on port 8080 and my post endpoint path is `/upload_files`, I use the URL `http://localhost:8080/upload_files`. Lastly, we tell the form that the fetch method is `POST`.\n\nDon't forget to include a `name=\"\"` with the name field containing the parameter's name in the Multer upload method used in your Express server. See my working code at this [CodePen](https://codepen.io/sandrarodgers/pen/QWMQGXa?editors=1010).\n\n### HTML and JS using a FormData object\n\nA common need when sending a file is to include extra information with the file that you may want to use for some purpose on your backend, such as data you need to store along with the file. In that case, a way to do this is to use a Javascript FormData object. So in this example, I'll show you an HTML form that uses Javascript to include the FormData object. See my working code at this [CodePen](https://codepen.io/sandrarodgers/pen/XWazGem?editors=1011).\n\n#### HTML\n\n```html\n<form>\n  <label for=\"file\">Select files</label>\n  <input id=\"file\" type=\"file\" name=\"file\" />\n  <input type=\"submit\" value=\"POST to server\"></input>\n</form>\n```\n\nWe don't have to include the enctype attribute in the HTML form since that is already clear by it being a Form Data object we send in the Javascript. Also, the fetch post and URL/endpoint information are also included in the Javascript, so these are not needed in the HTML.\n\n#### Javascript\n\n```js\nconst form = document.querySelector('form')\nconst fileInput = document.getElementById('file')\nlet file\n\n//input file upload gets the file we want to post:\nhandleAudioFile = (e) => {\n  file = e.target.files\n  for (let i = 0; i <= file.length - 1; i++) {\n    file = file[i]\n  }\n}\nfileInput.addEventListener('change', handleAudioFile)\n\n//on clicking the submit button, we create the Form Data object, add the data value of the username to send as part of the request body and add the file to the object\nform.addEventListener('submit', (e) => {\n  e.preventDefault()\n  const formData = new FormData()\n  formData.append('username', 'Sandra Rodgers')\n  formData.append('files', file)\n\n  fetch('http://localhost:8080/upload_files', {\n    method: 'post',\n    body: formData,\n  })\n    .then((res) => console.log(res))\n    .catch((err) => ('Error occurred', err))\n})\n```\n\nThe Form Data allows us to send more info as key-value pairs that we can pull from the req.body when it gets to the server. Add the `append(\"key\", \"value\")` to the form data object for any information you want to add. There are [other methods](https://javascript.info/formdata) you can use to set up the Form Data object the way you want it.\n\nWhen you click the submit button, you can go to the `/uploads` folder you set up and see that your file has arrived!\n\n## Conclusion\n\nNow you are able to use Multer to send files to your Express.js server. Try sending some audio files or even image files. This is a good starting point for taking those files and moving them to another storage place, such as your Cloudinary account or a database.";
						}
						async function compiledContent$1c() {
							return load$1c().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1c() {
							return (await import('./chunks/index.f475d69f.mjs'));
						}
						function Content$1c(...args) {
							return load$1c().then((m) => m.default(...args));
						}
						Content$1c.isAstroComponentFactory = true;
						function getHeadings$1c() {
							return load$1c().then((m) => m.metadata.headings);
						}
						function getHeaders$1c() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1c().then((m) => m.metadata.headings);
						}

const __vite_glob_0_200 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1c,
  file: file$1c,
  url: url$1c,
  rawContent: rawContent$1c,
  compiledContent: compiledContent$1c,
  default: load$1c,
  Content: Content$1c,
  getHeadings: getHeadings$1c,
  getHeaders: getHeaders$1c
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1b = {"title":"Sentiment Analysis and Emotion Recognition: What's the Difference?","description":"Sentiment analysis and emotion regulation are hot topics in speech recognition today, but the two are often confused. So what’s the difference?","date":"2022-04-01T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981414/blog/sentiment-analysis-emotion-regulation-difference/Sentiment-Analysis-Emotional-Recognition-thumb-554.png","authors":["chris-doty"],"category":"ai-and-engineering","tags":["sentiment-analysis","language","nlp"],"seo":{"title":"Sentiment Analysis and Emotion Recognition: Whats the Difference?","description":"Sentiment analysis and emotion regulation are hot topics in speech recognition today, but the two are often confused. So what’s the difference?"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981414/blog/sentiment-analysis-emotion-regulation-difference/Sentiment-Analysis-Emotional-Recognition-thumb-554.png"},"shorturls":{"share":"https://dpgr.am/dd94c45","twitter":"https://dpgr.am/acc49c8","linkedin":"https://dpgr.am/30b3228","reddit":"https://dpgr.am/a20a2c7","facebook":"https://dpgr.am/bee1fe5"}};
						const file$1b = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/sentiment-analysis-emotion-regulation-difference/index.md";
						const url$1b = undefined;
						function rawContent$1b() {
							return "Sentiment analysis and emotion recognition are two of the hottest topics in speech understanding today. But they're often confused for one another-so much so that people often say \"sentiment analysis\" when they're referring to emotion recognition. In this post, we'll explain what both sentiment analysis and emotional recognition are, [how they are used in business](https://blog.deepgram.com/voice-technology-customer-experience/), and some of the limitations and challenges of each.\n\n## 1. What is Sentiment Analysis?\n\n**Sentiment analysis** is a typically text-based machine learning classification task. It might operate on single sentences, paragraphs, or even entire articles. The typical goal of sentiment analysis is to determine whether the author of a text has a positive or a negative opinion about whatever the topic of the text is. To this end, the typical training sets for sentiment analysis models are things like IMDb reviews of movies and Amazon product reviews, where it's easy to tell how someone felt about a topic (that is, their star ratings can be used as part of the training data). Sentiment analysis has a variety of uses, including analyzing customer feedback, monitoring social media conversations, tracking brand reputation, gauging public opinion on a topic or issue, and evaluating customer satisfaction levels.\n\n### Limitations and Challenges of Sentiment Analysis\n\nThere are, of course, limitations to systems like this. Sarcasm, for example, can be hard for sentiment analysis to detect (which isn't surprising since humans also struggle to correctly identify sarcasm in written language). That might be less of a problem when you're training and have the groundtruth of someone's rating, but in the real world, the goal of sentiment analysis is to determine how someone felt in the absence of a rating.\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-how-deepgram-works\"></WhitepaperPromo>\n\n## 2. What is Emotion Recognition?\n\nThe second way that people use the term sentiment analysis is to refer to what is more appropriately known as **emotion recognition** (sometimes called emotion detection, or incorrectly called emotion*al* recognition). Unlike sentiment analysis, emotion recognition typically relies on audio data, rather than text, and uses things like intonation, volume, and speed to determine what emotion a speaker is feeling, usually coded as one of several categories like, happy, sad, angry, etc. It's important to note that this doesn't automatically correlate with how someone feels about a topic. Someone can be happy talking about how they don't like something (who doesn't love to vent?), so ultimately, emotion recognition is trying to get at different information than sentiment analysis. Uses of emotion recognition include helping call center agents understand how a caller is feeling, monitoring hospital patients for stress and pain, and even tracking responses to advertisements.\n\n### Limitations and Challenges of Emotion Recognition\n\nIf you've ever communicated with another human being-and we hope you have-you know that even with all of our experience with social interactions, it can still be tricky to determine someone's emotional state just from talking to them. This is doubly problematic for attempts at emotion recognition. Not only are you trying to make a system to do something that's tricky for humans, you're going to do so using a dataset that humans have labeled based on the emotion that they *think* is present, even though they might not agree, and even though their labels might not accurately match the emotion the speaker was actually feeling.\n\nThis is further complicated by the fact that the audio used to train the model might be acted data, and not people actually expressing the emotion that they're experiencing. Plus, most emotion recognition systems only look at audio data, and don't include other things that could help make a determination, such as body language or facial expressions. It's also the case that we do more with our voice than express emotion-for example, sarcasm in English carries a particular kind of intonation that's recognizable, but sarcasm isn't an emotion. This creates an added complication for emotion recognition systems.\n\n## 3. A Combination of Approaches\n\nBecause of the challenges of sentiment analysis and emotion recognition, some people have tried to combine the systems to try and better understand how people feel about a topic. If you have audio of someone speaking, in addition to conducting emotion recognition on that audio, you can also use a product like Deepgram to transcribe the audio into text, and then apply a text-based sentiment analysis model. This approach obviously only works when you have audio, and so isn't appropriate for all use cases, but it can provide additional insight when working from spoken data.\n\n## Concluding Thoughts\n\nUltimately, sentiment analysis, emotion recognition, or some combination of the two systems can help drive improvements in customer service and retention. By harnessing audio and text data to determine how customers (and employees) are feeling and communicating, you can recommend early steps to help customer service agents, improve retention, and extract valuable insights from unstructured data. If you want to learn more about what the future of voice tech looks like for customer experience, check out our recent webinar [Importance of Voice Technology for Customer Experiences](https://offers.deepgram.com/importance-of-voice-technology-for-customer-experiences-on-demand), which highlights some of the ways that voice tech tools like sentiment analysis and emotion recognition are being used today to power incredible customer experiences.";
						}
						async function compiledContent$1b() {
							return load$1b().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1b() {
							return (await import('./chunks/index.5e071724.mjs'));
						}
						function Content$1b(...args) {
							return load$1b().then((m) => m.default(...args));
						}
						Content$1b.isAstroComponentFactory = true;
						function getHeadings$1b() {
							return load$1b().then((m) => m.metadata.headings);
						}
						function getHeaders$1b() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1b().then((m) => m.metadata.headings);
						}

const __vite_glob_0_201 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1b,
  file: file$1b,
  url: url$1b,
  rawContent: rawContent$1b,
  compiledContent: compiledContent$1b,
  default: load$1b,
  Content: Content$1b,
  getHeadings: getHeadings$1b,
  getHeaders: getHeaders$1b
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1a = {"title":"Should AI be Regulated? — AI Show","description":"Should AI be regulated or not? Listen to this episode of the AI Show and see what you think.","date":"2019-01-11T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981342/blog/should-ai-be-regulated-ai-show-2/should-ai-be-regulated%402x.jpg","authors":["morris-gevirtz"],"category":"ai-and-engineering","tags":["machine-learning","deep-learning","voice-tech"],"seo":{"title":"Should AI be Regulated? — AI Show","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981342/blog/should-ai-be-regulated-ai-show-2/should-ai-be-regulated%402x.jpg"},"shorturls":{"share":"https://dpgr.am/f4c2af8","twitter":"https://dpgr.am/155967e","linkedin":"https://dpgr.am/22de798","reddit":"https://dpgr.am/c43597e","facebook":"https://dpgr.am/1fc0b5f"}};
						const file$1a = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/should-ai-be-regulated-ai-show-2/index.md";
						const url$1a = undefined;
						function rawContent$1a() {
							return " \n<iframe src=\"https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/570094422&color=%23ff5500&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&show_teaser=true\" width=\"100%\" height=\"166\" frameborder=\"no\" scrolling=\"no\"></iframe>\n\n**Scott:** Welcome to the AI show. Today we're asking the question, should AI be regulated?\n\n**Susan:** This is a big one-The government has figured out that AI exists.\n\n**Scott:** Like, \"Whoa. Wait a minute. This is important stuff.\"\n\n**Susan:** It's like the government has realized that maybe the world can change and all sorts of things can happen. And that means we're going to regulate it. Should A.I. be there? It's going to be.\n\nSo really the question is how's it going to be? What does it take to do that? How do you even define something from a legal standpoint?\n\n**Scott:** That's a pretty rough one. You can start with technology, right? Anything that is made from a computer.\n\n**Susan:** We've struggled with this question multiple times on this show.\n\n## How do you define this versus that?\n\nSusan: You go through the internets and the wikis, and suddenly there's eight different definitions for one set of tools, and over time these things blend together so seamlessly that you don't even realize where AI begins and other things begin and it's a big challenge.\n\n**Scott:** I don't know. It's a big problem.\n\n**Susan:** First, can we define it in a legal, well pinned down way that will be durable? So five, ten years from now this legal definition of AI can be used to then say, \"This needs to be regulated in these different ways.\"\n\n**Scott:** Probably not in any good way. You set something down now it seems all right. What are you going to do? Anything that looks smart? No, they're not going to do that. Anything that uses GPUS? Okay. Probably not. Software that gets better over time? Gets better with exposure to data? What definitions are you going to come up with?\n\n![curve](https://res.cloudinary.com/deepgram/image/upload/v1661976807/blog/should-ai-be-regulated-ai-show-2/Basic_Curve_Fit.jpg)\n\n**Susan:** Will curve fitting in Excel suddenly be controlled? You just did linear regression on those points there. So yeah, that's a huge challenge. I think that just the beginning is fraught with danger.\n\n**Scott:** I think there's no way that this could be regulated well, basically. You can regulate it, and maybe government will because of hype and things like that, but it's probably a mistake.\n\n**Susan:** I'm going to take a contrary view. Some regulations have helped in the past industries.\n\n**Scott:** Like what?\n\n## When has regulation helped in the past?\n\n**Scott:** Like banking. No, it's true. It helps with banking.\n\n**Susan:** Basically when it comes down to say, protecting consumers or something along those lines, there are examples where the regulations have helped. But this is a really big challenge here because first of all, the regulations that are starting come out are talking about export laws, export controls because of AI being weaponized. That's not really talking about consumer protections here. That's going to be really, really hard to find something beneficial for the average person coming out regulations like that. But the right legislation can also help increase an industry. If you're company x and you know regulations are coming, but you don't know what they're going to be, when they finally set down, then at least you know the playing field. So long as they're not too damaging, you can now step in understanding there's a safety net of here's my boundary, don't go past this and I can build with inside this niche. Now, the problem is we've already started talking about, can you build those boundaries in any meaningful way?\n\n**Scott:** There will be regulations of some type, but there are lenient regulations and then there are real regulations where you have to do things this way and comply that way, et cetera. The US was in a more unique position in the past, in other similar revolutions. We were the leaders, and we are the ones that had the resources to go after things. In the later part of the Industrial Revolution and the Tech Evolution, we didn't have to worry so much about other players. That's not true anymore. China is a powerhouse. They have a huge population. They have their own economy. They don't have to rely on the US, at least from a tech perspective. They've got their own Twitter, their own Facebook, their own, everything. And you could say, \"Oh, well they're all copies of the US.\" It doesn't matter. They're a huge economy there and they have tons of people and data flows freely. And models and algorithms flow freely. They're a population that's four times the size of the US, and now they're waking up too. Their middle class is only going to get bigger. In the US, we're pretty stagnant. So when you say, \"Hey, we're going to start throwing the ratchets straps on everything and tie it down and everything. Nothing can leave the US.\" Well, that's fine, except no money will be left in the US in order to develop AI. Not none, just less. And data won't be free to train the algorithms. Meanwhile, in China, data's flowing freely, money's flowing freely, and their economy is bolstered massively. And we are here saying we're the future Europe.\n\n**Susan:** It's definitely a challenge because like you said, a very good example is they have their Twitters, they have their Facebooks, but they're fairly blatant copies in some ways.\n\n**Scott:** Yeah. But they work.\n\n**Susan:** That's exactly the point. They don't have the controls that say, \"You're not allowed to really do that.\" So if we start throwing down the screws, stop getting the benefits of seeing the results of their research and incorporating that in and getting that cross pollination, I do think that we could do real harm here and quickly get behind if we put on brakes that are meaningless. Say arms controls, you can stop physical pieces from leaving the country. Some of the things that it takes to build big weapon systems just can't be easily reproduced in another country. Therefore, it's a physical thing. Arms controls makes sense.\n\n![arms](https://res.cloudinary.com/deepgram/image/upload/v1661976807/blog/should-ai-be-regulated-ai-show-2/Screen-Shot-2019-02-01-at-10.28.15-AM.png)\n\n_Physical objects can controlled more easily than bits and bytes on the web. Here container trucks are scanned with with X-ray machines._\n\n**Susan:** But when it comes to machine learning and all that stuff, you can be an 18 year old kid and you're in your basement with a computer, and come up with some great crazy new model tweak that rivals what others are doing. It's becoming a little bit harder. I'm not saying it's that easy.\n\n**Scott:** We can always do this with software though. Do we regulate software?\n\n**Susan:** We do some. And how well does it work? The challenge here is that regulating something where the people you're trying to stop are already on parity with us and they're going to continue on. It just doesn't make a lot of sense.\n\n**Scott:** But, okay. Regulations are coming.\n\n**Susan:** Yeah, they're coming. How do you deal with it?\n\n**Scott:** Are they going to go with some technological definition or are they going to say maybe, 'For civilian use you can do whatever you want, but for military use you can't.\" Or how are they going to drop the lines here?\n\n**Susan:** There's a request or a notice talking about regulations coming, and inside of there they're discussing how to possibly define it. And then part of the comments is how should we define this and all that stuff. But there it doesn't really break down military or civilian, and you got to think a lot of the civilian stuff could easily be used for military stuff. Think about drones. Hey, I want to build a drone that does a whole lot of image recognition stuff. I want to build that farm [drone I've always thought about](https://blog.deepgram.com/ai-show-what-does-an-ai-tranformation-look-like). Well, that exact same technology could obviously easily be used for military purposes. So you can't just say, \"Oh, if it's civilian, go for it\", Because you can flip that switch and in 10 seconds have turned it over to military. So these regulations are not going to be easily split between civilian and military uses there.\n\n**Scott:** But what are they going to regulate, the code? Like by saying: don't open-source AI anymore.\n\n**Susan:** That's a real challenge. But there's also a lot of different things that regulation can touch. How they're going to do it? This is to me a huge big question for the future. I don't know. I don't personally see an easy solution here, but I can see the intents are of course, military export, import, export on military weapons. But not only that. Think about biomedical research and applications in hospitals and stuff like that. Those regulations are definitely going to happen. Maybe autonomous vehicles. [Here's a set of standard data sets](https://blog.deepgram.com/ai-show-different-types-of-machine-learning/) that your machine learning algorithms must pass to be able to be considered a drivable vehicle in California. Those types of regulations, those types of laws are clearly coming and probably are warranted. I can easily see standard data sets from the government being a form of regulation or being a form of control on these things for specific applications. But saying, \"This piece of code, because it was built using some function in the toolkit of x, you can't ship it across the Internet.\", those types of things are really hard to even understand how that could happen.\n\n**Scott:** The closest thing I can think of that has real export restrictions on it are FPGAs. And there are really fancy FPGAs in the world that still have export restrictions on them from the US that are created in the US. It's a lot like a processor, almost as fast as a processor, but it performs like an ASIC-an application-specific integrated circuit.But, it's a programmable processor, if that makes any sense. It's a Field-Programmable Gate Array, so you can physically manipulate the gates on it. They're very small gates and there are millions of them. You can physically manipulate them in electric fields and the military doesn't want them exported... you can't ship those out of the country if they're above a certain performance level.\n\n![fpga](https://res.cloudinary.com/deepgram/image/upload/v1661976808/blog/should-ai-be-regulated-ai-show-2/Altera_StratixIVGX_FPGA.jpg)\n_Though they look like \"just a another chip\" FPGAs are an inherently different sort of data processing device which often have military application. This is because despite their lower clock speed, they are capable of massively parallel computation._\n\n**Scott:** The companies that make these, like Xilinx, can't ship out their high end stuff to China. But, that's a physical object. That's hard to produce. That is done with tooling that would be very hard to replicate it somewhere else. Code is a different story.\n\n**Susan:** But even on the FPGA front, honestly that seems like a technology that should be a lot more prevalent. And I can only imagine that saying above a performance level has really squashed the industry. Why try to go for that performance level if you know that you don't have a worldwide distribution for it, and that you have to go through all sorts of different regulatory hurdles? Just to even ship it within the US you're going to have to do stuff. So it probably has had a chilling effect. It's not my area, so I can't say for certain, but it just seems like there's some areas there. Another one that's pretty relevant is cryptography. That one's right in the wheelhouse here of being software. There's a long history of cryptography going all around the world. It really has not been stopped. Having limits on like bit sizes of keys and things like that, it's just ridiculous. It's like, \"Oh, this program is okay, so long as you can't increase this one counter above this level.\" You really think that stopped its use elsewhere? It's a challenge. How are they going to identify it? What are we going to do for it? What industries, what areas is going to affect?\n\n## What about importing?\n\n**Scott:** Well, we've talked about the export side of it. What about the import side? When we regulate to the point where we can't actually beat others and they have the good stuff, we'll be able to import the good stuff?\n\n**Susan:** Oh well, here's another question. Just like there's data havens that are starting to pop up for [GDPR](https://blog.deepgram.com/ai-show-how-will-data-influence-the-future-of-machine-learning) and stuff like that, will we have machine learning havens? Will Amazon put the the core guts of Alexa in China? Because they can't really rip you apart there because they were able to train and do stuff they can't do here or vice versa.\n\n**Scott:** Probably.\n\n**Susan:** Again, that's a huge speculation craziness there.\n\n**Scott:** At the end of the day, regulation, data, privacy, these things matter and you shouldn't abuse them, but people do want a better life. They do want more productivity. They do want good products. The way to get there is with data, training on it, building a smart system. The way that we used to do that is we'd just have humans do it. That's really expensive, and they can only do so much. Now we have the opportunity to have machines do things that humans could do - maybe at a hundred x the productivity, and we're worried about it.\n\n![food](https://res.cloudinary.com/deepgram/image/upload/v1661976809/blog/should-ai-be-regulated-ai-show-2/Screen-Shot-2019-02-01-at-1.57.04-PM.png)\n\n_This chart from ourworldindata.org shows us how over time workers have been able to afford more and more food for the same amount of labor. The greatest cause of this improvement is mechanization and technological improvements. AI has the same effect._\n\n**Scott:** Well, you should be worried about it, but also you should think about the productivity gains that you're squashing by that and saying : more people could be fed, more people can have better health care when you let this ride for a few decades, and just bring up the rising tide lifts all boats here. Everything becomes more productive. Of course it will only be the pretty developed countries that are benefiting from it in the beginning, but that that stuff does normalize across the world. Just like mobile phones.\n\n**Susan:** That's the hope.\n\n**Scott:** It takes time though. It might not be in our lifetime. It might be over the next 50 years that there are AI power houses. But, this stuff relaxes. It isn't like steam ... you can't find a steam engine anywhere. Okay, it's 200 years later, everything's fine. But yeah, there are other better things.\n\n**Susan:** In general we've got what's been happening more and more and faster and faster. I'm not going to go to the [Kurtzweil](https://www.ted.com/talks/ray_kurzweil_announces_singularity_university?language=en) stuff here.\n\n**Scott:** Oh no.\n\n**Susan:** But, society's struggling to catch up with technology. Technology used to move basically at this pace of society. As a new thing came along, we were integrating it into it over a generation.\n\n**Scott:** You didn't have to change yourself very much in your lifetime.\n\n**Susan:** The social norms would adapt to that one new thing, which is a little bit while ago. And now there's a new thing and a new thing, and a new thing and the new thing. And we're still trying to catch up with the fact that you can call someone anywhere in the world for basically nothing. We're still catching up with technology that came out in the '80s and '90s and finding regulations and ways of integrating the social implications of those things into us. So this is just another one that's just so massive and it's changing so fast. Going back to the definition problem, defining what is AI, what is not, what you can and can't do with it, just skip ahead to 10 years from now. The speech model that's been training off of x million hours worth of data and it's perfect and an amazing, but there's no longer tags to that data. It's just 100 megs worth of weights or something like that. Can you transfer that hundred megs worth of weights, but you can't touch the data? It's like separating those two now. You've got the knowledge store that represents an entire huge thing.\n\n**Scott:** But it represents experience.\n\n**Susan:** That represents a huge data cache in a lot of ways. These types of ideas of something that is small that can represent this huge thing. It's like saying, I've got my core engineer at a company and you've stolen them and moved them somewhere else. If you take the model weights, that kind of thing.\n\n**Scott:** But they're replicable. They're cloneable. Instantly. Which I think is actually a really interesting thing. This is going to have to be figured out in the legal landscape. We have a new being in some sense of the word that now has a bunch of experience that is now and you can pump electricity into it and it does stuff. That can be infinitely replicated and sent somewhere else and run on other hardware. But just as you said, it's not taking the data with it anymore. It's just taking its experience. Well, hey, we already have that. It's called humans. When they go get a job somewhere, they work and they do things. Can they reveal a specific data that was in that company that they shouldn't or something? They could, but they're not going to. It's hard to remember it. You're going through lines and lines of Excel data or whatever. You don't remember it. You remember the basic stuff. You remember the platonic truths of what you were working on there. But you know what you do when you get hired somewhere else. You're taking all of those learnings and you're using them somewhere else. It's a very, very similar thing. You're transporting this around, and hey, you can learn from something over here too, and then you can go take a new job and learn from something over here. And that's the trajectory of your life. What is that? This is like a gray area for sure.\n\n**Susan:** If someone steals that from your company, is it worth the storage cost... of 100 megs?\n\n**Scott:** Is it kidnapping?\n\n![kid](https://res.cloudinary.com/deepgram/image/upload/v1661976810/blog/should-ai-be-regulated-ai-show-2/Project_Paperclip_Team_at_Fort_Bliss.jpg)\n\n_The idea of stealing scientists is not fiction of movies like Wild Wild West or League of Extraordinary Gentlemen, in the 1940's the goal of Operation Paper Clip was to smuggle/kidnap/liberate German Rocket scientists. Wernher von Braun was one captured scientist who merely went on to spearhead the Saturn V project which put humans on the moon._\n\n**Susan:** Or is it worth billions of dollars? Like I said, we've been talking about regulations and stuff, but the legal aspects of what these new entities are is clearly a broad, wide open thing. Stealing all the code behind something is one thing, but stealing the model that took, I don't know how many GPU hours and how much data to build, that's a huge deal.\n\n**Scott:** The world is changing. The world is getting a lot smaller. One thing that's true though is communication can happen roughly instantly now. You can talk and see anybody anywhere in the world for the most part. You can't just go anywhere else in the world instantly. That holds things back. That holds things back a little bit right now, where if you want to go tell your friend ... we're in San Francisco ... in New Zealand something or show up to their wedding or whatever, it's not going to be two seconds later and you're there.\n\n**Scott:** And so that actually keeps some of the variants in the world still. Meaning there are still cultures.\n\n**Susan:** Geography still matters.\n\n**Scott:** Geography still matters. I don't come at this from the fact that ... well, the world is all the same. Hey, everybody is evolving to the point all of our cultures will just become one. I don't think that's extremely true, but I think that for the mean, is pretty true. But there are things that still stick around and this is just another step toward that essentially. Everybody becomes more ... not everybody, but all of the different cultures become more productive. They have better health, they have more free time. They have things like that. And everybody's lifted up by that. But they're not going to be identical mostly because of the geographical differences there. It's hard to move.\n\n**Susan:** That's one of the few things helping to slow down this stuff is geography, terrain.\n\n**Scott:** And this is where regulation's getting. Saying, \"Hey, moving one thing from this place to there ... because we are us and you're them and there's a real geographical difference between us.\" So, that's actually going to persist. There won't, probably in the next hundred years, there won't be just one country. Right?\n\n**Susan:** Yeah. Every single Sci Fi will have a ... what is the generic Sci Fi name for the one world government that comes up in all the Sci Fi stories?\n\n**Scott:** What, the empire?\n\n**Susan:** No. That's generally a thousand years. When you're in Sci Fi let's say it turns into an empire in a thousand years onwards. In the near term, most Sci Fi say one world government within generic 50 years.\n\n<iframe src=\"https://www.youtube.com/embed/H5GqwtuP-t4\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\n_In the Pixar Movie Wall-E, the defacto world government is the Buy N Large corporation._\n\n**Scott:** Got it.\n\n**Susan:** But really where we're heading is like most things, we're going to need some test cases in life to help us clarify these things. One example that's been recent is of course, Google search in China. This is clearly touching on the areas that we're talking about here. That's a test case for how we are reacting. Will we start building more and more legislation that says, \"Hey Google, you can't bring your search technology into China for reason x, y, and z. Or if you do, you have to do it under these conditions\"? That's actually a great example where the problem was that they were bending to another country's, like what they wanted to do, as opposed to opening up the full rein of what they could do. They were limiting themselves. But those types of tests cases are going to happen. They're probably already happening. We just aren't aware of many of them and we'll see them in the news pop up day after day and reach a certain momentum, and we'll find certain small key things that can help us improve them the next time they happen. The real challenge though is will we go too big too early? I think we both agree doing big rash things right now would probably be more harmful than helpful.\n\n**Scott:** You probably want to go the soft direction versus the hard direction like the United Nations or something like that, rather than hard laws, where you put people in jail. That type kind of thing.\n\n**Susan:** Especially when it's going to be fairly arbitrary, whatever definition you come up with. Suddenly just the fact that you have speech recognition in your app makes it that your app is now not allowed to be exported and it's like, \"What?\" That would be a huge challenge. And especially since it's being integrated in so many areas.\n\n## What are the benefits of regulation?\n\n**Susan:** Well, like said before, the benefits are, it can stabilize the market. It can give stability to companies.\n\n**Scott:** Like, \"Hey, this new thing is here. What should we do as a company? What should we plan for? What should we-\"\n\n**Susan:** You know your boundaries. You can work within boundaries. That is a benefit because it reduces risk. You now know that if you stay within here, you're not going to be arbitrarily smacked. Benefits again, we talked about the consumer side of the house. Good consumer protections, well-crafted consumer protections can be hugely beneficial to consumers and also to companies. It keeps everybody on a same playing field when they have to keep consumers first to some degree.\n\n**Scott:** Safe play area. It's a lot like children. \"Stay on your block. Hey, you can do a lot of things in this area, but don't set things on fire. Don't stick forks into electrical outlets. Don't cross the road.\"\n\n**Susan:** But like we talked about, even going down those routes, the fact that you start putting those things into place, you're pigeonholing down and you really run serious risks.\n\n**Naming something is a very powerful thing. As soon as you name the thing and say that something else is that name, nothing is a perfect fit.** And especially in this industry that's changing so quickly so much every single day-\n\n**Scott:** It's also hard to deregulate. That's not usually a thing that happens. Regulations come in and then 500 times more work goes into getting rid of them.\n\n**Susan:** It's far easier to say, \"Well, I'll just leave it in place and we'll add more to things.\" Man, it's a sticky wicket, right?\n\n## Do you think should there be punishments? What would the punishment be?\n\n**Susan:** Oh. Punishments?\n\n**Scott:** Is it just monetary?\n\n**Susan:** This is the whole self driving car dilemma. Should you hold car company x liable for the one death a year they cause?\n\n**Scott:** That's true. That's true.\n\n**Susan:** Do you treat a company like an individual? Since corporations are now individuals, does that mean you can execute a company if they maliciously killed someone on the road? Can you put them in jail? And now that's another thing. Again, going back to the regulation route, I can almost guarantee you that if we had a very clear, consistent standard applied to the laws or the damages and stuff that will happen when the collision happens, even if they're a little bit \"mean\" to the car companies, that would allow a lot more people to enter the market. A lot more companies to enter the market, because they know they now have a known risk, as opposed to an unknown risk, which is a lot worse. What should happen if you design something that's used to kill people? I don't know, gun manufacturers probably have a lot to think about on this too. That's an example of an industry that doesn't have a lot of the repercussions, but there are a lot of other industries that do. If you make a toy that chokes one child, you've got huge, huge damages there going on on.\n\n**Scott:** Well, it's like a ski resort versus the playground. Everything in a playground has to be so you can't hurt yourself, but ski resort's fine. Yeah, just send kids down. They can run into trees. Whatever. It's just history and how people work and what their norms are. It's just weird.\n\n**Susan:** And it comes down to a test case hit. Street or risky resorts versus manufacturer x versus whatever, and\n\nwhen that test case hit, we were forced to make a decision and that decision is stuck. Even if technology and time has changed, that decision still stands.\n\nKinder eggs is a good example. You can't import Kinder eggs because a kid may choke on them.\n\n**Scott:** Kinder eggs are a chocolate with a toy inside, in a little egg shape and in the US, you can't get him.\n\n![kinder](https://res.cloudinary.com/deepgram/image/upload/v1661976811/blog/should-ai-be-regulated-ai-show-2/Screen-Shot-2019-02-01-at-3.15.33-PM.png)\n\n**Susan:** Yeah. I know. My son loved them when we were living overseas.\n\n**Scott:** Because if a child eats it and chokes on it.\n\n**Susan:** For getting something this big, it's ridiculous.\n\n**Scott:** But yeah, that's the idea.\n\n## Where do you think we'll be 20 years from now?\n\n**Scott:** I think it's a decision point here. If you want the US to truly just become China's vacation spot, it's the new Europe, there wasn't really anything going on here. Everybody just chills out and coasts for the rest of their lives, then you regulate a whole bunch. And you keep your lifestyle the way it is, and that's that. If you still want to be the dominant world power in the world, you can't go hard on regulation in AI and/or software or whatever. You can't start protecting. You can't go into protect mode.\n\n**Susan:** If you crack down on innovation, you're going to to kill this industry. Obviously China's been the example, but my time is Scotland, there was a lot of push to get innovators to stay. We're seeing other areas, other regions of the world that are doing a lot more than the US to encourage things. When I first came to the Bay Area was a good example. People are coming into Bay Area because that's where all the ideas were at, and that's where everybody wanted to be. There was this huge upwelling of, \"This is where it's going to be at.\" Now it's still there, but there's a lot more people that are saying, \"I'm here because of the momentum.\" Whereas, go to say Edinburgh and there's lot of programs. They're building that momentum. People are going there because there's a lot of press to make this stuff happen, and they're building what we are now coasting on.\n\n\n";
						}
						async function compiledContent$1a() {
							return load$1a().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1a() {
							return (await import('./chunks/index.c2950650.mjs'));
						}
						function Content$1a(...args) {
							return load$1a().then((m) => m.default(...args));
						}
						Content$1a.isAstroComponentFactory = true;
						function getHeadings$1a() {
							return load$1a().then((m) => m.metadata.headings);
						}
						function getHeaders$1a() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1a().then((m) => m.metadata.headings);
						}

const __vite_glob_0_202 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1a,
  file: file$1a,
  url: url$1a,
  rawContent: rawContent$1a,
  compiledContent: compiledContent$1a,
  default: load$1a,
  Content: Content$1a,
  getHeadings: getHeadings$1a,
  getHeaders: getHeaders$1a
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$19 = {"title":"Building a Voice-Powered Song Search","description":"🎅  Let Deepgram detect the Christmas song","date":"2021-12-16T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1639662018/blog/2021/12/song-search-js/Building-Voice-Powered-Song-Search%402x.jpg","authors":["kevin-lewis"],"category":"tutorial","tags":["javascript","microphone"],"seo":{"title":"Building a Voice-Powered Song Search","description":"🎅  Let Deepgram detect the Christmas song"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453835/blog/song-search-js/ograph.png"},"shorturls":{"share":"https://dpgr.am/15b5006","twitter":"https://dpgr.am/73a2300","linkedin":"https://dpgr.am/3e7d48b","reddit":"https://dpgr.am/e2319b5","facebook":"https://dpgr.am/cd65bc2"}};
						const file$19 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/song-search-js/index.md";
						const url$19 = undefined;
						function rawContent$19() {
							return "Love it or hate it, Christmas is a period for music, and that comes the frustrating scenario of knowing lyrics but not quite knowing the song. Of course, you could just search the lyrics, but where's the fun in that? In this project, we will warm up our vocal cords and use Deepgram and the Genius Song Lyrics API to build a website that should correctly guess spoken or sung lyrics.\n\nWhile doing this, we'll learn how to stream microphone data to Deepgram via a server, so you don't need to worry about exposing your API Key.\n\nThis is what we'll be building:\n\n![A diagram showing 9 steps. 1 - emit mic data from browser to server using socket.io. 2 - send mic data from server to Deepgram via Deepgram SDK. 3 - Deepgram returns utterances to server. 4 - the server stores utterances. 5 - a user presses a button and emits search event to server. 6 - server searches song on Genius. 7 - Genius sends response to server. 8 - Server emits response to browser. 9 - browser shows result. Steps 1-3 have a green background and steps 4-9 have a blue background.](https://res.cloudinary.com/deepgram/image/upload/v1637178711/blog/2021/12/song-search-js/diagram.png)\n\nThe green area is one set of steps that gets us to the point of transcripts. The blue area covers searching for and displaying songs. Don't worry if that looks like a lot - we'll take it step by step. If you want to look at the final project code, you can find it at https://github.com/deepgram-devs/song-search.\n\n## Before We Start\n\nYou will need:\n\n* Node.js installed on your machine - [download it here](https://nodejs.org/en/).\n* A Deepgram API Key - [get one here](https://console.deepgram.com/signup?jump=keys).\n* A Genius API Access Token - [get one here](https://genius.com/api-clients).\n\nCreate a new directory and navigate to it with your terminal. Run `npm init -y` to create a `package.json` file and then install the following packages:\n\n```\nnpm install dotenv @deepgram/sdk express socket.io axios\n```\n\nCreate a `.env` file and add the following:\n\n```\nDG_KEY=replace_with_deepgram_api_key\nGENIUS_TOKEN=replace_with_genius_access_token\n```\n\nCreate an `index.js` file, a folder called `public`, and inside of the public folder create an `index.html` file. In `index.html` create a boilerplate HTML file:\n\n```html\n<!DOCTYPE html>\n<html>\n  <head>\n    <meta charset=\"UTF-8\" />\n  </head>\n  <body>\n    <!-- Further code goes here -->\n  </body>\n</html>\n```\n\n## Establish a Socket Connection\n\nThe socket.io library can establish a two-way connection between our server (`index.js`) and client (`index.html`). Once connected, we can push data between the two in real-time. We will use this to send data from the user's microphone to our server to be processed by Deepgram and show results from the server logic.\n\nIn the `index.html` `<body>` tag:\n\n```html\n<script src=\"/socket.io/socket.io.js\"></script>\n<script>\n  const socket = io()\n  // Further code goes here\n</script>\n```\n\nIn `index.js` create a combined express and socket.io server and listen for connections:\n\n```js\n// Require\nconst express = require('express')\nconst app = express()\nconst http = require('http').createServer(app)\nconst io = require('socket.io')(http)\n\n// Configure\napp.use(express.static('public'))\n\n// Logic\nio.on('connection', (socket) => {\n  console.log(`Connected at ${new Date().toISOString()}`)\n})\n\n// Run\nhttp.listen(3000, console.log(`Started at ${new Date().toISOString()}`))\n```\n\nFor this tutorial, I would leave the comments in as I refer to sections later by their names. Start the server in your terminal by navigating to the directory and running `node index.js`. Open your browser to `http://localhost:3000`, and you should see 'Connected at `date`' in your terminal. Once this connection is established, we can send and listen for events on both the server and the client.\n\n## Access and Send Audio\n\nIn [a blog post last month](/blog/2021/11/live-transcription-mic-browser/) we covered how to access and retreive data from user's mic in a web browser. Each of the steps are covered there, so we'll be lifting the examples from it without a deep explanation. In `index.html`:\n\n```js\nnavigator.mediaDevices.getUserMedia({ audio: true }).then((stream) => {\n  const mediaRecorder = new MediaRecorder(stream)\n\n  mediaRecorder.addEventListener('dataavailable', (event) => {\n    if (event.data.size > 0) {\n      socket.emit('microphone-stream', event.data)\n    }\n  })\n  mediaRecorder.start(1000)\n})\n```\n\nThis will immediately ask for access to the microphone and begin accessing data once permitted. When emitting events with socket.io, we can specify a specific event name which we can then listen for on the server. Here, we have called it `microphone-stream` and send it with the raw mic data.\n\n## Listening for Events\n\nIn `index.js` inside of the connection and below the `console.log()` statement:\n\n```js\nsocket.on('microphone-stream', (data) => {\n  console.log('microphone-stream event')\n})\n```\n\nRestart your server and then refresh your web page. Once you grant access to your microphone, you should see a steady stream of logs indicating that data is sent from your browser to the server. You may stop your server while we continue with the next step.\n\n![A terminal showing the server starting, a client connecting, and then 4 microphone stream event logs.](https://res.cloudinary.com/deepgram/image/upload/v1637178711/blog/2021/12/song-search-js/microphone-stream-event.png)\n\n## Setting Up Deepgram\n\nAt the top of the Require section in `index.js` add `dotenv` which will allow access to the `.env` file values.\n\n```js\nrequire('dotenv').config()\n```\n\nAt the bottom of the Require section require the Deepgram Node.js SDK which we installed earlier:\n\n```js\nconst { Deepgram } = require('@deepgram/sdk')\n```\n\nFinally, in configure, initialize the SDK and create a new live transcription service:\n\n```js\nconst deepgram = new Deepgram(process.env.DG_KEY)\nconst deepgramLive = deepgram.transcription.live({ utterances: true })\n```\n\n## Getting Live Deepgram Transcripts\n\nInside of the `microphone-stream` event handler comment out the `console.log()`. In it's place, take the provided data and send it directly to Deepgram:\n\n```js\nsocket.on('microphone-stream', (data) => {\n  // console.log('microphone-stream event')\n  deepgramLive.send(data)\n})\n\n// Further code goes here\n```\n\n`deepgramLive` provides an event when Deepgram has a transcript ready, and like the [browser live transcription blog post](/blog/2021/11/live-transcription-mic-browser/) we will wait for the final transcript for each of our utterances (phrases).\n\n```js\nlet transcript = ''\ndeepgramLive.addListener('transcriptReceived', (data) => {\n  const result = JSON.parse(data)\n  const utterance = result.channel.alternatives[0].transcript\n  if (result.is_final && utterance) {\n    transcript += ' ' + utterance\n    console.log(transcript)\n  }\n})\n```\n\nRestart your server, refresh your browser, and speak into your microphone. You should see a transcript appear in your terminal.\n\n![A terminal showing give phrases with words spoken, with each adding words on to the last and getting longer.](https://res.cloudinary.com/deepgram/image/upload/v1637178714/blog/2021/12/song-search-js/transcript-terminal-log.png)\n\n## Triggering Song Search\n\nBecause a set of lyrics can take up multiple utterances, we need to have a way to indicate that we are finished and the search should take place. We will attach an event listener to a button that, when pressed, will emit an event.\n\nIn `index.html` add a `<button>` at the top of your `<body>` tag:\n\n```\n<button>Search Song</button>\n```\n\nJust below `mediaRecorder.start(1000)` add the following logic:\n\n```js\nconst button = document.querySelector('button')\nbutton.addEventListener('click', () => {\n  button.remove()\n  mediaRecorder.stop()\n  socket.emit('search')\n})\n```\n\nWhen the button is pressed, it will be removed from the DOM, so we only can click it once; we stop the mediaRecorder (and, in doing so, stop emitting the `microphone-stream` events), and emit a new event called `search`.\n\nIn `index.js` add a new socket event listener just after the block for `microphone-stream` is closed:\n\n```js\nsocket.on('search', async () => {\n  console.log('search event', transcript)\n  // Further code here\n})\n```\n\nRestart your server and refresh the browser. Speak a few phrases and click the button. You should see the search event take place with the final transcript logged.\n\n## Searching for Songs\n\nWe will use the [Genius API](https://docs.genius.com) to search for songs based on lyrics. To make this API call, we'll utilize Node package `axios`. In the Require section of our `index.js` file, add the package:\n\n```js\nconst axios = require('axios')\n```\n\nAnd make the API call when the `search` event is received:\n\n```js\nconst { data } = await axios({\n  method: 'GET',\n  url: `https://api.genius.com/search?q=${transcript}`,\n  headers: {\n    Authorization: `Bearer ${process.env.GENIUS_TOKEN}`,\n  },\n})\nconst topThree = data.response.hits.slice(0, 3)\nconsole.log(topThree)\n\n// Further code here\n```\n\nRestart your server and refresh your browser.\n\n![A terminal showing an array with several items. Each item contains metadata for one song.](https://res.cloudinary.com/deepgram/image/upload/v1637178713/blog/2021/12/song-search-js/songs-returned.png)\n\n**Yay!**\n\n## Displaying Results\n\nThe final step is to show the output to the user by emitting an event from the server back to the client. Doing this is nearly identical to the other direction. In `index.js`:\n\n```js\nsocket.emit('result', topThree)\n```\n\nIn `index.html` add an empty `<ul>` under the `<button>`:\n\n```\n<ul></ul>\n```\n\nAt the bottom of the `<script>` tag, below all other code, listen for the `results` event and add items to the new list:\n\n```js\nsocket.on('results', (data) => {\n  const ul = document.querySelector('ul')\n  for (let song of data) {\n    const li = `\n    <li>\n      <img src=\"${song.result.song_art_image_url}\">\n      <p>${song.result.full_title}</p>\n    </li>\n  `\n    ul.innerHTML += li\n  }\n})\n```\n\nBefore we try this add this minimal styling inside of your `<head>` tag:\n\n```html\n<style>\n  ul {\n    display: grid;\n    grid-template-columns: 1fr 1fr 1fr;\n    grid-gap: 4em;\n    list-style: none;\n  }\n  img {\n    width: 100%;\n  }\n</style>\n```\n\nRestart your server, refresh your browser, and try it out! You can display any of the information provided by Genius.\n\n<YouTube id=\"IFuJiNNoYYw\"></YouTube>\n\nNo one ever said I was a good singer.\n\n## Wrapping Up\n\nThere are quite a lot of improvements you could make here:\n\n* Show utterances to users in the browser\n* Do searches as soon as utterances are available, and update them as more words are said\n* Allow multiple songs without needing to 'reset' by refreshing\n* Give it a festive theme\n\nThis post has also introduced you to the code required to stream your microphone from the browser to Deepgram via a server, thus protecting your API Key from being exposed.\n\nWe'll have some more posts coming out before Christmas, but from me, this is it until January, so please have a wonderful festive period and a wonderful new year. The complete project is available at https://github.com/deepgram-devs/song-search, and if you have any questions, please feel free to reach out on Twitter - we're [@DeepgramDevs](https://twitter.com/DeepgramDevs).";
						}
						async function compiledContent$19() {
							return load$19().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$19() {
							return (await import('./chunks/index.dfe89bf2.mjs'));
						}
						function Content$19(...args) {
							return load$19().then((m) => m.default(...args));
						}
						Content$19.isAstroComponentFactory = true;
						function getHeadings$19() {
							return load$19().then((m) => m.metadata.headings);
						}
						function getHeaders$19() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$19().then((m) => m.metadata.headings);
						}

const __vite_glob_0_203 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$19,
  file: file$19,
  url: url$19,
  rawContent: rawContent$19,
  compiledContent: compiledContent$19,
  default: load$19,
  Content: Content$19,
  getHeadings: getHeadings$19,
  getHeaders: getHeaders$19
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$18 = {"title":"Sonic Branding in the Enterprise - Audrey Arbeeny, CEO, Audiobrain - Project Voice X","description":"Sonic Branding in the Enterprise presented by Audrey Arbeeny, CEO of Audiobrain, presented on day one of Project Voice X. ","date":"2021-12-09T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981391/blog/sonic-branding-in-the-enterprise-audrey-arbeeny-ceo-audiobrain-project-voice-x/proj-voice-x-session-audrey-arbeeny-blog-thumb-554.png","authors":["claudia-ring"],"category":"speech-trends","tags":["project-voice-x","sonic-branding"],"seo":{"title":"Sonic Branding in the Enterprise - Audrey Arbeeny, CEO, Audiobrain - Project Voice X","description":"Sonic Branding in the Enterprise presented by Audrey Arbeeny, CEO of Audiobrain, presented on day one of Project Voice X. "},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981391/blog/sonic-branding-in-the-enterprise-audrey-arbeeny-ceo-audiobrain-project-voice-x/proj-voice-x-session-audrey-arbeeny-blog-thumb-554.png"},"shorturls":{"share":"https://dpgr.am/d7dc853","twitter":"https://dpgr.am/ac4634e","linkedin":"https://dpgr.am/2510148","reddit":"https://dpgr.am/79c5c9e","facebook":"https://dpgr.am/eb1f4d6"}};
						const file$18 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/sonic-branding-in-the-enterprise-audrey-arbeeny-ceo-audiobrain-project-voice-x/index.md";
						const url$18 = undefined;
						function rawContent$18() {
							return "*This is the transcript for the session “What’s Next for AI in the Contact Center” presented by Audrey Arbeeny, CEO of Audiobrain, presented on day one of Project Voice X.* \n\n*The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was **94% accurate.**  Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram’s API.  If you want to see if Deepgram is right for your use case, [contact us](https://deepgram.com/contact-us/).*\n\n\\[Audrey Arbeeny:] Can you all hear me? I think without the mic, being a very loud New Yorker, I could have probably done this without this. But thanks to the two presenters before me. I’m so inspired by what they had to say. And I’ll get into what I have to say here, but I’m gonna… I’m just gonna take a second. I wanna just make sure I check my time, and I would sit… tell you a very, very quick story just to bring a little levity to this. I was on Twenty Thousand Hertz. And I don’t know if you know that podcast, but it’s a real… it’s my favorite podcast. And it was on Domestic Symphony, and it was about sounds and the things we hear in in everyday life, and they do wonderful storytelling. A teacher from Montessori heard it, and I got many emails and many LinkedIn messages about that particular episode. So the Montessori teacher said, I find this fascinating, and I think my students would love to hear what you do. And I said, ok. Definitely. Absolutely. I’m all for education. And she said they’re second and third graders. And I said, ok. So we’ll we’ll do that. We’ll do that. My youngest students ever. And when I did the presentation the other day, the the first question they said was what’s… ’cause they listen to the the podcast. They said, what’s psychoacoustics? And I sat there, and I said, this is the future. These are second and third graders, and it’s very inspiring where all this is going. And I’ve been doing this for twenty five years, and I’ve had Audiobrain, which is a sonic branding firm, for eighteen. So I’ve seen the progression in everything that’s going on, and I saw the progression tremendously three years ago. And, Henry, every time I see you, I just like… your work amazes me. But what’s been going on in the past year is just beyond my comprehension as somebody who supplies music, sound, voice, and vibration. So I wanna just bring them into the equation, because we bring in a very big emotional context to the equation of using these technology. And I do believe that we can bring more empathy, and we we have the technology. Sorry. We have the technology, but I think that we need a little bit more of the emotion, and it it is coming, and it is coming pretty quickly.\n\nAnd so I wanna talk to you about the enterprise and what’s going on right now. It’s really changed a lot in the past few years, and they’re embracing intentional audio at a level that I have not seen before in twenty five years in the past year. So what sonic branding is, it’s an art, and it’s a science of creating a strategic development and deployment of consistent authentic sound. So if you’re working on AI or you’re working on a voice for something, a call center, there may be music underneath it, or there may be sounds within the product that you’re doing a voice for. They need to be consistent. They need to be within the framework of the enterprise or what it is that you’re working on, and that’s what we do. We do a lot of research, and we do a lot of testing. We do a lot of discovery, and we work on technology side. We do surgical robotics, and we could do something, you know, like YouTube Kids. So we work in every single industry. The passion point is science and health for me, personally. But we need to tie these things together because sonic branding is music, sound, voice, and vibration.\n\nSo when you’re working on your projects, you’re working on many of you on voice. We were complimentary because it has to be an ecosystem, and it has to make sense, and it has to work together in order for it to be effective. And the brands and the enterprise that are highly successful do know this. So once upon a time, little me starting in the industry. It was mostly a logo or jingle. That’s what sonic branding was about. I did work with IBM, and IBM did sounds for the ThinkPad and educational videos, executive walk-ons, what the brand was all about, and that was twenty years ago at least. So I’d love to speak to Lenovo people later on. So this is about where we were, but really smart brands, like Disney or HBO, they were doing sonic branding. And they… that’s why they’re still incredibly effective. Then we’ve rolled out to this, and we started doing on-hold music and network identities and things of that nature. Well, this is where… and I could have made four of these.\n\n> This is the landscape now. These are the opportunities for your voice. This is the opportunity for our sound, and this is the opportunity for a unified communication that strengthens these brands and strengthens the companies and brings us all opportunities to bring high technology with human characteristics of emotion and to leverage it across a lot of different touch points.\n\nSo when we all do things separately, and I do one thing, and you do another thing, and then we put them together, sometimes, they don’t really work together. They… they’re taking a different point of view because they’re not being done through the same brand lens. The top enterprise companies know this. They set the framework. We set the framework, and we’re the compass for what the sound of of the brand is gonna be like, which also includes what the right voice persona is, what the right localization is. And we then set that framework, and sometimes, we’re not the ones to do that. A lot of that could be done by you. And we would call upon our various partners or people that we need here that I would call in a second. We have clients that want synthetic voices. We want clients that want proprietary voices, and we have a lot of things going on, tremendous amounts. So that’s what I wanna get into. But when we all do it all scattered, it doesn’t make for that holistic experience that really resonates with the customer. I’m gonna go a little bit faster here. So these are the trends that we’re seeing from our end as sonic branding people.\n\nOk. So there’s large scale efforts going across entire organizations. Before, there were some companies that did what I just showed you. But now so many companies are calling to go across multiple touch points very quickly. I’ve never seen anything like this. We have people that are coming in with proposals that have, you know, maybe, like, fifty different items that they want done for the brand. Before, they would come in. And some would do that, but most would start and iterate and… which we do, but would build it. But another trend I see is that they were acquiring an investor… investing or partnering with voice, sound, and technology companies. So they will buy a company, and you saw the the Nuance before, but there’s many others out there that are being acquired very, very quickly because these enterprise companies know that this is where the future is. The future is about sound. The future is about voice. We’re seeing more c-suite and top players coming to the table with their teams, so the companies themselves are working as unified units where we might get brought in in the past by the product developer or the marketing director or the design director. Now, internally, they’re working as cohesive teams because one is interacting with the other, and they’re working on initiatives that, you know, involve a lot of different touch points. So we see a lot of emerging patterns regarding social audio, smart speakers, sponsorships. They want visibility in the industry. We see people that used to be guests on podcasts now having their own podcast. So their company’s having their own podcast who are having many podcasts. And we see several enterprises working together. Because different capabilities when combined, some of this is so large that we need the different capabilities, and we need to be sitting at the same table, and we need to work as a unit, whereas before, that never really happened years ago ever. Like, it was almost in a way a little bit competitive, but now we we bring it altogether. And every industry is looking at sonic branding.\n\nHere’s some different things that just have gone on pretty recently. So you have the Sharjah Airport. They launched their new sonic branding. It was a very, very large initiative. TikTok reveals six certified sonic sound partners, ok, that they’ve selected to lead their sonic initiatives. And so I spoke to a lot of different experts in the field, and and I’m like, what makes, you know, what makes a great enterprise? What makes success? And this is where we landed, and it’s about bringing consistency. So when you have consistency and your call center doesn’t sound different than your events, which don’t sound different than your your speakers and your products that have sounds in them or voices in them, that’s really important. Differentiation. Differentiation is really important because you wanna be at the top of the pyramid. You want to be known for something, something that differentiates you from anyone else. You want… they they want an emotional connection, and that’s really, really important. And customers want ’em, an emotional connection, because we live in a very digital world. You wanna be identified quickly. And brand alignment, and that’s why so many people are coming to the table, and comprehensive, well-thought-out, well-planned, good strategy to act upon, and trust. And trust is is gigantic because without trust, when a brand is inconsistent or what we’re creating is inconsistent, the consumer understands that, and they can move on. So I really… I’ve I’ve been the music supervisor for ten Olympic broadcast for NBC. I wanted to get to number ten so badly, and it took a very, very, very long time. There’s trust there. There’s trust in in their brand, but there’s trust between us and them for that relationship to go on that long. And that’s what you wanna do. You wanna make sure that you’re the trusted partner, and you wanna make sure that the enterprise is trusted by the customer. So why should you care about this?\n\nWell, we are in voice-first, sound-first experiential world, and it’s been said a few times today. If you’re… if you don’t get on board, you’re going to be left behind. This is for real. This is moving… as somebody had said before about the industrial age, and this is is moving at rapid pace. And you need to keep up with it, not just what you’re doing, but what other people are doing, because this is where the future is. And many people are coming in with, like I said, with these full sonic strategies. For my company, this has risen three hundred percent this year where they’re coming in with large initiatives, and, I mean, from around the world. And thirty percent of these companies were companies that we gave proposals to three years ago and, you know, wanted to work with, and they didn’t do it. They’re all doing it now. Everybody… or or a lot of companies are beginning to realize they really need to get on board with this, and that’s why we collaborate. So why now? First of all, it’s technology. We all know that. Now who would wanna hear a sound like that? That’s how sounds used to sound. Or my neighbor with GarageBand can do better than that.\n\nCustomers have a choice, and they could go anywhere they want. So they they can hear you on so many different devices and through so many different vehicles that you wanna make sure that you’re consistent, and, again, to bridge that emotional connection. So when we choose voices, when we work on these brands, we wanna make sure that the consumer… it’s all about emotion, and it’s all about getting them to feel confident in you and and confident in the work that you’re doing. So here’s, well, something that we did for Whirlpool, and Whirlpool is the caregiver. And the sound you just heard was their old sound, and this is the new sound. And it’s about the human touch, and it’s about, you know, giving care and the joy of taking care of one’s family. I’ll lower this a little bit. Kinda loud. The next thing that’s important is to make sure that people know what you’re doing so that they come to you. And what happened with the Whirlpool sounds, because they were so pleasing and so different, they got tremendous press out of it. And from there, it led to several of those initiatives that we’re doing multi-touch points for. And we will be doing synthetic voice. We will be doing different things within a lot of the clients that we work with. For Holland America, we did a global experience design. When we started with them, we would go to the… by… you know, did an assessment, and this beautiful ship was left to the complaining customer, the bartender, or the hotel manager. That’s who made the determination on what music played. We not only took the brand and took a a hard look at it, but we assessed what the… with them what the brand stood for. We did many different touch points. So we did things like… and we’ll see if these work. Brad… Bradley and I, we were having some fun with these earlier today. And if they don’t, I’m just moving on. So now we’re doing sound and stories for all the destinations. This is the one that’s been giving us the most problems.\n\n\\[SPEAKER 2:] Welcome on board. For your safety, please remain in your seats while the tender is moving. Do not stand up or leave your seats.\n\n\\[Audrey Arbeeny:] And all kinds of engagements. And as voice becomes more conversational, we’ll be able to do these.\n\n\\[SPEAKER 3:] — and recommending wines.\n\n\\[Audrey Arbeeny:] — AI, and we won’t have to go back to those talents and rerecord and rerecord and rerecord.\n\n\\[SPEAKER 3:] — law specifying the geographic location that a particular —\n\n\\[Audrey Arbeeny:] We did co-branding. Oprah is… has co-branding ships for particular cruises. So we did that, and a lot of that work, the peripheral work, could have been done with the AI. We do also… did the interactive voice response system, and we have some other features coming out because there’s noise. There’s different things that we need to rectify that the customer wants now. So they might’ve wanted pop, rock, jazz. Now they want meditation. Now they want noise cancellation. There’s a lot of things that we can do in that arena, and it’s been a long relationship. And we saved cost by seventy percent because we set a framework, and then we leveraged it out across an entire fleet. And that’s what a lot of brands are doing. They’re going global. So this is just six tips I have to say is… we’re gonna be working together quite a bit. And the more cohesive we are, and the more we communicate with each other, and the more we bring the right partners in… because our clients are coming into these meetings with twenty, thirty people, and and they want this to be consistent. And they want a custom voice, and they want this. And they want the call center, and they want everything. So as we work as a collective team, I think that we can all bring better outcome to our products, better outcome to the enterprise, better outcome for each other.\n\nSo you’ve got brand, marketing, product owners, designers, engineers, gathering info, field studies, understanding what’s going on in the landscape, what are your future plans, what sounds add value. Not everything on that map will apply to your client. But when we do our assessment, we know which ones do and which ones that they can focus on. Iterate. Being fearless doesn’t mean you don’t have fear. Being fearless means that, ok, I have a little fear on this one, but I’m gonna do it anyway. Try things. Try the next thing. Try the next thing. We tried so many different things that we never thought we could do and worked with people and got some pretty successful results from it. And seek those with specific expertise to guide you. This is a big playing field we’re in right now, and we need each other when the opportunity arises to call the right person. And plan for new ventures and new use cases. So in closing, sound is a key element. Music and sound is key. It could warm up artificial intelligence, and it is gonna live side by side. And we convey a lot of personality through music, sound, and voice, and, also, it has a really great return on investment. And that’s pretty much what I have to say. I hope this helped you understand what sonic branding is and where the opportunities are. And I’m gonna do a plug right now. This is my very good friend, Bianca Phillips, who I’m sure a lot of you know of. She’s from Australia. She wrote a book called Making the Digital Health Revolution. She’s an attorney. She was supposed to be speaking at this conference, but she can’t get out of Australia. So I just wanted to point out her book to you. It’s fantastic. She’s brilliant, and I’m really proud to call her my friend. So we’ll all work together, and we’ll bring this where it needs to be, which is the big future. Thank you.";
						}
						async function compiledContent$18() {
							return load$18().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$18() {
							return (await import('./chunks/index.25966ffd.mjs'));
						}
						function Content$18(...args) {
							return load$18().then((m) => m.default(...args));
						}
						Content$18.isAstroComponentFactory = true;
						function getHeadings$18() {
							return load$18().then((m) => m.metadata.headings);
						}
						function getHeaders$18() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$18().then((m) => m.metadata.headings);
						}

const __vite_glob_0_204 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$18,
  file: file$18,
  url: url$18,
  rawContent: rawContent$18,
  compiledContent: compiledContent$18,
  default: load$18,
  Content: Content$18,
  getHeadings: getHeadings$18,
  getHeaders: getHeaders$18
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$17 = {"title":"Sparking the Future of Conversation Design - Braden Ream, CEO, Voiceflow - Project Voice X","description":"Speaking the Future of Conversation design voiceflow. This is the transcript for the session presented by Braden Ream, CEO of Voiceflow","date":"2021-12-09T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981392/blog/sparking-the-future-of-conversation-design-braden-ream-ceo-voiceflow-project-voice-x/proj-voice-x-session-braden-ream-blog-thumb-554x22.png","authors":["claudia-ring"],"category":"speech-trends","tags":["conversational-ai","project-voice-x"],"seo":{"title":"Sparking the Future of Conversation Design - Braden Ream, CEO, Voiceflow - Project Voice X","description":"Speaking the Future of Conversation design voiceflow. This is the transcript for the session presented by Braden Ream, CEO of Voiceflow"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981392/blog/sparking-the-future-of-conversation-design-braden-ream-ceo-voiceflow-project-voice-x/proj-voice-x-session-braden-ream-blog-thumb-554x22.png"},"shorturls":{"share":"https://dpgr.am/2113016","twitter":"https://dpgr.am/f6fcfa7","linkedin":"https://dpgr.am/7a45a55","reddit":"https://dpgr.am/0429d31","facebook":"https://dpgr.am/a5d913b"}};
						const file$17 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/sparking-the-future-of-conversation-design-braden-ream-ceo-voiceflow-project-voice-x/index.md";
						const url$17 = undefined;
						function rawContent$17() {
							return "*This is the transcript for the session “Sparking the Future of Conversation Design” presented by Braden Ream, CEO of Voiceflow, presented on day one of Project Voice X.* \n\n*The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was **94% accurate.**  Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram’s API.  If you want to see if Deepgram is right for your use case, [contact us](https://deepgram.com/contact-us/).*\n\n\\[Braden Ream:] Alrighty. Hi, everyone. Thanks for having me here. So I’ve given a bunch of talks on conversation design in the past, and the one I most often give is how to structure a conversation design team, so how to build one internally, build out the best practices. I thought what may be kinda fun is… you know, we learn a lot about conversation design through the practices of actually building Voiceflow. And so I thought it could be interesting to talk about the challenges of building a conversation design tool, which will, hopefully, in effect, teach you a little bit about conversation design itself. So there’s gonna be a lot of product shots, talk about a lot of a a specific customer problems, you know, without naming any names, and kinda run you through the process of actually building a tool. I’ll try to keep it fairly short to sync to the point. And, you know, if you wanna chat a little bit more about Voiceflow, we’ve got a social later today as well as a booth at the back. Love chatting product, but I’ll try to keep it fairly succinct ’cause I know we’re going into into lunch here.\n\nSo this is thoughts on building a conversation design tool. First time running this, so bear with me. Little bit a… yep. We already went through the goal here. Got ahead of myself. Little bit about Voiceflow, so we’re used by over eighty thousand teams now. Lots of great customers out there, actually, lots of great customers here as well. I’ve raised about twenty five million dollars from some great investors, including Amazon and Google, who are are present as well as some other awesome design tool folks, including Figma and InVision and and others. Cool. So why does conversation design matter? A lot of folks might actually not be familiar with it. You know, it’s really come to prominent sites over the past two years.\n\n> Conversation design is essentially the UX branch of conversational AI. So it’s applying best practices on the… from the design world to conversational interfaces. Now why this is becoming increasingly relevant is as the world automates. Really, the industry that a lot of us are in is the talking to people industry. Right? It’s a very large space that’s quickly becoming more and more automated. And as we do this, there’s a lot of focus on the underlying technologies, the ASR, the TTS, the NLUs. And now we’re starting to see a lot more focus on the actual human side of it, the design side. And so that is where conversation design comes in. It is about the content structure as well as the content design itself.\n\nAnd so the challenge for a lot of teams today creating conversational AI is the tools, frankly, aren’t there on the collaborative side. In fact, the most common tool stack we see is going to be Word docs, Excel sheets, and Visio flowcharts. Now there’s lots of flowcharting softwares out there, and I’ve certainly heard people say, well, we don’t use Visio. We use Miro all in the same bucket. It is typically… next slide here. You’re gonna see spreadsheets are used to manage the NLU design. You’re gonna see, essentially, flowcharts are managed… manage the state machine. It manages the flow of the conversation. Word docs are often doing the script of the conversation, so a very lightweight, almost like a wire frame just to give people the feel. And the three of these are used in conjunction to create a conversation design. Now the problem with this is… well, two things. One, you don’t have a single source of truth. And so companies often have… you know, when you’ve one conversation designer, you can usually work with spreadsheets and flowcharts. That’s when you have two or more or ten or, you know, a hundred in some of these larger companies now, where they’re now getting to these very large teams. And when they have to communicate conversations across different teams and organizations, it might take a week just to understand the design. In fact, when I work with some customers, I’ll go into a design, and we’ll be chatting about it, and it might take me an hour or two just to understand the design before I can even comment on it. And so it gets really, really messy as its scales, have multiple sources of truth. And then further, you’re not able to prototype.\n\nThis is a big challenge for a lot of companies where if they’re coming from Visio flowcharts or, again, Miro, whatever your flowcharting tool of choice is, you’re not able to actually turn that into a usable prototype to run user testing. So a lot of companies will actually go straight from Word docs, Excel sheets, flowcharts, whatever it might be to a Jira ticket and then out to production. That’s a super common workflow we see all the time. Sometimes WoZ testing, which is Wizard of Oz testing, is implemented, and that’s essentially acting as though you are the assistant. So we hear companies where, you know, they’ll have a human on one end the line. They’ll call the other end if they’re trying to simulate a call center experience, and they’ll act as though they are the assistant. So that’s really the low fidelity testing we have available with the existing tool stack. So this is where conversation design tools come into play like Voiceflow. So a streamline way to go from design, prototype, user testing, and then ultimately handing off to development, trying to give them artifacts that are more battle-tested because you’ve actually done the prototyping, the user testing before handing it off to development as well as handing them in artifact is ultimately more readable as it’s all in one place without having to dig through a spreadsheet, like, you know, a flowchart, etcetera. Some nice quotes, but I’m gonna skip through these here and say conversation designs tools ultimately give you a single source of truth. And so this is the intro to a conversation design if you weren’t familiar with it and sort of what it is and what conversation design tools are.\n\nI wanna spend a little bit time about talking about the role of conversation design. So I think it’s fairly misunderstood what a conversation designer actually does on a daily basis. It’s essentially a mix of a few different a few different roles. So it’s going to be traditional UX. Most conversation designers actually come from a traditional UX background. It’s probably the most common one we see. We also see copywriting as a as a fairly common background, and then you have the NLU model. When you actually look at these in terms of structure, you really have conversation designs sitting on top of the NLU model. It sort of stands on on its shoulders there, and these three practices together create what we talk… call the conversation designer. And then you’re now starting to see, as these teams are getting larger, they’re getting more sophisticated. The role of the conversation designer is actually starting to split within companies. You’re starting to see AI trainers become a more common role, starting to see copywriters with a… you know, just very specific copyright and title, and the conversation designers really sort of the flow architect. And so you’re starting to see increased specialization on the UX side of these conversational AI teams. Cool.\n\nSo when we look at, you know, a conversation design… so here’s just a a sample when I pulled in from Voiceflow. You might have four different rules all working within this one design. You’re gonna have the copywriter who is focused primarily on the responses that the assistant is going to give. They’re going to be responsible for the persona often as well. From there, you’re going to have the NLU designer. They’re working with the different intents to make sure the utterances aren’t conflicting, the model’s actually working, and it’s gonna provide the best design experience. From there, the UX designer’s often responsible for the entire flow. What is the flow of the conversation? What intents are we handling, and how do they structure into each other? So that’s really sort of the role. They’re almost like the the flow architect almost. And then lastly is the UI designer. A lot of experiences are becoming multimodal. Voice is not… and I think, as the  previous presentation went over, voice is not the be-all, end-all interfaces one of many, and it’s incredibly good input, but it’s not very good at output. You know? If you’re choosing a Netflix movie, you would certainly would not want to have to be read the thirty different options before you make a choice. It’s gonna be crazy cognitive overload, but we’ve all sat there trying to use the the keyboard on the remote to try to pick a Netflix movie. And if you know what you want, it’s an amazing input interface. So you’re starting to see the rise of multimodal as well as these visual designers are being added to the conversation design teams. So one, you know, thing that, I I think, a lot of folks misunderstand about even voice or other conversations design tools is that we’re not content experts.\n\nYou know, a lot of folks who come from linguistics backgrounds, they might understand, you know, Grice’s Maxim is thrown around quite a bit coming from sociolinguistics. These are sort of the folks who are the best at creating the ideal responses. That’s not asset at Voiceflow. We actually view ourselves as giving you the tools to create the best responses. But as far as we’re concerned, we’re thinking about structure. We’re thinking about collaboration and the workflow. That’s really where we spend most of our time thinking in terms of what is actually said to the user. You know, we’re average average Joes when it comes to the actual content. And so what do I mean by content? Well, that’s going to be what’s actually said. We focus on the content structure. We focus on how can designers visualize the structure of the conversation. How can they actually put this together in a readable, and how can they ultimately collaborate? That’s really where we spend most of our time thinking. So some examples, on the far top top left there, you’ll see, like, a normal response. What the designer actually puts in there… I’m not a very good copywriter myself. You know, that’s that’s not our forte, but what we’re thinking about is… k. How can we add response variance? How can we do localization, text markup, speech SSML? How can we allow for all these different structures to be applied to the content more so than what is actually said inside the content itself?\n\nSo if you work at Voiceflow, probably, the number one thing you hear preached constantly is conversation design documentation. It is the number one thing we think about. And the reason for that is, ultimately, the art of conversation design is the art of conversation documentation. You know what you wanna say as a conversation designer, and if you’re a team of one, that’s where you’re able to put this into a… any kind of artifact you’d like, and how it’s actually presented is not that important. When you start to work in larger teams, it’s all about how you present it in a readable format so that other people can actually use it. So conversation design is the art of conversation documentation. And in order for a conversation design to be useful, it must be easily readable, and I think we’ve all seen flowcharts that look like this. This is… you know, oh, it’s a little blurry on the screen there, but you can kinda get the point. It is a crazy mess of nodes, and this is how a lot of internal design artifacts look at large enterprises as well as smaller teams. Frankly, there’s not much thought put into the actual design structure, and often what you’ll see is folks come in, and they’re unable to read it, and then they will often opt to completely redo it themselves. You just create a ton of redundancy.\n\nAnd so the best practice here is often to start out. Create a prototype. It might look really messy and spend a lot of time actually refactoring the designed to be highly readable so that other people can collaborate on it with you. So I wanted to spend some time going through essentially customers’ stories that is all along this theme of documentation, so some things that we’ve had to think about as a conversation design tool that you might not think about being a conversation designer. This was a big one. So when we were working with a large automaker, they were switching over their documentation base from Visio to Voiceflow, and we had a lot of seasoned VUI designers, voice user interface designers, sort of a branch within conversation design. Say they… you know, they’ve been working in Visio for twenty years now, and they’ve always done top to bottom, and therefore for Voiceflow is incompatible and will never work. This is something we hadn’t thought about. Voiceflow was always a left to right structure. That’s just… you know? In the early days of the company, it was just something we did. It was left to right. That was it. We didn’t really think much about it.\n\nAs the company evolved and now we work with customers that use all different orientations, you actually need to be able to mold the tool to mesh to people’s existing documentation systems so that when they’re going from reading a Visio chart to a Voiceflow chart, it’s going to look comparable, and they’re gonna be able to understand the lay of land instantly. So problem was not being able to go to left to right. Solution was fairly simple, being able to go vertical. Another problem we had was conversation designs aren’t just about the functional nodes on the canvas. It’s not just about the lines. It’s also about the surrounding contacts. There are sticky notes. There are little notes that designers jot down. There are comments. A lot of this wasn’t present in Voiceflow. We just had the functional nodes. We thought, well, if you can build it, that’s good enough. There’s not… you know? No need to add any sort of additional context. And what we found is design teams were going from Visio to Voiceflow to prototype, and then back to back to Visio to document. So then they would they would document their findings from Voice’s prototypes, and Voiceflow was just used as a prototyping system. So we ask why that was, and it was because they couldn’t add sticky notes. It was the simplest thing, but we just hadn’t thought about it as… and as a conversation design tool, we learned that the surrounding context is as important as the content itself. And so we added sticky notes, labels, images, all these things that allow your canvas to be high fidelity and flushed with context. Response variations take too much space. You don’t wanna hear the same thing from your system over and over and over again, and so response variations are a big thing in conversation design. Having five, six, you know, seven responses, or more to say the same thing just to keep the experience fresh. And so in Voiceflow, we had this feature. We thought this was great, and we allowed you to stack them all like this. Well, some companies might have a hundred, and they might have a… you know, two dozen. And it started to completely clutter the canvas, and so companies were again going back to spreadsheets to put all the response variations and then linking to Voiceflow and saying, you know, this should… is what the variant should have.\n\nAnd so we saw this, and we added the ability to choose your different visibilities, ’cause, again, it’s all about being able to either… essentially, layer your visibility. Are you gonna share everything, or are you just gonna share what’s needed to be seen by that particular individual? So we have… often see this a lot with even executive stakeholders. When they come and look at a conversation design, the designer will change the visibility to be just the bare essentials of the conversation. Anything more and the executives, you know, go asking about, you know, variation fifteen, what’s going on there? It’s way too much. You just wanna share just the bare minimum to understand the conversation flow. But then when you’re sharing with maybe a developer, you wanna be able to show all those different variations to really give the give the scope. So selective visibility has been… become incredibly important. Customers were adding a ton of designs to make the prototype work. This is a really curious one. So we were chatting with a bunch of customers and they were adding a ton of work to the designs, only to make the prototype work, and so the best example of this was personas. They wanted you test out. What happens if we have a a user who’s logged in versus logged out? So what they would do is they would actually add inside the design a whole bunch of logic and variables and things to actually create these different personas that would then be used inside the prototype. Now the problem here is by doing this, the… this… developers, when they saw the design, weren’t sure is this part of the design, or is this part just… to make the prototype work?\n\nAnd so this has become a super common problem, especially with the enterprise where they testing, you know, two personas, five personas when they’re doing user testing, and so we added the ability to test different personas. And so you start seeing a thing where it’s very customer led and that conversation design is such a new field. The UX side has been around on the IVR side for about twenty years, but in terms of the UX, sort of, adopting what’s the modern UI practices to conversation design now, that’s only really come in the past two years. And so we’re essentially building a new category of tooling, and we’re learning all this stuff as we go. Few more here. Customers were creating multiple projects for the same assistant. Designs are getting very, very large. If you run a contact center today, you’re going to have massive flows, and so we saw our customers creating tons and tons of projects just to handle different variations, so we added folders. I could go off this. Yeah.\n\nLast one here. Different stakeholders need different information. What we found is that the same node on a canvas might be used differently by two different people even on the same team. So the developer is gonna look at this and go, oh, great. There is the JavaScript logic I needed. I can now reference that easily into my actual code, but the designer looks at that and has no idea what’s going on often. And so we added the ability to do role-based views. You have a developer view or a designer view. It’s really up to you. And so this is sort of the evolving state. As these teams get larger and larger and different roles are being added, you’re gonna see these tools essentially become multifaceted in how they can actually display information depending on the rule. Since a conversation is such an abstract concept, it is going to be different to every stakeholder that views it. And so you need to be able to chunk and break down that layered visibility. It’s very similar too. In a design tool, you know, you might see the color red, and the developer needs to see the hex code. And so you’re gonna have that ability to break down a conversation by all of its different elements as you go through. Cool. I think we can go through this. Yeah. Some problems, I think, the industry has yet to solve, you know, intent manager… intent management. We call them an… essentially, an IMS internally.\n\nSome customers have thousands of intents, tens of thousands that you’re… you know, at some of these larger companies, almost having what you have for a content management system and applying that to the intent space, especially as you deal with different locals, different NLPs and NLUs, having a robust system, the industry has yet to solve that, but there are some exciting start-ups popping up. Content management. Content management, as it relates specifically to conversational assistance, there’s a couple start-ups that popped up that are exciting, but this is something that’s going to be increasingly important as these assistants are deployed, not just in one local, but in multiple locals, getting to the point where it’s not just countries but even down to the city level. Maybe you have a restaurant in a… one city that has a very different, you know, phrase for good morning than another, we wanna have conversational assistance, meet customers where they are, and actually be able to get down to that local level. So content management’s gonna be incredibly important.\n\nThen lastly, persona management. You know, certainly brands want to have one assistant that is going to be able to interact with customers across all channels, and so having a way to manage that persona in terms of its vocabulary and how it actually interacts with customers, I think you’re gonna start to see a a persona management software pop up as well. So that’s some exciting stuff there, but lots more in the industry to solve on the collaborative side. And that is it on my end. I wanted to essentially have a bunch of different product product shots to kinda run through some different solutions. From the conversation design tool side, hopefully, you learned a little bit about conversation design through that process. We’d love to chat with a bunch of folks. If you wanna chat about conversation design practices, the talk I’ve given previously on how to actually structure our team, happy to do that as well. We’ve got a booth at the back in a social today at four thirty, and there will be free drinks. So… and it’s a beautiful venue right on the beach. So feel free to come by. Say hi, and we’ll give you a ticket for that. And with that, thanks so much.";
						}
						async function compiledContent$17() {
							return load$17().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$17() {
							return (await import('./chunks/index.f254ff9f.mjs'));
						}
						function Content$17(...args) {
							return load$17().then((m) => m.default(...args));
						}
						Content$17.isAstroComponentFactory = true;
						function getHeadings$17() {
							return load$17().then((m) => m.metadata.headings);
						}
						function getHeaders$17() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$17().then((m) => m.metadata.headings);
						}

const __vite_glob_0_205 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$17,
  file: file$17,
  url: url$17,
  rawContent: rawContent$17,
  compiledContent: compiledContent$17,
  default: load$17,
  Content: Content$17,
  getHeadings: getHeadings$17,
  getHeaders: getHeaders$17
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$16 = {"title":"How Speech-to-Text Supports Content Moderation Companies","description":"Many content moderation companies specialize in assessing text—but with automatic speech recognition, they can also look at audio and video.","date":"2022-08-24T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981434/blog/speech-to-text-content-moderation-companies/how-ASR-supports-content-moderation-companies-thum.png","authors":["chris-doty"],"category":"speech-trends","tags":["voice-strategy"],"seo":{"title":"How Speech-to-Text Supports Content Moderation Companies","description":"Many content moderation companies specialize in assessing text—but with automatic speech recognition, they can also look at audio and video."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981434/blog/speech-to-text-content-moderation-companies/how-ASR-supports-content-moderation-companies-thum.png"},"shorturls":{"share":"https://dpgr.am/6c3209b","twitter":"https://dpgr.am/548715e","linkedin":"https://dpgr.am/b548551","reddit":"https://dpgr.am/cdcf645","facebook":"https://dpgr.am/76b5ac1"}};
						const file$16 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/speech-to-text-content-moderation-companies/index.md";
						const url$16 = undefined;
						function rawContent$16() {
							return "Content moderation companies and departments work hard to keep offensive language out of video games, off platforms like forums, out of ad campaigns, and more. Most content moderation looks specifically at text, meaning that videos and audio chats can slip past the moderation efforts that a company might have in place-or else be extremely expensive, hiring multiple people to review this kind of content.\n\nThat's where content moderation with speech-to-text comes in-by converting speech to text, the same processes that apply to written content can be applied to spoken content, providing additional options for moderation. To get started, let's look at what content moderation is and how it typically works, before we dive into some of the benefits of content moderation and how AI-powered automatic speech recognition solutions like Deepgram can help.\n\n## What is Content Moderation?\n\nContent Moderation refers to the process of monitoring user-generated content online and ensuring that it complies with any site rules and relevant laws. For example, companies like [Spectrum Labs](https://www.spectrumlabsai.com/) use artificial intelligence to identify problematic content like sexually charged messages, hate speech, radicalization, bullying, scams, grooming, and more. Moderation is used in a variety of different contexts, from social media sites to advertising platforms to video games. Any company that needs to ensure that the content that's being created and shared via its service has a need for some kind of content moderation. That moderation can come in a few different forms, including:\n\n* **Pre-moderation:** All content is reviewed before it's allowed to go live.\n* **Post-moderation:** Content is allowed to go live, but is still reviewed after being posted.\n* **Reactive moderation:** Content is only reviewed when it's flagged by other users as potentially problematic.\n* **Distributed moderation:** Content is upvoted or downvoted based on user feedback, and shown or hidden based on that voting, rather than the decision of moderators.\n\nAdditionally, moderation can happen in several different ways. In the most basic forms, humans review content to make sure that it complies with any relevant guidelines. But this process can be time consuming and tedious-and, in some cases, simply not possible with the amount of content that gets created. That's where automatic moderation comes in. \n\nAutomatic moderation occurs without a human intervening, and can be as simple as removing content that contains words from a pre-specified list, or as complex as training a neural network for AI content moderation. Automatic moderation is especially relevant when we talk about automatic speech recognition for media monitoring, because, as mentioned above, once the audio has been turned into text, the same rules and filters can be applied to it as would have been applied to written content. But before we get to the benefits of content moderation and how STT can help, let's explore some of the most common use cases for content moderation.\n\n## Top 5 Use Cases for Content Moderation\n\nContent moderation is used for a variety of different use cases across different industries, some of which might surprise you-let's take a look at the top five use cases for content moderation.\n\n### 1. Gaming\n\nOnline gaming communities aren't often known as friendliest of places. With content moderation, game companies can work towards creating friendlier, more welcoming communities.\n\n### 2. Forums and Social Media\n\nSites that rely on user-generated content-from forums like Reddit to social media like Facebook-rely on content moderation to review what's posted, ensuring it follows site guidelines.\n\n### 3. Advertising\n\nAdvertising platforms have a vested interest in making sure that any ad served through their platform complies with their guidelines and any relevant laws. Content moderation reviews user-created ads to make sure that they're all above board.\n\n### 4. Ecommerce\n\nContent moderation can serve a number of purposes for ecommerce platforms, from making sure that illegal or prohibited items aren't listed and sold to making sure that customer product reviews aren't offensive or spam.\n\n### 5. Health and Finance\n\nAlthough they might not be the first things that come to mind when you think of content moderation, the health care and finance industries can make use of content moderation technologies. With lots of personal identifiable information (PII) and the need for HIPAA compliance, content moderation companies like [Private AI](https://www.private-ai.com/) can help to clean and process data to remove identifying information before the data is used for other purposes.\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-make-application-voice-ready\"></WhitepaperPromo>\n\n## Benefits of Content Moderation\n\nThere are a number of benefits that come from using content moderation for your company. Here are a few of the biggest impacts that content moderation can have.\n\n### Protect Your Brand...\n\nWhether it's on a social media platform, in a video game, or on an ad, using content moderation ensures that what users experience is what they expect. For example, if a company says that they support the LBGTQ+ community, but you regularly find bigotted language used on their site, you're unlikely to believe them. Content moderation can help ensure that the face a company presents to the world reflects their values and beliefs.\n\n### ... and Your Users\n\nContent moderation also helps protect your brand by creating inclusive communities, spaces where everyone can feel safe. By monitoring what's posted by users and flagging or removing offensive or hateful content so that all users feel welcome.\n\n### Better Understand Your Customers\n\nAlthough you might think of content moderation as simply removing public-facing content, the process of analyzing everything posted can give you insights into your users. It can help you understand how they're feeling and what they're posting about (whether that content ultimately ends up being removed or not), giving you new insight into how to interact with them, what they're looking for, and even how they're feeling (see the section \"Sentiment Analysis\" below).\n\n### Avoid Legal Troubles\n\nDepending on what you're moderating, it's possible that users could be posting content that doesn't just run afoul of your own guidelines, but also of relevant local laws or others who hold copyrights to the content being posted. Content moderation efforts allow you to catch this content so that you aren't exposing yourself to possible legal action.\n\n## How Speech-to-Text Supports Content Moderation Companies\n\nWhenever you're choosing a speech-to-text solution, you want to make sure that it supports your specific needs. If you're interested in content moderation using STT, that means you need something that works quickly, returning transcripts in real time if you want to do pre-moderation or any kind of content evaluation before something is posted or is live.\n\nThat's because [AI-powered automatic speech recognition](https://blog.deepgram.com/deep-learning-speech-recognition/) is faster than the alternatives, enabling real-time monitoring and removal of content that violates your guidelines. While many companies today rely on post-moderation or reactive moderation-especially for audio and video-with real-time STT, these media can also be pre-moderated. Let's take a look at some of the specific ways that AI-powered STT solutions like Deepgram can support content moderation companies and departments.\n\n### Unlocks New Moderation Channels\n\nA lot of automatic moderation today happens based on text, with other options used for audio and video. But with an AI-powered STT solution that can turn speech into text in real time, you can use the same automated process you employ for text, opening new industries and potential customers. For example, [Modulate's](https://www.modulate.ai/) [TodMox](https://www.modulate.ai/tox-mod) product is a full-coverage voice moderation solution-something that it simply isn't possible to build without advanced automatic speech recognition solutions.\n\n### Cost Savings\n\nAs mentioned in the introduction, it's certainly possible to moderate video and audio with a person in the loop-but if your users are generating large amounts of content, it can become cost-prohibitive. With AI-powered speech-to-text, though, this content can be moderated quickly and easily-and more cheaply.\n\n### Sentiment Analysis\n\nIf you're only working with text, you can do some basic sentiment analysis to see what the tone of user-generated content is-positive or negative. But with the addition of audio streams, you can [add emotion recognition](https://blog.deepgram.com/sentiment-analysis-emotion-regulation-difference/) to the mix, getting even more insight into how customers are feeling than would be possible from pure text or human moderators.\n\n## Wrapping Up\n\nNow that you've had a chance to consider some of the most common use cases and benefits of content moderation, as well as the ways that AI-powered STT solutions can help, why not give Deepgram a try? You can [sign up for a free trial and get $150 in free credits](https://console.deepgram.com/signup). Or, [reach out to our team](https://deepgram.com/contact-us/) and we're happy to talk through what you're building and how we can help you succeed.";
						}
						async function compiledContent$16() {
							return load$16().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$16() {
							return (await import('./chunks/index.0d1b84fc.mjs'));
						}
						function Content$16(...args) {
							return load$16().then((m) => m.default(...args));
						}
						Content$16.isAstroComponentFactory = true;
						function getHeadings$16() {
							return load$16().then((m) => m.metadata.headings);
						}
						function getHeaders$16() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$16().then((m) => m.metadata.headings);
						}

const __vite_glob_0_206 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$16,
  file: file$16,
  url: url$16,
  rawContent: rawContent$16,
  compiledContent: compiledContent$16,
  default: load$16,
  Content: Content$16,
  getHeadings: getHeadings$16,
  getHeaders: getHeaders$16
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$15 = {"title":"Speech-to-Text Model for Ukrainian Released","description":"We created a Ukrainian speech-to-text model to help with humanitarian efforts in Eastern Europe. This post explains how to get access to it.","date":"2022-03-21T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981411/blog/speech-to-text-model-ukrainian/STT-model-for-ukranian-released-thumb-554x220%402x.png","authors":["chris-doty"],"category":"product-news","tags":["language"],"seo":{"title":"Speech-to-Text Model for Ukrainian Released","description":"We created a Ukrainian speech-to-text model to help with humanitarian efforts in Eastern Europe. This post explains how to get access to it."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981411/blog/speech-to-text-model-ukrainian/STT-model-for-ukranian-released-thumb-554x220%402x.png"},"shorturls":{"share":"https://dpgr.am/a6a965c","twitter":"https://dpgr.am/3cdce26","linkedin":"https://dpgr.am/1354d71","reddit":"https://dpgr.am/c87d7dc","facebook":"https://dpgr.am/8bc4171"}};
						const file$15 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/speech-to-text-model-ukrainian/index.md";
						const url$15 = undefined;
						function rawContent$15() {
							return "We've been asked by a few different organizations to help with humanitarian efforts in Eastern Europe by creating a speech model to transcribe Ukrainian. Such a model is needed in, for example, call centers that have been set up to help refugees fleeing the conflict. We'd like to offer access to that model to anyone who needs it to help with the current crisis-all Ukrainian speech transcription will be free for at least the next 6 months.\n\n## How to Get Access to Ukrainian ASR\n\nIf you'd like to get access to Ukrainian speech-to-text, you can [sign up for a Deepgram account](https://console.deepgram.com/) and select Ukrainian as your audio language in our drop down menu, as seen in the image below. \n\n![](https://res.cloudinary.com/deepgram/image/upload/v1661976853/blog/speech-to-text-model-ukrainian/image1-1024x732.png) \n\nTo transcribe Ukrainian audio with Deepgram's API, simply add **language=uk** to your transcription requests. You can visit [our documentation](https://developers.deepgram.com/documentation/features/language/) for more information about our language support.\n\n```\ncurl -X POST \\\n -H \"Authorization:Token YOUR_API_KEY\" \\\n -H 'content-type: application/json' \\\n -d '{\"url\":\"LINK_TO_YOUR_FILE\"}' \\\n\"https://api.deepgram.com/v1/listen?language=uk\"\n```\n\n## Conclusion\n\nWe're pleased that we've been able to use [Deepgram's end-to-end approach](https://blog.deepgram.com/deep-learning-speech-recognition/) and transfer learning to quickly train a model for Ukrainian to support those on the ground assisting the Ukrainian people. If there are other languages that would be useful to you, please see our [language page](https://deepgram.com/product/languages/) for a list of all of the models that we currently offer. And if you still have questions or need help getting started, you can reach out to us via the [contact page](https://deepgram.com/contact-us/).";
						}
						async function compiledContent$15() {
							return load$15().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$15() {
							return (await import('./chunks/index.151ff6c7.mjs'));
						}
						function Content$15(...args) {
							return load$15().then((m) => m.default(...args));
						}
						Content$15.isAstroComponentFactory = true;
						function getHeadings$15() {
							return load$15().then((m) => m.metadata.headings);
						}
						function getHeaders$15() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$15().then((m) => m.metadata.headings);
						}

const __vite_glob_0_207 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$15,
  file: file$15,
  url: url$15,
  rawContent: rawContent$15,
  compiledContent: compiledContent$15,
  default: load$15,
  Content: Content$15,
  getHeadings: getHeadings$15,
  getHeaders: getHeaders$15
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$14 = {"title":"State of Speech: Our New Data Report Reveals ASR’s Untapped Potential","description":"Speech is a powerful tool for the enterprise with the ability to unlock a whole new treasure trove of data. This space is growing and almost every company is investing in some form of Automatic Speech Recognition (ASR) - but the question remains, are they using speech to its full potential?","date":"2021-03-03T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981362/blog/state-of-speech-our-new-data-report-reveals-asrs-untapped-potential/2021-state-of-asr-infogfx%402x.jpg","authors":["scott-stephenson"],"category":"speech-trends","tags":["state-of-voice-tech"],"seo":{"title":"State of Speech: Our New Data Report Reveals ASR’s Untapped Potential","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981362/blog/state-of-speech-our-new-data-report-reveals-asrs-untapped-potential/2021-state-of-asr-infogfx%402x.jpg"},"shorturls":{"share":"https://dpgr.am/b04ca40","twitter":"https://dpgr.am/6b0c910","linkedin":"https://dpgr.am/ed77ce6","reddit":"https://dpgr.am/f3bd46e","facebook":"https://dpgr.am/793714c"}};
						const file$14 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/state-of-speech-our-new-data-report-reveals-asrs-untapped-potential/index.md";
						const url$14 = undefined;
						function rawContent$14() {
							return "Speech is a powerful tool for the enterprise with the ability to unlock a whole new treasure trove of data. This space is growing and almost every company is investing in some form of Automatic Speech Recognition (ASR) - but the question remains, are they using speech to its full potential?\n\nTo answer this question, we partnered with [Opus Research](https://opusresearch.net/wordpress/) to examine the state of ASR in the enterprise. We surveyed 400 North American decision-makers, from managers to the C-suite, and in industries from retail and hospitality to financial services and government, to understand how they are currently using ASR and what future investment looks like. Through our research, we discovered specific opportunities where companies could expand ASR use to better support company growth. We also found that while companies are interested in capitalizing on the insights that ASR can unearth, they have yet to take full advantage of the technology. This may be a result of company decision-makers not understanding what their ASR provider can offer, or due to the sophistication of the technology itself. Here are some of the key takeaways we discovered through our research:\n\n* **Siloed ASR use is hindering innovation**: Currently, ASR is mainly used for operational efficiency, compliance and regulatory mandates. Respondents are primarily leveraging ASR for cost savings (e.g., [Contact Center](https://deepgram.com/solutions/contact-centers/) operational efficiencies and improved training) or cost avoidance (compliance), but most aren't utilizing it to fuel overall enterprise growth or to improve products and services with the exception of the retail and telecom industries. However, our findings revealed that there is a growing appetite for innovation-focused use cases where speech data is leveraged for real-time [speech analytics](https://deepgram.com/solutions/speech-analytics/), virtual assistants or [voicebots](https://deepgram.com/solutions/voicebots/), market research, etc.\n* **Speech data is largely underutilized:** 88% of respondents are using ASR to transcribe or analyze spoken conversational content. However, two-thirds of respondents say they are significantly under-utilizing their audio. This indicates that many companies understand the importance of speech data, but are struggling to make the best use of it.\n* **Companies recognize the importance of ASR - and plan to keep investing**: While it was not a requirement for respondents to currently be using ASR, 99% of respondents indicated that they currently implement ASR in some capacity as part of their business strategy, and a large majority plan to significantly increase their investment in 2021. In addition, 85% indicate ASR as important or very important to their future enterprise strategy.\n* **COVID-19 accelerated Automatic Speech Recognition adoption:** The pandemic has transformed technology and business principles across industries forever. 80% of respondents list COVID-19 as a top reason they accelerated the adoption of ASR.\n\nOur findings are proof that ASR is a critical element of any enterprise strategy and has the power to unlock powerful insights into the overall customer experience, how a product or service is working and much more. At Deepgram, we believe that voice data presents enterprises with a significant opportunity to gain more insight into their business, customers and markets, and we are committed to helping enterprises unlock the potential hidden within their voice data. For more information about Deepgram and to download a copy of the full data report, click [here](https://deepgram.com/state-of-asr-report/).";
						}
						async function compiledContent$14() {
							return load$14().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$14() {
							return (await import('./chunks/index.3fb87040.mjs'));
						}
						function Content$14(...args) {
							return load$14().then((m) => m.default(...args));
						}
						Content$14.isAstroComponentFactory = true;
						function getHeadings$14() {
							return load$14().then((m) => m.metadata.headings);
						}
						function getHeaders$14() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$14().then((m) => m.metadata.headings);
						}

const __vite_glob_0_208 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$14,
  file: file$14,
  url: url$14,
  rawContent: rawContent$14,
  compiledContent: compiledContent$14,
  default: load$14,
  Content: Content$14,
  getHeadings: getHeadings$14,
  getHeaders: getHeaders$14
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$13 = {"title":"State of Voice Tech 2022: New Report Highlights Biggest Voice Tech Adoption Motivators","description":"Learn about the motivations for using voice technology, the most impactful uses of voice tech today, and what the future holds for voice.","date":"2022-01-25T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981405/blog/state-of-voice-report-2022/State-of-Voice-2022-thumb-554x220%402x.png","authors":["shadi-baqleh"],"category":"speech-trends","tags":["state-of-voice-tech"],"seo":{"title":"State of Voice Tech 2022: New Report Highlights Biggest Voice Tech Adoption Motivators","description":"Learn about the motivations for using voice technology, the most impactful uses of voice tech today, and what the future holds for voice."},"shorturls":{"share":"https://dpgr.am/d5a530e","twitter":"https://dpgr.am/2231b16","linkedin":"https://dpgr.am/90e3d4c","reddit":"https://dpgr.am/70b6698","facebook":"https://dpgr.am/aaa7e49"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981405/blog/state-of-voice-report-2022/State-of-Voice-2022-thumb-554x220%402x.png"}};
						const file$13 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/state-of-voice-report-2022/index.md";
						const url$13 = undefined;
						function rawContent$13() {
							return "\r\nIn last year's inaugural State of ASR report, we partnered with [Opus Research](https://opusresearch.net/wordpress/) to examine how companies of all sizes use voice technologies built on [ASR (Automatic Speech Recognition)](https://blog.deepgram.com/what-is-asr/) to drive efficiencies and productivity through their organizations. But businesses have come to understand the inherent value of voice and the data it holds, so this year, we wanted to expand beyond just ASR to the entire speech technology industry. We wanted to dig deeper into the motivations for using voice technology, what the future holds for voice, and the most impactful uses of speech and voice technology that companies are seeing today-the [State of Voice Technology](https://deepgram.com/state-of-voice-technology-2022/) report was born. To make it happen, we again partnered with Opus Research, who surveyed 400 North American decision makers-from managers to the C-suite in various industries from finance to healthcare to hospitality-to understand how they are currently using voice technology and what they think is coming in the future. Let's take a look at some of the major findings from this year's report.\r\n\r\n## Major Findings from the Report\r\n\r\nOne of the most interesting results is the speed with which companies are moving from voice technology as a cost-saving tool to a revenue-generating strategy. Companies are thinking about all the insights that can be gained and actions that can immediately be implemented to create more loyalty, build better products, and create additional revenues. Voice technology automation is moving away from, \"Say one for support, say two for sales...\" to a voice bot that might say, \"You said you need more bandwidth, you might want to upgrade to our premium package, so let's look at the cost difference for you.\" Voice technology is being recognized not as a cost but as an investment to increase revenues. Cost savings is a want-to-have but increasing revenues is a need-to-have, so this may be the time you really see voice technology takeoff in business.  Here are some of the other key findings that are revealed in the report:\r\n\r\n### Customer experience is the top motivator for speech technology adoption\r\n\r\nCustomer loyalty is harder to secure than ever, leading more and more companies to differentiate on CX. Speech technology is seen as a key piece of CX with 73% of respondents noting customer experience analysis as the most impactful use for speech technology and 75% reporting they plan to increase their speech technology budget in 2022.\r\n\r\n### Speech technology is creating a bottom-line impact\r\n\r\nOur data found that 83% of companies see 25-75% productivity gains, while 59% of companies are seeing revenue gains of 25-75%. Speech technology historically seemed like a costly investment for businesses of all sizes, but companies have started noticing the increased payoff. 60% of respondents noted that speech technology helps promote operational efficiencies, while 77% believe it helps identify new business opportunities and ways to engage and retain customers. \r\n\r\n### Concern around bias in speech technology is top of mind\r\n\r\nThe way we speak contains a multitude of information about who we are and how we identify. Bias in AI has been a highly debated topic, and the debate isn't much different for speech technology. Several high-profile cases of systemic bias have been in the news, and companies realize the importance and impact bias can have, with 92% of respondents agreeing that speech technology bias has an important impact on customers. The most significant kinds of bias that concern respondents?\r\n\r\n*   Gender: 85%\r\n*   Race: 72%\r\n*   Non-native speaker: 56%\r\n*   Age: 8%\r\n\r\n### Widespread speech & voice technology adoption is coming\r\n\r\nThe COVID-19 pandemic accelerated the adoption of speech technology as we pivoted to a remote world, but respondents are wary that the rapid growth will continue. While 64% of respondents expect speech technology to be one of the most critical aspects for the future of their enterprise strategy, only 15% of respondents felt it was being adopted at a massive scale now. The majority (77%) shared they believe the mass implementation of the technology will take place in the next one to five years. \r\n\r\n## It's Time for Voice\r\n\r\nWith investment increasing and a voice-enabled future on the horizon, it's time for enterprises to get serious about how they're using voice technology. Voice-enabled experiences are solving problems right now through positive customer experiences and will continue setting companies up for success down the line by reducing bias, increasing revenue, and identifying new business opportunities. With many respondents believing widespread adoption of the technology is coming in the next few years, the time is now to invest in speech technology. Read the full report and get even more insight into the [State of Voice Technology for 2022](https://deepgram.com/state-of-voice-technology-2022/).\r\n";
						}
						async function compiledContent$13() {
							return load$13().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$13() {
							return (await import('./chunks/index.c9bd8745.mjs'));
						}
						function Content$13(...args) {
							return load$13().then((m) => m.default(...args));
						}
						Content$13.isAstroComponentFactory = true;
						function getHeadings$13() {
							return load$13().then((m) => m.metadata.headings);
						}
						function getHeaders$13() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$13().then((m) => m.metadata.headings);
						}

const __vite_glob_0_209 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$13,
  file: file$13,
  url: url$13,
  rawContent: rawContent$13,
  compiledContent: compiledContent$13,
  default: load$13,
  Content: Content$13,
  getHeadings: getHeadings$13,
  getHeaders: getHeaders$13
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$12 = {"title":"How to Add Supabase Authentication to a Vue App","description":"In this post we will walk through getting authentication set up using Supabase and Vue 3.","date":"2022-02-10T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1646690723/blog/2022/02/supabase-authentication-vue/Getting-Started-with-supabase-blog%402x.jpg","authors":["brian-barrow"],"category":"tutorial","tags":["supabase"],"seo":{"title":"How to Add Supabase Authentication to a Vue App","description":"In this post we will walk through getting authentication set up using Supabase and Vue 3."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453998/blog/supabase-authentication-vue/ograph.png"},"shorturls":{"share":"https://dpgr.am/3afd559","twitter":"https://dpgr.am/f87b3f4","linkedin":"https://dpgr.am/99a7dcb","reddit":"https://dpgr.am/d5d0e76","facebook":"https://dpgr.am/60f3165"}};
						const file$12 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/supabase-authentication-vue/index.md";
						const url$12 = undefined;
						function rawContent$12() {
							return "Following on from my \"[Getting Started with Supabase](https://blog.deepgram.com/getting-started-with-supabase/)\" blog post, this guest post on the freeCodeCamp blog covers how to implement authentication into your Vue 3 application.\n\nWe cover how to include and set up Supabase, create components to manage registration and login, gate content behind authentication, how to log out, and finally how to complete the flow with a 'magic link' via email.\n\nYou can read the post now over on the [freeCodeCamp Blog](https://www.freecodecamp.org/news/add-supabase-authentication-to-vue/). If you ever have any questions about this post, please feel free to reach out to us on Twitter [@DeepgramDevs](https://twitter.com/DeepgramDevs). We’re happy to help!";
						}
						async function compiledContent$12() {
							return load$12().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$12() {
							return (await import('./chunks/index.63a71f7f.mjs'));
						}
						function Content$12(...args) {
							return load$12().then((m) => m.default(...args));
						}
						Content$12.isAstroComponentFactory = true;
						function getHeadings$12() {
							return load$12().then((m) => m.metadata.headings);
						}
						function getHeaders$12() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$12().then((m) => m.metadata.headings);
						}

const __vite_glob_0_210 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$12,
  file: file$12,
  url: url$12,
  rawContent: rawContent$12,
  compiledContent: compiledContent$12,
  default: load$12,
  Content: Content$12,
  getHeadings: getHeadings$12,
  getHeaders: getHeaders$12
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$11 = {"title":"How to Build a Podcast Player with Transcriptions using Vue and Supabase","description":"In this post we will walk through setting up a Podcast Player app using Supabase and Vue 3, including getting transcriptions for the podcasts.","date":"2022-02-28T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1646691089/blog/2022/02/supabase-podcast-player-vue/Build-Podcast-Player-app-w-transcriptions-using-Vue-Supabase%402x.jpg","authors":["brian-barrow"],"category":"tutorial","tags":["supabase","vuejs"],"seo":{"title":"How to Build a Podcast Player with Transcriptions using Vue and Supabase","description":"In this post we will walk through setting up a Podcast Player app using Supabase and Vue 3, including getting transcriptions for the podcasts."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454000/blog/supabase-podcast-player-vue/ograph.png"},"shorturls":{"share":"https://dpgr.am/fd6bd89","twitter":"https://dpgr.am/09738d2","linkedin":"https://dpgr.am/0125cde","reddit":"https://dpgr.am/1d58a82","facebook":"https://dpgr.am/1d718e0"}};
						const file$11 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/supabase-podcast-player-vue/index.md";
						const url$11 = undefined;
						function rawContent$11() {
							return "I've previously written two posts on Supabase - the first on getting started with their database, and the second on implementing authentication into your Vue 3 app.\n\nToday, we're building a Vue application which allows users to log in, subscribe to podcast feeds, and create a transcript for any episode with Deepgram.\n\nYou can read the post now over on the [freeCodeCamp Blog](https://www.freecodecamp.org/news/build-a-podcast-player-with-transcriptions-using-vue-supabase/).\n\nThe [final code is available on GitHub](https://github.com/briancbarrow/vue-supabase-auth/tree/final-podcast-feed-transcriptions) if you want to jump straight in.\n\nIf you ever have any questions about this post, please feel free to reach out to us on Twitter [@DeepgramDevs](https://twitter.com/DeepgramDevs). We’re happy to help!";
						}
						async function compiledContent$11() {
							return load$11().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$11() {
							return (await import('./chunks/index.2c7e077e.mjs'));
						}
						function Content$11(...args) {
							return load$11().then((m) => m.default(...args));
						}
						Content$11.isAstroComponentFactory = true;
						function getHeadings$11() {
							return load$11().then((m) => m.metadata.headings);
						}
						function getHeaders$11() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$11().then((m) => m.metadata.headings);
						}

const __vite_glob_0_211 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$11,
  file: file$11,
  url: url$11,
  rawContent: rawContent$11,
  compiledContent: compiledContent$11,
  default: load$11,
  Content: Content$11,
  getHeadings: getHeadings$11,
  getHeaders: getHeaders$11
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$10 = {"title":"Technical Writing: A Beginner's Guide","description":"A developer's guide for creating written and technical documentation.","date":"2022-03-07T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1646661847/blog/2022/03/technical-writing-a-beginners-guide/writing-a-technical-blog-post.jpg","authors":["bekah-hawrot-weigel"],"category":"best-practice","tags":["technical-writing"],"seo":{"title":"Technical Writing: A Beginner's Guide","description":"A developer's guide for creating written and technical documentation."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454049/blog/technical-writing-a-beginners-guide/ograph.png"},"shorturls":{"share":"https://dpgr.am/6611fa2","twitter":"https://dpgr.am/8a6c3fa","linkedin":"https://dpgr.am/b0c86cb","reddit":"https://dpgr.am/f0c9059","facebook":"https://dpgr.am/dd7af93"}};
						const file$10 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/technical-writing-a-beginners-guide/index.md";
						const url$10 = undefined;
						function rawContent$10() {
							return "As developers, not only do we need to know how to communicate our code, we also need to understand how to create written and technical documentation to ensure our meaning, decisions, and processes are understood. This technical writing happens as documentation, inter-team communication--communication between different teams--and even blog posts. But what if you’ve never been trained in technical communication? How can you get started in technical writing?\n\n## Key terms\n\n**Technical Writing**: The process of writing about complex concepts for a specific audience.\n\n**Audience**: Who you are writing for or to.\n\n**Jargon**: Often technical in nature, words that your wider audience might not easily understand. For example, I might use ASR if I’m working on a [Deepgram](https://deepgram.com/) post. Outside of a specific technical audience, this won’t mean much. Instead, I should write, “Automated Speech Recognition (ASR) uses Machine Learning to take speech and create text.”\n\n## Audience\n\nLet’s start with a checklist. Think about your audience and then answer the following questions:\n\n* How will that affect your word choice and tone?\n* What do your readers need to know?\n* How much information is enough?\n* Put yourself in the reader’s position. What do you need and want?\n* How can you make it clear, relevant, and provide practical examples?\n* Will using videos, images, or code samples benefit your audience’s understanding?\n\nAfter answering these questions, think about your topic and how you'll develop it as a blog post. This will help you determine how and when to avoid or explain jargon. Is your audience learning to code? Are they looking for advanced applications of a concept? How can you support them in their learning journey--because anyone who’s reading a blog post is on a learning journey, just at different levels.\n\nStart writing down ideas to help you to create an outline and eventually fill your post.\n\n## Types of Technical Writing\n\nIn this post, we’re focusing on technical writing, but we’re going to break that down into three categories: informational, instructional, and persuasive. As tech writers, we experience a little bit of these in each post. It’s okay to see the overlap. In fact, if you recognize it, you’ll be a more effective writer.\n\n**Informational**: With informational writing, your primary purpose is to give information. You might have a post, for example, “Explain it to me like I’m 5: Automatic Speech Recognition” or “Five ways to use Deepgram to increase accessibility in your school.” Your writing is factual, provides objective information, and can bring in external resources to provide adequate information to your reader.\n\n**Instructional**: Instructional writing, at its heart, is writing to teach. You want your reader to learn based on what you’ve written. This means you need to provide clear instructions, examples, and explanations. And as a result of reading your instructional writing, your readers should be able to follow along and complete a task with little to no questions. Try to anticipate the questions of the reader. You don’t have to answer all of their questions, but consider a combination of answering questions, linking to answers, and/or pointing out that there are topics that you haven’t covered in your post.\n\n**Persuasive**: In this form of writing, you’re trying to convince the reader of something. Maybe you want them to choose one technology or product over another. Maybe you’re asking them to come back to read your next blog post. With persuasive writing, you’re working towards asking your reader to decide on something.\n\n## Structure & Content\n\nWhatever type of writing you’re working on, having a beginning, middle, and end matters. In the beginning--often referred to as the introduction--you’re giving your reader a roadmap for what to expect. Are you going to teach them something, convince them to use a specific technology, or break down a concept? Your reader should never have to guess where you're going in your post.\n\nThe middle of your writing should provide supporting ideas for your topic and give examples. If you’re writing about Speech-to-Text, you should give a definition, examples, and explanations of what that means. Each paragraph--or chunk of text--should focus on one idea, connecting the ideas in the paragraphs around it.\n\nIf you’re taking an instructional point of view to show how to use Deepgram in the classroom, you might cite [Kevin’s Classroom Captioner post](https://blog.deepgram.com/classroom-captioner/), provide another example, and give the readers both step-by-step instructions as well as an explanation of *why* you’re doing those things. If you’re referring to code, it’s *always* better to include code samples.  Two great ways to do that are: [Ray.so](http://ray.so) and [Carbon](https://carbon.now.sh/). In Dev.to, you can also use markdown syntax with single backticks (\\` \\`)--those are the ones to the left of the 1 on the keyboard--or triple backticks for longer code samples (\\`\\`\\` \\`\\`\\`).\n\n## Proofreading\n\nEven if your code is perfect, you’re not done. Proofreading is one of those things that takes time to get right. I *highly* recommend adding the free [grammarly extension](https://www.grammarly.com/) to your chrome browser.\n\nSome other strategies I like include:\n\n* Read your writing aloud--hey, you can even have Deepgram transcribe that for you to read how you sound!\n* Read your paragraphs from the conclusion to the introduction,\n* Look at your coding samples to see if they clearly explain the meaning you want to convey.\n\nTechnical writing should be clear, have a purpose, and share examples that allow for understanding and application if necessary. I’ve heard many people say that technical writing is boring, but that doesn’t have to be the case. It’s interesting if you make it interesting. Stay tuned for the next post in this series where we'll go into some strategies for creating engaging content.";
						}
						async function compiledContent$10() {
							return load$10().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$10() {
							return (await import('./chunks/index.6b968521.mjs'));
						}
						function Content$10(...args) {
							return load$10().then((m) => m.default(...args));
						}
						Content$10.isAstroComponentFactory = true;
						function getHeadings$10() {
							return load$10().then((m) => m.metadata.headings);
						}
						function getHeaders$10() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$10().then((m) => m.metadata.headings);
						}

const __vite_glob_0_212 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$10,
  file: file$10,
  url: url$10,
  rawContent: rawContent$10,
  compiledContent: compiledContent$10,
  default: load$10,
  Content: Content$10,
  getHeadings: getHeadings$10,
  getHeaders: getHeaders$10
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$$ = {"title":"Technical Writing: A Developer's Guide to Storytelling","description":"A complete developer's guide with tips and tricks for implementing storytelling into technical writing and documentation. Read more here.","date":"2022-03-14T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1647092465/blog/2022/03/technical-writing-a-developers-guide-to-storytelling/storytelling-in-technical-blog-posts%402x.jpg","authors":["bekah-hawrot-weigel"],"category":"best-practice","tags":["technical-writing"],"seo":{"title":"Technical Writing: A Developer's Guide to Storytelling","description":"A complete developer's guide with tips and tricks for implementing storytelling into technical writing and documentation. Read more here."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454051/blog/technical-writing-a-developers-guide-to-storytelling/ograph.png"},"shorturls":{"share":"https://dpgr.am/66fd1e9","twitter":"https://dpgr.am/d61100d","linkedin":"https://dpgr.am/45312be","reddit":"https://dpgr.am/ec7d7cf","facebook":"https://dpgr.am/7419fdb"}};
						const file$$ = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/technical-writing-a-developers-guide-to-storytelling/index.md";
						const url$$ = undefined;
						function rawContent$$() {
							return "When I went to college, I wanted to be a Math major. Calculus II wasn’t offered my first semester, so I took *British and American Authors* instead. I have always loved reading, and to be able to talk about stories and share that experience with others made it a natural fit as my major. That chance class led to my ten years of teaching college English before coming into tech. Although I enjoyed reading novels and stories, I didn’t see the practicality in creative writing until I started learning how to write screenplays. That’s when I dove into *[The Writer’s Journey](https://en.wikipedia.org/wiki/The_Writer%27s_Journey:_Mythic_Structure_for_Writers)* by Christopher Vogler and learned more about why we tell stories, why we want to hear stories, and the connections we make through storytelling. And then suddenly I saw stories everywhere.\n\nAs I started teaching Technical Communication, I saw a lot of examples of technical writing that hadn’t seen the value of storytelling. In fact, my students probably would’ve called them “dry,” “boring,” or “personality-free.”  But that doesn’t mean that technical writing *should* be that way. Using storytelling in technical writing can actually enhance the effectiveness of your writing and help your audience to better comprehend and remember your content.\n\n## Why should we incorporate storytelling?\n\nIn my [last post on Technical Writing](https://blog.deepgram.com/technical-writing-a-beginners-guide/), we covered the basics. Adding storytelling makes your writing memorable. Storytelling, at its core, is about sharing: sharing your experience, your knowledge, the things that have inspired you. When we share stories, we’re inviting people to participate in the experience, and that’s why stories are so impactful. They allow us to relate to each other, better understand concepts, and acknowledge the voices of those around us. Because stories encourage personal connection, they’re an effective method for engaging an audience and helping them to remember the content we’re sharing.\n\nIf we look at this from [a neuroscience perspective](https://contentmarketinginstitute.com/cco-digital/april-2019/storytelling-neuroscience-joe-lazauskas/), storytelling is effective for two reasons:\n\n1. Humans are hard-wired to respond to storytelling. Our brains light up when we hear stories, which creates the connections we need to remember them.\n2. Oxytocin - a neurochemical that helps us to feel connected with others - is released when we hear and connect to stories.\n\nIt should be no surprise, then, that storytelling helps to connect us to our audience.\n\nAccording to Paul J. Zak in *[Why Your Brain Loves Good Storytelling](https://hbr.org/2014/10/why-your-brain-loves-good-storytelling)*, “When you want to motivate, persuade, or be remembered, start with a story of human struggle and eventual triumph. It will capture people’s hearts – by first attracting their brains.”\n\n## How to Storytell Effectively\n\nWe should think of storytelling as a decoration for our writing. Too much decoration and the reader won’t be able to see the main point or will get frustrated trying to find it and quit reading. Too little storytelling and your reader is less likely to be invested. Have you ever landed on a blog post for a recipe but the writer tells you the whole history of their grandma’s kitchen before getting to the actual recipe? That is *not* what we want in our writing. We want to grab the reader’s attention with the story, but make sure we’re not waiting to long to get to the point. And that means knowing your audience and the purpose of your writing.\n\n### Audience\n\nIn the [last post](https://blog.deepgram.com/technical-writing-a-beginners-guide/), we covered audience, but it’s important to note that *which* story you choose will largely depend on your audience. If you think about *who* your audience is, you can more effectively choose which story or example to tell. For example, if you’re writing for a general audience, you’ll need to choose a story that most of the readers can relate to. I have a talk on mentorship where I look at the mythological journey of the hero and the impact of the mentor. My go-to references are to *Lord of the Rings*, *Star Wars*, and *Harry Potter* and the mentors the heroes find in Gandolf, Yoda, and Dumbledore. They’re popular enough that I get broad understanding and also the connection that those characters have with readers.\n\n### Purpose\n\nWe should never tell a story for the sake of telling a story. We gain reader trust by giving them a meaningful story that enhances the writing and illuminates our main ideas. It should be like a piece of thread that fits into the pattern and helps to hold the cloth together. In my mentor talk, I don't just name drop the movies and mentors, I still connect it to the purpose of the talk and provide context for the examples and explanation of how those paths to the mentors relate to paths we can create for our teammates, mentees, and people coming into tech. One trick to determine if the story contributes to the purpose of your writing is to remove the story and see if your writing loses meaning. If it does, chances are you’ve used the story well.\n\n## Ways to Implement Storytelling in Your Writing\n\nDepending on the type of writing you’re doing, the approach will vary. For example, it’s more natural to see a story integrated in a blog post. We expect this in many of the blogs we read. However, we might not expect it from technical documentation or in user manuals. Here are some options for using storytelling in your writing:\n\n**Hook**: The hook is usually the beginning of the writing, in the introduction - the first paragraph. This is where you want to pull the reader into your writing and the journey you’re going to take them on. Let the reader connect through a story in your opening. If you're writing about a coding concept, you might begin with where you first learned the concept or the first time you used it well.\n\n**Examples**: If you’re working on technical documentation, storytelling throughout your writing might not seem natural, but if you’re providing examples, this is a great place to introduce the example with a story or to use the example to tell a story. Choosing examples that are meaningful to your audience encourages a more personal connection.\n\n**Response Elements**: If you’re writing a technical document and conclude each page with a knowledge check, a way for the reader to engage, or an element that asks the reader for a response, you have an opportunity for adding micro-stories or elements of storytelling to engage the reader.\n\n**Gamification**: Technical writing can be fun too! Look at the document as an opportunity for taking the reader through a journey, or providing a gamefied learning experience. You can make them the hero in the story as they make the journey through documentation.\n\n**Theme**: When I was writing a lot of [Cypress](https://www.cypress.io/) tests, I would frequently name things after characters from the tv show [Parks and Recreation](https://www.imdb.com/title/tt1266020/). In a sense, I wasn’t telling a story, but I was bringing in storytelling through my references. Beyond pop culture, other themes for writing can be derived from shared identities, cultures, and communities.\n\n**Visuals**: The Henrik Ibsen quote \"A picture is worth a thousand words\" tells how powerful images can be. Images can be woven into your writing and provide an opportunity for visual storytelling. Kevin's post on [\"Building a Live Transcription Badge with Deepgram\"](https://blog.deepgram.com/live-transcription-badge-video/), is an introduction to the video with essential links. However, if you look at [his incredibly popular tweet](https://twitter.com/_phzn/status/1478504862170161152?s=20&t=uNmXTFv2kvTpml4CH19sYQ) that he references, you can see the added value of the initial short video.\n\nWriting, of course is a skill learned on its own aside from technical knowledge. One of my favorite essays on writing is Anne Lamott's [\"Shitty First Drafts\"](https://wrd.as.uky.edu/sites/default/files/1-Shitty%20First%20Drafts.pdf) from *Bird by Bird* because it captures the process of writing drafts and making changes so well. I first read the essay when I was doing my undergraduate degree, and I taught it to my undergraduates when I was still teaching. When you're incorporating storytelling into your writing, it will be a process. You'll have to decide what story you want to tell, how to tell the story, and where you want to tell the story. And then you'll evaluate if you've chosen the best path for the story, if you need to cut it down, if you've missed the purpose and connection. It's worth it though. Next week, we'll continue diving into finding your writing voice when I cover honesty and ethics in our writing.\n\nIf you're enjoying this series, be sure to check out the Twitter Spaces we're doing by following us on Twitter at [@DeepgramDevs](https://twitter.com/DeepgramDevs).";
						}
						async function compiledContent$$() {
							return load$$().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$$() {
							return (await import('./chunks/index.9d51169d.mjs'));
						}
						function Content$$(...args) {
							return load$$().then((m) => m.default(...args));
						}
						Content$$.isAstroComponentFactory = true;
						function getHeadings$$() {
							return load$$().then((m) => m.metadata.headings);
						}
						function getHeaders$$() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$$().then((m) => m.metadata.headings);
						}

const __vite_glob_0_213 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$$,
  file: file$$,
  url: url$$,
  rawContent: rawContent$$,
  compiledContent: compiledContent$$,
  default: load$$,
  Content: Content$$,
  getHeadings: getHeadings$$,
  getHeaders: getHeaders$$
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$_ = {"title":"Technical Writing: Accessible Writing for Developers","description":"Accessible writing creates a more inclusive space, reduces gatekeeping, and provides clarity for your readers.","date":"2022-03-28T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1648142030/blog/2022/03/technical-writing-accessible-writing-for-developers/accessible-writing%402x.jpg","authors":["bekah-hawrot-weigel"],"category":"tutorial","tags":["technical-writing"],"seo":{"title":"Technical Writing: Accessible Writing for Developers","description":"Accessible writing creates a more inclusive space, reduces gatekeeping, and provides clarity for your readers."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454053/blog/technical-writing-accessible-writing-for-developers/ograph.png"},"shorturls":{"share":"https://dpgr.am/a6a5087","twitter":"https://dpgr.am/86cd2aa","linkedin":"https://dpgr.am/74b594a","reddit":"https://dpgr.am/129c6dc","facebook":"https://dpgr.am/c47d954"}};
						const file$_ = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/technical-writing-accessible-writing-for-developers/index.md";
						const url$_ = undefined;
						function rawContent$_() {
							return "Our primary goal as technical writers is to be understood. It’s not to sound smart, be elite, be gatekeepers of knowledge. It’s to convey information to our readers in a way that’s effective for understanding. Beyond that primary goal, there are other benefits. Using strategies to make your writing accessible makes you more likely to retain your audience, have a broader audience, and have readers return to your content. We’ve talked about some of the concepts in the post in [Technical Writing: A Beginner’s Guide](https://blog.deepgram.com/technical-writing-a-beginners-guide/), but in this post about accessible writing, we’ll take a deeper look at understanding our audience, our word choice, and even go into sentence structure.\n\n## Audience\n\nAs developers, we’re frequently writing for a general audience. Many of our readers aren’t going to be native English speakers, may be neurodiverse, or may be beginners in the field. Below are some ways to make technical writing accessible to a general audience.\n\n## Word Choice\n\n**Jargon**: Words often specific to a field that wouldn’t be understood by a general audience or those new to the field.\n\nOne example of tech jargon is “rubber ducking.” What does that mean? Well, it’s a shortened phrase for “rubber duck debugging” that comes from [*The Pragmatic Programmer* by Andrew Hunt and David Thomas](https://en.wikipedia.org/wiki/The_Pragmatic_Programmer).  Debugging means exploring why there’s a “bug” or problem preventing the code from running as expected. As a concept, rubber ducking means explaining aloud your code line by line to help you to identify the problem and find a solution. Developers sometimes have a toy rubber duck to talk to as they work through a coding problem.\n\n![Image of a rubber duck](https://res.cloudinary.com/deepgram/image/upload/v1648142036/blog/2022/03/technical-writing-accessible-writing-for-developers/rubber-duck.jpg)\nPhoto by [S. Tsuchiya](https://unsplash.com/@s_tsuchiya?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n\n**Acronyms**: Words formed from the beginning letters of a group of words. For example, UX/UI is an acronym for User Experience/User Interface, which could also be considered jargon. User Experience Design focuses on identifying and solving user problems. User Interface Design focuses on creating an intuitive design that a user will interact with.\n\n**Numeronym**: A numeronym is a word that contains a number either to represent a sound (k9 means canine) or to represent a number of letters that have been replaced to abbreviate a word. Most commonly, we see A11y represent the word Accessibility in tech. The eleven omitted letters between the ‘A’ and ‘y’ are represented by 11.\n\n## Organization\n\nThe organization plays a significant role in creating technical writing that your readers can more easily understand. Think about it this way: if each of your ideas or concepts are a puzzle piece and they’re all mixed up, it doesn’t make sense or form a beautiful picture. But once those pieces are put together, you can see the image and how each piece completes the puzzle. In the same way, each of our paragraphs is a piece of the puzzle that should help form a complete picture. They should hold a particular spot in your writing that helps to support the overall main idea.\n\n## Structure\n\nBreaking down your writing into paragraphs with headings and subheadings is another way to make your writing accessible. A long paragraph can deter your reader, especially if they have focus issues or visual impairments. Having a clear structure can also help ensure you have a well-organized piece of writing. Your headings can act as a type of outline for your technical writing.\n\n## Give Clear Directions\n\nIf you want your reader to perform an action, clearly tell them what you want them to do. For example, if you’re telling them to copy code and add it to an existing file in a repository, where do you want them to put that code? Should they copy the entire code block? Are they replacing any of the existing code?\n\n## Link Text\n\nIf you have link text, is it meaningful? Does it say “Read More” or “Read more about Accessible Writing”? There should be a way to differentiate your links on the page. You can use the title of the page or the subject to provide specificity.\n\n## Assessing your Writing\n\nCreating accessible content can be hard when you’re not used to it. Here are some tips to help determine if your writing is accessible:\n\n* Scan your document to see if there’s a clear sense of purpose and main ideas. This can help you determine if your section headings are clear and if your paragraphs are well organized.\n* Give your draft to a reader in your target audience. Are they able to understand and comprehend your writing?\n* Define terms that aren’t recognizable to your audience.\n* Proofread your writing or use a tool that helps to check grammar, sentence structure, and clarity.\n\n## Resources\n\n* The World Wide Web Consortium (WC3) provides [tips and instruction for how to write accessibly](https://www.w3.org/WAI/tips/writing/)\n* [ASE Simplified Technical English](https://asd-ste100.org/about.html) works to make “technical texts easy to understand by all readers.”\n* Amy Dickens's talk [“Improve Your Writing Using Accessible Language”](https://yougotthis.io/talks/improve-writing-using-accessible-language/) for [You Got This](https://yougotthis.io/) shares the importance of using accessible language and strategies and tips to do that.\n* [Jargon File](http://www.catb.org/jargon/): A site dedicated to hacker jargon.\n\nMaking technical writing accessible helps remove barriers to entry for learning new concepts and creates a more inclusive space for everyone. This post is an introduction to accessible writing for the web. If you want to know more about accessible writing, please check out the resources above, and hit us up with any questions or suggestions on our [@deepgramDevs Twitter account](https://twitter.com/DeepgramDevs).";
						}
						async function compiledContent$_() {
							return load$_().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$_() {
							return (await import('./chunks/index.df480aad.mjs'));
						}
						function Content$_(...args) {
							return load$_().then((m) => m.default(...args));
						}
						Content$_.isAstroComponentFactory = true;
						function getHeadings$_() {
							return load$_().then((m) => m.metadata.headings);
						}
						function getHeaders$_() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$_().then((m) => m.metadata.headings);
						}

const __vite_glob_0_214 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$_,
  file: file$_,
  url: url$_,
  rawContent: rawContent$_,
  compiledContent: compiledContent$_,
  default: load$_,
  Content: Content$_,
  getHeadings: getHeadings$_,
  getHeaders: getHeaders$_
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$Z = {"title":"Technical Writing: Ethics and Honesty for Developers","description":"Ethics protects your credibility as a writer and developer and helps to create trust with your readers. Read more about technical writing ethics here.","date":"2022-03-21T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1647623514/blog/2022/03/technical-writing-ethics-for-developers/ethics-in-technical-blog-posts%402x.jpg","authors":["bekah-hawrot-weigel"],"category":"best-practice","tags":["technical-writing"],"seo":{"title":"Technical Writing: Ethics and Honesty for Developers","description":"Ethics protects your credibility as a writer and developer and helps to create trust with your readers. Read more about technical writing ethics here."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454055/blog/technical-writing-ethics-for-developers/ograph.png"},"shorturls":{"share":"https://dpgr.am/18bea59","twitter":"https://dpgr.am/9a4fcc8","linkedin":"https://dpgr.am/aca0665","reddit":"https://dpgr.am/2c2a423","facebook":"https://dpgr.am/8784580"}};
						const file$Z = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/technical-writing-ethics-for-developers/index.md";
						const url$Z = undefined;
						function rawContent$Z() {
							return "When I taught Rhetorical Argument, we talked about the concept of the rhetorical triangle. The rhetorical triangle is based on Aristotle’s teaching that you need to appeal to your audience in three primary planes: ethos, pathos, and logos. When you approach an argument in this way, you’re more likely to convince your audience.\n\n- - -\n\n## Definitions\n\n**Ethos**: The Greek word for “character,” ethos reflects the credibility of the speaker.\n\n**Pathos**: Emotion. Something that evokes feelings. The Greek word pathos means \"suffering,\" \"experience,\" or \"emotion.\"\n\n**Logos**: Logic or meaning.\n\n- - -\n\nIn some ways, this series of posts on writing covers a piece of each of these concepts. In my [last post on storytelling](https://blog.deepgram.com/technical-writing-a-developers-guide-to-storytelling/), we talked about enhancing your writing with storytelling, which we can also see as an appeal to *pathos* as we form a connection to our audience. In the [first post on writing](https://blog.deepgram.com/technical-writing-a-beginners-guide/), we learned how to approach writing. In that post, I was appealing to writing knowledge and facts, which aligns with appealing to *logos*. In this post on ethics, we’re going to learn more about--you guessed it--*ethos*.\n\nThe word “ethics” stems from “ethos.” When we talk about ethics in technical writing, we’re completing the rhetorical triangle approach and adding another layer to how we write and think about writing. Ethics protects your credibility as a writer and helps to create trust with your readers.\n\n## What is Ethics in Technical Writing?\n\nAt the very least, ethics in writing means that you’re honest with your audience, you represent your evidence accurately by avoiding exaggeration or disinformation, and you give credit to the resources you use to complete your writing.\n\nThink about it from a reader’s perspective: you would want someone to give you credit if they use your idea, even if that’s in a tweet. You read articles and resources to gain knowledge, help you formulate opinions, and create strategies for problem-solving.\n\nIt is also worth noting that because something is legal, it doesn’t mean that it’s ethical. Ethical dilemmas in technical writing can stem from pressure from an employer, a short deadline, or by not thinking critically about our topics as we're writing. So what are some ways to ensure that your technical writing is ethical? Well, you can get started by evaluating your resources and your writing.\n\n### Questions to Consider in Ethical Writing\n\n* What information have you included? Does it represent the topic honestly and without trying to “trick” the reader?\n* What words have you used? Do they convey an accurate meaning?\n* Have you felt pressured to represent your topic in a certain way?\n* Have you given credit to resources you’ve used through linking, citations, or another form of attribution?\n* Have you given false information?\n* Have you concealed the truth?\n* Have you misused information?\n* Have you exploited cultural understanding to mislead your audience?\n\n### Don’t Be Deceptive\n\nThis might seem like a clear statement, but it means more than “don’t lie.” Avoiding deceptive writing means that you do your best to represent an idea without blurring any lines, leaving out pertinent information, or misrepresenting data.\n\nFor example, let’s say you’re writing about a particular framework, and you want your readers to think it’s the *best* technology in its category. You know that someone in high standing in the tech industry has said that framework is the best of all. So you add that into your blog post. That doesn’t *seem* deceptive, right? Here’s the catch: that tech person made that statement a year ago and has recently said that a new framework is even better. Despite knowing this, you use the quote to support your argument.\n\nAlthough you didn’t lie, and it’s not illegal, it’s still unethical because you deceive your audience.\n\n### Give Credit to Referenced Authors\n\nIn the tech world, where people often talk about copying and pasting code into their repositories from other resources, it might seem unnatural to credit others for their ideas and content. However, you should recognize the author of the information you’re using if at all possible in your writing. You can do this by linking to the resource, using a footnote that shares the resource, or by listing the source you’ve referenced at the end of your writing. You should do your best to make it clear what information you’ve used from the source.\n\n## Avoiding Unintentional Ethics Violations\n\nSome of the most common ethics violations in writing are unintentional. You might be curious *how* someone could do that. Presenting misinformation that’s been passed along to you or believing that you’re giving information truthfully when [you don’t recognize how your biases are impacting your judgment](https://open.library.okstate.edu/technicalandprofessionalwriting/chapter/chapter-4/) are examples of unintentional ethical violations. Here are some questions to help you avoid ethical violations:\n\n* Have I used reliable resources?\n* Have I evaluated my own bias relating to this subject?\n* Have I checked that the information provided to me is accurate?\n* Have I represented the research and opinions of others accurately?\n* Have I considered my primary audience and how they’ll interpret the information I’ve presented?\n* Have I disclosed any sponsorships or rewards I’ll receive for posting and/or linking information?\n\nLet’s go back to the example above. What if you didn’t know that the tech person came out in support of another framework? What if you just remembered that they shared that opinion? In cases like these, it’s helpful to do a quick search to verify that the information is up to date and accurate.\n\nYou can still update the content if you’ve already posted that blog. It’s also good practice to point out that you’ve edited your post and point to where and why that occurred.\n\n## Conclusion\n\nThis post, along with the previous two on writing, should give you an overview of how to approach writing and, hopefully, think a little deeper about the words you put on the page. That’s not to say overthink it. Writing requires practice and effort, but it’s a discovery to find your voice and a voice that your readers can connect to and trust. When you think deliberately about ethical technical communication, you're taking a step to improve your own writing. Next week, we’ll continue our writing journey and learn more about making your writing accessible.\n\nIf you have any questions, or you’d like to hear more about finding your voice, you can hit us up on Twitter and listen to our Tuesday Twitter Spaces, at [@DeepgramDevs](https://twitter.com/DeepgramDevs)";
						}
						async function compiledContent$Z() {
							return load$Z().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$Z() {
							return (await import('./chunks/index.907802bd.mjs'));
						}
						function Content$Z(...args) {
							return load$Z().then((m) => m.default(...args));
						}
						Content$Z.isAstroComponentFactory = true;
						function getHeadings$Z() {
							return load$Z().then((m) => m.metadata.headings);
						}
						function getHeaders$Z() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$Z().then((m) => m.metadata.headings);
						}

const __vite_glob_0_215 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$Z,
  file: file$Z,
  url: url$Z,
  rawContent: rawContent$Z,
  compiledContent: compiledContent$Z,
  default: load$Z,
  Content: Content$Z,
  getHeadings: getHeadings$Z,
  getHeaders: getHeaders$Z
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$Y = {"title":"Text Cleaning for ASR: The Case of Turkish","description":"Text cleaning can be challenging in any language—and Turkish is no exception! Let’s look at what text cleaning is and how it works.","date":"2022-08-30T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981434/blog/text-cleaning-asr-turkish/the-case-of-turkish-thumb-554x220-1.png","authors":["duygu-altinok","morris-gevirtz","chris-doty"],"category":"linguistics","tags":["deep-learning","language","nlu"],"seo":{"title":"Text Cleaning for ASR: The Case of Turkish","description":"Text cleaning can be challenging in any language—and Turkish is no exception! Let’s look at what text cleaning is and how it works."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981434/blog/text-cleaning-asr-turkish/the-case-of-turkish-thumb-554x220-1.png"},"shorturls":{"share":"https://dpgr.am/be99ec0","twitter":"https://dpgr.am/bd774ae","linkedin":"https://dpgr.am/0cd3f95","reddit":"https://dpgr.am/61d0221","facebook":"https://dpgr.am/ad549d8"}};
						const file$Y = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/text-cleaning-asr-turkish/index.md";
						const url$Y = undefined;
						function rawContent$Y() {
							return "Since Turkey is celebrating [Victory Day](https://en.wikipedia.org/wiki/Victory_Day_(Turkey)) today, and since we've been meaning to write a blog post about text cleaning, we figured we might as well write a post about text cleaning and Turkish! For every automatic speech recognition (ASR) system, cleaning the training data is a crucial part. Speech recognition systems learn how to map speech to its written counterpart by learning speech-text pairs. If the data that teaches it how to do this is messy, the system just won't work well. In this post, we'll explain what text cleaning is, how it works, and why it's so important for ASR. Let's look!\n\n## What is Text Cleaning?\n\nSimply put, text cleaning (also called text normalization) is a component of natural language processing that takes raw data and neatens it up, usually in order to use it for something like machine learning model training. While preparing our training data, we'd like the transcriptions to match the spoken phoneme sequences as much as possible. For example, if we hear the words \"J. Lo\" in the speech file, we'd like its transcription to be \"J. Lo\" as well. However, if the speech file includes the words \"Jennifer Lopez\", we wouldn't want the transcription to be \"J. Lo\"; we'd want it to be \"Jennifer Lopez\". Although \"Jennifer Lopez\" and \"J. Lo\" represent the same entity in the real world, phonetically they don't match at all. The former word is represented by the sequence of phonemes\n\n> JH EH N AH F ER . L OW P EH Z \n\nwhere the latter word sequence maps to a much shorter phoneme sequence\n\n> JH EY . L OW\n\nAll of this logic also applies to numbers, abbreviations, email addresses-i.e., nonstandard tokens, where we can end up with written text that doesn't necessarily match the pronunciation, need to match up to what's actually being said, and not the real-world entity. \n\n## Why Text Cleaning is So Important\n\nThere's an old saying in data science: garbage in, garbage out. This means that if you train a model with bad data, your model isn't going to be very good.  We want to make sure we have a good match between the phonetics-the actual speech sounds produced-and the phonetic transcription of that speech.\n\nThis is the first step in getting an accurate, readable transcript at the end of the transcription process. This is why text cleaning is so important for ASR training. By doing text cleaning before training, you give your model the best possibility of producing what you want. Let's take a look at an example of how text cleaning actually works.\n\n## How Text Cleaning Works\n\nText cleaning is language-dependent. This is because abbreviations, shortenings, and other special text forms have a language-unique relationship to their spoken tongue. For example, \"lb\" and its relationship to \"pound\" in English isn't at all obvious if you didn't already know about it, and isn't relevant to any other language. To apply text cleaning to training data, we need a multi-step processing pipeline. Each step transforms the text into a \"cleaner\" variation of the original.\n\nBy \"cleaner\" here we mean \"closer to the actual phonetics of what was said\". Take a look at Figure 1, below. There you can see an example sentence which is transformed, step by step, into a \"cleanest\" final version. It starts in the usual written form in Step 1, but that doesn't accurately represent the phonetics of what's being said, so we need to clean it up if we want to use it for model training. In Step 2, we spell out the numbers into words, rather than numerals. And in Step 3, we spell out the components of an email* (which, when said, is a string of words, just as it is in English-\"duygu at deepgram dot com\"). In the final step, Step 4, we remove the double period typo so that the sentence has [standard punctuation](https://blog.deepgram.com/complete-guide-punctuation-capitalization-speech-to-text/). \n\n![](https://res.cloudinary.com/deepgram/image/upload/v1661981074/blog/text-cleaning-asr-turkish/blog-image-1-1.png)\n\n**Figure 1.** Text cleaning for the Turkish sentence \"I sent an email to the email address `hello@company.com` yesterday.\"\n\n## Challenges for Turkish Text Cleaning\n\nIn the above example, we mostly saw things that would also make sense in other languages-like reformatting an email address into words. But there are unique text cleaning challenges for each and every spoken language, and Turkish is no exception. Let's consider a few, Turkish-specific examples.\n\n### The Turkish Apostrophe\n\nOne challenge that's specific to Turkish, and which we saw an example of in Figure 1 above, is with the use of the apostrophe. The apostrophe is used often in written Turkish, and is used for separating inflectional suffixes from the proper nouns, numbers, and abbreviations that they're attached to. We need to keep an eye on this when we do text cleaning because some of the changes that we make to the text will also influence whether or not we need to get rid of the apostrophe. For example, when we turn a numeral into a common noun (that is, we turn *3* into *üç)* we no longer need the apostrophe after we spell it out as text. In the above example, we saw the token *17.30'da* became *beş buçukta* and not *beş buçuk'ta;* we omitted the apostrophe in order to keep the grammaticality. However, proper nouns will still stay as proper nouns, no matter what text cleaning we do, so they should never lose an apostrophe. But what if our proper noun has a number in it? In Figure 2 below, the apostrophe is used to separate a proper noun from an inflectional suffix, and hence a good question arises: should we keep the apostrophe or not? \n\n![](https://res.cloudinary.com/deepgram/image/upload/v1661981075/blog/text-cleaning-asr-turkish/blog-image-2.png)\n\n**Figure 2.** Example of a possible text cleaning for the Turkish sentence \"150 accidents happened in the first 1000 km of the road E-5.\" \n\nIn Figure 2, we deleted the second apostrophe safely. However, keeping the first apostrophe creates a not-very-good looking token: *beş'in* (which should be *beşin* according to Turkish orthography). Here, *5* is part of an entity-the name of a road-and also a number itself. On the other hand, *beş* 'five' is just an ordinary noun and *beş'in* is an ordinary noun with an apostrophe (which is not quite grammatical). So what's the solution here? It totally depends on the model vocabulary of the ASR model that one wants to use. Here, it's indeed better to keep *E-5* intact and not normalize the *5* to *beş* because it's part of an entity. Hence, the final cleaning result should look like this:\n\n> ***E-5'in*** *ilk bin kilometresinde sadece geçen yıl yüz elli kaza oldu.* 150 accidents happened in the first 1000 km of the road E-5.\n\nLong story short, we need to pay attention to apostrophes to do text cleaning in Turkish correctly. Sometimes we delete them, sometimes we keep them; the important part is to get grammatically correct sentences even after the cleaning and to do so consistently.\n\n### Another Apostrophe Challenge\n\nBecause the apostrophe comes hand in hand with inflectional and derivational suffixes, we also have to pay attention to phonological processes such as consonant assimilation and vowel harmony rules. Let's look at what those terms mean, and an example of each. Consonant assimilation refers to two consonants becoming more similar to each other when they occur next to each other. We can see an easy example of this in English with the words *cat* and *dog*. When we add the plural *\\-s* to these words, we pronounce them differently based on the final sound: like an /s/ in *cats* but like a /z/ in *dogs*. (This is specifically an example of [voicing assimilation](https://en.wikipedia.org/wiki/Consonant_voicing_and_devoicing#Voicing_assimilation) if you're curious.) \n\nThis consonant assimilation in English is pronounced but not written. Modern Turkish writing, however, does reflect consonant assimilation when it occurs. And when it comes to consonant assimilation in Turkish, we're actually sort of lucky because even if consonant assimilation occurs with a suffix, the apostrophe is still used.\n\n> *Dün saat* ***3'te*** *beni görmeye geldi.* He came to visit me yesterday at 3PM.\n\nIn this example, the first consonant of the suffix /-dA/\\*\\* has assimilated and become /te/ after *üç* 'three', so we don't need to do any extra processing after deleting the apostrophe in this case. So far so good, but now let's look at a trickier case with a derivational suffix and abbreviation combo, along with a typo. One of the main issues that comes up with text cleaning is that you often can't assume that your input text follows the normal rules of grammar and orthography, so you have to do some checking.\n\n> *Yrd doçluk sınavı kalkmış.* *Yrd doç luk sınavı kalkmış.* The entrance exam for assistant professorship is canceled.\n\nIn both sentences, the words *yrd* and *doç* are abbreviations. *Yrd* is short for *yardımcı* 'assistant' and *doç* is short *doçent* 'professor', so *yrd doç* is a bit like *asst prof*. Here, there are no apostrophes between *doç* and *luk* at all to indicate a suffix is appended to an abbreviation. In the first sentence, the spelling *doçluk* is not great grammatically but still acceptable, while in the second sentence, the two words *doç* and *luk* should not be written separately. In perfect written Turkish, they should be written together, but with an apostrophe, like this: *Yrd doç'luk sınavı kalkmış.* Figuring out what is going on with either of the two original sentences could be difficult, but fortunately we have a hint-*luk* is not a Turkish word; however, it is a suffix. And, what's more, it is following the rules of Turkish vowel harmony here, as it harmonizes with the vowel of the previous word *doç*. [Vowel harmony](https://en.wikipedia.org/wiki/Vowel_harmony), *very* briefly, is a process that forces all of the vowels in a word to be part of the same set. Because /u/ and /o/ are part of the same set, we can conclude the second sentence is a misspelling and we can correct this mistake by deleting the space in between to get a \"cleaner\" version: *Yrd doçluk sınavı kalkmış.*\n\n### Processing Currencies in Turkish\n\nPhew, that was a lot! The good news is, we can relax a bit when processing some other types of tokens, though; processing currency symbols, for example, is quite easy. Unlike English, in Turkish we don't append a plural suffix to currencies whose value is more than 1. Please compare: \n\n![](https://res.cloudinary.com/deepgram/image/upload/v1661981076/blog/text-cleaning-asr-turkish/blog-image-3-1.png)\n\nHence, a simple piece of code is enough to clean up currencies. We first parse out the currency symbols with a tiny regex, look them up from a predefined dictionary of currency words, and finally replace them in the text. The code for this is below in Figure 3. \n\n```python\nimport re\n\nCURR_SYMS = {\n\"$\": \"dolar\",\n\"€\": \"euro\",\n\"£\": \"sterlin\",\n\"¥\": \"yen\",\n\"tl\": \"lira\",\n\"ytl\": \"lira\",\n\"₺\": \"lira\"\n}\n\nCURR_REGEX = r\"([$€£¥₺]|y?tl|try)\"\n\ndef convert_currency_syms(token):\n  token = token.lower()\n  words = CURR_SYMS.get(token, token)\n  return words\n\ndef clean_currency_symbols(sentence):\n  match = re.search(CURR_REGEX, sentence)\n  while match:\n    mstring = match.group()\n    mstart, mend = match.span()\n    sentence = sentence[:mstart] + \" \" + convert_currency_syms(mstring) + sentence[mend:]\n    match = re.search(CURR_REGEX, sentence)\n  return sentence\n\nsent = \"Hepsine 100$ verdim.\"\n>> clean_currency_symbols(sent)\n\"Hepsine 100 dolar verdim.\"\n```\n\n**Figure 3.** A sample code for running a regex to get correct Turkish currencies. \n\nWe literally did **nothing** to currency words; no plural suffix or any other suffixes were added by us. If we were processing English, though, we would need to parse the currency symbol together with the number to check if the number is 1 or not; if it's not 1, we need to add a plural suffix to the currency word. Please note that the code segment in Figure 3 shows only one step of processing, that related to processing the currency symbols. As we mentioned previously, all conversions are done in individual steps, hence the number *100* would be processed in a separate number-cleaning step.\n\n### Text Cleaning for Numbers\n\nNumbers can introduce other challenges as well. For example, converting ordinal cardinal and decimal number tokens into words; as well as converting times, dates, telephone numbers, postal codes, and other sorts of number tokens into words.  Parsing number-type tokens can be tricky; numbers come in different shapes. There are decimal numbers (which include commas), big numbers (which include periods), phone numbers (including parenthesis, dashes, plus signs, and even more), postal codes (address strings should be extracted first to spot them), and so on. \n\nCleaning numbers is challenging for each language indeed. Quite a number of regexes and other sorts of parsers (including NER and CF parsers) are used. For an example, this regex should parse out big numbers (such as 1,250,000) in English text: `\\b\\d{1,3}(,\\d{3})+\\b` Big numbers are written similarly in Turkish, but with a period instead of comma; hence, a tiny modification to this regex would suffice: `\\b\\d{1,3}([.]\\d{3})+\\b` It's totally expected, when we're building text cleaning pipelines, that we implement both language-specific and some \"universal\" cleaning methods. For each language we process, we find common token types and processing methods, but at the same time, we implement quite a lot of language specific text cleaning methods as well.\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-how-deepgram-works\"></WhitepaperPromo>\n\n## Wrapping Up\n\nWhen it comes down to it, all languages are beautiful and yet challenging at the same time; dealing with natural language is never easy. At Deepgram, we all enjoy working with speech and text, and we embrace both the difficulty and the beauty of natural language. We hope you enjoyed this article and share our joy for Turkish, too! Please join us for more content on our blog and follow us on [LinkedIn](https://www.linkedin.com/company/deepgram/).\n\nAnd, if you'd like to give Deepgram a try for yourself, [sign up for Console](https://console.deepgram.com/signup) for free and get $150 in free credits to give it a try. \n\n- - -\n\n\\* Of course, sometimes we might *want* an email to appear in a final transcript. In that case, we can do a bit of postprocessing to the model output to get the final transcription in order to reverse normalize *...-at-...-dot-com* word sequences into a single email token. But for the purposes of this post, because we're looking at the phonetic level, we'll want to represent the pronunciation rather than what we want the final output to look like. \n\n\\*\\* The /d/ becomes /t/ due to assimilation with the preceding sound, and the A represents a vowel that changes depending on vowel harmony, which in this word, becomes /e/. Thus, we end up with /te/ for the suffix.";
						}
						async function compiledContent$Y() {
							return load$Y().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$Y() {
							return (await import('./chunks/index.ddf0b7ee.mjs'));
						}
						function Content$Y(...args) {
							return load$Y().then((m) => m.default(...args));
						}
						Content$Y.isAstroComponentFactory = true;
						function getHeadings$Y() {
							return load$Y().then((m) => m.metadata.headings);
						}
						function getHeaders$Y() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$Y().then((m) => m.metadata.headings);
						}

const __vite_glob_0_216 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$Y,
  file: file$Y,
  url: url$Y,
  rawContent: rawContent$Y,
  compiledContent: compiledContent$Y,
  default: load$Y,
  Content: Content$Y,
  getHeadings: getHeadings$Y,
  getHeaders: getHeaders$Y
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$X = {"title":"The Contact Center of the Future with Real-time AI","description":"What is the current state of AI in Contact Centers and what does the future look like?","date":"2021-03-12T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981363/blog/the-contact-center-of-the-future-with-real-time-ai/contact-center-of-future-realtime-ai%402x.jpg","authors":["keith-lam"],"category":"speech-trends","tags":["contact-center"],"seo":{"title":"The Contact Center of the Future with Real-time AI","description":"What is the current state of AI in Contact Centers and what does the future look like?"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981363/blog/the-contact-center-of-the-future-with-real-time-ai/contact-center-of-future-realtime-ai%402x.jpg"},"shorturls":{"share":"https://dpgr.am/c787455","twitter":"https://dpgr.am/14fc62f","linkedin":"https://dpgr.am/f700724","reddit":"https://dpgr.am/586df97","facebook":"https://dpgr.am/be5be02"}};
						const file$X = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/the-contact-center-of-the-future-with-real-time-ai/index.md";
						const url$X = undefined;
						function rawContent$X() {
							return "Imagine the [contact center](https://deepgram.com/solutions/contact-centers/) of the future where your agent is sitting in a cockpit type workstation. All relevant sales and support information for that customer is readily available and automatically sent to the agent depending on the conversation topic through an AI knowledge base and [automatic speech recognition](https://blog.deepgram.com/what-is-asr/). 360 degree awareness of the customer is easily pulled from all customer facing channels. This future is already beginning but lets first talk about current AI use in the contact center.\n\n## **Current Use AI in the Contact Center**\n\nFrom a 2020 Canam Research study, 60% of contact centers use or plan to use AI in the next 12 months. They are focusing their AI efforts on the following: \n\n![](https://res.cloudinary.com/deepgram/image/upload/v1661976837/blog/the-contact-center-of-the-future-with-real-time-ai/Screen-Shot-2021-03-09-at-3.09.37-PM.png)\n\nHowever, 22% have no plans to implement AI due to the following reasons: \n\n![](https://res.cloudinary.com/deepgram/image/upload/v1661976838/blog/the-contact-center-of-the-future-with-real-time-ai/Screen-Shot-2021-03-09-at-3.10.32-PM.png)\n\nSo, should you jump into the AI pool now? And is the value there to overcome the barriers? We would answer, yes. We believe that AI has matured in the past 5 years to be a much more stable and usable tool, so dipping your toes into AI may be a good idea. Here is what the short term future may hold for AI.\n\n## Contact Center of the Future\n\nSo what does the contact center of the future look like? Customers are contacting you through various channels like email, SMS, chat or voice, but how you handle these conversations are different. Depending on the customer base, you may have 40% of your chat, SMS, and email answered by a chatbot, 30% of voice calls answered by a [virtual agent or voicebot](https://deepgram.com/solutions/voicebots/) and only 30% actually answered by a human. All of these customer communication options are supported by an AI Knowledge Base; the main brain of your company that knows about your products, all customers, issues, and solutions. You now have 70% of your calls being handled by an AI solution with only the highly technical or difficult calls coming to a human agent. \n\n![](https://res.cloudinary.com/deepgram/image/upload/v1661976838/blog/the-contact-center-of-the-future-with-real-time-ai/Screen-Shot-2021-03-09-at-3.25.58-PM.png)\n\nWhat can you gain from going moving to AI enabled sales and support? For your enterprise, you will gain cost savings, productivity, agent satisfaction (less mundane tasks), and lower churn. For your customers, you can provide shorter wait times, faster issue resolution, and choice for sales or support, leading to higher satisfaction. Is that value worth the investment? Check out our on-demand webinar, **[How Real-time AI will Transform the Call Center](https://offers.deepgram.com/how-real-time-ai-will-transform-the-contact-center-on-demand),** to learn more about other [contact center](https://deepgram.com/solutions/contact-centers/) focused AI applications, especially ones using real-time voice conversations including:\n\n* [Virtual agents](https://deepgram.com/solutions/voicebots/)\n* Agent enablement\n* Compliance monitoring\n* Churn reduction\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-how-deepgram-works\"></WhitepaperPromo>";
						}
						async function compiledContent$X() {
							return load$X().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$X() {
							return (await import('./chunks/index.465e06e1.mjs'));
						}
						function Content$X(...args) {
							return load$X().then((m) => m.default(...args));
						}
						Content$X.isAstroComponentFactory = true;
						function getHeadings$X() {
							return load$X().then((m) => m.metadata.headings);
						}
						function getHeaders$X() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$X().then((m) => m.metadata.headings);
						}

const __vite_glob_0_217 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$X,
  file: file$X,
  url: url$X,
  rawContent: rawContent$X,
  compiledContent: compiledContent$X,
  default: load$X,
  Content: Content$X,
  getHeadings: getHeadings$X,
  getHeaders: getHeaders$X
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$W = {"title":"The Evolution of Conversational AI in the Car and Beyond - Shyamala Prayaga, Sr Software Product Manager, Ford - Project Voice X","description":"The Evolution of Conversational AI in the Car and Beyond, presented by Shayamala Prayaga of Ford, presented on day one of Project Voice X. ","date":"2021-12-09T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981401/blog/the-evolution-of-conversational-ai-in-the-car-and-beyond-shyamala-prayaga-sr-software-product-manager-ford-project-voice-x/proj-voice-x-session-shyamala-prayaga-blog-thumb-5.png","authors":["claudia-ring"],"category":"speech-trends","tags":["automotive","conversational-ai","project-voice-x"],"seo":{"title":"The Evolution of Conversational AI in the Car and Beyond - Shyamala Prayaga, Sr Software Product Manager, Ford - Project Voice X","description":"The Evolution of Conversational AI in the Car and Beyond, presented by Shayamala Prayaga of Ford, presented on day one of Project Voice X. "},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981401/blog/the-evolution-of-conversational-ai-in-the-car-and-beyond-shyamala-prayaga-sr-software-product-manager-ford-project-voice-x/proj-voice-x-session-shyamala-prayaga-blog-thumb-5.png"},"shorturls":{"share":"https://dpgr.am/a89d1a9","twitter":"https://dpgr.am/9816f53","linkedin":"https://dpgr.am/b0a85b0","reddit":"https://dpgr.am/f2686fb","facebook":"https://dpgr.am/faef5b7"}};
						const file$W = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/the-evolution-of-conversational-ai-in-the-car-and-beyond-shyamala-prayaga-sr-software-product-manager-ford-project-voice-x/index.md";
						const url$W = undefined;
						function rawContent$W() {
							return "*This is the transcript for “The Evolution of Conversational AI in the Car and Beyond,” presented by Shyamala Prayaga, Senior Software Product Manager at Ford, presented on day one of Project Voice X.*\n\n*The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was **94% accurate.**  Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram’s API.  If you want to see if Deepgram is right for your use case, [contact us](https://deepgram.com/contact-us/).*\n\n\\[Shyamala Prayaga:] Thank you so much, Bradley. Hello, everyone. I know a couple of you, and it’s great to see you guys. And for the new folks who are out there, welcome. It’s a pleasure meeting you all. My name is Shyamala Prayaga, and I am the Product Owner slash Manager for Ford Motor Company. And I lead the autonomous digital assistant effort, so I look at all of the conversational AI work for Ford. So I launched the gen four of SYNC and worked on the Alexa integration, and now working on the autonomous digital assistant. So when I’m not working, I love doing gardening, and that’s my garden on the left-hand side and the produces from my garden. And fun fact, I don’t own a pet, but I love petting dogs. So if you see me outside, you know, petting any dog, that’s because I love dogs a lot, but I don’t own one.\n\nAnyways… so conversational AI products are everywhere, and we have seen it. And it’s not new. We have been talking about conversational AI products throughout the day-to-day and how the evolution has happened, right, and all the great things happening, the technology, and all of those things. And we’ve also seen, like, conversational AI products are out there at so many different locations and areas, like, from your home to the appliances you use and also in the drive-through or kiosk. In all these areas, retail is also picking up. So conversational AI is definitely picking up a lot, and a car is not a new space as well. And we have seen voice assistant in the car. It’s interesting to look at these statistics, which says, like, out of the two fifty nine million people in the United States, one twenty seven point five… or one million folks are using voice in the car, and this is more than the smart speakers in the house or people… utility of the smart speakers.\n\nBut I would tell you one thing. Voice assistant in the car is not new, and you’ll be surprised to see the statistics and the history that voice has been there even before Amazon was launched or even before Siri was launched. So let’s look at some of those things. The first voice assistant was launched in two thousand four, and this was Honda in collaboration with IBM. And they were the first OEM to launch the voice assistant. Although it was not a full-fledged voice assistant, it was a voice navigation system. And, basically, what it did was it was more of a command and control. It allowed the user to call someone or, you know, go to a POI and stuff like that. The biggest drawback with the the system was natural language was missing, so it was more like command-based thing.\n\nSo you have to say something exactly as it is entered here, which is, like, call the name or dial number or go home, go back. So it was a lot more structured-based, command-based system, and the second drawback with the system back in two thousand four was it was too screen dependent. So people did not know how to design a multimodal system back then, so it was too much screen dependent, which means, like, you are distracted. But the whole point of voice in the car is to reduce the distraction, so you have to think about how to do that. So Honda tried, and they kept improving eventually. In two thousand seven, Ford was the one who launched SYNC in collaboration with Microsoft, and this is when they actually designed an assistant called Samantha, which was able to make calls and text and controlling music and do all sorts of things. Again, with Samantha, the big problem was, again, it was not natural language.\n\nSystem and technology were still improving, so natural language understanding was something the industry was still evolving and learning and improving. So that was another thing. So after that, not that from two thousand seventeen to two thousand thirteen, nothing happened, all the companies were trying and improving. And there were lot of other things happening, and everyone was trying their own assistant. But in two thousand thirteen, Apple changed the game when they launched CarPlay for Siri in the car, and people started using voice assistant in the car like they would be. And this is when the embedded systems, you know, technology came into picture where you can just embed your phone. So Android Auto was another thing which came eventually. So this gave people so much flexibility and freedom to use their phone in the car, like the way they use and all the things including the voice. Now two thousand thirteen… and we know two thousand fourteen was a game changer in so many ways.\n\nAnd it is worth mentioning Amazon here because this changed the entire dynamics for a lot of different industries. Amazon launched Alexa in two thousand fourteen, and after that, you know, a lot of things happened. So after the launch of Amazon, companies were still working, and there were a lot of OEMs who are trying to think about what to do. And Ford was the first company who actually integrated Alexa into this navigation system where people can… it was not exactly, like, embedded in the system. It was a cloud-based system, so…\n\n> Ford was able to make a call to the cloud-based system. Because the cars became smarter, they had connectivity, so they were able to do that. But at the same time, what happened was users were now able to say, turn on my car or open the garage or lock my house, and all the things Amazon is capable of doing from the comfort of their house, but also through the car. So that was a game changer.\n\nAnd then Amazon and Ford had a lot of other integrations as well, including FordPass and stuff like that. After that, two thousand eighteen… because by now, voice became, like, the forefront of everything. Almost everyone attempted to do something in the conversational AI space. So almost every OEM started building voice assistants in collaboration with Cerence or SoundHound or any other player out there, and that’s when you know Mercedes also launched Mercedes-Benz User Experience, which was their voice assistant. So a lot started happening. Not that Cerence did not exist before that or somehow did not exist. They were also working with these OEMS and trying to do a lot of things, and we’re improving back and forth along with the OEMS. But two thousand eighteen start… really started seeing, like, lot of more adoption and lot of more acceptance in terms of making the assistants better. Now two thousand eighteen, it was interesting because Amazon was like, we have a device at home, and then it’s great. But then now we want people to use voice in the car, and they also did some studies. And I think there was a survey they did they did with J.D. Power, where people said they want a voice assistant like they use in the home, the same assistant in the car.\n\nSo Amazon thought, what should we do? And they designed something called Echo Show, Echo Auto, which was a small device which you can plug into the car, and you can use all of the Amazon features into it. But then, the biggest drawback with this thing is, although it is amazing, it can play music, and it can turn on your garage or home control system and all of those things, it would still not be able to do the in-car controls like changing the climate in the car or starting the car or all of the things which a car would need, not, like, just bringing in that device. So still, people used it, and it was only an invite-only program for a couple of people, which went well. But then after that, Amazon did realize, like, this industry is growing bigger, and lot of different companies are also getting into the space. And by now, what started happening is there were lot more companies in the conversational AI space who were trying to get into the cars, so the space became bigger.\n\nSo they did a partnership with lot of different companies, including Cerence and Baidu, BMW, Bose, and lot more companies, and they created an initiative called Voice Interoperability, where they thought, like, multiple assistants can reside in the vehicle. It doesn’t have to be a competition. They all can reside together, and depending on wake-up word, one of them can be invoked. So this is an effort which is still going on, and they are working together to come up with a solution to make the system more interoperable. Now as this is happening, Amazon also realized, like, the pact will not work in the car because it is not able to control the system, so they launched something called Alexa Auto SDK, which is a software development kit deployed in the vehicle. Because another problem with automotive is not all of the vehicles comes equipped with a connectivity. Right? So then how do you make the system work? You need to have embedded things in the vehicle, which is able to control your in-car system, like climate controls or your car in general like starting the car and stuff like that. So they came up with an Alexa Auto SDK, which is able to do all of the Amazon things, what Amazon is capable of doing, but additional things which are embedded to the system. And this SDK resides into the vehicle and then is able to control a lot of things because it is directly connecting to the vehicle modules and is able to work.\n\nSo this was interesting. So when they launched it, Lamborghini was the first brand to leverage this SDK and create their voice assistant. Although Ford was the first to use the cloud-based system, they were the first one to integrate the in-car… car voice assistant using the SDK. And after that, General Motors in twenty twenty one also gets a voice assistant in collaboration with Google because Google also came up with something called Google Auto Services, which is another SDK in the vehicle using which… people have… be able to get the entire infotainment system, but also the Google Assistant and create these actions into the system. So this has been a great evolution so far, as you can see, from two thousand four, where we did not have a natural language system. And it was so screen heavy, and everything was embedded. Software updates were so difficult back then because they were not possible at all. You have to get a USB to get an update, and all of those things were complicated, so you are stuck with that… the only voice assistant which was there without any updates to the point where we have software updates. We have hundred percent connectivity. Users have so many options to choose from.\n\nWe have come a long way. And I would say, like, as we get into the future, there are many platforms, and we heard from couple of vendors today about their technologies and what they have been working on. And that is a good news, because that is going to help feed into the automotive segment as well as other segments as well to create a much better system. There’s lot of technology advancement happening, and we have seen that as well. Right? Systems are becoming more natural language. The speech recognitions have much more accuracy, and they are able to understand people better, more accents, and stuff like that as well. And we’ve also started to see that there are many more use cases, like voice commerce and voice advertisements, and these kind of things will also pick up pretty fast.\n\nAnd as we start to think about these things, at some point, we have seen this, and we have heard this. Like, how can we start using sentiment analysis or emotions and stuff like that? So we’ll start to see, like, as the technology has matured to this point, we’ll start to see emotion control units, which are, like, small chips deployed in the vehicle, which is able to read the users’ emotions through the cameras in the car and then able to help the user, because now these systems are smart enough to tune and change the responses based on the user’s emotion like a normal human being would do. So it’s pretty much possible that we’ll start to see, like, the evolution of car would start to have these kind of things as well. And then we’ll also start to see more humanized technologies. There’s text-to-speech engines out there, which are becoming more humanized, but you are seeing lot of more humanistic voice in there, a lot of different emotive voice in there, and stuff like that. And the same thing will start to happen where technologies will evolve more and become more inclusive in terms of how they understand or they interpret different kind of users. So with that, I wrap my talk, and thank you so much.";
						}
						async function compiledContent$W() {
							return load$W().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$W() {
							return (await import('./chunks/index.6fe7b5c3.mjs'));
						}
						function Content$W(...args) {
							return load$W().then((m) => m.default(...args));
						}
						Content$W.isAstroComponentFactory = true;
						function getHeadings$W() {
							return load$W().then((m) => m.metadata.headings);
						}
						function getHeaders$W() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$W().then((m) => m.metadata.headings);
						}

const __vite_glob_0_218 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$W,
  file: file$W,
  url: url$W,
  rawContent: rawContent$W,
  compiledContent: compiledContent$W,
  default: load$W,
  Content: Content$W,
  getHeadings: getHeadings$W,
  getHeaders: getHeaders$W
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$V = {"title":"The History of Automatic Speech Recognition","description":"Learn about the history of automatic speech recognition (ASR) and how end-to-end deep learning is creating a new revolution in ASR.","date":"2021-01-15T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981358/blog/the-history-of-automatic-speech-recognition/history-of-asr-infogfx%402x.jpg","authors":["keith-lam"],"category":"speech-trends","tags":["education"],"seo":{"title":"The History of Automatic Speech Recognition","description":"Learn about the history of automatic speech recognition (ASR) and how end-to-end deep learning is creating a new revolution in ASR."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981358/blog/the-history-of-automatic-speech-recognition/history-of-asr-infogfx%402x.jpg"},"shorturls":{"share":"https://dpgr.am/799a8c3","twitter":"https://dpgr.am/bfedebb","linkedin":"https://dpgr.am/4fd7971","reddit":"https://dpgr.am/1e7447d","facebook":"https://dpgr.am/6844267"}};
						const file$V = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/the-history-of-automatic-speech-recognition/index.md";
						const url$V = undefined;
						function rawContent$V() {
							return "The most exciting time to be in the Automatic Speech Recognition (ASR) space is right now. Consumers are using Siri and Alexa daily to ask questions, order products, play music, play games, and do simple tasks. This is the norm, and it started less than 15 years ago with Google Voice Search. On the enterprise side, we see [voicebots & conversational AI](https://deepgram.com/solutions/voicebots/), and [speech analytics](https://deepgram.com/solutions/speech-analytics/) that can determine [sentiment and emotions](https://blog.deepgram.com/sentiment-analysis-emotion-regulation-difference/) as well as [languages](https://deepgram.com/product/languages/).\n\n## **Early Years: Hidden Markov Models and Trigram Models**\n\nThe history of Automatic Speech Recognition started in 1952 with Bell Labs and a program called [Audrey](https://astaspeaks.wordpress.com/2014/10/13/audrey-the-first-speech-recognition-system/), which could transcribe simple numbers. The next breakthrough did not occur until the mid-1970 when researchers started using [Hidden Markov Models](https://jonathan-hui.medium.com/speech-recognition-gmm-hmm-8bb5eff8b196) (HMM).HMM uses probability functions to determine the correct words to transcribe. These ASR speech models take snippets of audio to determine the smallest unit of sound for a word or what is called a [phoneme](https://www.britannica.com/topic/phoneme).  The phoneme is then fed into another program that uses the HMM to guess the right word using a most common word probability function. These serial processing models are refined by adding noise reduction upfront and [beam search language models](https://towardsdatascience.com/an-intuitive-explanation-of-beam-search-9b1d744e7a0f) on the back end to create understandable text and sentences. Bean search is a time-dependent probability function and looks at the transcribed words before and after the target word to find the best fit for the target word. This whole serial process is called the [\"trigram\" model](https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9), and 80% of the ASR technology currently being used is a refined version of this 1970's model.\n\n## **New Generation of ASR: Neural Networks**\n\nThe next big breakthrough came in the late 1980s with the addition of neural networks. This was also an inflection point for ASR. Most researchers and companies use these neural networks to improve their current trigram models with better upfront audio phoneme differentiation or better backend text and sentence creation. This trigram model works very well for consumer devices like Alexa and Siri that only have a small set of voice commands to respond to.  However, this model is not as effective with enterprise use cases, like meetings, phone calls, and automated voicebots. The refined trigram models require huge amounts of processing power to provide accurate transcription at speed. Businesses need to trade speed for accuracy or accuracy for costs. \n\n## **New Revolution in ASR: Deep Learning**\n\nOther researchers believed that neural networks were the key to having a new type of ASR. With the advent of big data, faster computers, and graphical processing unit (GPU) processing, a new ASR method was developed, [end-to-end deep learning ASR](https://heartbeat.fritz.ai/the-3-deep-learning-frameworks-for-end-to-end-speech-recognition-that-power-your-devices-37b891ddc380). This new ASR method could \"learn\" and be \"trained\" to become more accurate as more data is fed into the neural networks. No more developers re-coding each part of the trigram serial model to add new languages, parse accents, reduce noise, and add new words. The other big advantage of using an end-to-end deep learning ASR is that you can have the [accuracy, speed, and scalability without sacrificing costs](https://offers.deepgram.com/how-to-evaluate-deep-learning-asr-platform-solution-brief).\n\n![history of speech recognition](https://res.cloudinary.com/deepgram/image/upload/v1661976833/blog/the-history-of-automatic-speech-recognition/history-hmm-v-dg_2%402x-1024x580.png) \n\n*History of Speech Recognition and Hidden Markov Models*\n\nThis is how [Deepgram](https://deepgram.com/company/about/) was born; out of research that did not look at refining a 50-year-old ASR model but starting from deep learning neural networks. Check out the entire history of ASR in the image above. [Contact us](https://deepgram.com/contact-us/) to learn how you can decrease word error rate systematically without compromising speed, scale, or cost, or sign up for a [free API key](https://console.deepgram.com/signup) to get started today.\n\n<WhitepaperPromo whitepaper=\"latest\"></WhitepaperPromo>";
						}
						async function compiledContent$V() {
							return load$V().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$V() {
							return (await import('./chunks/index.4a3896b3.mjs'));
						}
						function Content$V(...args) {
							return load$V().then((m) => m.default(...args));
						}
						Content$V.isAstroComponentFactory = true;
						function getHeadings$V() {
							return load$V().then((m) => m.metadata.headings);
						}
						function getHeaders$V() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$V().then((m) => m.metadata.headings);
						}

const __vite_glob_0_219 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$V,
  file: file$V,
  url: url$V,
  rawContent: rawContent$V,
  compiledContent: compiledContent$V,
  default: load$V,
  Content: Content$V,
  getHeadings: getHeadings$V,
  getHeaders: getHeaders$V
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$U = {"title":"The History of the Word Hacker","description":"Ever wonder where the word \"hacker\" came from? We've got your answers.","date":"2019-02-22T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981348/blog/the-history-of-the-word-hacker-2/history-of-hacker%402x.jpg","authors":["morris-gevirtz"],"category":"linguistics","tags":["language"],"seo":{"title":"The History of the Word Hacker","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981348/blog/the-history-of-the-word-hacker-2/history-of-hacker%402x.jpg"},"shorturls":{"share":"https://dpgr.am/f928e72","twitter":"https://dpgr.am/c96f63b","linkedin":"https://dpgr.am/f9cedf9","reddit":"https://dpgr.am/508226c","facebook":"https://dpgr.am/db12d0e"}};
						const file$U = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/the-history-of-the-word-hacker-2/index.md";
						const url$U = undefined;
						function rawContent$U() {
							return "The words \"[hack](https://www.etymonline.com/word/Hackney?ref=etymonline_crossreference)\" and \"[hacker](https://www.etymonline.com/word/hacker#etymonline_v_29937)\" started in the same place in English language history, split in meaning to mean horse and a brutal action verb. Curiously, these two words were reunited 2000 years later in the world of silicon chips, code, and programming. According to one of the best [English etymological dictionaries](https://www.etymonline.com) available anywhere, the word \"hacker\", with the sense of evil/good and brilliant computer programmer was [born in the halls of the MIT](http://catb.org/esr/writings/hacker-history/hacker-history.html). This fact alone reminds us that ***culture and words begin in actual places.*** At that time, to hack code, or hack out code, had a definitely negative connotation. Where did this come from? It came from horses. Yes, horses.\n\n## Hack-An Ancient, Equine Word\n\nMore than a thousand years ago, in a place called Hackney-once pastureland, now well within London, a certain breed of horse was bred. These horses were known as nags, a word of uncertain origin. This breed of horses came to be called by the name of the patch of land they came from: Hackney. The word Hackney itself meant \"Hack's island\" or perhaps \"Hook's island.\" There are two possible reasons for this:\n\n1. the island is in a bend or \"hook\" in the river\n2. A person or family, named \"Hook/Hack\" lived there.\n3. The family may have been called \"Hook/Hack\" because they came from a hook in a river, because they maybe hooks/axes, because they had a neat family story about a [claw](https://external-preview.redd.it/a21Cm_avnE9uOJ7-n1oz_ZF3sB7IXqaeCZf5VUzUibQ.png?auto=webp&s=51602496f2364ae91abf2afad0884640adddcb66) (Russian word for claw \"[коготь](https://indo-european.info/pokorny-etymological-dictionary/whnjs.htm)\" comes from the same root).\n\nWe will revisit the word \"hook\" later. Hackney nags were general purpose horses, work horses: used for [drayage](https://www.etymonline.com/word/drayage#etymonline_v_31869) and riding about town. They were not the classy horses used in hunting or war. They were certainly not the horses that the nobility rode with great pomp or under any circumstance.\n\n![nag](https://res.cloudinary.com/deepgram/image/upload/v1661976821/blog/the-history-of-the-word-hacker-2/hackney-fares.jpg)\n\n*[Source](https://janeaustenslondon.com/2017/01/02/paying-the-correct-fare-hackney-carriages-and-watermen/)* \n\nEventually, it was these hackney nags that were often, if not principally, used to pull carriages-the sort that were used to shuttle folk from place to place. Even after Hackney was paved over and built up, the horses that pulled taxi carbriolets (taxicabs, i.e. cabs) where known as hacks. Taxicabs were hired on the street-they did not belong to nice families. In other words, **they were commonplace, ordinary and for-hire.** Such horse-and-cart drivers came to be known as hackney drivers, then just \"hacks\" for short. In the 18th century, those who did work for hire, like writers, came to be known as hack-writer or hack-`insert-profession-here`. The \"transportation professional\" use-of-the-word \"hack\" fell out of fashion as it was replaced by the French word \"taxi.\" (Soon, that word too might be replaced by the word \"uber\"). Hack authors produce copious works, often anonymously for public consumption. It's not art. Hack work is laborious, tedious, unrefined. This etymology of an earlier tech-world \"hack\" is nicely supported by the first [dictionary of tech terms](http://www.gricer.com/tmrc/dictionary1959.html). In 1959, a member of MIT's Tech Model Railroad Club, Peter R. Sampson, wrote a dictionary of tech terms used in then nascent computer world saying of \"hack\" the following: \n\n![sampson](https://res.cloudinary.com/deepgram/image/upload/v1661976822/blog/the-history-of-the-word-hacker-2/Screen-Shot-2019-02-19-at-10.21.29-AM.png)\n\nIt's worth noting that MIT's [TMRC](https://en.wikipedia.org/wiki/Tech_Model_Railroad_Club)'s members, the original hackers, eventually seeded MIT's Artificial Intelligence Laboratory which is now [CSAIL](https://www.csail.mit.edu/). A meaningful portion of the lingo of the tech world today was born at the TMRC, e.g. [cruft](https://en.wikipedia.org/wiki/Cruft), [Internet of Things.](https://www.rfidjournal.com/articles/view?4986) As the world of computers became more mainstream-in the mid 1970s-so did new terms come into common English usage. The word \"hacker\" appeared and came to describe intrepid, anti-authoritarian, even criminal computer engineers who would illegally bypass communications security measures gaining access to data and software. \n\n![ngram](https://res.cloudinary.com/deepgram/image/upload/v1661976823/blog/the-history-of-the-word-hacker-2/hack-ngram.png)\n\n*Google nGram viewer chart shows the frequency of the labeled terms as found in books scanned by Google (subject to sampling bias). Note the sudden uptick in incidence at about the year 1975.* \n\nDoes this use stem from the earlier \"bad, unoriginal\" hack? The [answer](https://www.etymonline.com/search?q=hacker) is a cautious no. It seems that this second meaning comes from the verb \"to hack.\"\n\n## Vikings, Violence, and Vernacular English\n\n`\"to hack\", verb` has meant roughly the same thing for thousands of years: to cut up roughly, as with an axe or hatchet. Take a look at this snippet from Anglo-Saxon-language Legends of St. Andrew and St. Veronica-apocryphal tales of the apostles/saints which became popular in Anglo-Saxon England in the 10th century A.D. \n\n![tohaccode](https://res.cloudinary.com/deepgram/image/upload/v1661976824/blog/the-history-of-the-word-hacker-2/Screen-Shot-2019-02-19-at-10.11.31-AM.png)\n\n**Translation into Modern English:**\n\n\\[...]and some were pierced through with a spear, and some were sold into slavery, and some they were hacked into four parts\\[...]\n\nIn the snippet above, we see the ancestor of the verb \"to hack\" (highlighted) in a passage where the Roman army, under orders from Titus and Vespasian Christian/Jewish citizens of Judea tortured, killed and also sold into slavery. Astoundingly enough, linguistic research shows that the `to hack verb` comes from the same place as the word hook-unsurprising when you consider what many Iron Age axes looked like. To be clear, `to hack verb` [harkens back](https://indo-european.info/pokorny-etymological-dictionary/whnjs.htm) to `hook/hack, noun` not the other way around. \n\n![](https://res.cloudinary.com/deepgram/image/upload/v1661976825/blog/the-history-of-the-word-hacker-2/axe.jpg)\n\n*This is an example of an Iron Age axe (a reproduction). The word in Anglo-Saxon was \"æx.\"*\n\nThe people who invaded and settled the place that would be called \"Hack's Island \\[meaning: Hook's Island\"] would later be invaded by a linguistically related people-The Vikings-who would alter the pre-existing language and culture to the point of [influencing](https://www.babbel.com/en/magazine/139-norse-words/) the sound change, which drove a split in the pronunciation of the words \"hook\" and the word \"hack.\"\n\n## When did Hackers Become Good?\n\nBrute force can be used for good and bad. Since the early days of the second \"hacker\" term there has been an ambivalence towards those who use the epithet. This contribution to the Association for Computing Machinery SIGART bulletin from 1976 makes this evident. \n\n![sigart](https://res.cloudinary.com/deepgram/image/upload/v1661976826/blog/the-history-of-the-word-hacker-2/Screen-Shot-2019-02-19-at-10.51.25-AM-1.png)\n\n*[Source](https://dl.acm.org/citation.cfm?id=1045340)*\n\nNo doubt, today's crowd can identify with the sentiments of Mr. McDermott of the MIT AI lab. As pressure for [regulation begins to build](https://blog.deepgram.com/should-ai-be-regulated-ai-show/), the dual notion of the Robin Hood begins to color us members of the AI field as well. Are we brilliant, motivated creators of the avant-garde? Or, are we reckless, undisciplined technologists? \n\nBy the mid-80's and into the 90's, `hacker noun` came to mean both things: brilliant programmers who could produce elegant code to solve difficult problems as well as evil, or pathological programmers who threatened public safety by infiltrating secure locations remotely. There is no doubt that this second, negative meaning was fueled by the end of the cold war and the rise of the internet. The world needed new enemies and the internet was a new, unknown land in need of antagonists.\n\n## Two Movies from 1995 Exemplify this Duality\n\n![sandra](https://res.cloudinary.com/deepgram/image/upload/v1661976827/blog/the-history-of-the-word-hacker-2/the-net.jpg)\n\n1. In the movie *The Net*, starring Sandra Bullock, the main character has been defrauded by hackers and must steal her life back using the internet, clicking/typing on a computer, and killing people with guns, and fire extinguishers (of course).\n2. In the film *Hackers*, miscreant, though-not-evil hackers must fight against an evil (older) hacker who has enlisted the help of the Secret Service to accomplish his evil ends. While the young, talented hackers only intend to have fun, show off and steal small money, the evil hacker has no qualms with hurting innocent people and capsizing an oil tanker-Exxon Valdez style.\n\nIn both movies we see that hackers are of two strains: talented, irreverent, but good and older, jaded, evil. Note that in both movies the problem of evil hackers is solved by adding more (good) hackers. This means that the concept of hacking is one of yin and yang-where the balancing force is integral to the thing itself. Life is not always this way, sometimes we need intrinsically different things to fight the evil in the world; e.g. penicillin: fungi fight bacteria.\n\n## Verbs (and Adjectives) also are Ambiguous\n\nHacking with a hatchet conjures images of violence and brutality. Yet, `to hack verb`, the \"active form\" also displays the ambiguity of meaning shown by `hacker noun`, the state form. We have already alluded to a possible reason for this verbal ambiguity: destruction, or in this case *hacking* can be used for good or bad. Likewise, a skillful programmer may \"hack something together\" to get a quick proof-of-concept working and thus save the day. Their haste and brutal code is a boon in these circumstances. At the same time, a bad, careless programmer may produce low quality, \"hack-y\" or \"hacked together\" code.\n\nHowever, in the back of the mind of the early programmers of the 1950s, 60s, and 70s, the notion of the pathetic hack, who wrote \"without constructive end\" may have influenced the use and reanalysis of `to hack verb` For these reasons, it is not clear if this use of the term comes from the earlier horse-based \"hack\" or from the axe-based metaphor, suggesting roughness. In the decidedly analog, stochastic, meat-based brains of homo sapiens programmerensis, the answer is both: these two words started as the same word in northern Europe, diverged only to be reunited in the foggy San Francisco Bay.\n\n## Hacking in the Modern Day\n\nToday we have a number of tech words that all come from the same root, but tend to be interpreted rather differently depending on context and speaker's point-of-view.\n\n* **Hack (coder)**, n./adj. -- bad\n* **Hacker**, n. --good or bad\n* **To hack (code)**, v. -- ambiguously good or bad\n* **Hacked (code)**, adj. -- ambiguously good or bad\n* **Hacky (code)**, adj. -- usually bad\n\nWhat is fascinating to consider, is how the notion of a <mark>bend, hook, curve</mark> came to mean different things with opposite connotations. Then, over time, two or three currents of meaning were brought together in a new human endeavor of logical thinking, binary logic and clear lines - the world of computing. This new endeavor is equal parts tedious, destructive, violent, daring and creative with some amounts of evil sprinkled in. Ironically (or predictably), harnessing ambiguity-the natural enemy of computing-is now the goal of the tech world. In the modern world of computing we are trying to build machines that behave in this distinctly non-binary, highly-stochastic way.\n\n[Neural nets, or more broadly AI](https://blog.deepgram.com/ai-show-different-types-of-machine-learning/), work because they are a mimesis of the squishy, non-binary, probabilistic brains that created all the meanings of \"hack\" from one original noun. And yet, the image of the human brain is still one that connotes a meaning of utmost intelligence for us.\n\n## Bonus Material\n\nThe word hack has another use in modern English. It is found in the compound word: \"sidehack.\" A sidehack is what some might call a sidecar. The term seems to have gained traction (pun intended) in the world of motorcycle racing. No etymological explanation of this seems to exist, though the likeliest explanation is that it is an extension of the first etymology: the horse from London.\n\nWhy? Because hack was associated with the notion of the taxicab, for so many centuries.Some might argue that the idea of a sidehack is born from the notion of hastily, or imprecisely building something (the verbal, violent form). But that verbiage is more modern than we realize. If you consider the data offered by Google nGram viewer, you will find that most results for \"hack together\" from before about 1994 are actually [OCR errors](https://searchengineland.com/when-ocr-goes-bad-googles-ngram-viewer-the-f-word-59181) of \"back together.\" Other results are of the type: last name, together, e.g. \"and John Hack, together with Molly Stephens...\"\n\n![trisikel](https://res.cloudinary.com/deepgram/image/upload/v1661976828/blog/the-history-of-the-word-hacker-2/side-hack.jpg)\n\n *A sidehack on a BMX in Manila, Philippines. There the \"Trisikels\" \\[phonetic spelling] are used as transportation, not just entertainment. Photo credit: Scott Stephenson, Co-founder and CEO [Deepgram](https://www.deepgram.com/)*.\n\nThe word sidehack is indeed associated with motorcycle racing publications from the 1970s ([Google search](http://thebannerisup.district37ama.org/results/1970s/1970-SGVMC-H&H.pdf)) and returns results featuring BMX bike contraptions from the last 20 years. The relationship between BMX and Motocross was already clear. Are sidehacks \"hacks\" because of their association with transportation, or because of their jerry-rigged nature? Given that motocross arose in the 1970s, it's anyone's guess. However, given the nGram viewer data and our knowledge of the evolution of \"hacking\" then we might surmise that the link between tech and Motocross is is tighter than previously believed.\n\n<WhitepaperPromo whitepaper=\"latest\"></WhitepaperPromo>";
						}
						async function compiledContent$U() {
							return load$U().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$U() {
							return (await import('./chunks/index.2a9713ab.mjs'));
						}
						function Content$U(...args) {
							return load$U().then((m) => m.default(...args));
						}
						Content$U.isAstroComponentFactory = true;
						function getHeadings$U() {
							return load$U().then((m) => m.metadata.headings);
						}
						function getHeaders$U() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$U().then((m) => m.metadata.headings);
						}

const __vite_glob_0_220 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$U,
  file: file$U,
  url: url$U,
  rawContent: rawContent$U,
  compiledContent: compiledContent$U,
  default: load$U,
  Content: Content$U,
  getHeadings: getHeadings$U,
  getHeaders: getHeaders$U
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$T = {"title":"The Importance of Testing with Voice Experiences and Conversational AI - John Kelvie, CEO, Bespoken - Project Voice X","description":"The Importance of Testing with Voice Experiences and Conversational AI presented by John Kelvie, CEO of Bespoken, presented on day one of Project Voice X. ","date":"2021-12-09T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981398/blog/the-importance-of-testing-with-voice-experiences-and-conversational-ai-john-kelvie-ceo-bespoken-project-voice-x/proj-voice-x-session-john-kelvie-blog-thumb-554x22.png","authors":["claudia-ring"],"category":"speech-trends","tags":["conversational-ai","genesys","project-voice-x"],"seo":{"title":"The Importance of Testing with Voice Experiences and Conversational AI - John Kelvie, CEO, Bespoken - Project Voice X","description":"The Importance of Testing with Voice Experiences and Conversational AI presented by John Kelvie, CEO of Bespoken, presented on day one of Project Voice X. "},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981398/blog/the-importance-of-testing-with-voice-experiences-and-conversational-ai-john-kelvie-ceo-bespoken-project-voice-x/proj-voice-x-session-john-kelvie-blog-thumb-554x22.png"},"shorturls":{"share":"https://dpgr.am/f82942c","twitter":"https://dpgr.am/153675f","linkedin":"https://dpgr.am/f622203","reddit":"https://dpgr.am/3f8025b","facebook":"https://dpgr.am/2ae1d1c"}};
						const file$T = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/the-importance-of-testing-with-voice-experiences-and-conversational-ai-john-kelvie-ceo-bespoken-project-voice-x/index.md";
						const url$T = undefined;
						function rawContent$T() {
							return "*This is the transcript for \"Voice in Healthcare,\" presented by Henry O'Connell, CEO at Canary Speech, presented on day one of Project Voice X.* *The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was **94% accurate.**  Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram's API.  If you want to see if Deepgram is right for your use case, [contact us](https://deepgram.com/contact-us/).*\n\n\n\n\\[John Kelvie:] Thanks, Bradley. It's really nice to be here. It's nice to see Bradley again. When the pandemic started, I guess, March of twenty twenty, I remember seeing Bradley on Twitter, and he was saying, oh, just wait. You all be begging to get back to in-person events. And I thought, no. I'm definitely not gonna be begging for that. But I have to say, Bradley, I... I'm so excited to be here. I'm really excited to get these events going again. It's not necessarily that I love traveling, but I really like having the chance to meet people, and the virtual events just really didn't work very well, so great to see everybody here. And so I'm gonna talk today about a case study that we did. We have carved out this niche for testing, and as we've gone farther along as a company working with voice and we've seen these use cases emerge, we've just seen really great applications of testing, and I would even expand it.\n\nWe think of it now as just managing the overall conversational AI life cycle. We see that as our responsibility, and we really try to help our customers do that. And so I'm gonna walk you through one of the scenarios where we did do that. But just to start off, I am gonna briefly opine on where AI is today. You know, there are folks who think that this is the current state of it. This is probably... it looks a little bit more menacing than it actually is. But people who think that the AI is really self-learning, autonomous, incredibly intelligent, etcetera, etcetera... we do talk to a lot of customers, and when we talk about how they're gonna train their models, where they're gonna get data from, they're just kinda like, I thought it just trained itself. That's not an uncommon answer. I've even heard people who work at Nuance say that kinda confusingly. You know? And so even though it's quite advanced, I I would say it's a it's a long way off from Skynet. Instead, the mental model I like people to work with is if you just... our new customer of Dialogflow?\n\nThink of it as Google dropped off a beautiful child that looks like this for you. This is what you've received. Yes. It's extremely intelligent. It's very capable. It someday will be very skillful and will respond to customer queries in a useful way, but that day is not today. It's not the day that you start using it. It's not the day that you launch it. You need to constantly train and mature this intelligent being so that it does something meaningful for your customers. And it's not gonna take eighteen years, thankfully, but it's probably gonna take... you know, think eighteen months. Think about that longer time frame. Alright. So onto the case study. I'm not gonna use the customer's name for this. We don't yet have permission for that. Hopefully, we will have that shortly. But it's it's a pretty basic one, and I think the simplicity of it is something I really love about it. It illustrates even for sort of basic cases how tricky it can be to do this stuff.\n\nSo this... the part that we helped the customer with initially was just doing an initial lookup of a user based on a member ID, and so that's an alphanumeric sequence. It's three letters, six numbers, and a dash. Really simple. Right? And then once that's been input, they're doing a lookup into their database, and then they know the first name and last name and ZIP code, and they're gonna verify the user as who they purport to be by matching that information. So they ask them what's your name, what's your date of birth, all this other verifying information, and they have to give the right response. I mean, that sounds easy. That sounds like something in twenty twenty one. This should not be hard. And they used what I would say is a pretty stay... state-of-the-art tech stack. They're using Genesys. We do a lot of work with Genesys. We really love those guys. They're using the Genesys flow architect.\n\nI think Voiceflow should really take a lot of business away from that partly 'cause it's not quite as good. And then they have Lex integrated for doing the speech recognition in the NLU, and then they've got a back end that's based on AWS Lambda. So this is this is a a pretty standard tech stack. If you were like, I'm gonna build a call center today, it's very likely that you would see this pop up at the top of your Google searches. You would find a lot of things that suggest this is a great way to go, but it's not. It does absolutely terrible on the alphanumeric recognition. We're not terrible. It does it does poorly on the alphanumeric recognition. On the first name and last name, it does horrendously. Even though you know what the first and last name is, it still can't get it right because Lex is missing features, like recognition hints.\n\nIt has an inability to measure... or the customer had inability to measure performance accurately and repeatedly. They didn't have the tools in place for those processes, and they didn't have an ability to improve Lex over time. So one sort of theme for this presentation is, you know, you need to have good models, but then you need to have the engines around it that have good capabilities because, you know, you could have... like, let's pick on Google. Obviously, their models are incredible, but they don't have as much tooling for actually tuning those models. And so that can be a limitation if you're using, like, Google's ASR. So what did we see with the member ID? We saw a thirteen... greater than thirteen percent error rate on matching the the correct member ID that the user said. Like I said, really not very good. \n\nA lot of users are not being understood with that. And then on the first name and last name, it was a forty one point five percent and fifty six point eight percent, respectively, error rates. So that's the part that's really, you know, horrendous and just really not workable. The type of project... and we hear this a lot from people using Lex and Dialogflow where people are like, gee. I don't know that we can go forward with this. We saw this as a big strategic initiative for our company, but these numbers, this is not gonna work for our customers. They're just gonna get frustrated. We're not gonna be delivering a satisfying customer experience, whatsoever. So then we do some training on it. And in this case, it was just actually... the training is in the form... because there's not actually much that we can do with Lex there, we're actually just applying a fuzzy search algorithm, a really simple algorithm after we get the results from the ASR slash NLU. And and that does actually really help a lot, putting that in place. On the first name, last name, though... I mean, we tried a bunch of stuff with Lex.\n\nThere really wasn't much that actually improves it. So there's some... in a way, these improvements are substantial, but they don't actually get it to a place where it really becomes usable. Thirty five point one percent error rate, forty eight point eight percent error rate are really not in an acceptable range. So then we said, well, let's try Azure speech. And, you know, in general, that's a product that we like. We think it works really well. By switching to that, what we found was that on that first name and last name, initially, it was seventy five percent correct. And then by just adding in recognition hints... and this almost feels like cheating, but when you say, ok. This is the name we're expecting.\n\nIf you feed that to Azure first, it then understands it correctly ninety eight point four percent of the time. Same thing on the last name. So vastly reducing the number of errors, really improving that experience, you know, getting it to a range that is gonna feel almost perfect for users. And this is not... I mean, I think most modern ASRs would deliver a performance that are similar to this with their models, so it's not really a model limitation that we're looking in here. Instead, it's about the tooling that's provided by these different vendors. And some of it's a limitation of Lex. Some of it's a limitation of Genesys. But a lot of these tech stacks are really early. They don't have, you know, features like recognition hints. I mean, on one hand, that's been something that's been around for twenty plus years for IVRs. \n\nAt the same time, though, some of the more newfangled ones... you know, the Dialogflow team has not had a chance to enter... to introduce this yet. Same thing with Lex. So they're sort of behind in a certain way while, at the same time, they have pushed the state of innovation forward. On the member IDs using Azure, here, we did not see a very good performance measure, though. It really struggled with the member IDs. Even with doing an extensive amount of training, it still did not do very well. I personally found this kinda surprising. I mean, we do like, in general, the Azure tool kit. It does give you a lot of power and flexibility, someone who's doing modeling, but this was a limitation that we did run into. \n\nI will say at the same time, I think this is probably overcomeable. We're reaching out to some people at Microsoft. There's likely a way to resolve this and get to a better performance level with Azure. That said, it's still... it's interesting to see that Lex here, out of the box, has really done better than Azure. And so that sorta leads us to, well, a couple overall larger points. You know, one, Azure provides more leverage for optimization than Lex. It performs very well for transcription. It does not perform as well on a tightly prescribed domain. And so that leads us to these sort of larger observations. If you're building a conversational application, the things that you should be looking for, the qualities of that system that you build are that it should be trainable, each component with it... within it should be able to be tuned and trained quite easily. It should be testable. It should be easy to measure. You should know what those measurements are. You should know what the accuracy level is. It should be modular. Each component should be able to be easily swapped out for another when and if necessary. \n\nI mean, the state of the art in this space is just changing all the time. And there's companies like Deepgram. I'm very interested in their presentation. They're doing really innovative stuff. There's a lot of other companies doing innovative stuff, and so you wanna be able to take the best of what's out there and easily bring it into your system. And then finally, it should be contextual. One thing I like about this case study a lot is you're just talking about... it's really just one intent, and we're really focused on two slots within that single intent. But even within them, you really need two different models. \n\nThe actual solution that we've ended up using going forward with this specific case is a a grammar that's built with IBM Watson for that member ID. Right? And so that... that's a specific way to solve that slot-filling challenge. But then you get to the first name, last name, and there's a different approach that you wanna take. That's not something people should be afraid of. In fact, that's that's just kind of the nature of the beast. You... even within using a single vendor, you may end up with using different models between those different slots between the different intents.\n\nSo you want it to be contextual. You wanna embrace making very granular decisions and doing really granular processing on these transactions to get results that are really gonna wow customers. And so we see this overall as part of a a larger ecosystem.\n\nWe are, as a company, expanding the footprint of our software where we we really see ourselves as a platform that is supporting this whole life cycle. We really see that as essential. And the analytics and the management that are coming out of this, we really see as essential. We're helping customers with this orchestrator piece that sits in the center that'll actually act as a runtime to interact with the different ASRs and NLUs that are out there, and then at the same time, capture all this data.\n\nNow, ideally, it would not be necessary for us to supply that orchestration piece. But, again, it's it's not easy. You know, if you use Twilio or Genesys, they don't really supply that. So we see this as a whole platform that then is gonna make it easy to maintain and manage your AI system over time. And so what are some... the benefits of this? You're removing the risk of any single vendor. You're creating a system that can be easily improved and optimized over time, really, nearly endlessly. That's that's not necessarily such a great thing. I mean... but it is optimization, so there's always room for improvement. You're never really gonna get to a hundred percent, and it allows you to hire the best tech for each job. You know? So you might use Lex for one thing, Watson for another, Azure for a third, Deepgram for another.\n\nAnd so just to kinda zoom in on that platform a little bit, this is what we see as that overall cycle where, you know, there's a gathering of data, testing, training, monitoring on an ongoing basis, and then that is basically repeated ad infinitum. I'm not saying that right. But, basically, it's it's repeated indefinitely as your system evolves and until it becomes truly a sort of mature, intelligent adult. And I did set a timer just to remind myself when it's fifteen minutes. So I'm on my last slide here. You wanna go... you wanna take your little intelligent baby, get it walking, running, flying, and this does come back to an ROI. I put in here... we have a nice ROI calculator on our website now. We think it's pretty cool. If you plug in the numbers, the ROI, if you make significant improvements to your accuracy, really huge. It's really huge. So there's an immense benefit from that. You know, for even fairly moderate usage systems, you can really see substantial savings. So it really does matter. This is not just an academic exercise that we're talking about this for. It's gonna make customers happier, and it's gonna really deliver to your bottom line. So that's my talk. Thanks a lot for having me, and I'm John Kelvie. I don't have a slide with my name, but it's j-p-k b-s-t on Twitter, and, you know, look forward to talking with and meeting all you.";
						}
						async function compiledContent$T() {
							return load$T().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$T() {
							return (await import('./chunks/index.6fcb8ef8.mjs'));
						}
						function Content$T(...args) {
							return load$T().then((m) => m.default(...args));
						}
						Content$T.isAstroComponentFactory = true;
						function getHeadings$T() {
							return load$T().then((m) => m.metadata.headings);
						}
						function getHeaders$T() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$T().then((m) => m.metadata.headings);
						}

const __vite_glob_0_221 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$T,
  file: file$T,
  url: url$T,
  rawContent: rawContent$T,
  compiledContent: compiledContent$T,
  default: load$T,
  Content: Content$T,
  getHeadings: getHeadings$T,
  getHeaders: getHeaders$T
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$S = {"title":"The Language of LGBTQ Inclusion and Allyship","description":"Happy pride! This post has everything you need to be an effective ally.","date":"2021-06-24T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981376/blog/the-language-of-lgbtq-inclusion-and-allyship/language-of-lgbtq-inclusion%402x.jpg","authors":["sam-zegas"],"category":"identity-and-language","tags":["inclusion","lgbtq"],"seo":{"title":"The Language of LGBTQ Inclusion and Allyship","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981376/blog/the-language-of-lgbtq-inclusion-and-allyship/language-of-lgbtq-inclusion%402x.jpg"},"shorturls":{"share":"https://dpgr.am/1eff158","twitter":"https://dpgr.am/9bfd162","linkedin":"https://dpgr.am/7045e3e","reddit":"https://dpgr.am/13e813f","facebook":"https://dpgr.am/37b5c0a"}};
						const file$S = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/the-language-of-lgbtq-inclusion-and-allyship/index.md";
						const url$S = undefined;
						function rawContent$S() {
							return "Happy Pride!  Pride is about expressing your authentic self, being visible, and telling your story. At Deepgram, that's right up our alley. One of our core beliefs is that every voice should be heard and understood. When it comes to the LGBTQ community, there are a lot of terms to keep track of - so many that it can get confusing, even to allies. We're here to help!\n\nThis post is a guide to inclusive language, common LGBTQ terms, and allyship. Throughout this post, you'll see us use LGBTQ as an umbrella term for all LGBTQ+ people, since this is one of the most widely recognized names for the community. There's a great deal packed into that name though, and many other letters that are sometimes represented in the list. We'll provide more information about *LGBTQ* and other terms in the *Guide to common LGBTQ terms* section below. But for starters, LGBTQ stands for *Lesbian, Gay, Bisexual, Trans, and Queer/Questioning*.\n\nBefore we dive in, it's important to understand that language is always changing. That's certainly true for the LGBTQ community. This post is a snapshot of some current issues in LGBTQ language and terminology at the time of publication. Things change, though, and we look forward to seeing how these topics develop in the future. \n\n## The language of inclusion\n\nAccording to [polling](https://news.gallup.com/poll/234863/estimate-lgbt-population-rises.aspx), LGBTQ people make up about 4.5% of the U.S. population. This relatively small group has made vast strides in the last 25 years. At the turn of the millennium, marriage excluded same-sex partners in all states and many states had enacted constitutional bans on marriage between same-sex partners. Employers could legally [fire people for being LGBTQ](https://time.com/5554531/equality-act-lgbt-rights-trump/). Federal policies like [Don't Ask, Don't Tell](https://www.hrc.org/our-work/stories/repeal-of-dont-ask-dont-tell) required LGBTQ people to hide their identities. Today, the situation is dramatically different: marriage throughout the U.S. now [includes same-sex partners](https://www.npr.org/sections/thetwo-way/2015/06/26/417717613/supreme-court-rules-all-states-must-allow-same-sex-marriages). LGBTQ people are [protected in the workplace](https://www.usatoday.com/story/news/politics/2020/06/15/supreme-court-denies-job-protection-lgbt-workers/4456749002/), and President Biden is generally recognized as an [LGBTQ ally](https://www.washingtonpost.com/politics/2021/01/11/biden-lgbtq-policies/) with a [record](https://www.hrc.org/resources/president-bidens-pro-lgbtq-timeline) of LGBTQ-friendly policymaking-although commentators acknowledge that he [could do more on trans issues](https://time.com/6053114/pride-transgender/).\n\nThe progress made over the last 25 years is a reason to celebrate, but the LGBTQ community still faces steep challenges. LGBTQ people throughout the U.S. and in other countries still face discrimination as a result of their identity. In 2021 so far, [more than 250 anti-LGBTQ bills](https://www.hrc.org/press-releases/2021-slated-to-become-worst-year-for-lgbtq-state-legislative-attacks) have been introduced in state legislatures across the U.S. A 2020 [study](https://www.americanprogress.org/issues/lgbtq-rights/reports/2020/10/06/491052/state-lgbtq-community-2020/) found that 1 in 3 LGBTQ people in the U.S. reported experiencing discrimination in the past year. Trans people, in particular, face [extraordinary levels of violence](https://www.hrc.org/press-releases/marking-the-deadliest-year-on-record-hrc-releases-report-on-violence-against-transgender-and-gender-non-conforming-people). There is still a great need for change, and that's where allies come in.\n\nAllies will play an important role in pushing for change and supporting their LGBTQ friends, family, and coworkers. One way of stepping up as an ally is to shift toward more inclusive language. We'll cover a few different kinds of inclusive language below, but let's start with pronouns*.* If your grammar is feeling rusty, no problem. Here's a refresher: pronouns are words like *I, me* and *my*; *she* and *her; he, him* and *his*; *they*, *them* and *their*, etc. These words refer to people, but the exact person a pronoun refers to changes with the context of the sentence. In recent years, the use of gender-neutral pronouns [has become more common](https://www.pewresearch.org/fact-tank/2019/09/05/gender-neutral-pronouns/) in the U.S.\n\nThe most widely used gender-neutral pronoun is the singular *they,* as in, \"I just saw Ana; *they're* throwing a party next week and asked me to help *them* set up.\" The use of singular *they* might seem unusual at first glance, but it has actually [been in use since at least the 1300s](https://public.oed.com/blog/a-brief-history-of-singular-they/). In Shakespeare's *Comedy of Errors,* for example, he [wrote](http://shakespeare.mit.edu/comedy_errors/comedy_errors.4.3.html), \"There's not a man I meet but doth salute me, as if I were their well-acquainted friend\" (Act 4, Scene 3). Jane Austen used singular *they* in [many of her books](https://pemberley.com/janeinfo/austhlis.html), including in instances where the gender of the person being referred to is clear. In fact, most English speakers today are likely already using singular *they* in cases where the gender of the person being referred to is unknown, as in \"If you run into the new neighbor, invite *them* over!\" There are [other gender neutral pronouns](https://uwm.edu/lgbtrc/support/gender-pronouns/) as well, but singular *they* is the most common.\n\n**Occasionally you'll hear people say things like, \"it's hard to remember to use a gender neutral pronoun, are pronouns really that important?\" The answer is yes.** One of the central struggles of the LGBTQ community is around visibility. For centuries, society forced LGBTQ people to live their lives in the closet. In a world where few LGBTQ people were visibly out, it was easy for others to fill that void with harmful, untrue stories about them: that LGBTQ people are deviant and dangerous, morally corrupt and unnatural. Visibility is the antidote to these harmful misconceptions about LGBTQ people. When LGBTQ people represent their own stories, it is easy to recognize that they are normal members of society: they are siblings and parents, friends, coworkers, active community members, and they contribute to our communities just like everybody else.\n\nIn short, LGBTQ folks are among the people you already know and like. Over the last few decades, the shift in public opinion toward supporting LGBTQ causes can be [explained in part](https://www.prweek.com/article/1716069/lgbtq-visibility-leads-acceptance) by the increasing visibility of the LGBTQ community itself. **The use of gender neutral pronouns is about visibility and authenticity.** By the time someone decides to start using gender neutral pronouns for themself, they have likely gone through years of self-exploration and the often-painful process of coming out. The choice to use gender neutral pronouns inevitably runs the risk of dealing with the negative reactions of people who don't understand that experience of coming out. Ultimately though, authenticity and self-respect are some of the greatest gifts we can give to ourselves and others.\n\n**So when someone asks you to use gender neutral pronouns, thank them for trusting you with their story.** Recognize the long journey they have been on and the bravery it took for them to reach that point. As an ally, using someone's gender neutral pronouns is a way of saying to them, \"I see you and am here to help you tell your story,\" and of modeling allyship to other straight people as well. Even if you use standard male or female pronouns for yourself, consider posting your preferred pronouns for others to see.\n\n**Being open about your pronouns is an invitation for others to be public about their own gender neutral pronouns.** **It's alright to ask someone, \"what pronouns do you prefer?\" in a private conversation, but you should avoid prompting people to share their pronouns in a group setting.** As an ally, it's important to allow LGBTQ people to be in control of when and how they come out to others. In that same spirit, you should avoid \"outing\" someone by sharing their pronouns with others without checking with them first. Inclusive language goes beyond pronouns. More broadly, inclusive language makes space for LGBTQ people both when communicating directly to them and when talking about them to others.\n\n**If you don't know someone well, don't use language that assumes they are straight.** For example, you might ask someone you just met if she has a *partner* rather than assuming her partner must be a *husband*. Or consider a familiar, comfortable phrase like *ladies and gentlemen*, which is handy in some situations but excludes non-binary people. For many of us, it's almost automatic to use terms that assume a gender binary. The advice here is not that you should *never* use those phrases, but that you should become more aware of them. That kind of language will still be useful in many settings, but **it's good to think about** **[inclusive alternatives](https://www.teenvogue.com/story/how-to-use-gender-neutral-words)** **so that you have them ready when the situation calls for it.** **Finally: if you mess up and use heteronormative language or get someone's pronouns wrong, it's alright.** \n\nMistakes happen and practice makes perfect. The mark of a good ally is someone who can make a blunder, correct themself if needed, dust themself off, and get back in the game. Next, let's take a look at some of the key terms that are used in the LGBTQ community.\n\n## A guide to common LGBTQ terms\n\nThe LGBTQ community is a diverse collection of many smaller identity groups that are brought together by the common experiences of being different from the majority in terms of gender expression and sexual orientation.\n\nLGBTQ stands for lesbian, gay, bisexual, transgender, and queer/questioning. This [initialism](https://www.grammarbook.com/blog/abbreviations/abbreviations-acronyms-and-initialisms-revisited/) covers two different categories: sexual orientation (LGB) and gender identity (T). The Q spans both categories, which we'll cover below. These are some of the largest identity groups in the community, but there are many others as well. While the initialism is most frequently seen as LGBTQ, occasionally you'll see expanded versions, such as LGBTQ+ or LGBTQIA+, in which the additional letters stand for Intersex and Asexual - also covered below. The \"+\" is a gesture toward including numerous smaller groups that fit under the LGBTQ umbrella.\n\nIf you think the \"alphabet soup\" approach to naming the community is a mouthful, you're not alone. Some LGBTQ people have adopted the term *queer* to refer to the LGBTQ community in all its diversity. Queer is a [reclaimed word](https://www.advocate.com/arts-entertainment/2017/8/02/21-words-queer-community-has-reclaimed-and-some-we-havent#media-gallery-media-1) that was considered derogatory in the past and is sometimes still seen in that light by older generations of LGBTQ people. But for younger generations in particular, queer has become a widely-used term for any member of the community - and allies are welcome to use it as well. Let's take a look at some common terms associated with the LGBTQ community. For starters, here are some basics:\n\n* ***Gender*** - The social and behavioral norms associated with sex in a given culture. Gender is not the same as biological sex.\n* ***Sex*** or ***Biological Sex*** - A physical characteristic based on genetics that people are born with: *female* or *male*. In rare cases, ***Intersex*** people are born with physical traits that don't clearly align with typical male or female anatomy.\n* ***Sexual Orientation*** - A romantic, emotional, and physical attraction to others. \n* ***Coming Out*** *\\-* The process through which LGBTQ people recognize their identity and begin to tell others. Coming out is not a one-time event. Even once a person is out, they are continually coming out again to new people they meet.\n* ***Ally*** - Someone who demonstrates support for LGBTQ people, affirms their rights and dignity, and seeks to understand their experiences.\n\nThere is a great deal of gender diversity within the LGBTQ community. Let's dig deeper into some of the concepts and identity groups associated with gender.\n\n* ***Gender Identity*** - A person's understanding of their own gender, which may or may not align with their biological sex.\n* ***Gender Expression*** - Ways of communicating gender identity to others, which can include mannerisms, clothing, speech patterns, and interests. The norms of gender expression vary by culture and change over time.\n* ***Cisgender*** or ***Cis*** - Someone whose gender identity generally aligns with their biological sex is called cisgender or cis.\n* ***Cis-het*** - Short for cisgender heterosexual, the identity group that the majority of straight people belong to. *Straight* is often used interchangeably with cis-het, but cis-het is more nuanced because it is also possible to be a transgender heterosexual.\n* ***Transgender*** or ***Trans*** - Someone whose gender identity or expression differs from their sex assigned at birth. Trans people should be treated according to their gender identity rather than their sex assigned at birth. Remember that gender identity is distinct from sexual orientation, meaning that trans people have the full range of sexual orientations; a trans woman attracted to men is straight, while a trans woman attracted to women is a lesbian. Some trans people choose to physically transition to the opposite sex (see *transitioning* below), but not all do. If you're wondering how *transgender* and *trans* relate to the term *transsexual,* that's a fair question. For most purposes, *trans* has replaced the term *transsexual*. Check out the \"terms to avoid\" section below for more information on this topic.\n* ***Transitioning*** - Transitioning is the process by which some trans people bring their physical traits and gender expression into alignment with their gender identity. Not all transgender people choose to transition; this is a deeply personal choice. Some trans people who physically transition to the opposite sex identify as *transsexual*, but this term should be used with caution, since the details of transition are personal and private. When in doubt, just call someone *trans*.\n* ***Non-Binary*** and ***Genderqueer*** - Someone whose gender identity or expression does not conform to typical female or male patterns. Some people consider non-binary and genderqueer identities to be part of the trans community while others do not. In general terms, the difference between trans and non-binary identities is that trans people typically do align to binary genders (male and female), while non-binary people understand their gender identity beyond the framing of those two categories.\n* ***Genderfluid*** - Someone whose gender identity shifts depending on the social context. For example, someone might have male gender expression at work but female or non-binary gender expression among friends - not because they are hiding their more authentic self at work, but because that pattern of gender expression simply feels most natural to them.\n* ***Two-Spirit*** - A term used in some Native American cultures to describe [non-Western forms of gender expression and sexual orientation](https://indiancountrytoday.com/archive/two-spirits-one-heart-five-genders).\n\nAs speech technology improves, we've seen increased demand for products that automatically detect demographic information from audio. *Automatic gender detection* is a particularly complex challenge. Even among cis-het people, there is significant overlap in typical female and male vocal ranges. Furthermore, as seen above, there is diversity in gender identity beyond cisgender female and male, making it difficult for speech companies to accurately detect gender based on audio inputs. This is a rapidly evolving area of speech technology and an active area of research at Deepgram.  Next, let's take a look at some of the main identity groups associated with sexual orientation\n\n* ***Queer*** - An umbrella term for anyone who is not straight (or more specifically, cis-het), encompassing the entire LGBTQ community.\n* ***Lesbian*** and ***Gay*** - Someone who is romatically, emotionally, or physically attracted to others of the same gender.\n* ***Bisexual*** - Someone who is romatically, emotionally, or physically attracted to both females and males.\n* ***Pansexual*** or ***Pan*** - Someone who is romantically, emotionally, or physically attracted to people of all genders, including non-binary and genderqueer gender expressions.\n* ***Questioning*** - Someone who is unsure of their sexual orientation or gender identity.\n* ***Asexual*** and ***Demisexual*** - Someone who experiences no physical attraction or who only experience it in limited circumstances.\n\nFinally, heads up! There are also some terms to avoid:\n\n* ***Transsexual*** - Use *trans* instead. There are two reasons for this. First, to many people in the community, *transsexual* harkens back to when \"transsexualism\" was [stigmatized by medical practitioners as a mental disorder](https://www.nbcnews.com/feature/nbc-out/transsexualism-removed-world-health-organization-s-disease-manual-n885141). Second, when this term is still used today, it refers specifically to a transgender person who has physically transitioned from one sex to another. However, because the details of transition are private and personal, you generally won't know who this term should be applied to. With these two things in mind, it's always better to simply say *trans,* which is more respectful and more broadly applicable.\n* ***Homosexual*** - Use *lesbian*, *gay*, or *queer* instead. The term *homosexual* is associated with a long history of stigmatization, when society saw *homosexuals* as deviants and [medical practitioners stigmatized *homosexuality*](https://www.psychologytoday.com/us/blog/hide-and-seek/201509/when-homosexuality-stopped-being-mental-disorder), too, as medical disorder. As LGBTQ people have fought to free themselves from this stigma, they have left the term *homosexual* behind as a relic of the past.\n* ***Sexual Preference*** - Use *sexual orientation* instead. If you listen to the experiences of LGBTQ people, you'll hear that these identities are not a choice. They are enduring, life-long orientations that cannot be changed, just as a straight person cannot change their sexual orientation. To call LGBTQ sexual orientations a \"preference\" perpetuates a dangerous misconception that a person can be \"cured\" of their LGBTQ identity. This misconception feeds into the tragic practice of [conversion therapy](https://www.thetrevorproject.org/get-involved/trevor-advocacy/50-bills-50-states/about-conversion-therapy/) and is harmful to young people who may be struggling to come out, leading to [high rates of suicide among LGBTQ youth](https://www.nbcnews.com/feature/nbc-out/40-percent-lgbtq-youth-seriously-considered-suicide-past-year-survey-n1233832). \n* ***Gay Lifestyle*** - Queer people are as diverse in the way they live their lives as straight people. *Gay Lifestyle* and similar terms typically only appear when someone is trying to promote negative stereotypes of LGBTQ people.\n\nThese lists cover some of the most common terms but are far from exhaustive. There are many organizations that provide [further information](https://outrightinternational.org/content/acronyms-explained) about additional identity groups and [guidance on terminology](https://www.glaad.org/sites/default/files/allys-guide-to-terminology_1.pdf).\n\n## Activate yourself as an ally\n\nIf you've made it this far, you probably count yourself as an ally. It's great to have you among us! So as we send you off to enjoy a Pride parade, keep these tips for allyship in your back pocket: \n\n* Get to know the LGBTQ people in your life. You may not live through all their life experiences, but learning about their stories will bring you closer.\n* Work to include LGBTQ people through your actions and words.\n* Avoid stereotyping and making assumptions about people's identities.\n* Speak up as an advocate for LGBTQ people.\n* Educate yourself about issues facing the LGBTQ community, such as how trans people are affected by higher rates of violence than other groups.\n* Remember that LGBTQ issues intersect with the struggles faced by other groups: women, people of color, immigrants, and others.\n\nPride is a celebration of visibility and authenticity - so get out there, live your truth, and support your family, friends, and coworkers!";
						}
						async function compiledContent$S() {
							return load$S().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$S() {
							return (await import('./chunks/index.a554faab.mjs'));
						}
						function Content$S(...args) {
							return load$S().then((m) => m.default(...args));
						}
						Content$S.isAstroComponentFactory = true;
						function getHeadings$S() {
							return load$S().then((m) => m.metadata.headings);
						}
						function getHeaders$S() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$S().then((m) => m.metadata.headings);
						}

const __vite_glob_0_222 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$S,
  file: file$S,
  url: url$S,
  rawContent: rawContent$S,
  compiledContent: compiledContent$S,
  default: load$S,
  Content: Content$S,
  getHeadings: getHeadings$S,
  getHeaders: getHeaders$S
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$R = {"title":"The New Age of Voice Commerce - Mike Zagorsek, COO, SoundHound - Project Voice X","description":"The New Age of Voice Commerce presented by Mike Zagorsek, COO of SoundHound, presented on day one of Project Voice X. ","date":"2021-12-09T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981399/blog/the-new-age-of-voice-commerce-mike-zagorsek-coo-soundhound-project-voice-x/proj-voice-x-session-mike-zagorsek-blog-thumb-554x.png","authors":["claudia-ring"],"category":"speech-trends","tags":["conversational-ai","genesys","project-voice-x"],"seo":{"title":"The New Age of Voice Commerce - Mike Zagorsek, COO, SoundHound - Project Voice X","description":"The New Age of Voice Commerce presented by Mike Zagorsek, COO of SoundHound, presented on day one of Project Voice X. "},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981399/blog/the-new-age-of-voice-commerce-mike-zagorsek-coo-soundhound-project-voice-x/proj-voice-x-session-mike-zagorsek-blog-thumb-554x.png"},"shorturls":{"share":"https://dpgr.am/7eda44d","twitter":"https://dpgr.am/a16cf9e","linkedin":"https://dpgr.am/8ec982a","reddit":"https://dpgr.am/3933e42","facebook":"https://dpgr.am/a760c62"}};
						const file$R = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/the-new-age-of-voice-commerce-mike-zagorsek-coo-soundhound-project-voice-x/index.md";
						const url$R = undefined;
						function rawContent$R() {
							return "*This is the transcript for “The New Age of Voice Commerce,” presented by Mike Zagorsek, COO at SoundHound, presented on day one of Project Voice X.* \n\n*The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was **94% accurate.**  Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram’s API.  If you want to see if Deepgram is right for your use case, [contact us](https://deepgram.com/contact-us/).*\n\n\\[Mike Zagorsek:] Thank you, everyone. It’s great to be here. I’m Mike Zagorsek. I’m the Chief Operating Officer at SoundHound. Thank you to Bradley for hosting this and bringing us all together. It’s always wonderful to get together as a group. You know, the voice AI community, what we do is is really powerful. It’s times like these that you really appreciate and recognize what a movement looks like. It always starts somewhere and and folks like Bradley are the ones who who help us get started. We have a very dynamic topic. It’s very exciting. But all drama aside, it’s something that we’re really interested and excited to talk about, which is really around voice commerce monetization. We talk about voice assistance, but where is it all going? Where do we see an interesting culmination point? That’s what we’re here to talk about. A quick glimpse into… to our vision at SoundHound as a company. It’s a four part vision. The the first thing we wanna do is build a conversational voice AI platform that exceeds human capabilities. There was examples actually with the the Alexa example how computers didn’t use to beat humans in chess. Now they do because we know if we invest in certain platforms in in in AI, we can accomplish great things. When we talk to voice assistants today, we tend to simplify what we say, because we don’t believe they can exceed human capabilities. Our vision is to surpass that so that they can do more than we can. But a key to this is delivering value and delight to customers. So that’s gonna be one of our themes here. If you’re not delivering value and you’re not delighting your customers, you can’t really move forward. We wanna bring that together and bring an ecosystem of products that are all connected and talking to each other and really enabling innovation and monetization opportunities.\n\nOne of the things that we focus on is empowering other organizations to accomplish great things using as much of our platform as they can. So the two things to focus on as we talked about voice commerce is delivering value and creating monetization opportunities ’cause that’s a benefit to the customer, it’s a benefit to the business, and that’ll be our theme moving forward. A quick glimpse into our our technology. So we’re SoundHound Inc. We have our platform called Houndify, described as a one stop independent voice AI developer platform, all the technology you need to add conversational intelligence. The key thing is you maintain customers con… control over your customers, data, and your brand. There’s a huge market out there for third party voice assistance. We think there’s an equally big, if not bigger, market out there for brands to extend their product experiences through voice using a custom voice interface and using a variety of technologies including ours. Three key technology breakthroughs just to talk about briefly. The first is speech to meaning. So for those of you who are familiar with us, this is something that we like to talk about. Typical voice experiences are a two step process. There’s a transcription from speech to text, and then the natural language understanding deciphers the meaning. When we developed our technology over a ten year process, we wanted to manage it the way the brain does. So when I speak, you’re not translating what I’m saying into text and then trying to understand the meaning you’re doing in a real time. We do that as well. It helps with accuracy and speed. Accuracy because we’re already processing using NLU as the voice transcription comes in, and it’s also… which makes it more accurate and it’s fast because it’s one step instead of two. The other is deep meaning understanding.\n\nSo this is parsing, multiple variables, complex and compound queries. When we speak, again, we don’t clip what we say into individual commands. We tend to say multi command statements. And then lastly, Collective AI, this is a vision to create domains of knowledge talking to each other, controlled by developers in order to enhance the world’s knowledge using a knowledge graph. A quick demonstration for those of you who haven’t heard it. I’m I’m using our voice assistant Hound, which is available. And imagine, for example, if you are looking for restaurants and you just had Chinese last night, if you went to a hotel concierge, for example, and you asked the hotel concierge, show me restaurants except Chinese, they would answer it in that way. But, typically, in most assistance, if you say, show me restaurants except Chinese, you’ll actually get Chinese restaurants back. So the build that they handle exceptions and details like that are really key. I’ll give you a quick demonstration. Hopefully, it comes through the microphone using Hound. Show me restaurants except Chinese.\n\n\\[SPEAKER 2:] Here are several restaurants excluding Chinese restaurants.\n\n\\[Mike Zagorsek:] k. So we handle that pretty well. A quick demonstration in the multivariant component. If you’re doing a restaurant search, you may wanna have a little bit more information. So you can say, show me restaurants in San Francisco, except Chinese and Japanese, that have at least three stars on Yelp, have a patio, and are open past nine PM on Wednesdays.\n\n\\[SPEAKER 2:] Here are several restaurants with more than three stars in San Francisco that are open after nine PM on Wednesdays that had outdoor seating excluding Chinese restaurants, Japanese restaurants, or sushi bars.\n\n\\[Mike Zagorsek:] So you can see it. Part of the… I talk a little bit about customer experience. It shows me the results here visually. Talk about the visual and the voice component. It also speaks it back to reassure me. We can do a follow-up queries and and variations. We can say, does the third one have free Wi-Fi?\n\n\\[SPEAKER 2:] I may be wrong, but according to my data, Octavia, the American restaurant located at seventeen o one Octavia Street in San Francisco does not provide free Wi-Fi.\n\n\\[Mike Zagorsek:] That’s ok. I’ll still eat there anyway. It’s a good restaurant. So that’s just a quick demonstration showing some of our speech to meaning and deep meaning understanding and action and how it ties together. So what we focus on is empowering Lar… currently large global brands to extend their product experience into voice. So we’re in automotive. We’re an IoT, retail restaurants, and multiple language globally. So our primary focus has been to take our technology and scale it into these different brands. So if you experience different voice components, whether it’s the voice mode in Pandora, it’s not here, but we power the the voice scan in in Snapchat or even now the new VIZIO voice mode capability in their TVs. And the reason this is important is… and this is why we’re here. The world is evolving into a voice-enabled reality. I mean, this is why we’re all excited about this business is what binds us together. If you look at the history of it, computers, they offered, you know, keyboard and mice and screen, and then we came with the mobile environment. So, you know, touch screen was the revolution, and, of course, now there’s this voice AI, which is really empowering IoT products. It’s hands free. It’s ambient. It’s everywhere. It’s a new interface. It’s a new reality. And, of course, when businesses… when computers became a reality, every company needed a website. I mean, it was just how it worked. Shortly after, and the time, obviously, is decreasing now with innovation.\n\nEvery company needed a mobile presence, whether it was an app or whether it was even a voice-enabled website. And and we believe, and I think we’re all here because we collectively believe that every company needs a a voice strategy and a path towards its own central voice AI. So this is the idea that every organization will have its own voice AI powered by a variety of technologies that is the extension of its own brand. However, we should point out that these interfaces are integrated and multimodal. Mobile didn’t replace computers. Voice is not gonna replace anything else. They’ll actually all work together, and we just get a better mix, better optimization between keyboards, mice, touchscreen visuals, voice only. The key is that it all empowered in together. So so so why voice AI? I think it’s an important question. Because we can fall in love with the technology, and we can all come together and see the possibilities and imagine a future. And it’s really keeping it simple. It’s it’s just about creating value. Right? We have to deliver something that’s valuable. And I think some of the earlier conversations was about, if you’re not creating technology that makes your life easier, better, then it’s just technology for technology sake. I like to say technologies made by people, for people. The technology is just the part and between. And for consumers, the list can be long, but it’s interaction from anywhere, safety of hands-free control, instant access, but it has to be value for business. You can’t just make it a one way street.\n\nSo you’re extending product capability, increasing competitiveness, customer retention. There’s all these stats that show if voice interfaces are valuable with customers, they become more loyal loyal to businesses. And there’s really different layers of value here. And there’s three, and I’ll talk about the first two. One is what we call just the core voice experience. This is really about voice enabling your product and service. You have to do that well. If there’s only one thing that you do, well, make sure that whatever it is that you’re offering is voice enabled. Because if if you mess that up, people aren’t gonna give you a lot of permission to do anything else. So this is a TV example. You can say something like go back thirty seconds or add this show to my favorite. It’s basic, but it has to work and it has to be clear. The next level is we call expanded. So this is connecting to the world’s information. So you could say, what will the temperature be at noon? Who won the baseball game last night? And this is really bringing the outside in. And this is really that where people start to think about voice assistance because they can answer a lot of questions from the outside and and… or IoT. And these two things alone have already generated a lot of business. I mean, companies are getting on board. You have sixty seven percent of companies who have adopted some kind of voice assist in technology and those that have have extended it to their mobile app. So once they’re in, they double down on it. Even adding to the websites, if it’s still early, but they were seeing some meaningful percentages there. And then, of course, even just embedding into into various products. So companies are getting on board. We know that the excitement is there. The market opportunity is massive. There’s different ways you can slice it. This is a hundred and sixty billion total addressable market scenario, some stats here, ninety percent of new vehicles globally, well, are projected to have voice assistance. Many of them already do. Many of them are our customers.\n\nSeventy five billion connected devices worldwide by twenty twenty five. That’s significant. Seventy five billion connected devices. Most of those won’t have a screen or an interface. So if you think about you have an IoT device that is limited, it has to connect to a phone or connect to another device simply with a microphone and speaker, which are very cheap and inexpensive, you can empower those IoT devices. There’s seventy five billion of them in ways that you couldn’t before with other interfaces. Eight billion voice assistant devices, it’s… you know, some are for a doomsday kind of prediction here, but, you know, expected to surpass the world population. I think we’re far away from anything that’s negative there, and then the vast vast majority of large companies are are already working on voice AI strategies. So back to this, we go to these layers of value and say, ok. Great. There’s a market here. There’s a business. We’re already seeing it. But if you look back at that hundred and sixty billion dollar total addressable market, the TAM, well, where is that gonna come from? How is it going to happen? If you’re in a position to do these two things well, you can then expand into a monetized environment.\n\n> So if you’re watching your TV and you you… you’re already using the voice experience and it works well and you’re connected to the outside world, why wouldn’t you ask, I’d like to order some pizza for delivery or maybe I need to order a soundbar for my bedroom. So this is the progression towards monetization. This is the path that customers were taking because you establish the core value there. By the way, this works for for… I’m talking about voice AI here. But this can work in any other form.\n\nI mean, you can have a chatbot that’s entirely text based. And if you’re delivering a core service and it might be through text, your ability to do it while opens the door to these opportunities to start monetizing. And the way it breaks out is is another simplification. It goes from commands to queries to transactions. Right? So I’m telling my product to do something. I might ask you the question. If you combine commands and queries, that allows you to transact. Obviously, there’s more that needs to happen underneath it, but that’s the gateway and the door moving forward. And and we conducted a study. We partnered with Opus Research to ask these questions. And we weren’t surprised, but it was validated to see how many businesses are really getting on board on monetization there. We ask them, is it more important, less important, or or not important at all? The vast majority in differ… across different industries said this is really important to me because the idea of monetizing, it’s a new form of revenue, and the theme starts to be, how do I turn something that’s in from a cost into a revenue stream in a way that empowers customers. Customers themselves are getting on board already. If if if you’re a customer of a voice-enabled device, forty three percent have used it to shop in some format, and just the shopping revenue alone by next year is expect to be somewhere around forty billion dollars that’s voice related. Not only do businesses and customers agree.\n\nThe experts agree, one from Bradley Metrock. You might know him. Successful voice assistance create a platform from which companies can upsell. Conversational AI decreases transactional friction. More space opens up, it increase per revenue. That’s a really important point, transactional friction. I have to… I can do less. I just ask for things, and if it works, I’m gonna do it over and over and over again. Great contributor to the community, doctor Bajorek. And this really segues into the next point, voice technology about inter… integration rather than replacement that speaks to the additive component. Meet users where they’re at. Try one use case with them and learn from them how they use the technology. The ROI for voice is about looking at user data, crafting high quality user experience based on how users are interacting with your product. So a lot of the the presentations you’ve already seen has been talking about listening, observing, and better understanding, but the key thing here is is adding value. Some… just for a quick examples, and I won’t go into this too much, but automotive, if you can think of food ordering, booking service appointments, parking, filling the gas tank, finding a charging statement. We already know that more than two x adults have used voices in the car… even more so than using this as a smart speaker using the voice experience, TVs and speakers, delivery, booking a car, the list goes on. You know, ten percent more people in twenty twenty are shopping from IoT devices than they did even just a few years ago. Smart home and appliances, product refills, maintenance history requests, just a simple idea that you can have an appliance, and you can transact through it in a way that keeps that customer relationship starts to generate revenue, and, of course, travel and hospitality. We’re all here traveling.\n\nWouldn’t it be nice to be able to do more with your voice specifically? Massive growth is expected there. But question’s how how do we get there? And this could be a full day seminar in and of itself. So I’ll focus on one… specific one. It… it’s always about the customer experience. If you deliver something that works that is a valuable to customers, they will continue to use it. And the… I kinda stole this from my my marketing days because it is actually a marketing challenge. Because what does good marketing do? It’s really about delivering the right message to the right customer at the right time. So it’s really understanding when do they need it proactively or reactively. But what’s the approach? We have to be where your customers are, and and they have to be able to find your high quality content when they want it and whenever and however they prefer to consume it. So if you push it on them… I mean, you think about what advertising is, people don’t like advertising because it doesn’t solve a problem. It’s just pussy… pushing a message, and it’s hopefully trying to get you to buy something maybe you want it, maybe you don’t. But a really powerful suggestion at the right time could be an ad. It doesn’t feel like an ad. It’s solving a problem. So it’s solving a problem for the business because they’re getting in front of customers that they want. It’s solving a problem for customer, ’cause they’re getting information they need. It stops feeling like marketing. It stops feeling like advertising. It starts feeling like an informed conversation, and that’s really where things are headed.\n\nAnother quote from our good friend Cross the Pond, is it working? He talks about whether in the process of implementing your strategy or just getting started, if you think what the pain points customers have throughout the customer journey, the kind of conversations you can have that’ll help, and the number of touch points that could be voice enabled for many brands, even a minor percentage increase in sales will be more than enough to justify the investment. The business case will rise itself. So you’ve got all these interactions. Millions and billions of interactions happening through voice. You’re learning about each other. Just simply making the right suggestion in the right time really will move the needle significantly without even getting into the numbers. And it’s really what we’re talking about here is creating what we call monetizable moments. It’s not saying, hey. We have a platform, and we wanna jam it into people’s lives. It’s… what are those moments that you can add value to the customer where transaction can take place? That’s where we really believe things are are headed in a conversational manner and monetization will actually take place. So it’s… you know, we’re live… we’re… live in the world surrounded by the power of voice, people are asking questions, getting their needs met, and simple suggestions make their lives easier and more convenient. We have a quick two minute video to share, something we put out a few months ago. I have not tested the audio. So let’s hope it plays, and if not, I’ll I’ll fiddle with it. Two minute. And this hopefully will will bring together some of the things I’ve just shared with you.\n\n\\[SPEAKER 3:] We’re living in a world surrounded by the power of voice. With Houndify, your voice assistant now delivers point of revenue and unlimited possibilities for your users.\n\n\\[SPEAKER 4:] Start navigate —\n\n\\[Mike Zagorsek:] Alright. Let me start over.\n\n\\[SPEAKER 3:] We’re living in a world surrounded by the power of voice. With Houndify, your voice assistant now delivers real revenue and unlimited possibilities for your users.\n\n\\[SPEAKER 4:] Start navigation for the fastest route home.\n\n\\[SPEAKER 5:] Sure, David. Starting route guide. You’ll arrive home at six thirty. Would you like to order dinner for delivery? \n\n\\[SPEAKER 4:] Sure. What about pizza?\n\n\\[SPEAKER 5:] Best Pizza has a promo. Any large specialty pizza for twelve ninety nine. Would you like to place an order? \n\n\\[SPEAKER 4:] What’s the most popular item?\n\n\\[SPEAKER 5:] Their top selling item is the combination with red sauce.\n\n\\[SPEAKER 4:] I’ll have one of those, large.\n\n\\[SPEAKER 5:] Ok. Anything else?\n\n\\[SPEAKER 4:] No. Thanks.\n\n\\[SPEAKER 5:] Great. I’ve submitted your order to Best Pizza, and it should arrive by six forty PM.\n\n\\[SPEAKER 3:] Proactive purchasing options unlock the revenue potential of IoT products so your users get what they need in real time.\n\n\\[SPEAKER 6:] What’s the next step? \n\n\\[SPEAKER 7:] Mix in two cups of milk and three tablespoons of butter. Looks like you’re running low on butter. Should I add it to your grocery list?\n\n\\[SPEAKER 6:] Yes, please.\n\n\\[SPEAKER 7:] Great. Your list currently has ten items. Would you like to place a delivery order with food shoppers?\n\n\\[SPEAKER 6:] Sure.\n\n\\[SPEAKER 7:] Please review your shopping list and confirm your order.\n\n\\[SPEAKER 6:] Order confirmed.\n\n\\[SPEAKER 7:] Great. I’ve placed your order with food shoppers, and your groceries should arrive at five PM.\n\n\\[SPEAKER 3:] With Houndify voice commerce, your interactive assistant can turn simple conversations into revenue generating opportunities. \n\n\\[SPEAKER 8:] Will I need an umbrella in Manhattan next week?\n\n\\[SPEAKER 9:] There’s no rain in the forecast next week in Manhattan. Do you need help with wider hotel accommodations for New York?\n\n\\[SPEAKER 8:] Sure.\n\n\\[Mike Zagorsek0:] Please show me round trip flight options from SFO to JFK departing next Monday morning and returning Thursday evening.\n\n\\[SPEAKER 9:] Premier Air has the lowest price and two options for your flight itinerary. Would you like me to book one or see other airlines?\n\n\\[Mike Zagorsek0:] Please book me one seat for option number two.\n\n\\[SPEAKER 9:] I booked your seat with Premier Air and sent a confirmation to your email.\n\n\\[SPEAKER 3:] Welcome to Houndify monetization, where voice transactions bring revenue to your business.\\[nonspeech:music] Unleash your earning potential and open up a world of purchasing possibilities with a custom voice assistant that boosts your bottom line. Powered by Houndify. \n\n\\[Mike Zagorsek:] So two key themes in there. There were suggestions that were made very easily and seamlessly. You could refer to those as voice ads. They don’t feel like voice ads, transactions take place. There you have voice commerce. Of course, that was a very, very simplified video trying to convey the idea. There’s many layers underneath it. But the seamlessness in these moments are the ones where you were adding value, companies are getting value and, ultimately, everybody wins. So, you know, we believe that monetization is the future of AI. I mean, we’re… it allows companies to optimize their technology investments. There’s revenue share to be made between the product creators and the the service providers. It delivers in the promise of engaging voice experience. It’s adding more value if done correctly and putting the customer first. You know, unintrusive conversational suggestions, they become purchasing opportunities, again, if done correctly with the right data. New revenue streams come… increase earning potential and, ultimately, we think that voice AI should generate revenue, not cost. So company should be asking themselves, what is my revenue earnings potential by moving forward in this technology versus saying how much is it ultimately going to cost me? So with that, thank you very much. Appreciate it.";
						}
						async function compiledContent$R() {
							return load$R().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$R() {
							return (await import('./chunks/index.453220ed.mjs'));
						}
						function Content$R(...args) {
							return load$R().then((m) => m.default(...args));
						}
						Content$R.isAstroComponentFactory = true;
						function getHeadings$R() {
							return load$R().then((m) => m.metadata.headings);
						}
						function getHeaders$R() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$R().then((m) => m.metadata.headings);
						}

const __vite_glob_0_223 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$R,
  file: file$R,
  url: url$R,
  rawContent: rawContent$R,
  compiledContent: compiledContent$R,
  default: load$R,
  Content: Content$R,
  getHeadings: getHeadings$R,
  getHeaders: getHeaders$R
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$Q = {"title":"The Trouble with Word Error Rate (WER)","description":"Word error rate (WER) is one of the most common ways to evaluate automatic speech recognition. But is it the best choice?","date":"2019-01-03T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981951/blog/the-trouble-with-wer/placeholder-post-image%402x.jpg","authors":["morris-gevirtz"],"category":"ai-and-engineering","tags":["word-error-rate"],"seo":{"title":"The Trouble with Word Error Rate (WER)","description":"Word error rate (WER) is one of the most common ways to evaluate automatic speech recognition. But is it the best choice?"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981951/blog/the-trouble-with-wer/placeholder-post-image%402x.jpg"},"shorturls":{"share":"https://dpgr.am/c9b9228","twitter":"https://dpgr.am/292279b","linkedin":"https://dpgr.am/2ac3719","reddit":"https://dpgr.am/ceeddb9","facebook":"https://dpgr.am/5214a16"}};
						const file$Q = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/the-trouble-with-wer/index.md";
						const url$Q = undefined;
						function rawContent$Q() {
							return "In the spring of 2017, Google announced that their voice recognition [WER (word error rate)](https://blog.deepgram.com/what-is-word-error-rate/) had fallen to just 4.7%-putting it on the par with human transcriptionists. \"Wow!\" said the world. Highly accurate speech recognition is changing how we interact with computers, how we advertise and so much more. Too bad that a 4.7% WER is advertising bunk. Several large companies have announced similarly low rates-and these low rates *are* real, but here's the catch: **They managed to get human-level accuracy by training their [ASR (automatic speech recognition)](https://blog.deepgram.com/what-is-asr/) systems on a small language corpus, like the [National Switchboard Corpus](https://catalog.ldc.upenn.edu/LDC97S62).**\n\nThe National Switchboard Corpus is a well used (overly-used) database of phone calls that have been carefully transcribed for linguistics research. When companies announce that their new speech recognition system has impossibly low word error rates, it's because they are trained and validated on this very limited data set. No company has yet to reliably deliver 4.7% accuracy on everyday audio-the sort of audio that comes through call centers and cloud conferencing companies and needs to be transcribed.\n\nIn fact, even the most highly trained (and expensive) human transcriptionists would struggle to get 4.7% accuracy on regular \"wild\" audio data. When data scientists look at different speech recognition APIs (ASR products), they evaluate them according to several metrics, WER being a principal one. Yet, WER is not a perfect metric. This is because WER is strongly affected by:\n\n1. Different kinds of noise\n2. Crosstalk\n3. Accents\n4. Rare words\n5. How transcripts are (or aren't normalized)\n\n## 1. Noisy Voice Data Lowers WER\n\nHave you ever wondered why the NATO phonetic alphabet exists? The NATO phonetic alphabet (NATO-PA for short) is the cool words that pilots use to communicate (both in real life and in the movies). You've heard it:\n\n> **\"Delta bravo eight nine six cleared to land two six right.\"**\n\nThe NATO phonetic alphabet was created to address the very same problem that causes WER to vary greatly for any one system: the variety of accents in language and the noisiness of communication methods make comprehension difficult. In 2018, top-performing call center representatives all know the NATO-PA because the [phone-POTS](https://blog.deepgram.com/is-your-infrastructure-supporting-you-or-weighing-you-down/) and VoIP-are noisy, compressed communication media. ![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976790/blog/the-trouble-with-wer/nato-alphabet.png) \n\nToday, we have a variety of voice communication channels, but we still face the same problem that WW2 pilots faced: The phone systems our customers call us on introduce a lot of noise into the voice data. The noise may come from problems in the line, compression, and ambient noise.\n\n### Line Noise\n\nPeople call businesses from office phone lines, from their cell phones, even occasionally from a sat-phone. All these systems introduce noise from bad or weak connections: bad jack, bad signal, squirrely satellite. All the long-distance communication systems introduce noise somehow and can seriously hamper ASR systems. But, line noise is not the only form of noise that causes problems for transcription.\n\n### Compression\n\nAll voice communication systems, VoIP, chat, Whatsapp and so on compress their audio. Compression means that the phone systems are designed to be more efficient by removing a lot of information. That makes talking cheap, but it also means that you can't understand as much. Have you ever tried understanding a new word over the phone? Impossible, no?\n\n### Ambient Noise\n\nFew of us have a landline anymore. Plus, all of us are very busy. This means that when we do call the companies we buy from, we are likely calling from noisy environments.\n\n* Noisy office\n* Echo-y conference room\n* Crowded restaurant\n* Rambunctious playground\n* Siren-filled street\n* Windy beach\n\nTo make matters worse, there is no way to control for the quality of the microphone or how far away people speak from the conference room microphone. The fact is, most real-world data is going to be noisy. Therefore, when using a speech recognition system to decode your voice data, make sure it's trained on *your kind of noisy* voice data.\n\n## 2. Crosstalk: Humans Come in Pairs\n\nThere is another form of 'noise' that really dizzies brains and off-the-shelf speech recognition systems: **crosstalk.** As long as we are in the room, know the topic and the speakers intimately, it isn't hard to follow a conversation when two people talk over each other. You may not even realize when someone starts their sentence before you can end your's. However, recordings of human conversation are hard to follow and even harder to transcribe. Most of the difficulty arises from crosstalk-the moment when two people speak at the same time. Crosstalk is incredibly common-it is a natural and somewhat unavoidable part of human conversation. Due to the fact that humans and machines have trouble sorting out the words said in crosstalk, you get unpredictable results when it gets transcribed.\n\n* How do you represent simultaneous speech in text?\n* What if crosstalk makes things inaudible for humans and machines?\n* Is it more important to transcribe one speaker versus another?\n\nThere are few standards developed to deal with the \"crosstalk error.\" As a result, what gets transcribed is done in many different ways: sometimes correctly but in the wrong line (so it looks like deletion AND substitution errors at the same time) or completely omitting one speaker's words entirely. Naturally, such confusing standards would confound ASR and greatly impact WER. Data scientists should therefore consider how much crosstalk there is likely to be in their data. Web conference audio data likely has a lot and YouTube recipe videos likely have little, unless it's Giada attempting to teach Nicole Kidman and Ellen Degeneres to cook.\n\n<iframe src=\"https://www.youtube.com/embed/PCSs_nj0938?start=10\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\nWhen choosing a speech recognition API, look at where the errors occur. If the crosstalk errors are not important, consider re-calculating the error rate for yourself.\n\n<WhitepaperPromo whitepaper=\"latest\"></WhitepaperPromo>\n\n## 3. Accents: They're Sexy and Confusing\n\nEnglish is spoken natively by nearly a billion people. You can imagine that in such a large number of speakers there is going to be some variety in accents. Plenty of academic research (and our own experience) tells us that people often have trouble recognizing accents they are unfamiliar with-even when these are dialectical accents (from their own language). As you can expect, speech recognition systems also need to be trained on different accents. The NATO phonetic alphabet we mentioned above was created to address two problems with aviation communications: noise and accent. At the beginning of World War II, the Australian, American and British pilots had trouble understanding each other over the radio. Several radio alphabets were devised as a fail-proof system to communicate vital flight data across English accents. These were eventually formalized and standardized in the 1950s for use in commercial aviation. If we could train all English speakers to spell out every word they say with the NATO-PA, speech recognition would be a solved problem. Such a proposition is absurd.\n\nWhen companies like Google announce a 4.7% WER, they are effectively saying our speech recognition systems are really good at understanding this one accent, in this kind of a noise, using a known and limited list of words. That's not real life.\n\nIn a global economy, where our suppliers, employees and customers come from every corner of the globe, no one speech model can yet accurately transcribe every conversation. Even Silicon Valley giant Google isn't proficient enough to account for all English accents. Yet, in globally competitive economy, leveraging diverse voice data at scale is soon to be indispensable. Just like humans need to become familiar with an accent or a speaking style they are unfamiliar with, speech recognition models need to be trained to do the same.\n\n## 4. The Words that Matter are Rare\n\nWe're all using the words everyone else is using. This is an observation made by greats such as Aristotle and George Carlin. And it has deep meaning for evaluating a speech recognition API. Most of the words we use are the \"glue\" of language.\n\n* Yes\n* The\n* And\n* Going\n* I\n* They\n* So\n\nFrom a linguistics point of view, these glue words **collectively carry a lot of meaning.** You change their order, add an adverb or adjective and you've said a lot. However, this also means that the rare words, the 80% or so of words we use only once or twice in a conversation, are **individually loaded with meaning.** ![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976791/blog/the-trouble-with-wer/podcast-words-1.jpg) *Take a look at an episode of our podcast and you'll see we used 922 unique words. Here we see the 50 most used words. This means 5% of the individual words account for 50% of all words said! **Notice that 3 of the words are \"meaningful\" and the rest are just grammar.** These are the meaningful words that actually made it into the top fifty most used words. Despite this they only account for 1.7% of all words. Most truly meaningful words are used less than 2 times in the 30 minute podcast (less than .03%).*\n\nThese rare words are the ones that ASR systems often fail to recognize. In our businesses and sciences, we have extra words: our own common glue words and our own rare used-once-in-a-conversation words. These words are called jargon. Jargon words are the loaded-with-meaning words that general model speech recognition APIs often miss.\n\n* \"Fraudulent payment\"\n* \"Action item\"\n* \"Convolutional neural network\"\n* \"SaaS\"\n* \"Exec-dev estimates\"\n\nThey are also usually the words that companies most want to transcribe accurately. Even a low WER transcript may not be useable if the keywords-the jargon-are not well transcribed. Since they are rare, missing them does not strongly influence the WER, yet since they are important to us, transcribing these incorrectly can be extremely debilitating. In other words, since the information-dense words are rare, a transcript may boast a great WER, but still not be useful. Always make sure you know what words matter to you in a transcription. When your data team evaluates automatic speech recognition transcripts, they should make sure that their keywords are accurately transcribed across all their audio data.\n\n## 5. Normalization Discrepancies\n\nOn its face, the word error metric seems straight forward: how many words are transcribed incorrectly when compared to some 'correct' version. However, to calculate WER you must decide what counts as a word and what counts as an error.\n\n* Does punctuation matter?\n* How about the standard um, uhs, and uh huh's of normal human speech?\n* Should numbers be written in Arabic numerals: \"53\" or written out: \"fifty three\"?\n* Are normal contracted words like \"it's\" written as two words \"it is\" or as the contraction?\n* Are minor discrepancies such as pluralizations wrong?\n* What about phonetic reductions like \"gonna\" or \"runnin'\"?\n\nThe variation in word-distance introduced by not normalizing results in WER causes great unpredictability in transcript outputs. When you compare speech API outputs to your \"perfect\" transcript, you have to normalize the transcripts so that you are comparing apples to apples, and not apples to pickles or tangerines.\n\n## Know your WER\n\nWhen looking at word error rates remember to consider:\n\n### **1. The Kind of Audio Recording Being Transcribed**\n\n* Is it phone calls?\n* Is it crisp, bassy podcast audio?\n* Is it cloud conference data?\n\n### **2. The Kind of Speaking Happening**\n\n* Is it a fast conversation with lots of crosstalk?\n* Is it a monologue (radio, TV)?\n\n### **3. Who is Speaking**\n\n* Do the speakers have the same accent?\n* Do the speakers have an accent not supported by a particular ASR?\n\n### **4. The Subject Matter of the Conversation**\n\n* Can the ASR recognize the words and terms that matter to you?\n\n### **5. Normalizing the Data**\n\n* What are you comparing your transcript to?\n* What punctuation and structure should your transcript have?\n* Have you converted dollar amounts and phone numbers to a consistent format?\n\nTo get a better read on how well an ASR system is performing on your audio data, also check out [How to Evaluate a Deep Learning ASR Platform](https://offers.deepgram.com/how-to-evaluate-deep-learning-asr-platform-solution-brief)**.**";
						}
						async function compiledContent$Q() {
							return load$Q().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$Q() {
							return (await import('./chunks/index.50d41eb2.mjs'));
						}
						function Content$Q(...args) {
							return load$Q().then((m) => m.default(...args));
						}
						Content$Q.isAstroComponentFactory = true;
						function getHeadings$Q() {
							return load$Q().then((m) => m.metadata.headings);
						}
						function getHeaders$Q() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$Q().then((m) => m.metadata.headings);
						}

const __vite_glob_0_224 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$Q,
  file: file$Q,
  url: url$Q,
  rawContent: rawContent$Q,
  compiledContent: compiledContent$Q,
  default: load$Q,
  Content: Content$Q,
  getHeadings: getHeadings$Q,
  getHeaders: getHeaders$Q
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$P = {"title":"Is Speech the Weak Link in Your Multichannel Strategy?","description":"Are you neglecting speech when it comes to your multichannel strategy? Learn why that's a bad plan and how to change it.","date":"2018-07-19T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981314/blog/the-weak-link-in-your-multichannel-strategy/speech-weak-link%402x.jpg","authors":["scott-stephenson"],"category":"speech-trends","tags":["voice-strategy","voice-tech"],"seo":{"title":"Is Speech the Weak Link in Your Multichannel Strategy?","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981314/blog/the-weak-link-in-your-multichannel-strategy/speech-weak-link%402x.jpg"},"shorturls":{"share":"https://dpgr.am/3e670d8","twitter":"https://dpgr.am/63dcaa0","linkedin":"https://dpgr.am/0e8d7ea","reddit":"https://dpgr.am/5515887","facebook":"https://dpgr.am/6970f46"}};
						const file$P = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/the-weak-link-in-your-multichannel-strategy/index.md";
						const url$P = undefined;
						function rawContent$P() {
							return "With the prevalence of mobile technologies like smartphones and laptops, it's no surprise that today's customers have come to expect seamless communication. They want answers in seconds, and companies are taking steps to make that expectation a reality.\n\n> [4 out of 5 executives](https://www.mckinsey.com/business-functions/operations/our-insights/why-your-call-center-is-only-getting-noisier) ranked providing digital tools to promote self-research and quick support via online chats, FAQs, and social media as one of the most important technologies over the next five years.\n\nHowever, there's one channel that's been left by the wayside - <mark>the phone</mark>. ![](https://images.unsplash.com/uploads/1413222992504f1b734a6/1928e537?ixlib=rb-0.3.5&ixid=eyJhcHBfaWQiOjEyMDd9&s=ae29adbe918cb18e89f9c06a97c0c878&auto=format&fit=crop&w=2250&q=80)\n\n## A Costly Mistake\n\nAlthough the phone is the backbone of the call center, the resulting experience is often lackluster. That's an incredibly costly mistake given that\n\n> #### **[74% of people who had a bad phone support experience are likely to choose another business the next time they shop](https://hbr.org/2017/07/your-customers-still-want-to-talk-to-a-human-being)** .\n\nCompanies have implemented measures and tools like warm transfers, IVR systems, advanced routing, and real-time metrics in an attempt to improve customer experience, yet calling a company's support line is still a dreaded, and necessary, activity. <mark>While improvements in alternate communication channels can help boost a company's customer service, the phone is a crucial link in the process that cannot be overlooked or deprioritized</mark> for a number of reasons:\n\n1. **[Communicating by voice is faster](https://hbr.org/2017/07/your-customers-still-want-to-talk-to-a-human-being)**, easier, and more effective than typing messages back and forth.\n2. **Call volumes are growing**, with calls to businesses [expected to exceed 169 billion per year by 2020.](https://hbr.org/2017/07/your-customers-still-want-to-talk-to-a-human-being)\n3. As customers are able to find answers to simple questions through alternate channels, **[the largest share of call volumes will be made up of complex interactions](https://www.mckinsey.com/business-functions/operations/our-insights/why-your-call-center-is-only-getting-noisier)** that involve higher average handling times.\n4. **[80% of customers say the experience a company provides is as important as its products and services](https://c1.sfdcstatic.com/content/dam/web/en_us/www/documents/e-books/state-of-the-connected-customer-report-second-edition2018.pdf)**, making a seamless transition between human and digital interactions crucial.\n\n## What's Hindering Success?\n\nWith these statistics, one might ask why companies haven't taken measures to improve their call experience. Simple, phone is the most expensive channel and audio data is much harder than text data to analyze for optimization. Maintaining a call center requires lots of human time and as a result, call centers are limited in their capabilities.\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-how-deepgram-works\"></WhitepaperPromo>\n\n## What Steps Can be Taken?\n\nThankfully, speech recognition driven by artificial intelligence has the potential to significantly amplify call center efficiency. With its ability to quickly process recorded audio, **[automatic speech recognition](https://blog.deepgram.com/what-is-asr/) makes it possible to [analyze one hundred percent of calls](https://deepgram.com/solutions/contact-centers/)**, vastly increasing a call center's ability to monitor calls to make strategic changes that better operations and reduce costs. It will support agents by navigating callers through intuitive IVRs, extract context from calls to aide transfers, and read the caller's sentiment and intent in real-time. <mark>In effect, its accuracy, scalability, and actionable insights will lead to increased customer retention, customer satisfaction, and sales</mark>. Recorded phone calls hold a wealth of information that companies would be shortsighted to leave untouched. While multichannel may be the latest challenge in the [contact center](https://deepgram.com/solutions/contact-centers/), critical business insights are likely hidden within the traditional contact center's bread and butter. **Investing in technology that helps unlock understanding into why your customers are calling, employee performance, and what steps need to be taken to grow your business is the key to delivering a winning and unified customer experience.**";
						}
						async function compiledContent$P() {
							return load$P().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$P() {
							return (await import('./chunks/index.d8b04256.mjs'));
						}
						function Content$P(...args) {
							return load$P().then((m) => m.default(...args));
						}
						Content$P.isAstroComponentFactory = true;
						function getHeadings$P() {
							return load$P().then((m) => m.metadata.headings);
						}
						function getHeaders$P() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$P().then((m) => m.metadata.headings);
						}

const __vite_glob_0_225 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$P,
  file: file$P,
  url: url$P,
  rawContent: rawContent$P,
  compiledContent: compiledContent$P,
  default: load$P,
  Content: Content$P,
  getHeadings: getHeadings$P,
  getHeaders: getHeaders$P
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$O = {"title":"Tips on Choosing a Call Analytics Development Path","description":"Here are our top tips on choosing a call analytics development path!","date":"2021-10-13T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981382/blog/tips-on-choosing-a-call-analytics-development-path/choosing-call-analytics-dev-path-blog-thumb-554x22.png","authors":["keith-lam"],"category":"ai-and-engineering","tags":["call-analytics"],"seo":{"title":"Tips on Choosing a Call Analytics Development Path","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981382/blog/tips-on-choosing-a-call-analytics-development-path/choosing-call-analytics-dev-path-blog-thumb-554x22.png"},"shorturls":{"share":"https://dpgr.am/32b250a","twitter":"https://dpgr.am/b295c2c","linkedin":"https://dpgr.am/447bbd8","reddit":"https://dpgr.am/7536bf0","facebook":"https://dpgr.am/3c00d41"}};
						const file$O = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/tips-on-choosing-a-call-analytics-development-path/index.md";
						const url$O = undefined;
						function rawContent$O() {
							return "Call analysis can be real-time or post-call to pull out relevant data or review to make sure the agent said, \"This call is being recorded for coaching purposes,\" or \"Prices can fluctuate before you book.\" Data analysis can look at how many times customers say, \"stop my service,\" use profanity, are angry, are happy, or used words that indicate customer churn. On the positive side, analysis can see how many times a salesperson recommended an add-on service, provided promotional free items, talked less than the customer, or was in a good mood. \n\n## How Call Analysis Works\n\nThis process starts with recording and collecting the audio from both the customer and the agent, normally on two channels, and sending it to a speech-to-text solution that converts the audio to text. The speech-to-text platform may also send other metadata to help the Natural Language Processing/Understanding solution determine the intent of the conversation. NLP/NLU solutions take the text and try to find the intent of the conversation and sentiment. Is the conversation about a product sale or product issue? Are the customer and agent happy or angry? This data is fed into a data integrator that organizes the text and meta-data, like sentiment, into a database that can be queried. The end-user business intelligence (BI) application uses the data query to create the dashboards for supervision and management of all the calls. The overall goal of call analytics is to improve customer experience, improve customer support, increase sales or reduce costs.\n\n![](https://res.cloudinary.com/deepgram/image/upload/v1661976848/blog/tips-on-choosing-a-call-analytics-development-path/call-analytics-reference-architecture%402x-1.png)\n\n### Audio and Meta-Data Collection\n\nAudio capture is normally done with existing infrastructure; UCaaS, CCaaS, on-device applications, smart speakers, PBX, or VOIP. Depending on the speech-to-text (STT) solution, the audio capture may need to be converted into different file formats for real-time streaming to the STT. More sophisticated systems may capture audio patterns, tone, and frequency.\n\n### Speech-to-Text\n\nSpeech-to-text transcribes the speech in the audio capture into text for the NLU/NLP to parse and use. The more accurate the STT the better the results from the NLU/NLP. In addition, some STT systems also provide diarization, audio sentiment, speaker ID, speaker isolation, noise reduction, and metadata on pitch, pace, tone, and utterances. This is normally a separate best-of-breed vendor. STT providers include Deepgram, Google speech-to-text, Amazon Transcribe, Nuance, and IBM Watson Natural Language Processing and Understanding NLU/NLP is the main processing to turn words, sentences, sentiment, audio metadata into intent and sentiment. What does the speaker want to convey? It matches the words to the intent so the data integration engine can organize the data and provide additional tags on the speakers. This can be part of a complete sales or support enablement solution or a separate best-of-breed vendor such as OneReach.ai, Rasa, or Cognigy.ai.\n\n### Data Integration\n\nData integration takes all the text and organizes it so you can perform queries on the data. It also organizes the meta-data to be able to query. Was the customer happy, neutral, or sad? Was this a sales inquiry or support call? This step buckets that conversation into these pieces.\n\n### Data Query\n\nData query engine does the work of pulling the right data for the business intelligence application. The end-user might ask the business intelligence application to show him all calls where there was profanity and the data query will pull back the results.\n\n### Business Intelligence\n\nThis business intelligence end-user application is where you see the analytics and gain insight into the business to make the right decisions. Maybe there is a compliance issue, the BI can search all the calls to see which ones and who is not providing the compliance message that could lead to fines. Or the BI can create a chart to see which salesperson is always mentioning a new add-on service and the success rate of these sales. When looking at call analytics solutions, most of all in one solution which handles all parts of the process, with some handling the audio collection also. All-in-one providers include Calabrio, Call Tracking Metrics, Five9, Genesys, Sharpen, Talkdesk and Tethr.\n\n## Tips for Choosing Your Development Path\n\n**Questions to Answer** - Before looking for a call analytics solution, you should be very clear on the questions you want to be answered by the BI. Are you using it for compliance? Using it for increased sales? Employee management, coaching, or ranking? Trying to tie support calls to churn? Finding ideas for new products or services?\n\n* **Tip** - Write these questions down before reviewing vendors so you already have the list of the top priority questions you want to be answered by the BI. This will also prevent you from being dazzled by great UI that does not provide actionable insights. Some of your questions may not be able to be answered by traditional logic and require AI to find the patterns.\n\n**Real-Time or Post-Call** - Currently you will find many more post-call call analytics solutions than real-time solutions. Mostly because real-time has not been real-time analysis, but most recently this has been changing with vendors adding in true streaming STT, NLU, and BI that may take seconds for updated analyses.\n\n* **Tip** - As you determine what questions you want to be answered, you can determine how quickly you want them. Do you need real-time analysis to escalate a customer who sounds like they might churn? Or can you wait a day or two? Do you want to provide immediate feedback and coaching? Or want to look for more patterns? In addition, you should ask your providers if they have real-time analysis on their roadmap, as well as what do they consider \"real-time\" (milliseconds, seconds, minutes, next day)?\n\n**Sample or Use All** - Do you want to just sample your audio or record and analyze it all? Less than 5 years ago, this would not be a question as recording, transcribing, and analyzing all your audio data was highly expensive, compute resource-intensive, and would take days to transcribe one day of calls. With the advent of end-to-end deep learning neural networks, cheaper computing, and better optimization, it can be affordable to analyze all your call data and get actionable insights the same day.\n\n* **Tip** - No data scientist ever said, \"You know what, I have enough data to analyze and I know it is not going to change.\" With ongoing churn in your employee base and changing customer needs, you should find a solution that can provide you with all the call data at an affordable price. Sampling 5-10% of your calls may bias your decision. Why stop there when you can get a more comprehensive picture of your customer base? \n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-how-deepgram-works\"></WhitepaperPromo>\n\n## Wrapping Up\n\nWe hope this gave you the insight you need to start getting more out of your call analysis data. We want to emphasize that this technology is changing rapidly so having a partner that's actively evolving with it and with your needs rather than a vendor with a one-size-fits-all approach is essential. They should have a product that includes real-time analysis powered by AI, which will find insights you missed. Waiting for data and insights to act upon in our real-time world may cost you revenue and customers.";
						}
						async function compiledContent$O() {
							return load$O().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$O() {
							return (await import('./chunks/index.5607fac3.mjs'));
						}
						function Content$O(...args) {
							return load$O().then((m) => m.default(...args));
						}
						Content$O.isAstroComponentFactory = true;
						function getHeadings$O() {
							return load$O().then((m) => m.metadata.headings);
						}
						function getHeaders$O() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$O().then((m) => m.metadata.headings);
						}

const __vite_glob_0_226 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$O,
  file: file$O,
  url: url$O,
  rawContent: rawContent$O,
  compiledContent: compiledContent$O,
  default: load$O,
  Content: Content$O,
  getHeadings: getHeadings$O,
  getHeaders: getHeaders$O
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$N = {"title":"Tips on Choosing a Conversational AI Development Path","description":"What is the general architecture of a Conversational AI voicebot and what are some tips on choosing a development path for a voicebot.","date":"2021-09-22T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981381/blog/tips-on-choosing-a-conversational-ai-development-path/choosing-convo-ai-dev-path%402x.png","authors":["keith-lam"],"category":"ai-and-engineering","tags":["conversational-ai"],"seo":{"title":"Tips on Choosing a Conversational AI Development Path","description":"What is the general architecture of a Conversational AI voicebot and what are some tips on choosing a development path for a voicebot."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981381/blog/tips-on-choosing-a-conversational-ai-development-path/choosing-convo-ai-dev-path%402x.png"},"shorturls":{"share":"https://dpgr.am/3e9e488","twitter":"https://dpgr.am/5cb8581","linkedin":"https://dpgr.am/8140dfc","reddit":"https://dpgr.am/813e1ab","facebook":"https://dpgr.am/e28d53c"}};
						const file$N = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/tips-on-choosing-a-conversational-ai-development-path/index.md";
						const url$N = undefined;
						function rawContent$N() {
							return "Conversational AI, and specifically voicebots, are a transformational technology for many industries. In this blog, we would like to share a high-level process flow of how a Conversational AI voicebot works, the types of technologies that are used to deliver a near-human experience, and tips on starting your development path. On a high level, the process should behave like this.  A speaker talks into a phone, intercom, or app, and the audio is captured by a system, which sends the audio to a speech-to-text platform to turn the audio into text.  The speech-to-text platform may also send other metadata to help the Natural Language Processing/Understand solution determine the intent of the conversation. NLP/NLU solutions take the text and try to find the intent of the conversation. Are they ordering food, do they need their billing information, do they want to make a payment, are they interested in new products, do they need technical support.  After the NLP/NLU determines the intent, it is fed into the business logic to find the correct response for that intent.  The response engine could create the following responses, \"your balance is $1000\", \"your next bill is $56.79\", \"Or your order is one Big Mag, one large fries, and a Diet Cherry Coke.\"  This response which is in text form is then fed into a text-to-speech solution to create a vocal response to the speaker.  Responding naturally requires this pipeline to have very low latency, typically less than 1 second with shorter latency being a large factor in the success of a solution.\n\n![](https://res.cloudinary.com/deepgram/image/upload/v1661976847/blog/tips-on-choosing-a-conversational-ai-development-path/convo-ai-reference-architecture%402x.png)\n\n**Audio Capture** Audio capture is normally done with existing infrastructure; UCaaS, CCaaS, on-device applications, smart speakers, PBX, or VOIP.  Depending on the speech-to-text (STT) solution, the audio capture may need to be converted into different file formats for real-time streaming to the STT.\n\n**Speech-to-Text** Speech-to-Text transcribes the speech in the audio capture into text for the NLU/NLP to parse and use. The more accurate the STT, the better the results from the NLU/NLP.  In addition, some STT systems also provide diarization, audio sentiment, speaker ID, speaker isolation, noise reduction, and metadata on pitch, pace, tone, and utterances.  This is normally a separate best-of-breed vendor. STT providers include Deepgram, Google STT, Amazon Transcribe, Nuance, and IBM Watson.\n\n**Natural Language Processing and Understanding** NLU/NLP is the main processing of the voicebot to turn words, sentences, sentiment, audio metadata into intent.  What does the speaker want to do or convey?  It matches the words to the intent so the business logic and response can be determined.  More advanced systems are contextually aware and know the user context and preferences. They add behavior prediction, other user data, and the conversational history to process the audio and provide a more accurate response. This can be part of a complete voicebot solution or a separate best-of-breed vendor such as OneReach.ai, Rasa, or Cognigy.ai.\n\n**Validation and Business Logic** After the intent is presented to this step, decisions are made to determine the response. Is the intent to query for an answer, such as an account balance, how to get a new pin, order a new product, create an account, or get technical support?  This can be a decision tree logic or an AI that can direct the intent to the right area.  This process is normally bundled with the NLU/NLP step.\n\n**Response** The response is the text output from the Validation and Business Logic step or the result of a query from a knowledge database or backend system. These responses can be pre-set, scripted responses, or AI-generated responses in text. More advanced systems can add small talk, compound responses, and summarize the discussion. This process is also normally bundled with the NLU/NLP step with connectors to knowledge bases, backend systems, or other AI systems.\n\n**Text-to-Speech** This last Text-to-Speech step takes the text response and presents it to the speaker in audio form.  In the design of this step, you would choose the voice to use, personality, language, accent, and dialect. Advanced systems can personalize the voice to what the user likes, add empathetic voice emotions (\"I hear you are frustrated\"), add intermediate dialogue for long processing (\"Hold on while I look that up\"), and add confirmation cues (\"aha\", \"hm\", \"huh\").  This step is normally a separate best-of-breed vendor.   Overall Conversational AI Voicebot providers include:  Agara.ai, Elerian.ai, and Uniphore These voicebots are focused on short conversations, not one-word responses from the customer.  For example, these voicebots intake conversations like \"I would like to know my account balance please\" versus IVRs that only accept one word, \"balance\".  This voicebot can parse out the keywords for the business logic to provide the right response.\n\n## **Tips for Choosing Your Development Path** **Metrics for Success**\n\nHow are you going to measure success for your Conversational AI voicebot?  Do you have hard metrics that directly relate to great customer experience? One metric to consider is the overall latency of the solution.  In other words, from the time the user stops talking, how long does it take your voicebot to respond?  Or can your voicebot respond when there is a short pause, like in real conversations?  For your STT, is a general Word Error Rate (WER) acceptable? Or do you need to be looking at more specific WER-the word error rate on specific terms, keywords, alphanumeric numbers, or jargon essential to interact with machines? Some metrics you might consider are listed in this technical whitepaper.\n\n* **Tip** - Map the performance metrics to the business case success or better customer experience before you start comparing solutions.  Find the metrics that matter and eliminate the metrics that don't.  In STT for example, is a general WER acceptable? Remember, even a low WER doesn't distinguish between important and unimportant words. But a specific WER focused on your product names might give you better performance.\n\n**Build or buy** - This is always one of the first questions to answer before selecting a new technology implementation. Variables in this decision are initial costs, implementation costs, maintenance costs, resources, timeline, internal experience and skills needed, learning curve, innovation control, etc. For voicebots, we are still in the early stages of technology evolution and hence there are many changes. Kevin Fredrick, the Managing Partner of OneReach.ai, expressed this best when he said, \"Building a Conversational AI voicebot is like planning to summit a mountain. Those who are looking for an 'easy button' get frustrated and quit. The ones who think it will be too hard, don't ever start. It is the ones who know the challenge is worth it and have the right partners and use the right tools who make the summit.\"\n\n* **Tip -** Unless you have a team very experienced in voicebots and NLP/NLU, buying might be your best bet at this time.  However, be flexible in your implementation as technology is changing rapidly and you don't want to be locked into one platform or vendor.\n\n**Communication Platform Add on, All in One, or Best of Breed** - As we are in the early stages of this technology there is a lot of choice from communication platforms that have an add on voicebot, voicebot specific companies, and best of breed solutions for STT, NLP/NLU, and TTS.  Which path you take again depends on your experience. Can your team manage the implementation and maintenance of various vendors that may make up your voicebot or do you want one vendor to handle it all? Is customization of the voicebot, control of innovation, speed of implementation, or ease of vendor management important? With overall trends toward specialization, best-of-breed solutions individually are generally better than an all-in-one solution, and an all-in-one is typically better than add-on solutions.\n\n* **Tip** - This decision hinges on time, experience, and resources but also control of your innovation and roadmap.  Best of breed solutions are great if you want to control your innovation and may want to switch out individual component providers as new technologies emerge,  All-in-one is the simpler solution for any company but you are stuck with the solution's innovation timeline.  Look very closely at Add-on solutions as they may be behind in technology and innovation.\n\n**General or Tailored** - Each technology provider may focus on industries, regions, markets, or use cases or offer a general technology tailored to your specific use case.  You need to determine how the technology fits your use case, language, culture, and compliance needs. Some voicebots may be general and you customize the responses and logic while others are specialized for banking and include customer ID by voice. Do you need a voicebot to be able to speak multiple languages or understand different accents or dialects?\n\n* **Tip** - Study what is available in the current market and what the analysts are saying. A good start is this Gartner paper, [\"Architecture of Conversational AI Platform\"](https://emtemp.gcom.cloud/ngw/globalassets/en/doc/documents/723272-architecture-of-conversational-ai-platforms.pdf).  List out the end customer priorities for your voicebot and the features you need in priority order.  Then screen the choices down. We have found that a more tailored approach works best and provides a better customer experience than a more general voicebot. \n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-how-deepgram-works\"></WhitepaperPromo>\n\n## Wrapping Up\n\nWe hope this gave you the insight you need to take the next step in starting or advancing your conversational AI experience. We want to emphasize that this technology is changing rapidly and a one-stop-shop solution may sound great but may end up locking you into technology that's quickly dated. So make your implementations are flexible enough to allow you to pivot for the best voicebot experience possible now and in the future.";
						}
						async function compiledContent$N() {
							return load$N().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$N() {
							return (await import('./chunks/index.9e7e6d84.mjs'));
						}
						function Content$N(...args) {
							return load$N().then((m) => m.default(...args));
						}
						Content$N.isAstroComponentFactory = true;
						function getHeadings$N() {
							return load$N().then((m) => m.metadata.headings);
						}
						function getHeaders$N() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$N().then((m) => m.metadata.headings);
						}

const __vite_glob_0_227 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$N,
  file: file$N,
  url: url$N,
  rawContent: rawContent$N,
  compiledContent: compiledContent$N,
  default: load$N,
  Content: Content$N,
  getHeadings: getHeadings$N,
  getHeaders: getHeaders$N
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$M = {"title":"Tips on Choosing a Sales and Support Enablement Development Path","description":"Looking to use voice for sales or support enablement, learn some tips on choosing the right solution and what is available out in the market.","date":"2021-11-01T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981384/blog/tips-on-choosing-a-sales-and-support-enablement-development-path/choosing-sales-support-enablement-dev-path-blog-th.png","authors":["keith-lam"],"category":"ai-and-engineering","tags":["sales-enablement","support-enablement"],"seo":{"title":"Tips on Choosing a Sales and Support Enablement Development Path","description":"Looking to use voice for sales or support enablement, learn some tips on choosing the right solution and what is available out in the market."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981384/blog/tips-on-choosing-a-sales-and-support-enablement-development-path/choosing-sales-support-enablement-dev-path-blog-th.png"},"shorturls":{"share":"https://dpgr.am/b784e32","twitter":"https://dpgr.am/f74c454","linkedin":"https://dpgr.am/91658fa","reddit":"https://dpgr.am/042402c","facebook":"https://dpgr.am/cbad04b"}};
						const file$M = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/tips-on-choosing-a-sales-and-support-enablement-development-path/index.md";
						const url$M = undefined;
						function rawContent$M() {
							return "Sales and support enablement has changed dramatically with the use of speech recognition. Sales and support teams can perform situational training and analysis before the employee takes their first call. They can analyze their voice, facial expressions, tone of voice, speed of delivery to provide coaching on better sales or support conversations. During calls, the software can record and analyze the conversation and provide on-screen feedback and tips for add-on sales or solutions to issues, thereby improving the customer experience. Post-call, the conversations can be analyzed for coaching purposes. As such, sales and support enablement reference architecture varies depending on the use case. The three main use cases can be bucketed as:\n\n* Onboarding\n* Coaching\n* Real-time sales or support assistance\n\nOverall, these software solutions aim to improve customer satisfaction, improve support, or increase sales. In this post, we would like to share a high-level process flow of the various processes for the types of software.\n\n## **Onboarding**\n\nOnboarding tools help sales to better understand and deliver the right message, sales pitch and offer to customers. Overall, reduce the onboarding time for a new salesperson. This could be analyzing the sales pitch or determining how well they did with sales objections or obstacles.\n\n## **Coaching**\n\nSales and support coaching solutions allow sales management to review sales and support calls and determine areas for improvement. There are various coaching solutions available, some focus on the sales call and combine it with sales funnel statistics and others combine the audio with call analytics to determine the best pitches, messaging, or methods for higher sales, and for support, they can provide call analytics of the call along with the transcript and audio for coaching.\n\n## **Real-Time Sales and Support**\n\nReal-time sales and support enablement is a new type of solution to provide hints and tips to the salesperson or support agent as they are making phone calls to customers. As they are speaking with the customer, the solution parses out keywords and phrases and return to the salesperson or support agent information to help them with the customer encounter, including tips to close the sale, recommended add-on products or services, tips to prevent customer churn, links to articles on a solution to the customer issue. It is very much like an advisor or coach pushing the agent information as they need it. The goal is to increase sales or increase customer satisfaction, but it can also be used to assist new salespeople or support agents. \n\n![](https://res.cloudinary.com/deepgram/image/upload/v1661976850/blog/tips-on-choosing-a-sales-and-support-enablement-development-path/real-time-sales-support-reference-architecture%402x.png)\n\n* **Audio and Meta-Data Collection** Audio capture is normally done with existing infrastructure; UCaaS, CCaaS, on-device applications, smart speakers, PBX, or VOIP. Depending on the speech-to-text (STT) solution, the audio capture may need to be converted into different file formats for real-time streaming to the STT. More sophisticated systems may capture audio patterns, tone, and frequency. \n* **Speech-to-text** speech-to-text transcribes the speech in the audio capture into text for the NLU/NLP to parse and use. The more accurate the STT the better the results from the NLU/NLP. In addition, some STT systems also provide diarization, audio sentiment, speaker ID, speaker isolation, noise reduction, and metadata on pitch, pace, tone, and utterances. This is normally a separate best-of-breed vendor. STT providers include Deepgram, Google speech-to-text, Amazon Transcribe, Nuance, and IBM Watson.\n* **Natural Language Processing and Understanding** NLU/NLP is the main process to turn words, sentences, sentiment, audio metadata into intent and sentiment. What does the speaker want to convey? It matches the words to the intent so the data integration engine can organize the data and provide additional tags on the speakers. This can be part of a complete coaching solution or a separate best-of-breed vendor, such as OneReach.ai, Rasa, or Cognigy.ai.\n* **Data Integration** Data integration takes all the text and organizes it so you can perform queries on the data. It also organizes the meta-data to be able to query also. Was the customer happy, neutral, or sad? Was this a sales inquiry or support call? This step sorts that conversation into these buckets. \n* **Data Query** Data query engine does the work of pulling the right data for the business intelligence application. The end-user might ask the business intelligence application to show him where the salesperson lost the sale. \n* **Business intelligence** This business intelligence end-user application is where you see the analytics and gain insight into the sales calls. Or the BI can create a chart to see which salesperson is always mentioning a new add-on service and the success rate of these sales.\n\n#### **Tips for Choosing Your Development Path**\n\n* **Find Biggest Needs/Issues** - Because there are so many solutions out there, you need to determine and prioritize your biggest needs. Are you onboarding a lot of new salespeople this year? Is your NPS score too low due to inexperience? Do you not have enough experienced salespeople for coaching? Are you losing customers to churn without knowing about it? *Tip - This is the normal risk and rewards assessment or ROI assessment, but this needs to be done for the entire business, not just Sales or Support crying the loudest for help.\n* **Fix the Issues** - Many of these solutions have great UI and UX and great dashboards to dazzle you, but you have to ask, does the solution truly fix the main issues you have, and do you have the experienced management to implement? These solutions are tools but you still need to make sure they solve the problems. You can implement a coaching solution but you better have experienced sales and support management to implement this coaching solution and actually do the coaching. *Tip - Make sure you have the resources and experienced team to fix the issues and that you are looking at the right issues. Don't be dazzled by all the other features.*\n* **Measure Twice, Cut Once** - Before implementing these solutions, make sure you have a method to measure the success and key results you want to achieve. As they say, you cannot improve what you cannot measure. *Tip - Establish your success metrics and key results then look for solutions that will measure these metrics. Ideas include closed win-rates with new sales hires first call issue resolution with new support hires, etc.*\n* **Create a Win-Win** - You and your provider both want the same thing: for their solution to work for your company, track the metrics, and meet your goals. The solution provider wants a great case study, so find a win-win for both of your organizations. *Tip - See if the deal you have with the solution provider includes incentives for them to meet your key results. As they work with other customers like you, will they provide you with best practices and advice? Will have skin in the game for your success? Will they be a true partner, not just a vendor?*\n\n## Wrapping Up\n\nWe hope this gave you the insight you need to take the next step in starting or advancing your sales and support enablement experience. There are so many solutions out there but make sure they solve your main needs and issues, you have metrics preset to analyze success, and you create a win-win with the solution provider. Don't buy a hammer when you need to paint a wall. Find the right tools for the job.";
						}
						async function compiledContent$M() {
							return load$M().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$M() {
							return (await import('./chunks/index.91cab9da.mjs'));
						}
						function Content$M(...args) {
							return load$M().then((m) => m.default(...args));
						}
						Content$M.isAstroComponentFactory = true;
						function getHeadings$M() {
							return load$M().then((m) => m.metadata.headings);
						}
						function getHeaders$M() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$M().then((m) => m.metadata.headings);
						}

const __vite_glob_0_228 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$M,
  file: file$M,
  url: url$M,
  rawContent: rawContent$M,
  compiledContent: compiledContent$M,
  default: load$M,
  Content: Content$M,
  getHeadings: getHeadings$M,
  getHeaders: getHeaders$M
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$L = {"title":"Tonya Sims Joins the Developer Relations Team","description":"Meet Deepgram's new Developer Advocate and learn her story of getting into tech.","date":"2022-02-01T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1643739380/blog/2022/02/tonya-sims-joins-deepgram/tonya-city.jpg","authors":["tonya-sims"],"category":"devlife","tags":["team"],"seo":{"title":"Tonya Sims Joins the Developer Relations Team","description":"Meet Deepgram's new Developer Advocate and learn her story of getting into tech."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454001/blog/tonya-sims-joins-deepgram/ograph.png"},"shorturls":{"share":"https://dpgr.am/1823103","twitter":"https://dpgr.am/19b24a3","linkedin":"https://dpgr.am/74ab82e","reddit":"https://dpgr.am/6a40500","facebook":"https://dpgr.am/9c02c30"}};
						const file$L = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/tonya-sims-joins-deepgram/index.md";
						const url$L = undefined;
						function rawContent$L() {
							return "Fainting in an operating room during surgery launched my tech career.\n\nYes, you read that correctly. As a pharmaceutical sales representative for the company I worked for, we observed someone undergoing surgery as part of our training.\n\nWhen the doctor made the incision, my stomach started doing backflips. Five minutes later, he was doing things that you only see on TV!\n\nI passed out in my chair.\n\nEven though working in pharma sales taught me a lot, I decided to embark on a career that didn't involve surgical instruments. Ha!\n\nI just wanted to get back to my roots.\n\n## My Story in 2 Minutes\n\nMy parents bought a computer when I was eight years old, and that's when my passion for technology started. My older brother and I would write code following the instructions in the accompanying manual to make the computer do cool stuff!\n\nAs much as I loved tech, sports started to consume my life and didn't code again until many years later.\n\nI was pretty much born with a basketball in my hands and started playing competitively at the age of five. I went on to play basketball in college and professionally in Europe and the WNBA.\n\nAfter hanging up my gym shoes and working in sales, I started to develop an interest in coding again. So I picked up a book and taught myself how to do it. A few coding languages later, I got hooked on Python! I love the supportive Python community, the beauty of the syntax, and everything about it.\n\nOver the years, I have worked in the tech industry as a software tester and developer. I'm super excited to be working in my dream job at Deepgram as a Python Developer Advocate as my passion for AI and nurturing the developer community runs deep.\n\nOutside of work, I enjoy listening to music (anything from Tiesto!) and playing the piano (anything classical!). I also love to read and listen to audiobooks. My favorite thing is spending time with my nieces and nephews. They bring me joy and have coined a nickname for me: Auntie Mama!\n\n## Let's Connect\n\nIt would be great to connect with you! I'm looking forward to learning more about you and what you're working on! Please feel free to reach out to me on [Twitter](https://twitter.com/tonyasims). I hope we have a chance to connect throughout your journey!";
						}
						async function compiledContent$L() {
							return load$L().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$L() {
							return (await import('./chunks/index.15e9f355.mjs'));
						}
						function Content$L(...args) {
							return load$L().then((m) => m.default(...args));
						}
						Content$L.isAstroComponentFactory = true;
						function getHeadings$L() {
							return load$L().then((m) => m.metadata.headings);
						}
						function getHeaders$L() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$L().then((m) => m.metadata.headings);
						}

const __vite_glob_0_229 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$L,
  file: file$L,
  url: url$L,
  rawContent: rawContent$L,
  compiledContent: compiledContent$L,
  default: load$L,
  Content: Content$L,
  getHeadings: getHeadings$L,
  getHeaders: getHeaders$L
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$K = {"title":"Top 3 Use Cases for Speech-to-Text in Gaming","description":"Learn three of the top ways that companies are using speech-to-text to improve everyones gaming experience.","date":"2022-06-02T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981422/blog/top-3-use-cases-speech-to-text-gaming/top-use-cases-asr-video-games-thumb-554x220%402x.png","authors":["chris-doty"],"category":"speech-trends","tags":["gaming"],"seo":{"title":"Top 3 Use Cases for Speech-to-Text in Gaming","description":"Learn three of the top ways that companies are using speech-to-text to improve everyones gaming experience."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981422/blog/top-3-use-cases-speech-to-text-gaming/top-use-cases-asr-video-games-thumb-554x220%402x.png"},"shorturls":{"share":"https://dpgr.am/7acb112","twitter":"https://dpgr.am/a67806b","linkedin":"https://dpgr.am/30fbcbd","reddit":"https://dpgr.am/5ab6b59","facebook":"https://dpgr.am/5200007"}};
						const file$K = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/top-3-use-cases-speech-to-text-gaming/index.md";
						const url$K = undefined;
						function rawContent$K() {
							return "In the gaming industry, speech-to-text and voice recognition technology are becoming more and more commonplace. This is due to the fact that they offer a number of advantages for both developers and players alike. In this blog post, we'll cover some of the most common use cases for speech-to-text and voice technology in the video game industry. We'll also discuss the benefits that these technologies offer to both developers and players. Let's get started!\n\n## 1. Voice-to-Text Chat\n\nOne of the main benefits of speech-to-text and voice recognition technology is improved communication. Although many video games offer voice chats, in some, players can only communicate with others via text. That means you have to stop playing the game briefly to type out your message in order to communicate. But with in-game speech transcription, players can communicate by speaking and having the game transcribe what they've said so they don't have to stop playing. This can be extremely beneficial for players who are trying to strategize or coordinate their gameplay, or when time is of the essence. In addition, text chat can also be used to simply chat with friends and other players.\n\n## 2. Accessibility\n\nAnother benefit of speech-to-text and voice recognition technology is improved accessibility. This is because these technologies can allow players with disabilities to play video games that they otherwise would not be able to play. This can be extremely beneficial for players who want to be able to enjoy their favorite hobby, regardless of their physical abilities. Voice commands, for instance, allow players to issue commands to their characters without having to use a controller or type them out. This can help players who have limited mobility or otherwise encounter challenges with traditional gaming interfaces. This winning project from the Gram Gamers category of our recent Deepgram + Dev Hackathon, for example, [lets you control a video game character using only your voice](https://dev.to/sandy_codes_py/play-real-steel-boxing-with-your-voice-atom-the-peoples-champion-e8h). This technology can also let players use voice commands to navigate menus without having to use a controller or mouse.\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-make-application-voice-ready\"></WhitepaperPromo>\n\n## 3. Content Moderation\n\nSpeech-to-text and voice recognition technology can also be used for content moderation purposes by allowing developers to automatically moderate chat rooms and in-game chat. By converting speech to text, game developers can apply all of the same rules that they apply to text chats. And, with technologies like Deepgram that return results in real time, you can moderate conversations as they happen. This helps keeping players safe from inappropriate or offensive content. In addition, content moderation can also be used to simply keep players from being disruptive to other players. Two companies in this space are [Modulate.ai](https://www.modulate.ai/) and [Spectrum Labs](https://www.spectrumlabsai.com/).\n\n## Want to Add Deepgram to Your Game?\n\nIf you're curious how you might add speech-to-text to your game? We've got developer resources for that. Check out [How to Add Deepgram Speech Recognition to Your Unity Game](https://blog.deepgram.com/deepgram-unity-tutorial/) and [Playing With P5.js: Creating a Voice-Controlled Game](https://blog.deepgram.com/p5js-deepgram-game/) for some tutorials looking at how easily Deepgram can be incorporated into what you're building. You can also check out this project that we did to turn [the 404 page on our developer site into a game](https://blog.deepgram.com/building-404-pages-that-bring-joy/) if you need more gamespiration.\n\n## Wrapping Up\n\nAs you can see, there are a number of use cases for speech-to-text and voice recognition technology in the gaming industry. These technologies offer a number of benefits to both developers and players alike. If you're a developer, we encourage you to consider implementing these technologies into your next project. If you are a player, we encourage you to try out games that use these technologies. You might be surprised at how much they can improve your gaming experience!";
						}
						async function compiledContent$K() {
							return load$K().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$K() {
							return (await import('./chunks/index.8ca04472.mjs'));
						}
						function Content$K(...args) {
							return load$K().then((m) => m.default(...args));
						}
						Content$K.isAstroComponentFactory = true;
						function getHeadings$K() {
							return load$K().then((m) => m.metadata.headings);
						}
						function getHeaders$K() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$K().then((m) => m.metadata.headings);
						}

const __vite_glob_0_230 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$K,
  file: file$K,
  url: url$K,
  rawContent: rawContent$K,
  compiledContent: compiledContent$K,
  default: load$K,
  Content: Content$K,
  getHeadings: getHeadings$K,
  getHeaders: getHeaders$K
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$J = {"title":"Top 6 Dutch ASR Challenges: Diverse Dialects, Data, and Dictionaries","description":"The history and diversity of Dutch present unique challenges when building a Dutch ASR system. Here are the top six.","date":"2022-04-21T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981418/blog/top-6-dutch-asr-challenges/top-6-dutch-asr-challenges-thumb-554x220%402x.png","authors":["conner-goodrum"],"category":"linguistics","tags":["language"],"seo":{"title":"Top 6 Dutch ASR Challenges: Diverse Dialects, Data, and Dictionaries","description":"The history and diversity of Dutch present unique challenges when building a Dutch ASR system. Here are the top six."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981418/blog/top-6-dutch-asr-challenges/top-6-dutch-asr-challenges-thumb-554x220%402x.png"},"shorturls":{"share":"https://dpgr.am/e54e446","twitter":"https://dpgr.am/4076e4f","linkedin":"https://dpgr.am/c84af2d","reddit":"https://dpgr.am/ee61a6c","facebook":"https://dpgr.am/d99589f"}};
						const file$J = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/top-6-dutch-asr-challenges/index.md";
						const url$J = undefined;
						function rawContent$J() {
							return "Dutch is an Indo-European language, spoken in Northern Europe, primarily in Holland and Belgium. The language has about 25 million native speakers, and another 5 million second language learners. The term \"Dutch\" actually encompasses two different language varieties-*Nederlands*, the variety of the language spoken in Holland, and *Vlaams* (or Flemish in English), spoken mainly in Northern Belgium. The two varieties are so similar that they are largely mutually intelligible, and have the same International Organization for Standardization (ISO) code, although there are non-trivial differences in vocabulary and pronunciation between the two. This blog post outlines some of the challenges in training a Dutch automatic speech recognition model that works well on different varieties of the language, given the diversity that we see on the ground.\n\n## 1. Inflection\n\n**Inflection** refers to the addition of prefixes or suffixes (or, indeed, infixes) to root words to indicate things like person, number, or tense. English has a relatively limited number of these inflectional suffixes. Regular verbs, for example, get an *\\-s* in the present tense if the subject of the sentence is third person singular. *I sing* and *you sing* but *he/she/it sing**s***. Nouns can also be inflected-regular nouns take an *\\-s* suffix to indicate the plural. \n\nIn Dutch, however, nouns and verbs have more inflectional possibilities than English-there are two options for the plural marker, for example, (*\\-en* and *\\-s*) depending on the noun, as well as other suffixes-the [diminutive](https://en.wikipedia.org/wiki/Diminutive) *\\-tje* or *\\-je* 'small' is very common in Dutch-consider *aardbei* 'strawberry' and *aardbeitje* 'little strawberry'-and is used **productively,** meaning that it occur on almost any noun. In terms of ASR, all of this means that the dictionary that's used by a Dutch speech recognition model to tell it what words it might expect to find needs to be much larger than an English dictionary.\n\n## 2. Compound Words\n\nFurther complicating the issue with dictionary size, Dutch also has a large number of compounds (similar to German), and, like *\\-tje/-je* discussed above, these compounds can be created productively by speakers. In English, these would be represented by different words, all of which would appear in the dictionary on their own. But with Dutch, if the compound a speaker creates isn't in the dictionary, the system can have trouble transcribing it correctly. Moreover, it's often the case that the Dutch speech recognition model will transcribe each portion of the compound word as its individual parts, which are more common, but put them into a compound. For example, *autoverzekering* 'car insurance' is more likely to be transcribed as *auto verzekering*, as these individual words are more common than the compound.\n\nEven more so than with inflection, compounds create the potential for lots of unknown words to appear in speech that won't be present in the dictionary. And even if the compounds are known, it can still create vocabulary size issues. Numbers in Dutch are a great example of this. In English, larger numbers are written as a sequence of words-221 is *two hundred twenty one.* But in Dutch, this number is *tweehonderdeenentwintig*. In English, you only need 30 entries in the dictionary to get the numbers from zero to 100, but in Dutch, you need 101 unique entries.\n\n## 3. Different Dialects\n\nThere are many dialects spoken across the Netherlands and Belgium, and they aren't always widely understandable by people who speak a different variety. In the introduction, two large dialect groups were defined-*Nederlands* and *Vlaams*-but the situation is actually much more complicated and diverse, with each of those two varieties having multiple different dialects subsumed under them. And Frisian-technically a different language, but extremely similar to Dutch (and English!)-is also sometimes included in the general Dutch language area, further complicating matters. This creates challenges when training an ASR model due to different spellings of words, different pronunciations, and altogether different vocabulary items-Flemish, for example, tends to have more words borrowed from French due to being in Belgium.\n\nThe dictionaries that models use have limited vocabularies, as discussed above, so you have two options. The first is to standardize everything to one type of Dutch, which requires laborious and careful preprocessing of data and can introduce ambiguities. This also brings into question whether it is actually correct to 'translate' a specific word in one variety of Dutch into another variety-especially if you want to create a model that can handle any kind of Dutch thrown at it. These changes can also [introduce bias](https://blog.deepgram.com/detecting-and-reducing-bias-in-speech-recognition/) into your work, making the model favor some varieties of the language over others. The second option is simply training your model on all of the data with the hopes it does decently well across all varieties. This is [easiest to do with deep learning](https://blog.deepgram.com/deep-learning-speech-recognition/), as you can iterate through different data sets quickly and easily.\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-how-deepgram-works\"></WhitepaperPromo>\n\n## 4. Getting Large Amounts of Diverse Dutch Data\n\nAnother challenge of building a model for Dutch is finding a large enough *amount* of the diverse data that you need to represent all of these different dialects. If you're going for the second option above and training a Dutch speech recognition model on as many varieties as possible, it's important that any data used for model training accurately represent the possible variation in the language. However, not all varieties are equally represented in readily available media and corpora. Maybe people might be bidialectal-they grew up speaking one dialect but might switch to the standard version when, for example, appearing on TV or lecturing in a classroom. This can make it difficult to get the data you need so that the model can understand everyone's words regardless of the speech context.\n\n## 5. Finding Diverse Validation Data\n\nRelated to the above, even if you have a large variety of data to train your model, it's still going to be a problem if you can't find sufficient data to validate and test it. During many machine learning projects, data is split at the beginning into a training set and testing set (the split can vary, but it's often something like 80/20). The model is trained on the training set, and then validated on the test set to see how well it performs. But if you already have limited data for some varieties, splitting it like this can leave you with too little data to train the model, too little data left over to test it on those varieties, or both.\n\n## 6. Time Required to Train and Test Models\n\nAll of the above issues-the diverse dialects, the amount and types of data needed, and limits on dictionary size-come together to create a temporal burden as well. It takes time to collect data, train models, and test them, and each of the complications above increases the amount of time needed to go from no model to accurate model. This problem is further amplified when you consider the time it takes to locate and collect-and potentially transcribe-data from the various dialects, given its rarity. We're fortunate at Deepgram that we use [end-to-end deep learning to train our models](https://blog.deepgram.com/deep-learning-speech-recognition/), which means that the retraining process is relatively straightforward. But if you're using a legacy system that combines several different systems together to generate text from audio, there's likely to be much more work involved to get an accurate model up and running for a language like Dutch.\n\n## Wrapping up\n\nThese six challenges for Dutch automatic speech recognition can certainly be tricky, but they aren't insurmountable. With effective tools (like end-to-end deep learning) and data collection and validation procedures, accurate Dutch ASR models are possible. If you'd like to give our Dutch model a try (or [any of the other languages we offer](https://deepgram.com/product/languages/)), you can [sign up for a free API key](https://console.deepgram.com/signup) and get started today. Still have questions? [Contact us](https://deepgram.com/contact-us/) and we can talk through your use case and see how we can help.";
						}
						async function compiledContent$J() {
							return load$J().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$J() {
							return (await import('./chunks/index.6f1ceaf4.mjs'));
						}
						function Content$J(...args) {
							return load$J().then((m) => m.default(...args));
						}
						Content$J.isAstroComponentFactory = true;
						function getHeadings$J() {
							return load$J().then((m) => m.metadata.headings);
						}
						function getHeaders$J() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$J().then((m) => m.metadata.headings);
						}

const __vite_glob_0_231 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$J,
  file: file$J,
  url: url$J,
  rawContent: rawContent$J,
  compiledContent: compiledContent$J,
  default: load$J,
  Content: Content$J,
  getHeadings: getHeadings$J,
  getHeaders: getHeaders$J
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$I = {"title":"Top 7 Uses for Speech-to-Text in Education","description":"Speech recognition can have a big impact in education. Here are 7 of the top use cases.","date":"2022-07-29T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981427/blog/top-7-uses-speech-to-text-education/use-cases-for-speech-recognition-in-education-thum.png","authors":["chris-doty"],"category":"speech-trends","tags":["education"],"seo":{"title":"Top 7 Uses for Speech-to-Text in Education","description":"Speech recognition can have a big impact in education. Here are 7 of the top use cases."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981427/blog/top-7-uses-speech-to-text-education/use-cases-for-speech-recognition-in-education-thum.png"},"shorturls":{"share":"https://dpgr.am/ccbd569","twitter":"https://dpgr.am/c3b7901","linkedin":"https://dpgr.am/e375afd","reddit":"https://dpgr.am/0977cc5","facebook":"https://dpgr.am/e4b2401"}};
						const file$I = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/top-7-uses-speech-to-text-education/index.md";
						const url$I = undefined;
						function rawContent$I() {
							return "When most people think about speech recognition, they think about personal assistant applications like Siri or Alexa. But as speech technology becomes cheaper and faster, we're seeing more and more use cases in domains and industries where speech-to-text tools haven't been used much in the past. In this post, we're going to discuss 7 of the most common use cases of speech recognition for learning and education, as well as who can benefit most from this technology. But to start, let's clarify what speech recognition in learning is.\n\n## What is Speech Recognition for Learning?\n\nIn short, any use of speech-to-text for education is an example of speech recognition in learning. As will see below, this can refer to different things, but at their core, they all involve turning the spoken interactions that happen in the classroom into transcribed text. But beyond that, there are a lot of different ways that these transcriptions can be used.\n\n## 7 Top Use Cases for Speech Technology in Education\n\nLet's take a look at the top 7 use cases for speech-to-text in education, including how they can benefit students and teachers alike.\n\n### 1. Classroom Transcripts\n\nSpeech recognition can be used to create transcripts of lectures and classroom discussions. We've previously [talked about the benefits of classroom captions](https://blog.deepgram.com/automatic-speech-recognition-education/), and even [built a project to showcase the technology](https://blog.deepgram.com/classroom-captioner/), but in short, transcripts are especially useful for students who are hard of hearing or have difficulty taking notes, as well as non-native speakers who might not understand every word of a lecture. Transcripts let these students refer to lecture content after the fact to help them master the material.\n\n### 2. Study Aids\n\nCreating materials for study and test prep is another domain where speech recognition can help. For example, there are apps that can create flashcards based on a student's lecture notes-or, even better, a transcript of a lecture. And even if you don't make flashcards automatically, having the transcription of what was said can make it much easier to prep study materials manually, as well as search to find specific terms or things that weren't understood during the lecture.\n\n### 3. Video Subtitles & Captioning\n\nSpeech recognition can also be used to create subtitles for educational videos. This is helpful for students who are deaf or hard of hearing. It can also be helpful for students who speak English as a second language. These subtitles can also be used as closed captions on live videos. With hybrid class environments, this can be a boon for anyone who is attending remotely, as they might not be able to hear what was said in the class well. For example, [Habitat Learn offers a service like this](https://www.habitatlearn.com/product/messenger-pigeon), providing real-time transcripts of classes. If you want to learn more about the work that Habitat Learn is doing, [check out our upcoming webinar with them](https://offers.deepgram.com/habitat-learn-webinar)!\n\n### 4. Research Projects\n\nSome research projects require students to transcribe interviews or speeches. Speech recognition can be used to make this task easier. This can have impacts beyond the classroom as well, helping researchers to better understand data they glean from interviews, by giving them access to a searchable database of all of their interviews, without having to spend time manually transcribing them.\n\n### 5. Pronunciation Assessments\n\nSpeech recognition can be used to help students learn languages. For example, there are applications that allow students to practice their pronunciation by speaking into the app. Automatic speech recognition can be used to assess a student's pronunciation skills. This is valuable for language learners who need to practice their speaking skills, or want to check how well they're pronouncing certain words or phrases.\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-how-deepgram-works\"></WhitepaperPromo>\n\n### 6. Assistive Technologies\n\nAutomatic speech recognition can also be used to create accessible education materials for students with disabilities. For example, students who are blind or have low vision can use text-to-speech applications to listen to textbooks and other educational materials. Students who are deaf or hard of hearing can use automatic speech recognition to create subtitles for education videos. Speech recognition can be used to help students with dyslexia write essays by allowing dictation and readback of the content.\n\n### 7. Class Prep\n\nOur final benefit isn't for students, but for teachers. Teachers often have to teach the same material over and over, whether to different classes on the same day or week, or year after year. By referring to their transcripts after class, instructors can make updates to their lesson plans  and materials for future classes, review and better understand student questions, and remind themselves of content they promised to follow up on. And, with features like diarization and speaker ID, these transcripts can even help with assigning participation and attendance after class.\n\n## Who Can Benefit from Voice Technology in Education?\n\nAs we've seen, almost any student can benefit from having the ability to revisit class material after the fact to study. But some students see particular benefits from this technology. This applies to anyone who might have trouble hearing or understanding what was said in class, whether because they are [hard of hearing](https://blog.deepgram.com/asr-important-deaf-hoh-community/), not a native speaker of the language being used in the classroom, or have a learning disability that makes understanding lectures difficult. Additionally, in the current COVID environment, these tools can help keep all students on track. If students have to miss class due to illness, or attend remotely due to quarantining, tools like captioning and class transcripts can help make sure they stay on top of what's happening in the classroom. These technologies, used correctly, can truly help to put everyone in the classroom on the same footing when it comes to understanding and engaging with the lecture material.\n\n## Wrapping up\n\nIn conclusion, automatic speech recognition solutions have a wide range of applications in education. These technologies can be used to create transcripts, assess pronunciation skills, and create accessible education materials for students with disabilities. If you're looking to get started with a speech-to-text solution, [feel free to contact us](https://deepgram.com/contact-us/) and we'll be happy to discuss your use case and help you get started. Or, you can [sign up for Console for free and get $150 in credits](https://console.deepgram.com/signup) to give Deepgram a try.";
						}
						async function compiledContent$I() {
							return load$I().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$I() {
							return (await import('./chunks/index.632dd6c2.mjs'));
						}
						function Content$I(...args) {
							return load$I().then((m) => m.default(...args));
						}
						Content$I.isAstroComponentFactory = true;
						function getHeadings$I() {
							return load$I().then((m) => m.metadata.headings);
						}
						function getHeaders$I() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$I().then((m) => m.metadata.headings);
						}

const __vite_glob_0_232 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$I,
  file: file$I,
  url: url$I,
  rawContent: rawContent$I,
  compiledContent: compiledContent$I,
  default: load$I,
  Content: Content$I,
  getHeadings: getHeadings$I,
  getHeaders: getHeaders$I
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$H = {"title":"Top Six Uses Cases for Automatic Speech Recognition (ASR) in Social Media","description":"The social media industry is full of use cases for ASR and NLU—here are the top six, and how these solutions drive business value.","date":"2022-10-17T22:55:19.866Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1666047379/blog/Top%20Six%20Uses%20Cases%20for%20Automatic%20Speech%20Recognition%20%28ASR%29%20in%20Social%20Media/how-ASR-supports-content-moderation-companies-thumb-554x220_uzmvz9.png","authors":["chris-doty"],"category":"best-practice","tags":["social"],"shorturls":{"share":"https://dpgr.am/d93ac78","twitter":"https://dpgr.am/5b6f9dc","linkedin":"https://dpgr.am/eae57cd","reddit":"https://dpgr.am/ca9e72f","facebook":"https://dpgr.am/4c0d76d"}};
						const file$H = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/top-six-uses-cases-for-asr-social-media/index.md";
						const url$H = undefined;
						function rawContent$H() {
							return "\nSocial media platforms have truly changed the landscape of how people all over the world engage with each other, keep up on news, explore their interests, and more.\n\nAnd their reach and impact continues to grow. Every second, [approximately 6,000 tweets are sent](https://www.internetlivestats.com/twitter-statistics/) and, as of this writing, [97,306 YouTube videos were being viewed](https://www.internetlivestats.com/one-second/#youtube-band) in one second. With this much content being created, it’s hard for social media platforms to keep up. This content has the potential to be a treasure trove of information, but with more and more uploads taking the form of video and audio content, much of this content has been somewhat opaque to the platforms that host them.\n\nThat’s where automatic speech recognition (ASR) and Natural Language Understanding (NLU) comes in. By taking audio and video files and transcribing the speech that occurs in them, social media companies can gain access to a whole new source of information about what’s benign discussed on their platforms and provide additional value to their users. And, the more accurate the transcriptions and classifications, the more effectively social media platforms can take action based on this information.\n\nIn this blog post, we’ll look at some of the top ways that social media companies are using transcribed audio in their businesses today, as well as the ways that ASR and NLU solutions can drive increased revenue for social media platforms.\n\n## Top Uses Cases for Social Media Transcription\n\nSocial media companies use automatic speech recognition and speech-to-text tools for a variety of purposes. Let’s take a look at six of the most common ways that this technology is used.\n\n### 1. Closed Captioning\n\nSpeech-to-text solutions allow social media platforms to add captioning to audio and video that’s uploaded by users. These captions not only support people in the deaf and hard-of-hearing community, but also anyone who is browsing social media in a place where they can’t listen to audio. Captioning like this serves to increase engagement for posts, even if people can’t hear what’s being said.\n\n### 2. Add-on Analytics \n\nSocial media companies can use automatic speech recognition and NLU features such as [Language detection](https://developers.deepgram.com/documentation/features/detect-language/), Topic detection and Entity Detection to provide monitoring services to customers about  their products and services. This information can be used to improve the user experience and make changes to the products and services based on user feedback. It can also be used to moderate the content that’s being produced and uploaded to ensure that it confirms with site guidelines and any relevant laws and regulations.\n\n### 3. Improved Ad Targeting\n\nSocial media companies can use speech-to-text tools to help with ad targeting. In the past, anything that was uploaded in the form of a video or audio was largely opaque to the platforms. While a social media platform could target ads based on text posts, this wasn't possible for video and audio beyond the title and any metadata included as part of the upload. But with ASR solutions, social media services such as podcast platforms can target ads based on the content of audio and video posts themselves by using features such as [Keywords](https://developers.deepgram.com/documentation/features/keywords/), helping to better monetize the platforms.\n\n### 4. Improved Search\n\nSocial media companies can use automatic speech recognition to improve the search functionality of their platforms. In the past, as with ad targeting (discussed above), search had to rely on titles, descriptions, and metadata to help users find the content that they're looking for. But with speech-to-text functionality, videos can be transcribed and served up for search, making it easier for users to find the content they're looking for, regardless of its format. Developers building social media applications with search functionality based on audio content can now leverage ASR features that identify terms or phrases by [matching acoustic patterns](https://developers.deepgram.com/documentation/features/search/) in audio.\n\n### 5. Insights & Automation \n\nBetter understanding their users is widespread use of automatic speech recognition and speech-to-text tools. By analyzing the audio and video content that users upload, social media companies can get reliable information that can be used to surface trends and insights to the business, and/or automate downstream business process workflows. One example is to route posts for content moderation to the appropriate team based on [language detected](https://developers.deepgram.com/documentation/features/detect-language/) or [filtering profanity](https://developers.deepgram.com/documentation/features/profanity-filter/) in real-time. \n\n## How ASR Supports Social Media Companies\n\nAs mentioned above, the main benefit that ASR tools provide to social media companies is the ability to unlock what’s happening in audio and video that’s been published on their platform.\n\n*   Save Money — Many of the use cases above support saving money. Whether it’s optimizing customer service calls or better understanding customers, deploying speech solutions can save social media platforms big bucks. By providing Closed Captioning services to their customers you can also enable your customers to demonstrate compliance and avoid regulatory fines.\n*   Generate New Revenue — By providing assistance with ad targeting and highly accurate search, ASR solutions can increase the revenue generated by social media platforms.\n*   Increase Productivity — With customer insights, ad targeting, and improved search ASR solutions can increase productivity for social media companies. Instead of having to do these things manually, social media platforms can rely on software solutions to enable those tasks. For example, Deepgram Customer This allows them to focus on revenue-generating areas such as additional ad packages, new analytics offerings, new products and features, and more.\n\n## Wrapping Up\n\nAs we’ve seen, ASR solutions can help social media companies in many different ways, such as improving customer service, saving money, and better monetizing their platforms. In the future, we can expect social media companies to use these technologies in even more ways to improve the user experience.\n\nIf you’d like to see how Deepgram can support your speech-to-text needs, you can [sign up for a free trial of Console](https://console.deepgram.com/signup) and get $150 in free credits to try us out. Have questions? [Reach out to our sales team](https://deepgram.com/contact-us/); we’d be happy to talk you through your use case and see how we can help.\n\n";
						}
						async function compiledContent$H() {
							return load$H().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$H() {
							return (await import('./chunks/index.ca2fff70.mjs'));
						}
						function Content$H(...args) {
							return load$H().then((m) => m.default(...args));
						}
						Content$H.isAstroComponentFactory = true;
						function getHeadings$H() {
							return load$H().then((m) => m.metadata.headings);
						}
						function getHeaders$H() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$H().then((m) => m.metadata.headings);
						}

const __vite_glob_0_233 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$H,
  file: file$H,
  url: url$H,
  rawContent: rawContent$H,
  compiledContent: compiledContent$H,
  default: load$H,
  Content: Content$H,
  getHeadings: getHeadings$H,
  getHeaders: getHeaders$H
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$G = {"title":"Topic Detection in Podcast Episodes with Python","description":"This tutorial will use Python and the Deepgram API speech-to-text to perform Topic Detection using the TF-IDF Machine Learning Algorithm and KMeans Clustering.","date":"2022-08-23T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1660892766/blog/2022/08/topic-detection-with-python/2208-Detect-Topics-in-Podcast-Episodes-with-Python-blog%402x.jpg","authors":["tonya-sims"],"category":"tutorial","tags":["python"],"seo":{"title":"Topic Detection in Podcast Episodes with Python","description":"This tutorial will use Python and the Deepgram API speech-to-text to perform Topic Detection using the TF-IDF Machine Learning Algorithm and KMeans Clustering."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454121/blog/topic-detection-with-python/ograph.png"},"shorturls":{"share":"https://dpgr.am/c1098c5","twitter":"https://dpgr.am/335341b","linkedin":"https://dpgr.am/11c3f1c","reddit":"https://dpgr.am/e460418","facebook":"https://dpgr.am/26c7cfa"}};
						const file$G = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/topic-detection-with-python/index.md";
						const url$G = undefined;
						function rawContent$G() {
							return "Imagine you’re a Python Machine Learning Engineer. Your work day is getting ready to start with the dreaded stand-up meeting, but you're looking the most forward to deep diving into topic detection algorithms.\n\n<Panel title=\"Important Note\">\n\nIf you want to see the whole Python code snippet for topic detection, please scroll to the bottom of this post.\n\n</Panel>\n\nYou step out to get coffee down the street. A black SUV pulls up next to you, the door opens, and someone tells you to get in the truck.\n\nThey explain that your Machine Learning Python prowess is needed badly.\n\nWhy?\n\nThey need you to transcribe a podcast from speech-to-text urgently. But not just any podcast. It’s Team Coco’s podcast, the legendary Conan O’Brien. Not only do they need it transcribed using AI speech recognition, but they also require a topic analysis to quickly analyze the topics to discover what the podcast is about.\n\nThey can’t say too much about the underground Operation Machine Learning Topic Detection, other than if you can’t deliver the topic modeling results or tell anyone, something terrible may happen.\n\nWeird. Ironic but weird. Yesterday, you learned about the TF-IDF (Term Frequency - Inverse Document Frequency) topic detection algorithm.\n\nYou should feel confident in your Python and Machine Learning abilities, but you have some reservations.\n\nYou think about telling your manager but remember what they said about something terrible that may happen.\n\nYou’re going through self-doubt, and most importantly, you’re not even sure where to start with transcribing audio speech-to-text in Python.\n\nWhat if something bad does happen if you don’t complete the topic detection request?\n\nYou decide to put on your superhero cape and take on the challenge because your life could depend on it.\n\n## Discovery of Deepgram AI Speech-to-Text\n\nYou’re back at your home office and not sure where to start with finding a Python speech-to-text audio transcription provider.\n\nYou try using Company A’s transcription with Python, but it takes a long time to get back a transcript. Besides, the file you need to transcribe is over an hour long, and you don’t have time to waste.\n\nYou try Company B’s transcription again with Python. This time, the transcription comes back faster, but one big problem is accuracy. The words in the speech-to-text audio transcript you’re getting back are inaccurate.\n\nYou want to give up because you don’t think you’ll be able to find a superior company with an API that provides transcription.\n\nThen you discover Deepgram, and everything changes.\n\nDeepgram is an AI automated speech recognition voice-to-text company that allows us to build applications that transcribe speech-to-text.\n\nYou loved how effortless it is to sign up for Deepgram by quickly grabbing a Deepgram API Key [from our website](https://console.deepgram.com/signup?jump=keys). You also immediately get hands-on experience after signing up by trying out their console missions for transcribing prerecorded audio in a matter of a few minutes.\n\nThere’s even better news!\n\nDeepgam has much higher transcription accuracy than other providers, and you receive a transcript back super fast. You also discover they have a Python SDK that you can use.\n\nIt’s do-or-(maybe)-die time.\n\nYou hear a tornado warning siren, but disregard it and start coding.\n\nYou won’t let anything get in your way, not even a twister.\n\n## Python Code for AI Machine Learning Topic Detection\n\nYou first create a [virtual environment](https://blog.deepgram.com/python-virtual-environments/) to install your Python packages inside.\n\nNext, from the command line, you `pip install` the following Python packages inside of the virtual environment:\n\n```\npip install deepgram-sdk\npip install python-dotenv\npip install -U scikit-learn\npip install -U nltk\n```\n\nThen you create a `.env` file inside your project directory to hold your Deepgram API Key, so it’s not exposed to the whole world. Inside of your `.env` file, you assign your API Key from Deepgram to a variable `DEEPGRAM_API_KEY, like so:\n\n```\nDEEPGRAM_AP_KEY=”abc123”\n```\n\nNext, you create a new file called `python_topic_detection.py. You write the following code that imports Python libraries and handles the Deepgram prerecorded audio speech-to-text transcription:\n\n```python\nfrom ast import keyword\nfrom posixpath import split\nfrom deepgram import Deepgram\nfrom dotenv import load_dotenv\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom nltk.corpus import stopwords\nimport asyncio\nimport json\nimport os\nimport nltk\n\n\nload_dotenv()\n\nPATH_TO_FILE = 'conan_podcast.mp3'\n\nasync def transcribe_with_deepgram():\n  # Initializes the Deepgram SDK\n  deepgram = Deepgram(os.getenv(\"DEEPGRAM_API_KEY\"))\n  # Open the audio file\n  with open(PATH_TO_FILE, 'rb') as audio:\n      # ...or replace mimetype as appropriate\n      source = {'buffer': audio, 'mimetype': 'audio/mp3'}\n      response = await deepgram.transcription.prerecorded(source)\n\n      if 'transcript' in response['results']['channels'][0]['alternatives'][0]:\n          transcript = response['results']['channels'][0]['alternatives'][0]['transcript']\n\n          return transcript\n```\n\nThe `transcribe_with_deepgram()` function comes from our Deepgram Python SDK, located [here in Github](https://github.com/deepgram/python-sdk).\n\nIn this method, you initialize the Deepgram API and open our .mp3 podcast file to read it as audio. Then you use the `prerecorded` transcription option to transcribe a recorded file to text.\n\nYou’re on a roll!\n\nNext, you start writing the code for the TF-IDF Machine Learning algorithm to handle the topic detection. The tornado knocks out your power, and you realize you only have 20% laptop battery life.\n\nYou need to hurry and continue writing the following code in the same file:\n\n```python\nasync def remove_stop_words():\n  transcript_text = await transcribe_with_deepgram()\n  words = transcript_text.split()\n  final = []\n\n  nltk.download('stopwords')\n  stops = stopwords.words('english')\n\n  for word in words:\n      if word not in stops:\n          final.append(word)\n\n  final = \" \".join(final)\n\n  return final\n\n\nasync def cleaned_docs_to_vectorize():\n  final_list = []\n  transcript_final = await remove_stop_words()\n\n  split_transcript = transcript_final.split()\n\n  vectorizer = TfidfVectorizer(\n                              lowercase=True,\n                              max_features=100,\n                              max_df=0.8,\n                              min_df=4,\n                              ngram_range=(1,3),\n                              stop_words='english'\n\n                          )\n\n\n  vectors = vectorizer.fit_transform(split_transcript)\n\n  feature_names = vectorizer.get_feature_names()\n\n  dense = vectors.todense()\n  denselist = dense.tolist()\n\n  all_keywords = []\n\n  for description in denselist:\n      x = 0\n      keywords = []\n      for word in description:\n          if word > 0:\n              keywords.append(feature_names[x])\n          x=x+1\n\n      [all_keywords.append(x) for x in keywords if x not in all_keywords]\n   topic = \"\\n\".join(all_keywords)\n  print(topic)\n\n  k = 10\n\n  model = KMeans(n_clusters=k, init=\"k-means++\", max_iter=100, n_init=1)\n\n  model.fit(vectors)\n\n  centroids = model.cluster_centers_.argsort()[:, ::-1]\n\n  terms = vectorizer.get_feature_names()\n   with open(\"results.txt\", \"w\", encoding=\"utf-8\") as f:\n      for i in range(k):\n          f.write(f\"Cluster {i}\")\n          f.write(\"\\n\")\n          for ind in centroids[i, :10]:\n              f.write(' %s' % terms[ind],)\n              f.write(\"\\n\")\n          f.write(\"\\n\")\n          f.write(\"\\n\")\n\nasyncio.run(cleaned_docs_to_vectorize())\n```\n\nIn this code, you create a new function called `cleaned_docs_to_vectorize()`, which will get the previous method's transcript and remove any stop words. Stop words are unimportant, like `a, the, and, this` etc.\n\nThe algorithm will then perform the TF-IDF vectorization using these lines of code:\n\n```python\nvectorizer = TfidfVectorizer(\n                              lowercase=True,\n                              max_features=100,\n                              max_df=0.8,\n                              min_df=4,\n                              ngram_range=(1,3),\n                              stop_words='english'\n\n                          )\n```\n\nYou quickly read about the options passed into the vectorizer like `max_features` and `max_df` [on sciki-learn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n\nYou have a little bit on time with 15% battery life, so you decide to use K-Means to create 10 clusters of topics. This way, they can get a more meaningful sense of the data structure from the podcast. You write the K-Means clusters to a file called `results.txt`.\n\nTo run the program, type `python3 python_topic_detection.py` from the terminal.\n\nWhen you print the topics, you see a list like the following:\n\n```\nsort\nlittle\nsitting\nwent\nnew\nknew\ncomedy\nremember\nguys\nfunny\njerry\nclub\npoint\ngilbert\nyork\nchris\nrock\nfamous\nlater\ngetting\nlong\nlove\nnight\nyear\nbob\nnorm\ncar\nnews\nspace\nastronauts\nnasa\n```\n\nBingo!\n\nYou can now make inferences about the AI Topic Detection to determine the subject matter of the podcast episode.\n\nThen, peek at your `results.txt` file to verify that you received 10 clusters. Here’s an example of four of the ten groups of words using KMeans clustering:\n\n```\nCluster 0\nyeah\nthink\nve\nroast\ngot\nspace\ncat\nsay\njoke\noh\n\n\nCluster 1\nperson\nyork\njoke\ngonna\ngood\ngot\ngreat\nguy\nguys\nheard\n\n\nCluster 2\nknow\nyork\njokes\ngonna\ngood\ngot\ngreat\nguy\nguys\nheard\n\n\nCluster 3\nright\nyork\njoke\ngonna\ngood\ngot\ngreat\nguy\nguys\nheard\n```\n\nJust before your laptop battery dies, you show them the topics for Team Coco. They are very happy with your results and drive off.\n\nYou’re feeling more confident than ever.\n\nYou’ll never know why they needed the Machine Learning topic detection or why they chose you, but you’re on top of the world right now.\n\n## Conclusion\n\nCongratulations on building the Topic Detection AI Python project with Deepgram. Now that you made it to the end of this blog post, Tweet us at [@DeepgramAI](https://twitter.com/DeepgramAI) if you have any questions or to let us know how you enjoyed this post.\n\n## Full Python Code for the AI Machine Learning Podcast Topic Detection Project\n\n```python\nfrom ast import keyword\nfrom posixpath import split\nfrom deepgram import Deepgram\nfrom dotenv import load_dotenv\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom nltk.corpus import stopwords\nimport asyncio\nimport json\nimport os\nimport nltk\n\n\nload_dotenv()\n\nPATH_TO_FILE = 'conan_podcast.mp3'\n\nasync def transcribe_with_deepgram():\n  # Initializes the Deepgram SDK\n  deepgram = Deepgram(os.getenv(\"DEEPGRAM_API_KEY\"))\n  # Open the audio file\n  with open(PATH_TO_FILE, 'rb') as audio:\n      # ...or replace mimetype as appropriate\n      source = {'buffer': audio, 'mimetype': 'audio/mp3'}\n      response = await deepgram.transcription.prerecorded(source)\n\n      if 'transcript' in response['results']['channels'][0]['alternatives'][0]:\n          transcript = response['results']['channels'][0]['alternatives'][0]['transcript']\n\n          return transcript\n\nasync def remove_stop_words():\n  transcript_text = await transcribe_with_deepgram()\n  words = transcript_text.split()\n  final = []\n\n  nltk.download('stopwords')\n  stops = stopwords.words('english')\n\n  for word in words:\n      if word not in stops:\n          final.append(word)\n\n  final = \" \".join(final)\n\n  return final\n\n\nasync def cleaned_docs_to_vectorize():\n  final_list = []\n  transcript_final = await remove_stop_words()\n\n  split_transcript = transcript_final.split()\n\n  vectorizer = TfidfVectorizer(\n                              lowercase=True,\n                              max_features=100,\n                              max_df=0.8,\n                              min_df=4,\n                              ngram_range=(1,3),\n                              stop_words='english'\n\n                          )\n\n\n  vectors = vectorizer.fit_transform(split_transcript)\n\n  feature_names = vectorizer.get_feature_names()\n\n  dense = vectors.todense()\n  denselist = dense.tolist()\n\n  all_keywords = []\n\n  for description in denselist:\n      x = 0\n      keywords = []\n      for word in description:\n          if word > 0:\n              keywords.append(feature_names[x])\n          x=x+1\n\n      [all_keywords.append(x) for x in keywords if x not in all_keywords]\n   topic = \"\\n\".join(all_keywords)\n  print(topic)\n\n  k = 10\n\n  model = KMeans(n_clusters=k, init=\"k-means++\", max_iter=100, n_init=1)\n\n  model.fit(vectors)\n\n  centroids = model.cluster_centers_.argsort()[:, ::-1]\n\n  terms = vectorizer.get_feature_names()\n   with open(\"results.txt\", \"w\", encoding=\"utf-8\") as f:\n      for i in range(k):\n          f.write(f\"Cluster {i}\")\n          f.write(\"\\n\")\n          for ind in centroids[i, :10]:\n              f.write(' %s' % terms[ind],)\n              f.write(\"\\n\")\n          f.write(\"\\n\")\n          f.write(\"\\n\")\n\nasyncio.run(cleaned_docs_to_vectorize())\n```";
						}
						async function compiledContent$G() {
							return load$G().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$G() {
							return (await import('./chunks/index.377fd69f.mjs'));
						}
						function Content$G(...args) {
							return load$G().then((m) => m.default(...args));
						}
						Content$G.isAstroComponentFactory = true;
						function getHeadings$G() {
							return load$G().then((m) => m.metadata.headings);
						}
						function getHeaders$G() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$G().then((m) => m.metadata.headings);
						}

const __vite_glob_0_234 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$G,
  file: file$G,
  url: url$G,
  rawContent: rawContent$G,
  compiledContent: compiledContent$G,
  default: load$G,
  Content: Content$G,
  getHeadings: getHeadings$G,
  getHeaders: getHeaders$G
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$F = {"title":"Track Brand Mentions Across Podcast Episodes","description":"Learn how to generate a report for every podcast episode in a date range for mentions of your brand.","date":"2022-09-13T09:01:01.100Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1662992286/blog/2022/09/find-podcast-brand-mentions/cover.jpg","authors":["kevin-lewis"],"category":"tutorial","tags":["python","podcasts"],"shorturls":{"share":"https://dpgr.am/87bab2b","twitter":"https://dpgr.am/b9de264","linkedin":"https://dpgr.am/0ec29a6","reddit":"https://dpgr.am/795f099","facebook":"https://dpgr.am/8f5bfdf"}};
						const file$F = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/track-brand-mentions-across-podcast-episodes/index.md";
						const url$F = undefined;
						function rawContent$F() {
							return "\nIn this post, we'll cover how to check podcast episodes for mentions of your brand. This can be particularly useful for ensuring sponsorship obligations are met, or to check when competitors are spoken about.\n\nGiven an input of a podcast feed URL, start/end dates, and a brand name, the script will generate a report of all mentions as detected by Deepgram's fast and accurate speech recognition API.\n\n## Before You Start\n\nYou must have Python installed on your machine - I'm using Python 3.10 at the time of writing. You will also need a Deepgram API Key - [get one here](https://console.deepgram.com/signup?jump=keys).\n\nCreate a new directory and navigate to it in your terminal. [Create a virtual environment](https://developers.deepgram.com/blog/2022/02/python-virtual-environments/) with `python3 -m venv virtual_env` and activate it with `source virtual_env/bin/activate`. Install dependencies with `pip install deepgram_sdk asyncio python-dotenv feedparser`.\n\nOpen the directory in a code editor, and create an empty `.env` file. Take your Deepgram API Key, and add the following line to `.env`:\n\n    DEEPGRAM_API_KEY=\"replace-this-bit-with-your-key\"\n\n## Dependency and File Setup\n\nCreate an empty `script.py` file and import the dependencies:\n\n```py\nimport os\nimport json\nfrom datetime import datetime\nfrom time import mktime\nimport asyncio\nfrom dotenv import load_dotenv\nfrom deepgram import Deepgram\nimport feedparser\n```\n\nLoad values from the `.env` file and initialize the Deepgram Python SDK:\n\n```py\nload_dotenv()\ndeepgram = Deepgram(os.getenv('DEEPGRAM_API_KEY'))\n```\n\nFinally, set up a `main()` function that is executed automatically when the script is run:\n\n```py\nasync def main():\n    print('Hello world')\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n\n## Define Parameters\n\nAbove the `main()` function, create a set of variables with settings for your report:\n\n```py\npodcast_feed = 'http://feeds.codenewbie.org/cnpodcast.xml' # CodeNewbie Podcast\nbrand_name = 'stellar'\nrequired_confidence = 0.9\nstart_date = '2022-05-01' # Start of season 20\nend_date = '2022-06-27' # End of season 20\n```\n\nEach time Deepgram returns a search result, it will come with a confidence between 0 and 1. The `required_confidence` value will only report results above the specified confidence level.\n\n## Fetch Episodes with Feedparser\n\nRemove the `print()` statement in the `main()` function, fetch the podcast, and take a look at the returned data by pretty-printing it:\n\n```py\nasync def main():\n    rss = feedparser.parse(podcast_feed)\n    print(json.dumps(rss.entries, indent=2))\n```\n\nTry it out! In your terminal, run the file with `python3 script.py` and you should see a bunch of data for each episode.\n\n### Filter Episodes Within Date Range\n\nFeedparser will take in many different date formats for when a RSS entry is published/updated and normalize them to a standard format. Using the standardized output, create a helper function just below the `main()` function:\n\n```py\ndef check_if_in_date_range(episode):\n    date_with_time = datetime.fromtimestamp(mktime(episode.published_parsed))\n    date = date_with_time.replace(hour=0, minute=0, second=0)\n    is_not_before_start = date >= datetime.fromisoformat(start_date)\n    is_not_after_end = date <= datetime.fromisoformat(end_date)\n    return is_not_before_start and is_not_after_end\n```\n\nThis function takes in an episode, gets the date (without time), and returns `True` if it is within the range between and including `start_date` and `end_date`.\n\nRemove `print(json.dumps(rss.entries, indent=2))` and replace it with the following:\n\n```py\nepisodes = list(filter(check_if_in_date_range, rss.entries))\n```\n\nThe `episodes` array now contains only episodes within the date range.\n\n## Transcribe Episodes with Keyword Boosting and Search\n\nInside of the `main()` function, extract the podcast media URL, set transcription options, and request a Deepgram transcription. Finally, extract search results from the result:\n\n```py\nfor episode in episodes:\n    # Get podcast episode URL\n    source = { 'url': episode.enclosures[0].href }\n    # Increase chance of hearing brand_name\n    # Return search results for brand_name\n    transcription_options = { 'keywords': f'{brand_name}:2', 'search': brand_name }\n    # Request transcription\n    response = await deepgram.transcription.prerecorded(source, transcription_options)\n    # Extract search results\n    search_results = response['results']['channels'][0]['search'][0]['hits']\n```\n\n### Filter Only High Confidence Results\n\nAdd the following line below `search_results` to filter out any values which are below the required confidence:\n\n```py\nstrong_results = list(filter(lambda x: x['confidence'] > required_confidence, search_results))\n```\n\n## Save Mentions Report\n\nBelow `strong_results`, take each episode (and each result within the episode), and add it as a new line in a report file:\n\n```py\n# Define file name\nfilename = f'{brand_name}-mentions-{start_date}-to-{end_date}.txt'\nwith open(filename, 'a+') as f:\n    # Format publish date\n    pub = datetime.fromtimestamp(mktime(episode.published_parsed))\n    # Create line per episode\n    f.write(f'{pub}: \"{episode.title}\" ({len(strong_results)} mentions)\\n')\n    # Create line per result (indented two spaces)\n    for result in strong_results:\n        f.write(f'  Mention at {result[\"start\"]}s of \\\"{result[\"snippet\"]}\\\"\\n')\n```\n\nThat's it! Rerun the script with `python3 script.py` and, once completed, you should see a new file called `stellar-mentions-2022-05-01-to-2022-06-27.txt` - perfect if you want to run several reports.\n\n## Extending This Project\n\nThis project should equip you with the information you need to understand if and how often brand mentions occur throughout several podcast episodes. You should extend this further by creating more complex or graphical reports, allowing several brands to be searched for in one request, or by building a UI around this logic.\n\nAs ever, if you have any questions please feel free to get in touch or post in our [community discussions](https://github.com/orgs/deepgram/discussions).\n\nThe final code is as follows:\n\n```py\nimport asyncio\nimport os\nfrom datetime import datetime\nfrom time import mktime\nfrom dotenv import load_dotenv\nfrom deepgram import Deepgram\nimport feedparser\n\nload_dotenv()\ndeepgram = Deepgram(os.getenv('DEEPGRAM_API_KEY'))\n\npodcast_feed = 'http://feeds.codenewbie.org/cnpodcast.xml'\nbrand_name = 'stellar'\nrequired_confidence = 0.9\nstart_date = '2022-05-01'\nend_date = '2022-06-27'\n\nasync def main():\n    rss = feedparser.parse(podcast_feed)\n    episodes = list(filter(check_if_in_date_range, rss.entries))\n    print(len(episodes))\n\n    for episode in episodes:\n        source = { 'url': episode.enclosures[0].href }\n        transcription_options = { 'keywords': f'{brand_name}:2', 'search': brand_name }\n        response = await deepgram.transcription.prerecorded(source, transcription_options)\n        search_results = response['results']['channels'][0]['search'][0]['hits']\n        strong_results = list(filter(lambda x: x['confidence'] > required_confidence, search_results))\n\n        filename = f'{brand_name}-mentions-{start_date}-to-{end_date}.txt'\n        with open(filename, 'a+') as f:\n            pub = datetime.fromtimestamp(mktime(episode.published_parsed))\n            f.write(f'{pub}: \"{episode.title}\" ({len(strong_results)} mentions)\\n')\n            for result in strong_results:\n                f.write(f'  Mention at {result[\"start\"]}s of \\\"{result[\"snippet\"]}\\\"\\n')\n\ndef check_if_in_date_range(episode):\n    date_with_time = datetime.fromtimestamp(mktime(episode.published_parsed))\n    date = date_with_time.replace(hour=0, minute=0, second=0)\n    is_not_before_start = date >= datetime.fromisoformat(start_date)\n    is_not_after_end = date <= datetime.fromisoformat(end_date)\n    return is_not_before_start and is_not_after_end\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n\n";
						}
						async function compiledContent$F() {
							return load$F().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$F() {
							return (await import('./chunks/index.b4ca124c.mjs'));
						}
						function Content$F(...args) {
							return load$F().then((m) => m.default(...args));
						}
						Content$F.isAstroComponentFactory = true;
						function getHeadings$F() {
							return load$F().then((m) => m.metadata.headings);
						}
						function getHeaders$F() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$F().then((m) => m.metadata.headings);
						}

const __vite_glob_0_235 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$F,
  file: file$F,
  url: url$F,
  rawContent: rawContent$F,
  compiledContent: compiledContent$F,
  default: load$F,
  Content: Content$F,
  getHeadings: getHeadings$F,
  getHeaders: getHeaders$F
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$E = {"title":"Train a Deep Learning Speech Recognition Model to Understand Your Voice","description":"Learn how to build a speech recognition system to understand your voice with the power of deep learning.","date":"2020-06-19T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981354/blog/train-a-deep-learning-speech-recognition-model-to-understand-your-voice/trouble-w-wer%402x.jpg","authors":["natalie-rutgers"],"category":"ai-and-engineering","tags":["deep-learning"],"seo":{"title":"Train a Deep Learning Speech Recognition model to understand your voice","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981354/blog/train-a-deep-learning-speech-recognition-model-to-understand-your-voice/trouble-w-wer%402x.jpg"},"shorturls":{"share":"https://dpgr.am/2bbab35","twitter":"https://dpgr.am/6ff1b78","linkedin":"https://dpgr.am/fa3e037","reddit":"https://dpgr.am/c8e778b","facebook":"https://dpgr.am/758e888"}};
						const file$E = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/train-a-deep-learning-speech-recognition-model-to-understand-your-voice/index.md";
						const url$E = undefined;
						function rawContent$E() {
							return "Here we'll train a speech recognition model to transcribe a phrase that it previously failed to recognize. To do this, we'll record some audio files and train a model using <a target=\"_blank\" rel=\"noopener noreferrer\">Deepgram MissionControl</a> - Deepgram's all-in-one platform for custom training and deploying custom trained speech recognition. To explore custom training with MissionControl, we'll train our model to recognize the phrase: \"The proof is in the pudding.\" We'll do this by recording 50 audio files, pairing those files with accurate labels, and then training a model. That should get us from Deepgram General models belief that I'm saying, \"is this in this footing\" to a model that knows \"The proof is in the pudding.\"\n\n## Sign up for MissionControl\n\nFirst, you'll want to [create an account](https://missioncontrol.deepgram.com/signup). Your account comes preloaded with a few freebies:\n\n```\n1\\. 20 audio hours per month of Automatic Speech Recognition  \n2\\. The ability to create 2 Custom-Trained Models  \n3\\. The ability to deploy 1 of your Custom-Trained Models  \n4\\. 10 minutes of professional data labeling to help create training data  \n5\\. 2 Free Training-ready datasets  \n6\\. Access to 3 of Deepgram's General models  \n```\n\n## Create an API Key\n\nTo avoid reusing your username and password in requests, it can be helpful to create a new API key to use for development. You can do this at [in MissionControl](https://missioncontrol.deepgram.com/accounts-and-billing) or by running the following request in the command line, being sure to swap in your credentials and a name of your choosing. We highly recommend running these requests through `jq` for easy-to-read outputs.\n\n```\ncurl -X POST -u USERNAME:PASSWORD https://missioncontrol.deepgram.com/v1/keys?name=\"test\"  \n```\n\n## Create a dataset\n\nFirst, we'll create a new dataset. This dataset will hold all our example recordings and labels so that we can use them for training. Use the following command, being sure to give your dataset a useful name.\n\n```\ncurl -X POST -u SECRET:KEY https://missioncontrol.deepgram.com/v1/datasets?name=\"Proof\"  \n```\n\nIn no time, you'll get back a response telling you your new dataset's `dataset-id`. Grab that `dataset-id` for later.\n\n## Record and upload your audio and labels\n\nYou can go about this step a number of ways. Ultimately, you want to record and label 50 files of yourself saying, \"The proof is in the pudding.\" This tutorial walks you through two ways to do that:\n\n1. With the Deepgram MissionControl Recorder\n2. With the Command Line\n\n### With the Deepgram MissionControl Recorder\n\nFirst, navigate to the Deepgram MissionControl Recorder. There, you'll be asked to to fill in some information so that we can send your recordings to your dataset in your MissionControl account. Paste in the `dataset-id` that you received earlier, as well as your API credentials to authenticate your resulting POST requests. For the sentence, we'll be entering in our \"The proof is in the pudding.\" Once you've filled in all the fields, go ahead and click next. You'll see your script displayed, as well as a counter for how many recordings you have left to record. Record and submit yourself saying this phrase until you see a surprise. \n\nIn the background, the app uses the information you supplied to submit each recording to your selected dataset. It then submits the supplied alongside that recording for its label. You can check that your recordings are being captured by viewing your dataset in your [DataFactory in MissionControl](https://missioncontrol.deepgram.com/data). \n\n### With the command line\n\nTo install a tool to record from command line, use SoX (Sound eXchange).\n\n```\nsudo apt install sox  \nbrew install sox  \n```\n\nAnd, to record a file in your current directory in your terminal, do this (using SoX's built in rec command):\n\n```\nrec -b 16 -r 16000 -c 1 yourFile.wav  \n```\n\nThis will record a wav file from your computer's onboard microphone (make sure it has an onboard microphone that is in your proximity, this will not work on a remote server, gotta use a laptop or workstation). The wav file has these properties: 16 bit depth, 16,000 Hertz sample rate, mono channel and is recorded to the file yourFile.wav (you should might want to pick a better name than that!). Record yourself saying \"The proof is in the pudding.\" 50 times, giving the resulting file a unique name for each recording. Now you're ready to upload your recordings and labels to your MissionControl dataset. You can upload your recordings to MissionControl using this command:\n\n```\n    curl -X POST -u SECRET:KEY --data-binary @path/to/yourFile.wav \"https://missioncontrol.deepgram.com/v1/resources?name=myfile.wav&dataset-id=dddddddd-1111-2222-3333-444444444444\"  \n```\n\nUsing the resulting `resource-id`s, you'll then want to pair labels with each recording. This can be uploaded with the following command:\n\n```\n    curl -X PUT -u SECRET:KEY -H \"Content-Type: text/plain\" -d'The proof is in the pudding.'  \n     \"https://missioncontrol.deepgram.com/v1/resources/{resource-id}/transcript\" \n```\n\nRepeat with all of your recordings.\n\n## Train a Custom Model with your dataset\n\nNow that your dataset is training-ready, you're ready to build your Custom Model. Go ahead and submit a `curl` command that names your model and associates the dataset you prepared with it.\n\n```\n    curl -X POST -u SECRET:KEY https://missioncontrol.deepgram.com/v1/models?dataset-id=dddddddd-1111-2222-3333-444444444444 -H 'content-type: application/json' -d '{\"name\": \"Proof-in-the-Pudding\"}'  \n```\n\nTo associate additional datasets to your model, take advantage of [`PUT /models/{model-id}/datasets`](https://missioncontrol.deepgram.com/docs). You'll quickly get back a response that shows your new model.\n\n```\n    {\n      \"model_id\": \"aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee\",\n      \"version_id\": \"12345678-1234-1234-1234-1234567890ab\",\n      \"model_name\": \"Proof-in-the-Pudding\",\n      \"created\": \"2020-05-01T18:56:40.316185Z\",\n      \"model_type\": \"USER\",\n      \"wer\": null,\n      \"trained_at\": null,\n      \"status\": \"CANCELLED\"\n    }\n```\n\nGo ahead and copy the `model_id`. We'll use that to submit the model for training. Perfect, plug that `model_id` in and run the following command:\n\n```\n    curl -X POST -u SECRET:KEY \"https://missioncontrol.deepgram.com/v1/train?model-id={model-id}&base-model-id=e1eea600-6c6b-400a-a707-a491509e52f1\"  \n```\n\nYou'll see a response confirming that your model has been submitted and its status has changed to `PENDING`\n\n```\n    {\n     \"id\":\"a21e82a7-5bac-4b2a-a816-cb2f84e08ca8\",\n     \"model_id\":\"aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee\",\n     \"submitted\":\"2020-05-01T19:12:24.913587Z\",\n     \"finished\": null,\n     \"status\":\"PENDING\"\n    }\n```\n\nTraining will take some time, but you'll be emailed once your model has finished. Once it's finished training, take a look at the steps for [reviewing your custom model's performance and deploying it for use at scale](https://blog.deepgram.com/quickstart-guide-for-the-deepgram-missioncontrol-api/). To transcribe with your new model, you'll need to [deploy it to SpeechEngine](https://blog.deepgram.com/quickstart-guide-for-the-deepgram-missioncontrol-api/).\n\n![]()";
						}
						async function compiledContent$E() {
							return load$E().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$E() {
							return (await import('./chunks/index.41efaf5f.mjs'));
						}
						function Content$E(...args) {
							return load$E().then((m) => m.default(...args));
						}
						Content$E.isAstroComponentFactory = true;
						function getHeadings$E() {
							return load$E().then((m) => m.metadata.headings);
						}
						function getHeaders$E() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$E().then((m) => m.metadata.headings);
						}

const __vite_glob_0_236 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$E,
  file: file$E,
  url: url$E,
  rawContent: rawContent$E,
  compiledContent: compiledContent$E,
  default: load$E,
  Content: Content$E,
  getHeadings: getHeadings$E,
  getHeaders: getHeaders$E
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$D = {"title":"Automatically Transcribe Google Drive Files with Pipedream","description":"Learn how to automate the creation of transcriptions by simply dropping files into a Google Drive folder.","date":"2022-08-08T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1659956235/blog/2022/08/transcribe-google-drive-files-pipedream/cover.png","authors":["kevin-lewis"],"category":"tutorial","tags":["javascript","low-code"],"seo":{"title":"Automatically Transcribe Google Drive Files with Pipedream","description":"Learn how to automate the creation of transcriptions by simply dropping files into a Google Drive folder."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454123/blog/transcribe-google-drive-files-pipedream/ograph.png"},"shorturls":{"share":"https://dpgr.am/76f6af6","twitter":"https://dpgr.am/23ffeae","linkedin":"https://dpgr.am/0ead30b","reddit":"https://dpgr.am/d45be9f","facebook":"https://dpgr.am/7c87314"}};
						const file$D = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/transcribe-google-drive-files-pipedream/index.md";
						const url$D = undefined;
						function rawContent$D() {
							return "Pipedream is one of my new discoveries, and I've quickly become a fan. It's a low-code automation builder with a solid offering for developers with the ability to install npm packages and write code for the Node.js runtime (as well as other languages). There's ~2GB of ephemeral storage available to your workflows. With hundreds of integrations available, much of the time spent doing CRUD operations is taken away. You can focus on the task-specific 'glue' that brings your automation to life.\n\nIn this tutorial, you will build a Pipedream workflow that will listen for new files to be added to a Google Drive folder, transcribe them with the [Deepgram Node.js SDK](https://developers.deepgram.com/sdks-tools/sdks/node-sdk/), and email you the result.\n\nBefore you start, you will need a Pipedream account, a Google account, and a free [Deepgram API Key](https://console.deepgram.com/signup?jump=keys).\n\nCreate a new, empty, Pipedream workflow for this project.\n\n## Trigger Your Workflow By Adding New Files\n\nEach workflow can have a single trigger that starts it - anything from it being a certain time, a new tweet matching specific criteria, or a new form submission.\n\nIn your new empty workflow, pick a **Google Drive New Files (Instant)** Trigger. Connect your Google Drive account, ensuring access to see, edit, delete, and download your Google Drive files.\n\nBy default, this trigger will begin your workflow when a file is added anywhere in your Drive. That can be a bit aggressive, so you can set specific folders in the trigger step. Finally, click **Create Source**.\n\n![New Files Google Drive trigger with one folder specified called Pipedream Demo.](https://res.cloudinary.com/deepgram/image/upload/v1658143658/blog/2022/08/transcribe-google-drive-files-pipedream/trigger.png)\n\nTest the step by adding a file to your Google Drive and selecting the event in your Pipedream workflow. This example data can be used in future steps in this workflow.\n\n![Step output showing drive file data, which includes a file ID, name, and mimeType.](https://res.cloudinary.com/deepgram/image/upload/v1658143658/blog/2022/08/transcribe-google-drive-files-pipedream/trigger-exports.png)\n\n## Download File To Temporary Pipedream Storage\n\nIt can't be assumed that your file has public permissions, so the next step in the workflow is to download the Google Drive file to Pipedream's temporary storage. This storage is ephemeral, so you can't rely on it for long periods, but it is safe to use for the length of this workflow.\n\nCreate a new step with a **Google Drive Download File** action and select your Google Drive account. For File, enter a custom expression of the File ID from the trigger: `{{steps.trigger.event.id}}`.\n\nTo download the file to Pipedream's ephemeral storage, set the Destination File Path to `/tmp/{{steps.trigger.event.name}}`. If the filename is `hello.mp3`, the destination will be `/tmp/hello.mp3`.\n\nTest the step, and you should see a success message. This now-local file will be sent to Deepgram in the next step.\n\n![The Downlod File step showing a success message](https://res.cloudinary.com/deepgram/image/upload/v1658143658/blog/2022/08/transcribe-google-drive-files-pipedream/download.png)\n\n## Transcribe File with Deepgram\n\nCreate a new step and select Deepgram. Connect a Deepgram account and provide your Deepgram API Key. Replace all of the code in this step with the following:\n\n```js\nconst fs = require('fs')\nconst { Deepgram } = require('@deepgram/sdk')\n\nmodule.exports = defineComponent({\n  props: {\n    deepgram: {\n      type: \"app\",\n      app: \"deepgram\",\n    }\n  },\n  async run({ steps }) {\n\n  }\n})\n```\n\nThere's no need to separately install the `@deepgram/sdk` package - simply importing or requiring it is enough.\n\nBy adding the `deepgram` prop, we can easily and securely access the Deepgram API Key we provided when connecting our account. The `run` method is triggered when this step is executed, and the `steps` object will contain all exported data from the trigger and previous steps.\n\nInside of `run`, add the following:\n\n```js\nconst { name, mimeType } = steps.trigger.event\nconst buffer = await fs.promises.readFile(`/tmp/${name}`)\nconst source = { buffer, mimetype: mimeType }\n\nconst deepgram = new Deepgram(this.deepgram.$auth.api_key)\nconst { results } = await deepgram.transcription.preRecorded(source, { punctuate: true })\nreturn results.channels[0].alternatives[0].transcript\n```\n\nThis code snippet uses the [Deepgram Node.js SDK](https://developers.deepgram.com/sdks-tools/sdks/node-sdk/) and the [punctuation](https://developers.deepgram.com/documentation/features/punctuate/) feature, and returns only the transcript string. You can customize this by changing the returned value.\n\nDue to the nature of ephemeral storage, the downloaded file from the last step may have now been deleted. Instead of clicking **Test**, use the dropdown and **Test all above** - this will repeat all steps, up-to-and-including generating transcriptions.\n\n![Dropdown showing several options - highlighted is \"Test all above\"](https://res.cloudinary.com/deepgram/image/upload/v1658143658/blog/2022/08/transcribe-google-drive-files-pipedream/deepgram.png)\n\n## Send Transcript\n\nNow a transcript is generated, you can store it or send it however you like. You could store it in an Airtable database, create a new Google Docs file with the same filename, or send a text message with the contents.\n\nCreate a new step with the **Send Yourself an Email** action. For the subject use `Transcription for {{steps.trigger.event.name}} is ready`. For the email text use `{{steps.deepgram.$return_value}}` to include the Deepgram transcription.\n\nTest your workflow, and you should receive an email with a newly-generated transcript!\n\n## Cleanup Storage\n\nFinally, go ahead and delete the file from the `tmp` directory. While this isn't strictly necessary, it is recommended. Create a new Node.js step, delete all of the code, and replace it with the following:\n\n```js\nconst fs = require('fs')\n\nmodule.exports = defineComponent({\n  async run({ steps }) {\n    return await fs.promises.unlink(`/tmp/${steps.trigger.event.name}`)\n  },\n})\n```\n\nYou can now deploy your workflow and automatically receive Deepgram transcripts via email. You can, of course, edit any of the steps to be more useful to your specific use case. If you have any questions, please feel free to reach out to us - we love to help!";
						}
						async function compiledContent$D() {
							return load$D().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$D() {
							return (await import('./chunks/index.93bd89f4.mjs'));
						}
						function Content$D(...args) {
							return load$D().then((m) => m.default(...args));
						}
						Content$D.isAstroComponentFactory = true;
						function getHeadings$D() {
							return load$D().then((m) => m.metadata.headings);
						}
						function getHeaders$D() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$D().then((m) => m.metadata.headings);
						}

const __vite_glob_0_237 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$D,
  file: file$D,
  url: url$D,
  rawContent: rawContent$D,
  compiledContent: compiledContent$D,
  default: load$D,
  Content: Content$D,
  getHeadings: getHeadings$D,
  getHeaders: getHeaders$D
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$C = {"title":"Transcribe Phone Calls with Twilio Functions and Deepgram","description":"Learn how to use Deepgram's Speech Recognition API to transcribe Twilio phone calls with Twilio Functions and Node.js","date":"2022-08-29T23:48:20.204Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661256278/blog/2022/08/transcribe-twilio-calls-functions/cover.jpg","authors":["kevin-lewis"],"category":"tutorial","tags":["twilio","serverless"],"shorturls":{"share":"https://dpgr.am/a20fc02","twitter":"https://dpgr.am/2f19bb2","linkedin":"https://dpgr.am/3c905b6","reddit":"https://dpgr.am/49ab62b","facebook":"https://dpgr.am/081054f"}};
						const file$C = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/transcribe-phone-calls-with-twilio-functions-and-deepgram/index.md";
						const url$C = undefined;
						function rawContent$C() {
							return "\nTwilio is a cloud communication platform that lets developers integrate a whole set of communication technologies into applications. On top of providing APIs, they also have Twilio Runtime - which allows the development and deployment of serverless applications directly in the Twilio Console. Super cool.\n\nToday you will be using Twilio Functions to build a phone number that gives callers a prompt, then records a message similar to a voicemail. Once completed, a transcript will be generated with Deepgram and sent to the caller.\n\n![A diagram showing a person talking into a phone call with a Twilio number, then ending the call, and then a text message thread with the spoken text sent from Twilio.](https://res.cloudinary.com/deepgram/image/upload/v1658239971/blog/2022/08/transcribe-twilio-calls-functions/twilio-diagram.png)\n\n## Before You Start\n\nYou will need a free [Deepgram API Key](https://console.deepgram.com/signup?jump=keys). You will also need a [Twilio account](https://console.twilio.com) and a phone number in your account with SMS and Voice capabilities.\n\n## Set Up Twilio Functions Space\n\nInside the Twilio Console, head to **Developer Tools -> Functions & Assets** and create a new service. A service can contain multiple Twilio Functions and assets related to a single project. It's important that you create a new service here and not a standalone function.\n\nHead to the *Dependencies* section and add `@deepgram/sdk` (you can omit the version for the latest). Then head to the *Environment Variables* section and add a key called `DEEPGRAM_KEY` with the value of your API Key generated in your Deepgram console.\n\nFunctions can have one of three visibility levels - public, protected, and private. The default of 'protected' is totally fine for this project and means that only Twilio webhooks can trigger them.\n\n## Record Inbound Call\n\nRename the default `/welcome` function to `/inbound`. Replace the whole file with the following:\n\n```js\nexports.handler = function(context, event, callback) {\n  let twiml = new Twilio.twiml.VoiceResponse()\n\n  twiml.say({ voice: 'woman', language: 'en' }, 'Try Deepgram transcription by speaking after the beep. Talk about what you see around you right now.')\n\n  twiml.record({\n    maxLength: 30, // seconds, 30 is default\n    playBeep: true,\n    recordingStatusCallback: '/transcribe'\n  })\n\n  return callback(null, twiml)\n};\n```\n\nTo respond to incoming calls and texts, Twilio lets you form and respond to requests with TwiML (Twilio Markup Language). It looks a lot like XML and can be generated with the Node Helper Library, which is included in Twilio Functions by default.\n\nThis snippet creates a new TwiML response, speaks the phrase, beeps, and begins the recording. Once the call is ended (hang up or ended after 30 seconds of recording), a payload is sent to `/transcribe` (which will be created in the next section).\n\nSave the function, and click *Deploy All*. Once deployed, this function is ready to be used. Go to your Twilio number settings, and when a call comes in, select *Function*. Select your service and the `/inbound` function path.\n\n![When a call comes in, use a Function. Default service with the /inbound function path.](https://res.cloudinary.com/deepgram/image/upload/v1661255979/blog/2022/08/transcribe-twilio-calls-functions/set-inbound-endpoint.png)\n\nCall your number, and you should hear it speaking, then beep. If you speak now, a recording will take place, and data will be sent to `/transcribe`, but that endpoint does not exist yet - let's fix that.\n\n## Transcribe Call\n\nCreate a new function - `/transcribe`. Delete the boilerplate and set up the function with the following code:\n\n```js\nconst { Deepgram } = require('@deepgram/sdk')\nconst deepgram = new Deepgram(process.env.DEEPGRAM_KEY)\n\nexports.handler = async function(context, event, callback) {\n  const { RecordingUrl, CallSid } = event\n  // Further code here\n  return callback(null, true)\n};\n```\n\nThe recording data will be available in the `event` object, which destructures to the `RecordingUrl` and `CallSid` values. Unfortunately, this payload doesn't include the caller's phone number, but it can be looked up from the `CallSid`. Where the `Further code here` comment is situation, add the following:\n\n```javascript\nconst twilioClient = context.getTwilioClient()\nconst { from: caller, to: twilioNumber } = await twilioClient.calls(CallSid).fetch()\n```\n\nThe caller's phone number is now available in a variable called `caller`, and the number they called as `twilioNumber`. Now generate a transcription with Deepgram's Node.js SDK:\n\n```js\nconst transcriptionFeatures = { punctuate: true, tier: 'enhanced' }\nconst { results } = await deepgram.transcription.preRecorded({ url: RecordingUrl }, transcriptionFeatures)\nconst { transcript } = results.channels[0].alternatives[0]\n```\n\nThis request uses Deepgram's [punctuation](https://developers.deepgram.com/documentation/features/punctuate/) feature, along with a request to use the [enhanced tier](https://developers.deepgram.com/documentation/features/tier/) for higher-accuracy transcripts.\n\n## Send Transcription via SMS\n\nNow that a transcript has been generated, it's time to send it to the caller. Just after the transcript is generated, add the following to send an SMS message:\n\n```js\nconst message = await twilioClient.messages.create({\n  body: `Here is what you said:\\n\\n${transcript}`,\n  to: caller,\n  from: twilioNumber\n})\n```\n\nFinally, change the callback value from `true` to `{ results, message }`. This is purely for logging to your Twilio Console.\n\nSave both files again and deploy all functions in your service. Call your Twilio number, speak after the beep, then hang up. You should receive a message a few seconds later.\n\n## In Summary\n\nThe final code is as follows:\n\n```js\n// /record\nexports.handler = function(context, event, callback) {\n\tlet twiml = new Twilio.twiml.VoiceResponse()\n  twiml.say({ voice: 'woman', language: 'en' }, 'Try Deepgram transcription by speaking after the beep. Talk about what you see around you right now.');\n  twiml.record({\n    maxLength: 30,\n    playBeep: true,\n    recordingStatusCallback: '/transcribe'\n  })\n  return callback(null, twiml)\n}\n\n// /transcribe\nconst { Deepgram } = require('@deepgram/sdk')\nconst deepgram = new Deepgram(process.env.DEEPGRAM_KEY)\nexports.handler = async function(context, event, callback) {\n  const { RecordingUrl, CallSid } = event\n  const twilioClient = context.getTwilioClient()\n  const { from: caller, to: twilioNumber } = await twilioClient.calls(CallSid).fetch()\n  const transcriptionFeatures = { punctuate: true , tier: 'enhanced' }\n  const { results } = await deepgram.transcription.preRecorded({ url: RecordingUrl }, transcriptionFeatures)\n  const { transcript } = results.channels[0].alternatives[0]\n  const message = await twilioClient.messages.create({\n    body: `Here is what you said:\\n\\n${transcript}`,\n    to: caller,\n    from: twilioNumber\n  })\n  return callback(null, { results, message })\n}\n```\n\nDon't forget to install dependencies and set your environment variables. If you have any questions about this project, feel free to get in touch.\n\n";
						}
						async function compiledContent$C() {
							return load$C().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$C() {
							return (await import('./chunks/index.a0d40915.mjs'));
						}
						function Content$C(...args) {
							return load$C().then((m) => m.default(...args));
						}
						Content$C.isAstroComponentFactory = true;
						function getHeadings$C() {
							return load$C().then((m) => m.metadata.headings);
						}
						function getHeaders$C() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$C().then((m) => m.metadata.headings);
						}

const __vite_glob_0_238 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$C,
  file: file$C,
  url: url$C,
  rawContent: rawContent$C,
  compiledContent: compiledContent$C,
  default: load$C,
  Content: Content$C,
  getHeadings: getHeadings$C,
  getHeaders: getHeaders$C
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$B = {"title":"Transcribe Videos With Node.js","description":"Convert and Transcribe Videos with Node.js and Deepgram","date":"2021-11-11T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1635374324/blog/2021/11/transcribe-videos-nodejs/transcribe-videos-with-nodejs-blog%402x.png","authors":["kevin-lewis"],"category":"tutorial","tags":["nodejs","sdk","javascript"],"seo":{"title":"Transcribe Videos With Node.js","description":"Convert and Transcribe Videos with Node.js and Deepgram"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453821/blog/transcribe-videos-nodejs/ograph.png"},"shorturls":{"share":"https://dpgr.am/c5961f4","twitter":"https://dpgr.am/dc34079","linkedin":"https://dpgr.am/3621965","reddit":"https://dpgr.am/977352f","facebook":"https://dpgr.am/9df6807"}};
						const file$B = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/transcribe-videos-nodejs/index.md";
						const url$B = undefined;
						function rawContent$B() {
							return "Whether your video is hosted online or on your machine, for accessibility or analysis, Deepgram can provide accurate transcriptions in just a few lines of code.\n\nI'm glad you're here, but I must confess that I am leading you down a path slightly different to what you would expect. Instead of transcribing video directly, this post will cover converting video files to audio files and then sending them to Deepgram. First, we will transcribe local files, and then we will download files programatically before transcribing them.\n\n## Before We Start\n\nYou will need:\n\n* Node.js installed on your machine - [download it here](https://nodejs.org/en/).\n* A Deepgram API Key - [get one here](https://console.deepgram.com/signup?jump=keys).\n* A video file to transcribe - [here's one you can download](https://github.com/deepgram-devs/transcribe-videos/blob/main/deepgram.mp4) and place in your new project directory.\n* A link to a hosted video file - [here is the same video's direct URL](https://github.com/deepgram-devs/transcribe-videos/raw/main/deepgram.mp4).\n\nCreate a new directory and navigate to it with your terminal. Run `npm init -y` to create a `package.json` file and then install the following packages:\n\n```\nnpm install @deepgram/sdk ffmpeg-static\n```\n\nCreate an `index.js` file, and open it in your code editor.\n\n## Preparing Dependencies\n\nAt the top of your file require these packages:\n\n```js\nconst fs = require('fs')\nconst https = require('https')\nconst { execSync: exec } = require('child_process')\nconst { Deepgram } = require('@deepgram/sdk')\nconst ffmpegStatic = require('ffmpeg-static')\n```\n\n`fs` is the built-in file system module for Node.js. It is used to read and write files which you will be doing a few times throughout this post. `ffmpeg-static` includes a version of ffmpeg in our node_modules directory, and requiring it returns the file path.\n\nInitialize the Deepgram client:\n\n```js\nconst deepgram = new Deepgram('YOUR DEEPGRAM KEY')\n```\n\n## Running ffmpeg Commands\n\n[ffmpeg](https://ffmpeg.org) is a toolkit for developers to work with audio and video files - which includes conversion between formats. It's used most commonly in a terminal, so below is a utility function to add to your `index.js` file. It allows us to fire off terminal commands directly from our Node.js application:\n\n```js\nasync function ffmpeg(command) {\n  return new Promise((resolve, reject) => {\n    exec(`${ffmpegStatic} ${command}`, (err, stderr, stdout) => {\n      if (err) reject(err)\n      resolve(stdout)\n    })\n  })\n}\n```\n\n## Transcribing Local Video\n\nThis function will convert and transcribe local video files:\n\n```js\nasync function transcribeLocalVideo(filePath) {\n  ffmpeg(`-hide_banner -y -i ${filePath} ${filePath}.wav`)\n\n  const audioFile = {\n    buffer: fs.readFileSync(`${filePath}.wav`),\n    mimetype: 'audio/wav',\n  }\n  const response = await deepgram.transcription.preRecorded(audioFile, {\n    punctuation: true,\n  })\n  return response.results\n}\n\ntranscribeLocalVideo('deepgram.mp4').then((transcript) =>\n  console.dir(transcript, { depth: null })\n)\n```\n\n``ffmpeg(`-hide_banner -y -i ${filePath} ${filePath}.wav`)`` takes in the provided file, and converts it to a `.wav` audio file. `-hide_banner` reduces the amount of information printed in the terminal and`-y` will overwrite an existing file (useful for development).\n\nSave and run the file in your terminal with `node index.js` and you should see transcripts appear.\n\n## Transcribing Remote Video\n\nAdd this utility to the bottom of your file:\n\n```js\nasync function downloadFile(url) {\n  return new Promise((resolve, reject) => {\n    const request = https.get(url, (response) => {\n      const fileName = url.split('/').slice(-1)[0] // Get the final part of the URL only\n      const fileStream = fs.createWriteStream(fileName)\n      response.pipe(fileStream)\n      response.on('end', () => {\n        fileStream.close()\n        resolve(fileName)\n      })\n    })\n  })\n}\n```\n\nThis allows us to download a file to our machine. The file name will be derived from the last part of the URL - for example https://example.com/directory/directory2/file.mp4 becomes `file.mp4` locally.\n\nWith this in place, we first download the video and then use our existing `transcribeLocalVideo()` function:\n\n```js\nasync function transcribeRemoteVideo(url) {\n  const filePath = await downloadFile(url)\n  const transcript = await transcribeLocalVideo(filePath)\n  console.dir(transcript, { depth: null })\n}\n```\n\nThe complete project is available at https://github.com/deepgram-devs/transcribe-videos and if you have any questions please feel free to reach out on Twitter - we're [@DeepgramDevs](https://twitter.com/DeepgramDevs).";
						}
						async function compiledContent$B() {
							return load$B().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$B() {
							return (await import('./chunks/index.4be7290e.mjs'));
						}
						function Content$B(...args) {
							return load$B().then((m) => m.default(...args));
						}
						Content$B.isAstroComponentFactory = true;
						function getHeadings$B() {
							return load$B().then((m) => m.metadata.headings);
						}
						function getHeaders$B() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$B().then((m) => m.metadata.headings);
						}

const __vite_glob_0_239 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$B,
  file: file$B,
  url: url$B,
  rawContent: rawContent$B,
  compiledContent: compiledContent$B,
  default: load$B,
  Content: Content$B,
  getHeadings: getHeadings$B,
  getHeaders: getHeaders$B
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$A = {"title":"How To Transcribe YouTube Videos From Your Terminal","description":"A five-year old snippet shared by our CEO still stacks up. Learn how to transcribe a YouTube video entirely from the terminal with youtube-dl and jq.","date":"2022-08-22T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661178745/blog/2022/08/transcribe-youtube-videos-from-terminal/2208-Transcribing-YouTube-Videos-From-Your-Terminal-blog%402x.jpg","authors":["kevin-lewis"],"category":"tutorial","tags":["youtube"],"seo":{"title":"How To Transcribe YouTube Videos From Your Terminal","description":"A five-year old snippet shared by our CEO still stacks up. Learn how to transcribe a YouTube video entirely from the terminal with youtube-dl and jq."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454124/blog/transcribe-youtube-videos-from-terminal/ograph.png"},"shorturls":{"share":"https://dpgr.am/0efb794","twitter":"https://dpgr.am/bf6fe19","linkedin":"https://dpgr.am/59d015d","reddit":"https://dpgr.am/fb31c03","facebook":"https://dpgr.am/2d52871"}};
						const file$A = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/transcribe-youtube-videos-from-terminal/index.md";
						const url$A = undefined;
						function rawContent$A() {
							return "In our internal Deepgram Slack workspace, there's a channel where folks can share fun and wacky things they've achieved on the terminal (`#bash-hall-of-fame`). Over five years ago, our CEO Scott shared a nice little snippet that allows you to download just the audio from a YouTube video. Today, I'm going to take that still-functional piece of code and show you how to download audio from a YouTube video and then transcribe it with Deepgram's Speech Recognition API.\n\nThe steps are remarkably similar to our [Transcribing YouTube Videos with Node.js](https://blog.deepgram.com/transcribe-youtube-videos-nodejs/) post, but entirely on the terminal.\n\nYou will need to download [`youtube-dl`](http://ytdl-org.github.io/youtube-dl/download.html), [`ffmpeg`](http://ffmpeg.org/download.html), and [`jq`](https://stedolan.github.io/jq/) for this tutorial to work. If you use macOS and have homebrew installed, this is `brew install youtube-dl`, `brew install ffmpeg`, and `brew install jq`. You will also need a Deepgram API Key - [get one here](https://console.deepgram.com/signup?jump=keys).\n\n## Download Audio From YouTube Video with youtube_dl\n\nWe'll use the following YouTube ID: `9NZDwZbyDus`. Starting with Scott's original snippet:\n\n```bash\nyoutube-dl 9NZDwZbyDus --extract-audio --audio-format wav -o 9NZDwZbyDus.wav\n```\n\nGiven that we use the same value twice, let's abstract the video ID into a variable:\n\n```bash\nVIDEO_ID=9NZDwZbyDus; youtube-dl $VIDEO_ID --extract-audio --audio-format wav -o $VIDEO_ID.wav\n```\n\n## Transcribe With Deepgram\n\nNow that we have a local file and know its file format, we can use cURL to get a transcript from Deepgram:\n\n```bash\ncurl https://api.deepgram.com/v1/listen?punctuate=true -H \"Authorization: Token YOUR_DEEPGRAM_API_KEY\" -H \"Content-Type: audio/wav\" --data-binary @${VIDEO_ID}.wav\n```\n\nUsing `jq` to extract just the transcript text and saving that to a file:\n\n```bash\ncurl https://api.deepgram.com/v1/listen?punctuate=true -H \"Authorization: Token YOUR_DEEPGRAM_API_KEY\" -H \"Content-Type: audio/wav\" --data-binary @${VIDEO_ID}.wav | jq '.results.channels[0].alternatives[0].transcript' > \"$VIDEO_ID.txt\"\n```\n\n## Delete Audio File\n\nFinally, if you no longer require the audio file, delete it:\n\n```bash\nrm $VIDEO_ID.wav\n```\n\n## Bringing It All Together\n\nWhen we first introduced a variable to this script, we separated the declaration and the cURL command with a semicolon. We can do exactly the same with all subsequent steps. The one-liner for this project is:\n\n```bash\nVIDEO_ID=EmIhbFeJgiE; youtube-dl ${VIDEO_ID} --extract-audio --audio-format wav -o ${VIDEO_ID}.wav; curl https://api.deepgram.com/v1/listen\\?punctuate\\=true -H \"Authorization: Token YOUR_DEEPGRAM_API_KEY\" -H \"Content-Type: audio/wav\" --data-binary @${VIDEO_ID}.wav | jq '.results.channels[0].alternatives[0].transcript' > \"$VIDEO_ID.txt\"; rm \"$VIDEO_ID.wav\"\n```\n\nIf you have any questions, please let us know - we love to help!";
						}
						async function compiledContent$A() {
							return load$A().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$A() {
							return (await import('./chunks/index.ac9156ca.mjs'));
						}
						function Content$A(...args) {
							return load$A().then((m) => m.default(...args));
						}
						Content$A.isAstroComponentFactory = true;
						function getHeadings$A() {
							return load$A().then((m) => m.metadata.headings);
						}
						function getHeaders$A() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$A().then((m) => m.metadata.headings);
						}

const __vite_glob_0_240 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$A,
  file: file$A,
  url: url$A,
  rawContent: rawContent$A,
  compiledContent: compiledContent$A,
  default: load$A,
  Content: Content$A,
  getHeadings: getHeadings$A,
  getHeaders: getHeaders$A
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$z = {"title":"Transcribe YouTube Videos with Node.js","description":"Generate and save transcripts from YouTube videos with the Deepgram SDK for Node.js.","date":"2021-11-01T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1635374324/blog/2021/11/transcribe-youtube-videos-nodejs/getting-transcripts-from-youtube-videos-blog%402x.png","authors":["kevin-lewis"],"category":"tutorial","tags":["nodejs","sdk","javascript"],"seo":{"title":"Transcribe YouTube Videos with Node.js","description":"Generate and save transcripts from YouTube videos with the Deepgram SDK for Node.js."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453823/blog/transcribe-youtube-videos-nodejs/ograph.png"},"shorturls":{"share":"https://dpgr.am/0f005c0","twitter":"https://dpgr.am/5091038","linkedin":"https://dpgr.am/94f4999","reddit":"https://dpgr.am/429a550","facebook":"https://dpgr.am/a63d46e"}};
						const file$z = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/transcribe-youtube-videos-nodejs/index.md";
						const url$z = undefined;
						function rawContent$z() {
							return "In this blog post we will be creating transcripts for YouTube videos using Deepgram's Speech Recognition API. First, we will download videos and convert them to mp3 audio files. Then, we will use Deepgram to generate a transcript. Finally, we will store the transcript in a text file and delete the media file.\n\nThe final project code can be found at https://github.com/deepgram-devs/youtube-transcripts.\n\nWatch this tutorial as a video:\n\n<YouTube id=\"LrNS_q886uQ\"></YouTube>\n\nWe need a sample video, so I am using a [Shang-Chi and The Legend of The Ten Rings teaser trailer](https://www.youtube.com/watch?v=ir-mWUYH_uo) - if that is a spoiler for you please go ahead and grab another video link.\n\n<YouTube id=\"ir-mWUYH_uo\"></YouTube>\n\n## Before We Start\n\nYou will need:\n\n* Node.js installed on your machine - [download it here](https://nodejs.org/en/).\n* A Deepgram API Key - [get one here](https://console.deepgram.com/signup?jump=keys).\n* A YouTube Video ID which is part of the URL of a video. The one we will be using is `ir-mWUYH_uo`.\n\nCreate a new directory and navigate to it with your terminal. Run `npm init -y` to create a `package.json` file and then install the following packages:\n\n```\nnpm install @deepgram/sdk ffmpeg-static youtube-mp3-downloader\n```\n\nCreate an `index.js` file, and open it in your code editor.\n\n## Preparing Dependencies\n\nAt the top of your file require these four packages:\n\n```js\nconst fs = require('fs')\nconst YoutubeMp3Downloader = require('youtube-mp3-downloader')\nconst { Deepgram } = require('@deepgram/sdk')\nconst ffmpeg = require('ffmpeg-static')\n```\n\n`fs` is the built-in file system module for Node.js. It is used to read and write files which we will be doing a few times throughout this post. `ffmpeg-static` includes a version of ffmpeg in our node_modules directory, and requiring it returns the file path.\n\nInitialize the Deepgram and YouTubeMp3Downloader clients:\n\n```js\nconst deepgram = new Deepgram('YOUR DEEPGRAM KEY')\nconst YD = new YoutubeMp3Downloader({\n  ffmpegPath: ffmpeg,\n  outputPath: './',\n  youtubeVideoQuality: 'highestaudio',\n})\n```\n\n## Download Video and Convert to MP3\n\nUnder the hood, the `youtube-mp3-downloader` package will download the video and convert it with `ffmpeg` on our behalf. While it is doing this it triggers several events - we are going to use the `progress` event so we know how far through the download we are, and `finished` which indicates we can move on.\n\n```js\nYD.download('ir-mWUYH_uo')\n\nYD.on('progress', (data) => {\n  console.log(data.progress.percentage + '% downloaded')\n})\n\nYD.on('finished', async (err, video) => {\n  const videoFileName = video.file\n  console.log(`Downloaded ${videoFileName}`)\n\n  // Continue on to get transcript here\n})\n```\n\nSave and run the file with `node index.js` and you should see the file progress in your terminal and then have the file available in your file directory.\n\n![A terminal showing various percentages downloaded ending with 100%. The final log states the final filename.](https://res.cloudinary.com/deepgram/image/upload/v1635374325/blog/2021/11/transcribe-youtube-videos-nodejs/downloaded.png)\n\n## Get Transcript from Deepgram\n\nWhere the comment is above, prepare and create a Deepgram transcription request:\n\n```js\nconst file = {\n  buffer: fs.readFileSync(videoFileName),\n  mimetype: 'audio/mp3',\n}\nconst options = {\n  punctuate: true,\n}\n\nconst result = await deepgram.transcription\n  .preRecorded(file, options)\n  .catch((e) => console.log(e))\nconsole.log(result)\n```\n\nThere are lots of options which can make your transcript more useful including diarization which recognizes different speakers, a profanity filter which replaces profanity with nearby terms, and punctuation. We are using punctuation in this tutorial to show you how setting options works.\n\nRerun your code and you should see a JSON object printed in your terminal.\n\n![A terminal showing the file being downloaded, and then an object containing data from Deepgram. Within the object is a results object with a channels array. Further content is ommitted from the screenshot as it is nested too far.](https://res.cloudinary.com/deepgram/image/upload/v1635374324/blog/2021/11/transcribe-youtube-videos-nodejs/transcript.png)\n\n## Saving Transcript and Deleting Media\n\nThere is a lot of data that comes back from Deepgram, but all we want is the transcript which, with the options we provided, is a single string of text. Add the following line to access just the transcript:\n\n```js\nconst transcript = result.results.channels[0].alternatives[0].transcript\n```\n\nNow we have the string, we can create a text file with it:\n\n```js\nfs.writeFileSync(\n  `${videoFileName}.txt`,\n  transcript,\n  () => `Wrote ${videoFileName}.txt`\n)\n```\n\nThen, if desired, delete the mp3 file:\n\n```js\nfs.unlinkSync(videoFileName)\n```\n\n## Summary\n\nTranscribing YouTube videos has never been easier thanks to Deepgram's Speech Recognition API and the Deepgram Node SDK. You can find the final project code at https://github.com/deepgram-devs/youtube-transcripts.\n\nCheck out the other options supported by the [Deepgram Node SDK](https://github.com/deepgram/node-sdk) and if you have any questions feel free to reach out to us on Twitter (we are [@DeepgramDevs](https://twitter.com/DeepgramDevs)).";
						}
						async function compiledContent$z() {
							return load$z().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$z() {
							return (await import('./chunks/index.71848df5.mjs'));
						}
						function Content$z(...args) {
							return load$z().then((m) => m.default(...args));
						}
						Content$z.isAstroComponentFactory = true;
						function getHeadings$z() {
							return load$z().then((m) => m.metadata.headings);
						}
						function getHeaders$z() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$z().then((m) => m.metadata.headings);
						}

const __vite_glob_0_241 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$z,
  file: file$z,
  url: url$z,
  rawContent: rawContent$z,
  compiledContent: compiledContent$z,
  default: load$z,
  Content: Content$z,
  getHeadings: getHeadings$z,
  getHeaders: getHeaders$z
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$y = {"title":"Transcribing Browser Tab Audio with Chrome Extensions","description":"Build your first Google Chrome browser extension and learn how to use Deepgram's speech-to-text API to record tab audio, with and without your microphone.","date":"2022-07-21T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1658240278/blog/2022/07/transcribing-browser-tab-audio-chrome-extensions/cover.jpg","authors":["kevin-lewis"],"category":"tutorial","tags":["extension","javascript"],"seo":{"title":"Transcribing Browser Tab Audio with Chrome Extensions","description":"Build your first Google Chrome browser extension and learn how to use Deepgram's speech-to-text API to record tab audio, with and without your microphone."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454112/blog/transcribing-browser-tab-audio-chrome-extensions/ograph.png"},"shorturls":{"share":"https://dpgr.am/56c127e","twitter":"https://dpgr.am/02a6d9c","linkedin":"https://dpgr.am/8fa7312","reddit":"https://dpgr.am/f2281f2","facebook":"https://dpgr.am/4561947"}};
						const file$y = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/transcribing-browser-tab-audio-chrome-extensions/index.md";
						const url$y = undefined;
						function rawContent$y() {
							return "\nChances are you have installed at least one browser extension before. With over 2.5 billion (yes, with a b) global users of Chrome, it's an excellent platform to build and release apps on. In this tutorial, you will create a Chrome extension that captures browser tab audio and transcribes it with Deepgram.\n\n![Transcripts are showing in the popup](https://res.cloudinary.com/deepgram/image/upload/v1657316996/blog/2022/07/transcribing-browser-tab-audio-chrome-extensions/transcripts-in-extension.png)\n\n## The Manifest\n\nCreate a `manifest.json` file. This file contains critical information about our extension, which is required by the browser to load it (and publish it to the Chrome Web Store). Add the following to it:\n\n```json\n{\n    \"name\": \"Transcribe Tab Audio\",\n    \"version\": \"1.0\",\n    \"manifest_version\": 3,\n    \"host_permissions\": [\"*://*/\"],\n    \"permissions\": [\"storage\", \"tabs\", \"scripting\"]\n}\n```\n\nThe `host_permissions` specify which webpages this extension will be active on - the `*` matches everything, so this will work on every page. You can alter this if you only want it to work on specific pages or domains.\n\nThe `permissions` specified are also needed for this project - `\"storage\"` allows the extension to store small amounts of data on the machine, `\"tabs\"` provides access to all data fields regarding tabs in the browser, and `\"scripting\"` allows us to execute JavaScript files - more on this later.\n\nAt this point, you actually have a valid Chrome Extension - let's load it in. Head to `chrome://extensions`, toggle Developer Mode on and click *Load Unpacked*. Select the folder with your `manifest.json` file, and you should see the extension appear in your browser.\n\nIt's a bit rubbish right now—time to fix that.\n\n## Creating a Popup\n\nAn extension popup is the small pane that appears when you click on the extension icon in your address bar.\n\nCreate a `popup.html` file:\n\n```html\n<!DOCTYPE html>\n<html>\n  <head>\n  </head>\n  <body style=\"padding: 1em;\">\n    <button id=\"start\">Start transcription</button>\n    <p id=\"transcript\"></p>\n    <script src=\"popup.js\"></script>\n  </body>\n</html>\n```\n\nIn your `manifest.json` file, specify the popup file by adding this property:\n\n```json\n\"action\": {\n    \"default_popup\": \"popup.html\"\n}\n```\n\n![The extension icon is clicked, and a small white popup shows one button reading 'start transcription'](https://res.cloudinary.com/deepgram/image/upload/v1657316996/blog/2022/07/transcribing-browser-tab-audio-chrome-extensions/popup-hello-world.png)\n\nYou may have noticed that the linked JavaScript file does not yet exist. Before we create it, it's important to note that as soon as the popup is closed, it's as if that page no longer exists, and the code will cease to run. For this reason, the extension must inject some code to run in the current webpage. This means the code will continue to run even once the popup is closed.\n\nWith this in mind, create a `popup.js` file:\n\n```js\ndocument.getElementById('start').addEventListener('click', async () => {\n    const tab = await getCurrentTab()\n    if(!tab) return alert('Require an active tab')\n    chrome.scripting.executeScript({\n        target: { tabId: tab.id },\n        files: ['main.js']\n    })\n})\n\nasync function getCurrentTab() {\n    const queryOptions = { active: true, lastFocusedWindow: true }\n    const [tab] = await chrome.tabs.query(queryOptions)\n    return tab\n}\n```\n\nWhen the start button is clicked, it will get the active tab and inject a `main.js` file. Go and create one:\n\n```js\nalert('This is an injected script!')\n```\n\nOpen the extension and press the button. You should see the alert! Delete the alert before moving on.\n\n## Transcribing Tab Audio\n\nIn your `main.js` file, ask for access to a user's display, check it has audio attached, and plug it into a MediaRecorder:\n\n```js\nnavigator.mediaDevices.getDisplayMedia({ video: true, audio: true }).then(stream => {\n    if(stream.getAudioTracks().length == 0) return alert('You must share your tab with audio. Refresh the page.')\n    const recorder = new MediaRecorder(stream, { mimeType: 'audio/webm' })\n\n    // Further code here\n})\n```\n\nTry it out. When you share a tab, ensure you are also sharing the tab audio. If not, we've set up an alert to show the error and stop further code from running.\n\n![A popup shows a screen scaring dialog. A chrome tab is selected, and a big red arrow is pointing to a checked checkbox reading 'share tab audio'.](https://res.cloudinary.com/deepgram/image/upload/v1657316996/blog/2022/07/transcribing-browser-tab-audio-chrome-extensions/share-audio.png)\n\nConnect to Deepgram using a WebSocket and, as soon as the connection is open, begin sending tab audio data:\n\n```js\nsocket = new WebSocket('wss://api.deepgram.com/v1/listen?tier=enhanced', ['token', 'YOUR_DEEPGRAM_API_KEY'])\n\nrecorder.addEventListener('dataavailable', evt => {\n    if(evt.data.size > 0 && socket.readyState == 1) socket.send(evt.data)\n})\n\nsocket.onopen = () => { recorder.start(250) }\n\n// Further code here\n```\n\nNote that the `socket` is being placed in global scope (shown by the lack of a `var`, `let`, or `const` keyword) so we can later close the connection.\n\nThen, listen for Deepgram's returned transcripts:\n\n```js\nsocket.onmessage = msg => {\n    const { transcript } = JSON.parse(msg.data).channel.alternatives[0]\n    if(transcript) {\n        console.log(transcript)\n    }\n}\n```\n\nGo to a tab with audio, start transcribing and look in your browser developer tools.\n\n![Several logs to the console with transcripts](https://res.cloudinary.com/deepgram/image/upload/v1657316997/blog/2022/07/transcribing-browser-tab-audio-chrome-extensions/logging-transcripts.png)\n\nNice! It's certainly coming together.\n\n## Passing Data From Content Script to Popup\n\nYou can't expect users to open up their browser console to see transcripts. You can send 'messages' from the injected script to the popup, but if the popup is closed, it won't be received. So, here's the plan:\n\n1.  When a new transcript is available, put it in chrome storage.\n2.  Send a message from the injected script to the popup to say there's a new transcript available.\n3.  If the popup is open, display the latest transcript from storage.\n4.  When the popup opens, get the latest transcript (even if messages are missed, this will get us up to date).\n\n[Chrome Storage](https://developer.chrome.com/docs/extensions/reference/storage/) is an extension-specific API that acts similarly to localStorage, but is more specialized towards the needs of extensions and may be synced using Chrome Sync (this extension won't be).\n\nAt the very top of `main.js`, above all other code, create a new transcript key in Chrome storage and set the initial value to an empty string:\n\n```js\nchrome.storage.local.set({ transcript: '' })\n```\n\nReplace `console.log(transcript)` with:\n\n```js\nchrome.storage.local.get('transcript', data => {\n    chrome.storage.local.set({\n      transcript: data.transcript += ' ' + transcript\n    })\n\n    // Throws error when popup is closed, so this swallows the errors with catch.\n    chrome.runtime.sendMessage({\n      message: 'transcriptavailable'\n    }).catch(err => ({}))\n})\n```\n\nThis gets the existing transcript and adds the new transcript to the end of it. Then, a message is sent with the value 'transcriptavailable,' which we can now listen for in `popup.js`.\n\nAt the bottom of `popup.js`:\n\n```js\nchrome.runtime.onMessage.addListener(({ message }) => {\n    if(message == 'transcriptavailable') {\n        showLatestTranscript()\n    }\n})\n\nfunction showLatestTranscript() {\n    chrome.storage.local.get('transcript', ({ transcript }) => {\n        document.getElementById('transcript').innerHTML = transcript\n    })\n}\n```\n\nAlso, get the latest transcript at the very top of `popup.js`, above all other code:\n\n```js\nshowLatestTranscript()\n```\n\n![Transcripts are showing in the popup](https://res.cloudinary.com/deepgram/image/upload/v1657316996/blog/2022/07/transcribing-browser-tab-audio-chrome-extensions/transcripts-in-extension.png)\n\n## Stopping Transcription\n\nAdd a button, just below the start button, to `popup.html`:\n\n```html\n<button id=\"stop\">Stop transcription</button>\n```\n\nWhen the button is pressed, send a message back to the injected script. In `popup.js`:\n\n```js\ndocument.getElementById('stop').addEventListener('click', async () => {\n    const tab = await getCurrentTab()\n    if(!tab) return alert('Require an active tab')\n    chrome.tabs.sendMessage(tab.id, { message: 'stop' })\n})\n```\n\nAt the very bottom of `main.js`, below all other code, receive the message and close the WebSocket connection to Deepgram:\n\n```js\nchrome.runtime.onMessage.addListener(({ message }) => {\n    if(message == 'stop') {\n        socket.close()\n        alert('Transcription ended')\n    }\n})\n```\n\nExcellent.\n\n## Creating an Options Page\n\nRight now, your Deepgram API Key is coded right into the application. Next, you will build an options page for the user to enter their key, save it to Chrome storage, and use that value when connecting to Deepgram.\n\nIn `manifest.json`, add the following property:\n\n```js\n\"options_page\": \"options.html\"\n```\n\nCreate and open an `options.html` file:\n\n```html\n<!DOCTYPE html>\n<html>\n  <body>\n    <h1>Provide your Deepgram API Key</h1>\n\n    <input type=\"text\" id=\"api\">\n    <button>Save</button>\n\n    <script src=\"options.js\"></script>\n  </body>\n</html>\n```\n\nCreate and open an `options.js` file:\n\n```js\nconst api = document.getElementById('api')\nconst button = document.querySelector('button')\n\n// If it exists, load it in\nchrome.storage.local.get('key', ({ key }) => {\n  if(key) api.value = key\n})\n\nbutton.addEventListener('click', () => {\n  const key = api.value\n  chrome.storage.local.set({ key }, () => {\n    alert('Deepgram API Key Set')\n  })\n})\n```\n\nTime to use the key. At the top of `main.js`, above all other code:\n\n```js\nlet apiKey\nchrome.storage.local.get('key', ({ key }) => apiKey = key)\n```\n\nAfter this, `apiKey` will either be `undefined` or be a string with the API Key.\n\nReplace the following in `main.js`:\n\n```js\nsocket = new WebSocket('wss://api.deepgram.com/v1/listen?tier=enhanced', ['token', 'YOUR_DEEPGRAM_API_KEY'])\n\n// Replace with 👇\n\nif(!apiKey) return alert('You must provide a Deepgram API Key in the options page.')\nsocket = new WebSocket('wss://api.deepgram.com/v1/listen?tier=enhanced', ['token', apiKey])\n```\n\nRight-click the extension and click *Options* to open the new page. Save your Deepgram API Key, and the extension should still work.\n\n## Accessing Browser Tab Audio and Microphone\n\nA hypothetical situation - you want to transcribe a browser-based video call with this extension. Everyone's voice is transcribed, except yours - this is because your audio doesn't come through the tab (or you would hear yourself!), so let's alter this extension to allow for both your mic and tab audio to be transcribed together.\n\nIf you only want to transcribe tab audio, skip to the end.\n\nAt the moment, in `main.js`, you are requesting a user display, checking there is audio, and piping the resulting stream into a MediaRecorder. Now, we must:\n\n1.  Get access to a user display and check there if is audio.\n2.  Get access to a user audio device (microphone).\n3.  Create a new, empty `AudioContext`.\n4.  Mix the two audio sources together as sources in the single `AudioContext`.\n5.  Create a MediaRecorder with the `AudioContext`, now containing two sources.\n\nAt the very bottom of main.js, below all other code:\n\n```js\n// https://stackoverflow.com/a/47071576\nfunction mix(audioContext, streams) {\n    const dest = audioContext.createMediaStreamDestination()\n    streams.forEach(stream => {\n        const source = audioContext.createMediaStreamSource(stream)\n        source.connect(dest)\n    })\n    return dest.stream\n}\n```\n\nReplace the following in `main.js`:\n\n```js\nconst recorder = new MediaRecorder(stream, { mimeType: 'audio/webm' })\n\n// Replace with 👇\n\nconst micStream = await navigator.mediaDevices.getUserMedia({ audio: true })\nconst audioContext = new AudioContext()\nconst mixed = mix(audioContext, [stream, micStream])\nconst recorder = new MediaRecorder(mixed, { mimeType: 'audio/webm' })\n```\n\nAdd the `async` keyword just before `stream` in the `.then()` function:\n\n```js\nnavigator.mediaDevices.getDisplayMedia({ video: true, audio: true }).then(await stream => {\n```\n\nBoom. Done.\n\n## Next Steps\n\nThere's so much you can do to improve your Chrome extension - make it look nicer with some CSS, change how you display transcripts or alter the extension icon when it is recording. You may also consider using Deepgram features such as [diarization](https://developers.deepgram.com/documentation/features/diarize/) to detect different speakers and display them differently.\n\nYou can find the full finished code for this project on GitHub at [deepgram-devs/transcription-chrome-extension](https://github.com/deepgram-devs/transcription-chrome-extension). As ever, if you have any questions, please feel free to reach out on Twitter (we are [@DeepgramDevs](https://twitter.com/DeepgramDevs)).";
						}
						async function compiledContent$y() {
							return load$y().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$y() {
							return (await import('./chunks/index.bf4a61a7.mjs'));
						}
						function Content$y(...args) {
							return load$y().then((m) => m.default(...args));
						}
						Content$y.isAstroComponentFactory = true;
						function getHeadings$y() {
							return load$y().then((m) => m.metadata.headings);
						}
						function getHeaders$y() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$y().then((m) => m.metadata.headings);
						}

const __vite_glob_0_242 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$y,
  file: file$y,
  url: url$y,
  rawContent: rawContent$y,
  compiledContent: compiledContent$y,
  default: load$y,
  Content: Content$y,
  getHeadings: getHeadings$y,
  getHeaders: getHeaders$y
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$x = {"title":"Transcriptions Without a Server Using Netlify and Deepgram","description":"Use Netlify Functions to transcribe pre-recorded audio without a server","date":"2022-01-31T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1643628688/blog/2022/01/transcription-netlify-functions/Transcribe-without-server-Netlify-Deepgram%402x.jpg","authors":["kevin-lewis"],"category":"tutorial","tags":["nodejs","netlify","serverless"],"seo":{"title":"Transcriptions Without a Server Using Netlify and Deepgram","description":"Use Netlify Functions to transcribe pre-recorded audio without a server"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453852/blog/transcription-netlify-functions/ograph.png"},"shorturls":{"share":"https://dpgr.am/a0ed7cf","twitter":"https://dpgr.am/838386f","linkedin":"https://dpgr.am/038d7a4","reddit":"https://dpgr.am/f2a916e","facebook":"https://dpgr.am/1f580fd"}};
						const file$x = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/transcription-netlify-functions/index.md";
						const url$x = undefined;
						function rawContent$x() {
							return "\nTraditional server applications typically need to be always on, always using resources and require maintenance to ensure availability. Serverless works differently - functionality is exposed via URLs. When a request is made they spin up, execute logic, and spin back down. Each serverless script (known as a 'function') can be run many times in parallel, so this approach may be suitable for scale, dependent on your use case.\n\nIn this tutorial, you will set up serverless functions with [Netlify](https://www.netlify.com/products/functions/) to get transcripts using [Deepgram's Node.js SDK](https://developers.deepgram.com/sdks-tools/sdks/node-sdk/) - one for hosted files, and one for local files. Because serverless functions do not lend themselves to long-living scripts, it's not recommended to use this approach for live transcription.\n\n## Before We Start\n\nYou will need:\n\n*   Node.js installed on your machine - [download it here](https://nodejs.org/en/).\n*   A Deepgram API Key - [get one here](https://console.deepgram.com/signup?jump=keys).\n*   The Netlify CLI installed - [get started guide here](https://docs.netlify.com/cli/get-started/). Also, make sure to log in.\n\nCreate a new directory and navigate to it with your terminal. Run `npm init -y` to create a `package.json` file and then install the Deepgram Node.js SDK:\n\n    npm install @deepgram/sdk\n\n## Set Up Netlify Project\n\nYou can set up a Netlify project from the web dashboard, but as we need the Netlify CLI to test our functions, we may as well use it here. Inside of your project directory, run `netlify init`, and when prompted, choose *Create and deploy site manually*.\n\nA new project will now be visible in your Netlify web dashboard - you can open it with `netlify open`.\n\n## Your First Netlify Function\n\nNetlify offer zero-configuration serverless functions if you put your logic in a specific directory - `/netlify/functions`. Create a new file at `/netlify/functions/hello.js` and populate it with the following:\n\n```js\nexports.handler = async (event) => {\n  try {\n    // Any logic goes here, but we'll return a fixed response\n    return { statusCode: 200, body: JSON.stringify({ message: 'ok' }) }\n  } catch (err) {\n    return { statusCode: 500, body: String(err) }\n  }\n}\n```\n\n### Test Your Function\n\nRun `netlify dev` and wait for the local server to start - usually at `http://localhost:8888`. Open another terminal and run the following command to see the response:\n```\n    curl http://localhost:8888/.netlify/functions/hello\n```\n<Alert type=\"info\">Don't be alarmed by the . in the URL - your local directory, which contains your functions, should just be /netlify</Alert>\n\nYour terminal should look something like this:\n\n![A terminal showing the curl command and a response of a json object with message ok](https://res.cloudinary.com/deepgram/image/upload/v1640794183/blog/2022/01/transcription-netlify-functions/hello.png)\n\n## Adding Your Deepgram API Key\n\nLike most hosting providers, Netlify provides a way to set sensitive keys as environment variables. Netlify CLI will inject any variables from your web dashboard to your local runtime for you - super cool.\n\nOpen your project dashboard with `netlify open` while in your project directory. Heard to **Site settings > Build & deploy > Environment > Environment variables** and create a new variable called `DEEPGRAM_API_KEY` with the value from the [Deepgram Console](https://console.deepgram.com).\n\nIf you are still running your `netlify dev` server, stop it with `ctrl + c` and restart it. You should see the key being injected,meaning it is now available with `process.env.DEEPGRAM_API_KEY`\n\n![netlify dev being run, and then the log 'Injected build settings env var DEEPGRAM\\_API\\_KEY'](https://res.cloudinary.com/deepgram/image/upload/v1640794183/blog/2022/01/transcription-netlify-functions/key-injection.png)\n\n## Transcribe Hosted Files\n\nInside of your `functions` directory, create `hosted.js` with the following content:\n\n```js\nconst { Deepgram } = require('@deepgram/sdk')\nconst deepgram = new Deepgram(process.env.DEEPGRAM_API_KEY)\n\nexports.handler = async (event) => {\n  try {\n    const { url } = JSON.parse(event.body)\n    const { results } = await deepgram.transcription.preRecorded({ url })\n    return { statusCode: 200, body: JSON.stringify(results) }\n  } catch (err) {\n    return { statusCode: 500, body: String(err) }\n  }\n}\n```\n\nOnce you save the file, the new URL is immediately available. This function requires a data payload with a `url` property. You can test it by once again using cURL:\n```\n  curl -X POST -H \"Content-Type: application/json\" -d '{\"url\": \"https://static.deepgram.com/examples/nasa-spacewalk-interview.wav\"}' http://localhost:8888/.netlify/functions/hosted\n```\n## Accessing Functions From The Web\n\nNetlify makes your functions available on the same domain as your main application (just under the `/.netlify/functions` path). Due to this, we can call Netlify Functions from our main application by specifying the relative URL. This means it will work both locally and once deployed.\n\nCreate an `index.html` file in your main directory:\n\n```html\n<!DOCTYPE html>\n<html>\n  <head>\n    <meta charset=\"UTF-8\" />\n  </head>\n  <body>\n    <button>Transcribe from URL</button>\n    <script>\n      document.querySelector('button').addEventListener('click', () => {\n        const url = prompt('Please provide an audio file URL')\n        fetch('/.netlify/functions/hosted', {\n          method: 'POST',\n          body: JSON.stringify({ url }),\n        })\n          .then((r) => r.json())\n          .then((data) => {\n            console.log(data)\n          })\n      })\n    </script>\n  </body>\n</html>\n```\n\nNavigate to `http://localhost:8888` in your browser, click the button, and provide a static file URL (if you don't have one, use https://static.deepgram.com/examples/nasa-spacewalk-interview.wav). Open your browser console, and you should see the response from Deepgram.\n\n![Browser console showing a large object from Deepgram](https://res.cloudinary.com/deepgram/image/upload/v1640794184/blog/2022/01/transcription-netlify-functions/console.png)\n\n## Transcribe Local Files\n\nCreate a new functions file - `file.js`:\n\n```js\nconst { Deepgram } = require('@deepgram/sdk')\nconst deepgram = new Deepgram(process.env.DEEPGRAM_API_KEY)\n\nexports.handler = async (event) => {\n  try {\n    const { results } = await deepgram.transcription.preRecorded({\n      buffer: Buffer.from(event.body, 'base64'),\n      mimetype: 'audio/wav',\n    })\n    return { statusCode: 200, body: JSON.stringify(results) }\n  } catch (err) {\n    return { statusCode: 500, body: String(err) }\n  }\n}\n```\n\nAdd a `<form>` just below the `<button>` in `index.html`:\n\n```html\n<form\n  enctype=\"multipart/form-data\"\n  action=\"/.netlify/functions/file\"\n  method=\"POST\"\n>\n  <input id=\"file\" type=\"file\" name=\"file\" />\n  <input type=\"submit\" value=\"POST to server\" />\n</form>\n```\n\nRefresh your browser and upload a file - you should see the results in your browser. If you want to handle the results within the page, [Sandra details how to submit a form using JavaScript here](https://blog.deepgram.com/sending-audio-files-to-expressjs-server/#html-and-js-using-a-formdata-object).\n\n## Deploying Functions\n\nReady? `netlify deploy`. That's it.\n\nOnce deployed, you'll be able to access your Netlify functions at **random-name.netlify.app/.netlify/functions/function-name**. Your webpage will work without modifications because it will be served at the same subdomain.\n\nNetlify also supports push-to-deploy with GitHub if you configure your project to deploy from a repo.\n\n## Wrapping Up\n\nNetlify makes deploying serverless functions reasonably straightforward, but if you have any questions after reading this guide, we are here to help! Just pop us a line at [@DeepgramDevs](https://twitter.com/deepgramdevs).";
						}
						async function compiledContent$x() {
							return load$x().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$x() {
							return (await import('./chunks/index.714bf9ff.mjs'));
						}
						function Content$x(...args) {
							return load$x().then((m) => m.default(...args));
						}
						Content$x.isAstroComponentFactory = true;
						function getHeadings$x() {
							return load$x().then((m) => m.metadata.headings);
						}
						function getHeaders$x() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$x().then((m) => m.metadata.headings);
						}

const __vite_glob_0_243 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$x,
  file: file$x,
  url: url$x,
  rawContent: rawContent$x,
  compiledContent: compiledContent$x,
  default: load$x,
  Content: Content$x,
  getHeadings: getHeadings$x,
  getHeaders: getHeaders$x
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$w = {"title":"Transfer Learning from Spanish to Portuguese: How Neighbors on the Map Also Share Vectors","description":"Spanish and Portuguese are very similar languages, which makes them a great example of the power of transfer learning. Read on to learn more.","date":"2022-04-13T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981415/blog/transfer-learning-spanish-portuguese/transfer-learning-from-spanish-to-portuguese-thumb.png","authors":["duygu-altinok"],"category":"ai-and-engineering","tags":["deep-learning","language","nlu"],"seo":{"title":"Transfer Learning from Spanish to Portuguese: How Neighbors on the Map Also Share Vectors","description":"Spanish and Portuguese are very similar languages, which makes them a great example of the power of transfer learning. Read on to learn more."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981415/blog/transfer-learning-spanish-portuguese/transfer-learning-from-spanish-to-portuguese-thumb.png"},"shorturls":{"share":"https://dpgr.am/3e97acd","twitter":"https://dpgr.am/85b3695","linkedin":"https://dpgr.am/3b4c570","reddit":"https://dpgr.am/7d13a38","facebook":"https://dpgr.am/9c9ac3b"}};
						const file$w = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/transfer-learning-spanish-portuguese/index.md";
						const url$w = undefined;
						function rawContent$w() {
							return "Transfer learning is one of the hottest topics of natural language processing-and, indeed, machine learning in general-in recent years. In this post, I want to share with you what transfer learning is, why it's so helpful when thinking about language-related tasks, and how we've used it to create a high-accuracy model for Portuguese based on the work that we'd already done for Spanish.  In this blog post, we'll discuss some of our specific logic here, including the intuition of picking Spanish for helping Portuguese model training and the similarities between these languages on many levels. But to get started, let's talk about what transfer learning is and why it's so valued at Deepgram before diving into the specifics of Spanish and Portuguese.\n\n## What is Transfer Learning? A Very Brief History\n\nIn short, we can say that transfer learning is taking a model that has been trained on one task, and using it for a different one by changing its training data. Transfer learning started its journey with word vectors, which are static vectors for each word in the corpus. For our purposes, you can think of a vector as a way of describing the relationship between words in a large, abstract space. To get an idea of how this works, you can see an example of word vectors in Figure 1, below. If you look at the 1st box for the words *man* and *woman*, they're the same, but the word *king* is different because he's not an ordinary human; he's the king. Also *man*, *woman*, and *king* share the same third box, because they're all human. The fourth box of *king* is identical to *man* (baby blue), but *woman* has a different fourth box, pink. So *king* is more similar to *man* in this regard. ![](https://res.cloudinary.com/deepgram/image/upload/v1661976854/blog/transfer-learning-spanish-portuguese/pic1.png) \n\n**Figure 1.** Word2vec, the ancestor of transfer learning, showing word similarities and differences (from https://jalammar.github.io/illustrated-word2vec/).\n\nNext, contextual word vectors came with ELMo, or Embeddings from Language Models, which is \"a type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy)\" ([source](https://paperswithcode.com/method/elmo#:~:text=Embeddings%20from%20Language%20Models%2C%20or,i.e.%2C%20to%20model%20polysemy).)). This provided a way to look at the relationship between words at a greater depth and with more . Finally transformers-a type of deep learning model that differentially weights input data-were developed to generate contextual word vectors as well as a sentence-level vector, which is even better for linguistic analysis.\n\nAt each step of this process, though, the goal remained the same-to look for good representations for our corpus words, which happen to be vectors. Both pretrained word vectors and transformers are trained on giant corpora, hence they know a lot about the target language's syntax and semantics. We feed pretrained vectors to our downstream models and the vectors bring what they know about the language, semantics of the words, and many surprising features to our models.\n\nYou can probably already see how this could be useful for language-related tasks. Speech recognition is the task of converting speech to text. Though speech recognition models are more sophisticated algorithms than text oriented statistical models, a neural network is still a neural network and weights are certainly used.\n\nHence, some weight re-using techniques are applicable to speech recognition, along with more sophisticated acoustic tricks. That is to say, once you have a model that works well for one language, it's relatively easy to give that model data for another language-especially one that's closely related-and see better results than if you started from scratch.\n\n## Why We Use Transfer Learning\n\nAt Deepgram, transfer learning is highly valued. For a specific language, when we want to train a new version of a specific model, we don't want to start from scratch. Instead, when we want to train a model for a brand new language, we want to transfer some knowledge from a similar language's model when possible. To illustrate the power of these processes, we'll look at a specific case of transfer learning-going from Spanish to Portuguese-to show how you can train a model for a new language from scratch by the help of a similar language's model. \n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-how-deepgram-works\"></WhitepaperPromo>\n\n## Why Spanish to Portuguese\n\nLet's get started with the basic question of why we picked Spanish to help train our Portuguese model from scratch. Spanish and Portuguese come from the same language family and are closely related. Italian, French, Spanish, and Portuguese all belong to this language family-the Romance family. However, even if you didn't know that, a quick glance is enough for one to see the similarities between Spanish and Portuguese just by looking at a piece of text. Here's an example text pair, taken from Alice in Wonderland, to explain what we mean:\n\n**Spanish**\n\n> No había nada tan notable en eso; Alicia tampoco pensó que fuera tan extraño escuchar al Conejo decirse a sí mismo: '¡Dios mío! ¡Oh querido! ¡Llegaré tarde!' (cuando lo pensó después, se le ocurrió que debería haberse preguntado por esto, pero en ese momento todo parecía bastante natural); pero cuando el Conejo sacó un reloj del bolsillo de su chaleco, lo miró y siguió corriendo, Alicia se puso de pie, porque le pasó por la mente que nunca antes había visto un conejo con chaleco... bolsillo, o un reloj para sacar de él, y ardiendo de curiosidad, corrió por el campo tras él, y afortunadamente llegó justo a tiempo para verlo caer por una gran madriguera debajo del seto.\n\n**Portuguese**\n\n> Não havia nada tão notável nisso; Alice também não achou tão estranho ouvir Rabbit dizer para si mesmo: 'Meu Deus! Oh querida! Chegarei tarde!' (Quando ele pensou sobre isso depois, ocorreu-lhe que deveria ter se perguntado sobre isso, mas na época tudo parecia bastante natural); mas quando o Coelho tirou um relógio do bolso do colete, olhou para ele e correu, Alice levantou-se, porque lhe passou pela cabeça que nunca tinha visto um coelho com um colete... bolso, ou um relógio para levar. longe dele, e queimando de curiosidade, ela correu pelo campo atrás dele, felizmente chegando bem a tempo de vê-lo cair em uma grande toca sob a cerca viva.\n\nFor non-speakers of Spanish and Portuguese, it's totally reasonable to identify the above languages as the same-and you can quickly and easily find a lot of similarities between the two texts above, even if you don't know what they mean.\n\n## Similarities between Spanish and Portuguese\n\nIn this section, we'll compare Spanish and Portuguese phonetically, vocabulary-wise, and syntax-wise to better understand the ways that these languages are so similar in more depth.\n\n### Phonetic Similarities\n\nThe first major similarity between Spanish and Portuguese is acoustic similarity, which plays a key role for our transfer learning purposes. Here, acoustic or phonetic similarity means that the two languages use a set of sounds in their words that are very similar to one another. Consonants are almost identical in both languages, although Spanish has three extra affricates that Portuguese does not. Other than that, the consonants look the same on paper and they sound the same. Below, in Figure 2, we can see the phonetic alphabet for consonants of both languages, taken from the [SAMPA website](https://www.phon.ucl.ac.uk/home/sampa/).\n\n![](https://res.cloudinary.com/deepgram/image/upload/v1661976854/blog/transfer-learning-spanish-portuguese/transfer2.png) ![](https://res.cloudinary.com/deepgram/image/upload/v1661976855/blog/transfer-learning-spanish-portuguese/transfer3.png)\n\n**Figure 2.** Spanish and Portuguese consonants, represented in SAMPA.\n\nVowels are a bit different. The Portuguese phonetic system has more vowels, and perceptually, Portuguese vowels are described as sounding different by Spanish speakers. In Figure 3 below we can see the Spanish and Pt vowels side by side, again taken from SAMPA website. ![](https://res.cloudinary.com/deepgram/image/upload/v1661976856/blog/transfer-learning-spanish-portuguese/transfer4.png) ![](https://res.cloudinary.com/deepgram/image/upload/v1661976857/blog/transfer-learning-spanish-portuguese/transfer5.png) \n\n**Figure 3.** Spanish and Portuguese vowels in SAMPA.\n\nHere, Spanish vowels look more minimalistic and the Portuguese side looks more crowded, since it has [nasal vowels](https://en.wikipedia.org/wiki/Nasal_vowel), like French. Still, there's a certain level of similarity.\n\n### Vocabulary Similarities\n\nThe final similarity between Spanish and Portuguese we want to discover is vocabulary similarity. Here, similar vocabulary means vocabulary strings that are either common or differ by a small edit distance-that is to say, one or two different letters or sounds. Compare the vocabulary words below, in Table 1, with Spanish on the left and Portuguese given on the right.\n\n<table>\n\n<tbody>\n\n<tr>\n\n<td style=\"padding: 10px;\">\\*\\*Spanish\\*\\*</td>\n\n<td></td>\n\n<td style=\"padding: 10px;\">\\*\\*Portuguese\\*\\*</td>\n\n</tr>\n\n<tr>\n\n<td style=\"padding: 10px;\">casa</td>\n\n<td></td>\n\n<td style=\"padding: 10px;\">casa</td>\n\n</tr>\n\n<tr>\n\n<td style=\"padding: 10px;\">señora</td>\n\n<td></td>\n\n<td style=\"padding: 10px;\">senhora</td>\n\n</tr>\n\n<tr>\n\n<td style=\"padding: 10px;\">para</td>\n\n<td></td>\n\n<td style=\"padding: 10px;\">para</td>\n\n</tr>\n\n<tr>\n\n<td style=\"padding: 10px;\">ahora</td>\n\n<td></td>\n\n<td style=\"padding: 10px;\">ahora</td>\n\n</tr>\n\n<tr>\n\n<td style=\"padding: 10px;\">cabeza</td>\n\n<td></td>\n\n<td style=\"padding: 10px;\">cabeça</td>\n\n</tr>\n\n<tr>\n\n<td style=\"padding: 10px;\">toma</td>\n\n<td></td>\n\n<td style=\"padding: 10px;\">toma</td>\n\n</tr>\n\n<tr>\n\n<td style=\"padding: 10px;\">vamos</td>\n\n<td></td>\n\n<td style=\"padding: 10px;\">vamos</td>\n\n</tr>\n\n<tr>\n\n<td style=\"padding: 10px;\">cama</td>\n\n<td></td>\n\n<td style=\"padding: 10px;\">cama</td>\n\n</tr>\n\n<tr>\n\n<td style=\"padding: 10px;\">gano</td>\n\n<td></td>\n\n<td style=\"padding: 10px;\">gano</td>\n\n</tr>\n\n<tr>\n\n<td style=\"padding: 10px;\">bonita</td>\n\n<td></td>\n\n<td style=\"padding: 10px;\">bonita</td>\n\n</tr>\n\n<tr>\n\n<td style=\"padding: 10px;\">cara</td>\n\n<td></td>\n\n<td style=\"padding: 10px;\">cara</td>\n\n</tr>\n\n<tr>\n\n<td style=\"padding: 10px;\">centro</td>\n\n<td></td>\n\n<td style=\"padding: 10px;\">centro</td>\n\n</tr>\n\n<tr>\n\n<td style=\"padding: 10px;\">estados</td>\n\n<td></td>\n\n<td style=\"padding: 10px;\">estados</td>\n\n</tr>\n\n<tr>\n\n<td style=\"padding: 10px;\">libros</td>\n\n<td></td>\n\n<td style=\"padding: 10px;\">livros</td>\n\n</tr>\n\n<tr>\n\n<td style=\"padding: 10px;\">opinión</td>\n\n<td></td>\n\n<td style=\"padding: 10px;\">opinião</td>\n\n</tr>\n\n</tbody>\n\n</table>\n\n**Table 1.** Comparison of Spanish and Portuguese vocabulary items.\n\nAs we see, some words are literally the same and some words differ by an edit distance of one or two. This is great for transferring the word vectors from the Spanish model into the Portuguese model. \n\n### Syntactic Similarity\n\nThe final aspect of similarity between the two languages that we'll look at here is syntactic similarity-that is, how similar is the way that sentences and phrases are constructed? This is also related to transferring weights, as syntactic constructions are one of the things learned by the model. Since we'd like to uncover how these two languages relate to each other, let's compare the dependency trees for the phrases *un perro pequeño* and *um cachorro pequeno* meaning 'a small dog' in each language. These trees show us how words within a phrase or sentence are related to each other. I generated both dependency trees with [displaCy](https://explosion.ai/demos/displacy), shown below in Figure 4.\n\n ![](https://res.cloudinary.com/deepgram/image/upload/v1661976857/blog/transfer-learning-spanish-portuguese/transfer6.png) ![](https://res.cloudinary.com/deepgram/image/upload/v1661976858/blog/transfer-learning-spanish-portuguese/transfer7.png)\n\n**Figure 4.** Dependency trees of 'a small dog' (literally, \"a dog small\") in Spanish (top) and Portuguese (bottom).\n\nIn both dependency trees, we notice that the adjective comes after the noun. In both trees, the head is the noun *perro/cachorro* 'dog' and the adjective is attached to the head by the dependency relation **adjective modifier**, or **amod**. A determiner *un/um* 'a' is attached to the noun by the **determiner,** or **det** relation. We see that the above trees are identical; hence, if one learns the syntax of one Spanish/Portuguese pair, one can easily figure out the other language's syntax. Putting together this information, now we're ready to understand how we make use of transfer learning for training the very first speech recognition model of Portuguese from scratch.\n\n## Transferring our Model's Learning from Spanish to Portuguese\n\nAs we've seen, Spanish and Portuguese are very similar to one another in terms of their sounds, their vocabulary, and their sentence structures. If we go back to our discussion of vectors and weighting at the beginning, you can see why starting with a Spanish model would be an effective choice for creating a Portuguese model-the weights and vectors are likely to be extremely similar, with only a few small points of difference that the model will learn when it sees Portuguese data, making small adjustments as it goes. We can see how useful this is if we imagine instead starting with an English model to create one for Portuguese. \n\nAlthough this would better than starting from zero (Portuguese and English are, after all, both languages, and they are related to one other, albeit more distantly), the number and magnitude of the adjustments would be much greater than when starting with Spanish, requiring a longer training process and more data from Portuguese to get the model to the same level of accuracy.\n\n## Wrapping Up\n\nI hope this article gives you a good sense of what transfer learning is and why it can be so impactful in a case like Spanish and Portuguese where the languages are very similar. We've got more transfer learning material coming in the next few months, so stay tuned to learn more-you can sign up for our newsletter below to keep up-to-date on what's happening at Deepgram. Want to see the power of transfer learning first hand? [Sign up for a free API](https://console.deepgram.com/signup) key or [contact us](https://deepgram.com/contact-us/) to get started using end-to-end deep learning for your speech recognition projects.";
						}
						async function compiledContent$w() {
							return load$w().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$w() {
							return (await import('./chunks/index.6f477e0e.mjs'));
						}
						function Content$w(...args) {
							return load$w().then((m) => m.default(...args));
						}
						Content$w.isAstroComponentFactory = true;
						function getHeadings$w() {
							return load$w().then((m) => m.metadata.headings);
						}
						function getHeaders$w() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$w().then((m) => m.metadata.headings);
						}

const __vite_glob_0_244 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$w,
  file: file$w,
  url: url$w,
  rawContent: rawContent$w,
  compiledContent: compiledContent$w,
  default: load$w,
  Content: Content$w,
  getHeadings: getHeadings$w,
  getHeaders: getHeaders$w
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$v = {"title":"Adding Translation to Your Transcription Project","description":"Use iTranslate's API to translate both pre-recorded and live transcription.","date":"2022-01-26T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1642934713/blog/2022/01/translation-itranslate/Adding-Live-Translation-to-Your-Transcription-Project%402x.jpg","authors":["kevin-lewis"],"category":"tutorial","tags":["translation","nodejs"],"seo":{"title":"Adding Translation to Your Transcription Project","description":"Use iTranslate's API to translate both pre-recorded and live transcription."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453854/blog/translation-itranslate/ograph.png"},"shorturls":{"share":"https://dpgr.am/ea626e0","twitter":"https://dpgr.am/291f11b","linkedin":"https://dpgr.am/2f4977d","reddit":"https://dpgr.am/4810389","facebook":"https://dpgr.am/dbbcb17"}};
						const file$v = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/translation-itranslate/index.md";
						const url$v = undefined;
						function rawContent$v() {
							return "\nGetting fast and accurate transcripts with Deepgram is often just one step in a broader project. We frequently get asked about adding translations to projects once transcripts are returned, and that's what we'll be doing in this project.\n\nThere are plenty of translation APIs available to developers, but I've become rather fond of [iTranslate](https://itranslate.com/api) after using them in a project earlier this month. It's a fast an straightforward API with a generous free tier and no rate limits at the time of writing.\n\n## Before We Start\n\nYou will need:\n\n*   Node.js installed on your machine - [download it here](https://nodejs.org/en/).\n*   A Deepgram API Key - [get one here](https://console.deepgram.com/signup?jump=keys).\n*   An iTranslate API Key - [get one here](https://itranslate.com/api).\n\nCreate a new directory and navigate to it with your terminal. Run `npm init -y` to create a `package.json` file and then install the following packages:\n\n    npm install dotenv @deepgram/sdk cross-fetch\n\nCreate a `.env` file and add the following:\n\n    DG_KEY=replace_with_deepgram_api_key\n    ITRANSLATE_KEY=replace_with_itranslate_api_key\n\nCreate an `index.js` file and add the following to it:\n\n```js\nrequire('dotenv').config()\nconst fetch = require('cross-fetch')\nconst { Deepgram } = require('@deepgram/sdk')\nconst deepgram = new Deepgram(process.env.DG_KEY)\n```\n\n## An Introduction to iTranslate\n\niTranslate supports text translation for over 50 languages. You may either specify the 'source dialect' with a value such as `en` (English) or `es` (Spanish), or set the value to `auto` and let iTranslate detect the language automatically. You must also specify a 'target dialect' for translation to work. An API request would look like this:\n```\n    POST https://dev-api.itranslate.com/translation/v2/\n    data: {\n       'source': { 'dialect': 'en', 'text': 'Hello World' },\n       'target': { 'dialect': 'es' }\n    }\n    headers: {\n        'Authorization': 'Bearer YOUR-API-KEY'\n        'Content-Type': 'application/json'\n    }\n```\nThe result looks like this:\n\n```js\n{\n  'source': { 'dialect': 'en', 'text': 'Hello World' },\n  'target': { 'dialect': 'es', 'text': 'Hola, Mundo' },\n  'times': { 'total_time': 0.051 }\n}\n```\n\n## Create A Translation Function\n\nAdd the following to the bottom of your `index.js` file:\n\n```js\nasync function translate(source, target, text) {\n  const url = 'https://dev-api.itranslate.com/translation/v2/'\n  const headers = {\n    Authorization: 'YOUR ITRANSLATE API KEY',\n    'Content-Type': 'application/json',\n  }\n  const data = {\n    source: { dialect: source, text: text },\n    target: { dialect: target },\n  }\n\n  const result = await fetch(url, {\n    method: 'POST',\n    headers,\n    body: JSON.stringify(data),\n  }).then((r) => r.json())\n\n  return result\n}\n```\n\nTry it out by adding the following code underneath the translate function:\n\n```js\ntranslate('en', 'es', 'Hello world').then((data) => console.log(data))\n```\n\nRun this with `node index.js`, and you should see the output in your terminal. Once you know it works, delete the line you just wrote.\n\n## Pre-Recorded Transcript Translation\n\nTo provide transcripts in languages which are different from the source audio, we will first get a transcript with Deepgram. Once the transcript is returned, we will translate the text. An example would look like this:\n\n```js\nconst url = 'https://static.deepgram.com/examples/nasa-spacewalk-interview.wav'\ndeepgram.transcription.preRecorded({ url }).then(async (response) => {\n  const { transcript } = response.results.channels[0].alternatives[0]\n  const translated = await translate('en', 'es', transcript)\n  console.log(translated)\n})\n```\n\n## Live Transcript Translation\n\niTranslate does not impose a rate limit at the time of writing, so transcribing live results from Deepgram is possible. This example gets live radio data and transcribes it with Deepgram. Once data is returned, we use the `translate` function:\n\n```js\nconst deepgramLive = deepgram.transcription.live({ punctuate: true })\n\nconst url = 'http://stream.live.vc.bbcmedia.co.uk/bbc_radio_fourlw_online_nonuk'\nfetch(url)\n  .then((r) => r.body)\n  .then((res) => {\n    res.on('readable', () => {\n      if (deepgramLive.getReadyState() == 1) {\n        deepgramLive.send(res.read())\n      }\n    })\n  })\n\ndeepgramLive.addListener('transcriptReceived', async (transcript) => {\n  const data = JSON.parse(transcript)\n  const response = data.channel.alternatives[0]\n  if (response.transcript && data.is_final) {\n    translate('en', 'es', response.transcript).then((data) => console.log(data))\n  }\n})\n```\n\n## In Summary\n\nBecause iTranslate is such a fast translation service, it is a good pairing with Deepgram's super fast speech recognition API.\n\nIf you have any questions, please feel free to reach out on Twitter - we're [@DeepgramDevs](https://twitter.com/DeepgramDevs).";
						}
						async function compiledContent$v() {
							return load$v().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$v() {
							return (await import('./chunks/index.91dc64a9.mjs'));
						}
						function Content$v(...args) {
							return load$v().then((m) => m.default(...args));
						}
						Content$v.isAstroComponentFactory = true;
						function getHeadings$v() {
							return load$v().then((m) => m.metadata.headings);
						}
						function getHeaders$v() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$v().then((m) => m.metadata.headings);
						}

const __vite_glob_0_245 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$v,
  file: file$v,
  url: url$v,
  rawContent: rawContent$v,
  compiledContent: compiledContent$v,
  default: load$v,
  Content: Content$v,
  getHeadings: getHeadings$v,
  getHeaders: getHeaders$v
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$u = {"title":"Tune In to Our Inaugural Deepgram Summit on 11/18 [and on demand!]","description":"Watch our inaugural Deepgram Summit on demand!","date":"2021-11-09T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981385/blog/tune-in-deepgram-summit-11-18-21/deepgram-summit-whats-next-blog-thumb-554x220%402x.png","authors":["scott-stephenson"],"category":"dg-insider","tags":["voice-tech","voice-strategy"],"seo":{"title":"Tune In to Our Inaugural Deepgram Summit on 11/18 [and on demand!]","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981385/blog/tune-in-deepgram-summit-11-18-21/deepgram-summit-whats-next-blog-thumb-554x220%402x.png"},"shorturls":{"share":"https://dpgr.am/8708da9","twitter":"https://dpgr.am/042e8c9","linkedin":"https://dpgr.am/29b76d5","reddit":"https://dpgr.am/2930940","facebook":"https://dpgr.am/9482f72"}};
						const file$u = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/tune-in-deepgram-summit-11-18-21/index.md";
						const url$u = undefined;
						function rawContent$u() {
							return "Five years ago, as consumer adoption of voice assistants exploded, discussions began about bringing the [power of voice technology to the office](https://venturebeat.com/2017/11/30/amazon-launches-alexa-for-business-platform-bringing-voice-services-to-the-office/). It was an exciting idea-leveraging the ease and convenience of consumer voice assistants in our everyday work lives-yet, in actuality, the real value and power of voice doesn't lie within quick call and response commands. Instead, the benefits of voice technology lie in the rich (and previously untapped) customer insights it can reveal.\n\nToday, with advancements in Artificial Intelligence (AI) and Natural Language Processing (NLP), modern speech recognition solutions are able to be utilized for so much more than simple queries-they have the ability to power the next wave of applications and give businesses and vendors a competitive advantage. That's why today, I'm excited to announce our inaugural virtual Deepgram Summit: What's Next in Voice.\n\nOn November 18, we are bringing together thought leaders from leading enterprises and high-growth startups who are tapping into the power of voice and will provide a glimpse into what the future holds for voice technology.\n\n## **So, what is next?**  \n\nAs a former academic researcher, it was my job to never settle for \"what works in the moment.\" We were challenged to constantly ask what's next and to think about how we could improve our techniques and research practices. That's why, as we set out to plan our very first Deepgram Summit, we chose to focus on the future. As an industry we cannot simply rest on what has worked to date in voice, but need to work together to push the envelope and challenge ourselves to leverage voice data to its full potential. It's not enough to settle for what's working now-we want to understand what's working and how we can improve upon it.\n\nDeepgram Summit: What's Next in Voice will reveal the power of voice and the possibilities across industries such as retail/ecommerce, healthcare, finance, and even space. We have an incredible lineup of speakers including Zachary DeWitt, partner at Wing Venture Capital, Pete Ellis, chief product officer at Red Box and Joe Wilson, head of product at Volley and many more who will share personal insights into the future of voice technology. Our vision at Deepgram is to empower every organization to unlock critical voice data to better understand employees and customers.\n\nOur Deepgram Summit is a natural progression of this vision. During our virtual event, we will not only look at what's working today but also what must be done to deliver on the promise of a voice-enabled tomorrow. Although the event has passed, you can watch on-demand videos from the event at [Deepgram Summit: What's Next in Voice](https://deepgram.com/deepgram-summit-on-demand/).";
						}
						async function compiledContent$u() {
							return load$u().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$u() {
							return (await import('./chunks/index.de9a003a.mjs'));
						}
						function Content$u(...args) {
							return load$u().then((m) => m.default(...args));
						}
						Content$u.isAstroComponentFactory = true;
						function getHeadings$u() {
							return load$u().then((m) => m.metadata.headings);
						}
						function getHeaders$u() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$u().then((m) => m.metadata.headings);
						}

const __vite_glob_0_246 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$u,
  file: file$u,
  url: url$u,
  rawContent: rawContent$u,
  compiledContent: compiledContent$u,
  default: load$u,
  Content: Content$u,
  getHeadings: getHeadings$u,
  getHeaders: getHeaders$u
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$t = {"title":"Build a Searchable Phone Call Dashboard with Twilio","description":"Automatically log and search your inbound Twilio calls","date":"2021-12-02T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1638306971/blog/2021/12/twilio-crm-log-js/Build-CRM-w-Searchable-Call-Transcripts-twilio-blog%402x.jpg","authors":["kevin-lewis"],"category":"tutorial","tags":["nodejs","twilio"],"seo":{"title":"Build a Searchable Phone Call Dashboard with Twilio","description":"Automatically log and search your inbound Twilio calls"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661453836/blog/twilio-crm-log-js/ograph.png"},"shorturls":{"share":"https://dpgr.am/6b12fa8","twitter":"https://dpgr.am/66efe38","linkedin":"https://dpgr.am/20d5645","reddit":"https://dpgr.am/91a22f2","facebook":"https://dpgr.am/87e4c65"}};
						const file$t = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/twilio-crm-log-js/index.md";
						const url$t = undefined;
						function rawContent$t() {
							return "\nImagine being able to search phrases in every call you've had to find exactly what was said. That's the dream, right? Well - it's my dream, and it's what we're building today.\n\nUsers can call a Twilio phone number, which will be forwarded to your agent while recording. Once the call is completed, we will get a transcript using Deepgram's Speech Recognition API and make it available through a searchable dashboard we'll put together with Vue.js. The front-end is super light, so you could build it with any framework (or none at all).\n\n![Webpage with three parts - a text box along the top, a list of phone calls on the left with phrases matching the keyword, and a full transcript on the right.](https://res.cloudinary.com/deepgram/image/upload/v1638306976/blog/2021/12/twilio-crm-log-js/final-project.png)\n\nIf you want to look at the final project code, you can find it at https://github.com/deepgram-devs/twilio-voice-searchable-log.\n\n## Before We Start\n\nYou will need:\n\n*   Node.js installed on your machine - [download it here](https://nodejs.org/en/).\n*   A Deepgram API Key - [get one here](https://console.deepgram.com/signup?jump=keys).\n*   A Twilio Account SID and Auth Token and a Twilio phone number - [get one here](https://console.twilio.com).\n*   Use of two phones to test your project - one to make the call and one to receive.\n\nCreate a new directory and navigate to it with your terminal. Run `npm init -y` to create a `package.json` file and then install the following packages:\n```\nnpm install @deepgram/sdk twilio dotenv express body-parser nedb-promises\n```\nCreate a `.env` file, open it in your code editor, and populate it with your credentials and settings:\n\n```\nDG_KEY=replace_with_deepgram_key\nTWILIO_ACCOUNT_SID=replace_with_sid\nTWILIO_AUTH_TOKEN=replace_with_auth_token\nTWILIO_NUMBER=replace_with_twilio_phone_number\nFORWARDING_NUMBER=replace_with_your_phone_number\n```\n<Alert type=\"warning\">You should not share this .env file as it contains sensitive credentials. If you are using git version control, make sure to ignore this file.</Alert>\n\nCreate an `index.js` file, and open it in your code editor.\n\n## Preparing Dependencies\n\nAt the top of your file require these packages:\n\n```js\nrequire('dotenv').config()\nconst express = require('express')\nconst bodyParser = require('body-parser')\nconst nedb = require('nedb-promises')\nconst Twilio = require('twilio')\nconst { Deepgram } = require('@deepgram/sdk')\n```\n\nCreate a new nedb database. This will either load an existing file, or create one if it doesn't exist:\n\n```js\nconst db = nedb.create('calls.db')\n```\n\nInitialize the Twilio Helper library and [Deepgram Node SDK](https://developers.deepgram.com/sdks-tools/sdks/node-sdk/):\n\n```js\nconst twilio = new Twilio(\n  process.env.TWILIO_ACCOUNT_SID,\n  process.env.TWILIO_AUTH_TOKEN\n)\nconst deepgram = new Deepgram(process.env.DG_KEY)\n```\n\n## Creating An Express Server\n\nInitialize an `express` instance, configure `body-parser` and a `public` directory, and start it at port 3000:\n\n```js\nconst app = express()\napp.use(bodyParser.urlencoded({ extended: false }))\napp.use(express.static('public'))\n\n// Further code goes here\n\napp.listen(3000, console.log(`Listening at ${new Date().toISOString()}`))\n```\n\n## Project Overview\n\nWhen a user rings our Twilio number, our application will be sent a POST request with a bunch of information - we will grab their phone number and the call identifier and add it to our database. We will instruct our application to forward the call to our real number and begin recording. Once the call is completed (or after the default one-hour limit), the recording will be saved, and, once ready, Twilio will send recording information back to our application.\n\nWe send the recording URL to Deepgram and receive a transcript in return. The recording details also contain the call identifier, so we can add the recording URL and the transcript to the existing database entry.\n\nOnce we have data in a database - the final step is to build the web dashboard with client-side search, which we will do towards the end of this blog post.\n\n## Configure Your Twilio Number\n\nWhen Twilio sends data to an application, it expects a publicly-accessible URL, but our application is only on our local machine. Fortunately, we can use [ngrok](https://ngrok.com) to provide a temporary public URL that will forward requests to our locally-running application. Open a new terminal and run the following:\n\n```bash\nnpx ngrok http 3000\n```\n\nOnce running, you will see a Ngrok URL in the terminal dashboard which we can provide to Twilio. If you restart your terminal, ngrok, or after 24 hours, the URL will change, and you need to complete the next step again.\n\n![Accept incoming Voice Calls, configure with Webhook, a call comes in webhook to ngrok url /inbound](https://res.cloudinary.com/deepgram/image/upload/v1638306975/blog/2021/12/twilio-crm-log-js/voice-config.png)\n\nNavigate to your phone number's settings in the Twilio Console, and configure incoming calls to send a HTTP POST request to your ngrok URL followed by `/inbound`.\n\n## Handle Inbound Calls\n\nIn your `index.js` file add a new route handler for receiving Twilio calls:\n\n```js\napp.post('/inbound', async (req, res) => {\n  const { Caller, CallSid } = req.body\n\n  // Add caller number, call indetifier, and date to the database\n  await db.insert({ Caller, CallSid, date: new Date() })\n\n  // Create Twilio instructions\n  const twiml = new Twilio.twiml.VoiceResponse()\n  const dial = twiml.dial({\n    record: 'record-from-answer-dual',\n    recordingStatusCallback: '/recordings',\n  })\n  dial.number(process.env.FORWARDING_NUMBER)\n\n  // Send response to Twilio\n  res.type('text/xml')\n  res.end(twiml.toString())\n})\n\n// Further code here\n```\n\nThe `dial` variable is the most critical bit here - we tell Twilio to record the call once it is answered and to record each speaker in their own channel. This isn't required but will lead to higher-quality transcripts later. Once the recording is ready, we tell Twilio to send a POST request with data to `/recordings`. Finally, we tell Twilio to forward this call to the number in our `.env` file - perhaps a reception or sales rep.\n\nBefore we test this, create an empty route handler for recordings:\n\n```js\napp.post('/recordings', async (req, res) => {\n  console.log('Recording received')\n  // Further code here\n\n  res.send('ok')\n})\n```\n\n**Test your code!** Run with `node index.js` while making sure ngrok is still running. Call your Twilio number from a phone number that is different from the one in .env and your forwarding number should ring. Answer, speak, hang up, and you should see 'Recording received' in your terminal a few seconds later.\n\n## Generate and Save Transcripts\n\nReplace your `/recordings` route handler:\n\n```js\napp.post('/recordings', async (req, res) => {\n  console.log('Recording received')\n\n  // Get values from data\n  const { CallSid, RecordingUrl } = req.body\n\n  // Get transcript\n  const transcriptionFeatures = {\n    punctuate: true,\n    utterances: true,\n    model: 'phonecall',\n    multichannel: true,\n  }\n  const transcript = await deepgram.transcription.preRecorded(\n    { url: RecordingUrl },\n    transcriptionFeatures\n  )\n\n  // Format response\n  const utterances = transcript.results.utterances.map((utterance) => ({\n    channel: utterance.channel,\n    transcript: utterance.transcript,\n  }))\n\n  // Update database entry\n  await db.update({ CallSid }, { $set: { RecordingUrl, utterances } })\n  res.send('ok')\n})\n```\n\nTwilio sends a body of data along with this request - we only care about the call identifier and the URL of the recording, so we pull those out by destcructuring the object.\n\nThen, we ask Deepgram for a transcript. Let's recap the features we use:\n\n*   The `punctuate` feature adds punctuation for easier reading.\n*   The `utterances` feature will return the spoken phrases rather than just words, and specifies the user who spoke them.\n*   The `model` feature lets us specify a model to use. We have a few, and the default model would probably perform quite well, but we specifically have one trained for phone calls, so we specify it here.\n*   The `multichannel` feature tells Deepgram that we are sending an audio file with multiple audio channels (one for each speaker). It improves transcription quality as we can accurately identify who is speaking at any point in time.\n\nOnce we provide the recording URL and receive a response from Deepgram, we format the response. The output is an array that looks like this:\n\n```json\n[\n  { \"channel\": 1, \"transcript\": \"Hi, Kev.\" },\n  { \"channel\": 0, \"transcript\": \"Hello. How are you mum?\" },\n  { \"channel\": 1, \"transcript\": \"I'm fine. Thank you. How are you?\" },\n  { \"channel\": 0, \"transcript\": \"Yes. Not too bad at all. Thank you\" }\n]\n```\n\nFinally, we update the existing database entry for this call to include both the recording URL and the utterances spoken.\n\n**Test your code!** Repeat the previous test steps, and you should see the database file (`calls.db`) now containing the new data.\n\n## Get All Recordings\n\nThe final step to the server-side of this project is to build a route handler that will return all calls which have a transcript included:\n\n```js\napp.get('/recordings', async (req, res) => {\n  const recordings = await db\n    .find({ utterances: { $exists: true } })\n    .sort({ date: -1 })\n  res.json(recordings)\n})\n```\n\nThis route handler will also sort results, so it's the latest-first.\n\n## Receive Data In The Browser\n\nOur search will consist of three parts:\n\n1.  A search box which, when typed in, will [fuzzy search](https://en.wikipedia.org/wiki/Approximate_string_matching) all call transcripts fetched from the database. We will use [Fuse.js](https://fusejs.io) for the fuzzy search.\n2.  An updating list of results which will show the phrases that matched the earch term.\n3.  When a result is clicked, a full transcript of the call.\n\nCreate a `public` directory and an `index.html` file inside of it. Open the file and scaffold a page:\n\n```html\n<!DOCTYPE html>\n<html>\n  <head>\n    <meta charset=\"UTF-8\" />\n    <title>Twilio x Deepgram Call Transcripts</title>\n    <style></style>\n  </head>\n  <body>\n    <div id=\"app\"></div>\n    <script src=\"https://cdn.jsdelivr.net/npm/vue@2.6.14\"></script>\n    <script src=\"https://cdn.jsdelivr.net/npm/fuse.js@6.4.6\"></script>\n    <script>\n      const app = new Vue({\n        el: '#app',\n        data: {\n          fuse: false,\n          calls: {\n            all: [],\n            selected: false,\n          },\n          filter: {\n            term: '',\n            results: [],\n          },\n        },\n        async created() {\n          // Further code goes here\n        },\n        methods: {\n          search() {},\n        },\n      })\n    </script>\n  </body>\n</html>\n```\n\nOur first goal is to populate `data.calls.all` with the items from the database returned from the GET `/recordings` route handler, and then create a Fuse search index with those items. Inside the `created()` method:\n\n```js\nthis.calls.all = await fetch('/recordings').then((r) => r.json())\nthis.fuse = new Fuse(this.calls.all, {\n  keys: ['Caller', 'utterances.transcript'],\n  includeMatches: true,\n  threshold: 0.5,\n})\n```\n\nRestart your server, navigate to `http://localhost:3000` in your browser. Open the DevTools Console and type `app.calls.all` to see if the value was populated correctly:\n\n![Browser console showing app.calls.all as an array containing four objects](https://res.cloudinary.com/deepgram/image/upload/v1638306976/blog/2021/12/twilio-crm-log-js/app-calls-all.png)\n\n## Search Transcripts\n\nInside of your `<div id=\"app\">` add a text input:\n\n```html\n<input\n  type=\"text\"\n  placeholder=\"Search term\"\n  v-model=\"filter.term\"\n  @keyup=\"search\"\n/>\n```\n\nThen, inside of the empty `search()` method conduct a search with Fuse and assign the result to `data.filter.results`:\n\n```js\nthis.filter.results = this.fuse.search(this.filter.term)\n```\n\nNow we have results being computed, let's display them right under the `<input>`:\n\n```html\n<ol id=\"results\">\n  <li v-for=\"result in filter.results\">\n    <p><b>{{ result.item.Caller }} on {{ result.item.date }}</b></p>\n    <p v-for=\"match in result.matches\">{{ match.value }}</p>\n  </li>\n</ol>\n```\n\n![A text box with the word \"yesterday\" in it, and three results. Each result shows the matching phrases.](https://res.cloudinary.com/deepgram/image/upload/v1638306975/blog/2021/12/twilio-crm-log-js/filtered-results.png)\n\n## Show Full Transcript\n\nWhen an search result is clicked, let's show the whole transcript. Add this attribute to the end of the `<li>` element - it will set `data.calls.selected` to the clicked item:\n\n```html\n<li v-for=\"result in filter.results\" @click=\"calls.selected = result\"></li>\n```\n\nThen, below the `<ol>` add another `<ol>` to show the utterances in the transcript:\n\n```html\n<ol id=\"full-call\" v-if=\"calls.selected\">\n  <li v-for=\"utterance in calls.selected.item.utterances\">\n    <small>{{ utterance.channel == 0 ? 'Receiver' : 'Caller' }}</small>\n    <p>{{ utterance.transcript }}</p>\n  </li>\n</ol>\n```\n\nWith the predictability of multichannel audio from Twilio, the receiver of the forwarded call is always on channel 0, so we can display in nicer terms who is speaking.\n\nFinally, add the following CSS to the `<style>` tag in the `<head>`:\n\n```css\n* { margin: 0; padding: 0; font-family: sans-serif; }\n#app { display: grid; grid-template-columns: 1fr 1fr; paddi g: 1em; gap: 2em }\ninput { grid-column: 1 / span 2; font-size: 1em; padding: 0.5em; }\nli { list-style: none; margin-bottom: 1em; }\n#results li { cursor: pointer; }\n```\n\nRefresh and you should see the finished project:\n\n![Webpage with three parts - a text box along the top, a list of phone calls on the left with phrases matching the keyword, and a full transcript on the right.](https://res.cloudinary.com/deepgram/image/upload/v1638306976/blog/2021/12/twilio-crm-log-js/final-project.png)\n\n## Wrapping Up\n\nWe've done quite a lot here - how to forward and record calls with Twilio, how to handle recording payloads to get transcripts, and implement basic client-side fuzzy search. There's a lot more we could do to expand on this project:\n\n*   Implement outbound call recording\n*   Group calls under specific callers\n*   Allow users to make notes for calls or callers\n*   Use a more permanent database like Supabase - [Brian wrote a getting started post here](https://blog.deepgram.com/getting-started-with-supabase/)\n\nThe complete project is available at https://github.com/deepgram-devs/twilio-voice-searchable-log and if you have any questions please feel free to reach out on Twitter - we're [@DeepgramDevs](https://twitter.com/DeepgramDevs).\n";
						}
						async function compiledContent$t() {
							return load$t().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$t() {
							return (await import('./chunks/index.48ec6c41.mjs'));
						}
						function Content$t(...args) {
							return load$t().then((m) => m.default(...args));
						}
						Content$t.isAstroComponentFactory = true;
						function getHeadings$t() {
							return load$t().then((m) => m.metadata.headings);
						}
						function getHeaders$t() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$t().then((m) => m.metadata.headings);
						}

const __vite_glob_0_247 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$t,
  file: file$t,
  url: url$t,
  rawContent: rawContent$t,
  compiledContent: compiledContent$t,
  default: load$t,
  Content: Content$t,
  getHeadings: getHeadings$t,
  getHeaders: getHeaders$t
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$s = {"title":"Understanding Webhooks","description":"Let's talk about why, when, and how webhooks are used in development.","date":"2022-02-28T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1645796672/blog/2022/02/understanding-webhooks/Understanding-Webhooks%402x.jpg","authors":["nicole-ohanian"],"category":"tutorial","tags":["webhooks","beginner"],"seo":{"title":"Understanding Webhooks","description":"Let's talk about why, when, and how webhooks are used in development."},"shorturls":{"share":"https://dpgr.am/037383e","twitter":"https://dpgr.am/43d85fb","linkedin":"https://dpgr.am/f3c8896","reddit":"https://dpgr.am/f8ec009","facebook":"https://dpgr.am/c2d2619"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454005/blog/understanding-webhooks/ograph.png"}};
						const file$s = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/understanding-webhooks/index.md";
						const url$s = undefined;
						function rawContent$s() {
							return "\nWhen using a web application, have you tried to change the information being displayed on the web page you're on? Even if you don't realize it you've probably done so, many times, through your computer's use of client-server communication.\n\nFor example, you click on something on a web application's UI on your machine (the client), which then sends an HTTP request to the application's server. The server then sends a response back to your device, which then triggers a change in the UI of the web application.\n\n![A digram shows a web browser with four posts numbered 111 to 108. A button on the top of the list reads \"Load new posts\" with a cursor on it. An arrow labelled \"fetch new data\" points at an updated browser window with a list numbered 113 to 109.](https://res.cloudinary.com/deepgram/image/upload/v1645796695/blog/2022/02/understanding-webhooks/update-ui.png)\n\nHowever, what happens when the web application’s server wants to trigger an event based on something that’s happening on a remote server instead of a user action? That’s where webhooks come in!\n\n## What Is a Webhook?\n\nA webhook is a 'reverse HTTP request' between servers rather than a client and a server. A remote server sends an HTTP POST request to a public URL on your application’s server every time an event occurs on their end so that you may trigger an event in your own application based on that update.\n\n## Exploring Some Examples\n\nNow that you have an idea of what a webhook is, let's look at a couple of sample use cases to solidify your understanding.\n\n### Triggering Actions On Successful Payment\n\n![When a payment form is submitted, the server submits a payment request to Stripe. Stripe them sends an immediate repsonse, and triggers some work before responding a second time with a success webhook. The server then sends an SMS message.](https://res.cloudinary.com/deepgram/image/upload/v1645796801/blog/2022/02/understanding-webhooks/stripe-flow.png)\n\nYou have an e-commerce website with a third-party payment processing integration. The process of completing  may be instantly successful but it may also be delayed or end in an error. Since payment processing is done by an external service, you will not have direct access to the payment process happening on their end. Yet, what if you wanted an event triggered on your application after a customer's successful purchase?\n\nA customer purchases on your website, which uses Stripe for payment processing. When a purchase is completed, you send a thank you text. Stripe supports webhooks, to alert us when a purchase has been successful. You provide a URL (that you control) to Stripe, and it receives details about the purchase instantly. Your application then takes the information received and sends an SMS message in response.\n\n### Waiting For a Transcript\n\nWhen requesting a transcript from Deepgram, you can wait for it to be generated, but this can take a few more seconds than you might be able to wait for larger files. You can access Deepgram's webhook by including the callback feature in your request, which allows the user to redirect the transcription results to the URL of your choice.\n\nYour request for a transcript can be answered immediately, allowing you to receive a response immediately. At the same time, Deepgram works in the background before sending the results are sent to your server through the provided URL.\n\n![A server submits a transcription request to Deepgram. Deepgram sends an immediate repsonse, and triggers some work before responding a second time with a success webhook.](https://res.cloudinary.com/deepgram/image/upload/v1645796695/blog/2022/02/understanding-webhooks/deepgram.png)\n\nBecause you control the application that receives a webhook payload, you can build any additional business logic to run once you have data. You might:\n\n*   Send an email to your client to let them know that their transcript is complete with the results.\n*   Translate the transcript provided to your server to be displayed on your application’s UI.\n*   Send an SMS text to the user's phone with a brief preview of the results.\n\n## Webhooks With Node.js\n\nA webhook consumer is just a route handler. Instead of receiving requests from a user action, it will be triggered by the service emitting webhooks. Here’s an example with Express:\n\n```js\n// Require, initialize, and configure Express\nconst express = require('express');\nconst app = express();\napp.use(express.json());\n\n// This is the route handler our webhook will POST data to\napp.post('/hook', (req, res) => {\n\n    /*\n        You could do anything here, such as:\n        Add data to a database\n        Trigger an email or SMS\n        Automatically schedule an event on your application's UI\n    */\n\n\tconsole.log(req.body); // See the data\n\tres.status(200).end(); // Return empty response to server\n})\n\n// Start express server\napp.listen(3000);\n```\n\nSince webhooks create a POST request to your application, you will need to create a POST route handler in your application. Assuming our application's URL is https://myDIYstore.com, our webhook consumer’s URL will be https://myDIYstore.com/hook.\n\n## Webhooks vs. Polling\n\nIn the examples above, we see that the remote server is sending data to our application using webhooks. However, an alternative to this method is polling from your application's server to the remote server of choice.\n\nPolling means your server will periodically and continuously request to check if there has been an update on the remote server. If there is one, it comes back with the requested information, and your application can stop checking.\n\nThe main difference between using webhooks and polling is that webhooks send a request from a remote server to your server as soon as an event occurs. With polling, a request is being made by your server periodically until it detects an update in the remote server.\n\nTo take advantage of webhooks, the third-party service needs to support them. Where they aren't available, you'll need to poll for updates. While this method can be more resource-intensive for applications, at times, it may be your only option (or even a more fitting one considering the context, but that's outside of the scope of this article). See below for an example of what polling would look like on an Express server.\n\n```js\n// Require cross-fetch library to bring fetch() to Node.js\nconst fetch = require('cross-fetch');\n\n// Require and initialize Express\nconst express = require('express');\nconst app = express();\n\n// Creating a function that once invoked, will poll repeatedly.\nasync function checkStatus() {\n\n  // Asynchronously making an HTTP GET request to our external API\n  let response = await fetch(\"external-api-example-here\").then(r => r.json());\n\n  if (!response.data.status == 'completed') {\n    // If status not completed, rerun this function after 2 seconds\n    await new Promise(r => setTimeout(r, 2000));\n    await checkStatus();\n  } else {\n    // Criteria was met, continue with logic.\n  }\n}\n\n// Invoke the function you have just written\ncheckStatus();\n\n// Start express server\napp.listen(3000);\n```\n\nIn this example, we are continuously querying our chosen third-party API every 2 seconds until the criteria are met. Once the criteria is met, we proceed with our business logic, and the polling stops. As you can see, this can be a resource-intensive method with many requests with no change in data. However, it may be the most appropriate (or only) option, so understanding both polling and webhooks is valuable.\n\nWebhooks are a wonderfully convenient and intuitive tool once you get the hang of it! But, it is important to remember that webhooks need to be supported by the service you are requesting the data from to work! Some platforms that use them include Deepgram, Twitter, Discord, and Stripe.\n\nIf you have any questions, please feel free to reach out on Twitter - we're [@DeepgramDevs](https://twitter.com/DeepgramDevs).\n\n        ";
						}
						async function compiledContent$s() {
							return load$s().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$s() {
							return (await import('./chunks/index.e9acb239.mjs'));
						}
						function Content$s(...args) {
							return load$s().then((m) => m.default(...args));
						}
						Content$s.isAstroComponentFactory = true;
						function getHeadings$s() {
							return load$s().then((m) => m.metadata.headings);
						}
						function getHeaders$s() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$s().then((m) => m.metadata.headings);
						}

const __vite_glob_0_248 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$s,
  file: file$s,
  url: url$s,
  rawContent: rawContent$s,
  compiledContent: compiledContent$s,
  default: load$s,
  Content: Content$s,
  getHeadings: getHeadings$s,
  getHeaders: getHeaders$s
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$r = {"title":"Upcoming January Releases","description":"Learn about some of the language and model improvements coming to Deepgram this January.","date":"2021-01-01T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1662069487/blog/upcoming-january-releases/placeholder-post-image%402x.jpg","authors":["natalie-rutgers"],"category":"product-news","tags":["language","speech-models"],"seo":{"title":"Upcoming January Releases","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1662069487/blog/upcoming-january-releases/placeholder-post-image%402x.jpg"},"shorturls":{"share":"https://dpgr.am/1ab8fb6","twitter":"https://dpgr.am/2c214bb","linkedin":"https://dpgr.am/ecc7afb","reddit":"https://dpgr.am/7a16a37","facebook":"https://dpgr.am/1fe2dd1"}};
						const file$r = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/upcoming-january-releases/index.md";
						const url$r = undefined;
						function rawContent$r() {
							return "Deepgram regularly releases new Automatic Speech Recognition (ASR) models and other enterprise features. These releases are planned on a monthly basis with releases occurring throughout the month.\n\n## Opting In\n\nBecause new models and features may change client workflow and functions, we want to make sure you can control what you want to Opt-in.  For example, a model may have a new punctuation method, diarization method, or an updated vocabulary which needs to be post-processed in domain-specific ways by client code. Therefore, we recommend that customers with high production uptime requirements **opt-in** to most updates rather than always running with the latest release. Deepgram provides two methods for opting in depending on the type of change being made:\n\n* All Deepgram *models* ([`model`](https://developers.deepgram.com/api-reference/speech-recognition-api)) support **version selection** (`version`). This means that in each API call customers can specify which version of a given model they wish to use. New model versions, as well as deprecation of versions, will be announced in the Deepgram Changelog. We recommend that customers with high-uptime requirements always select the model version that they wish to use and opt-in to new models by updating the version number after having tested the new model separately.\n* Almost all new ASR features are configured to be off by default. Customers can **opt-in** to new features through **the use of API parameters**.  As with model version selection, we recommend that customers first test new ASR features separately from their production pipeline and, once they are proven to work well with other customer software and workflow, begin use at a larger scale.\n\nIn certain circumstances (e.g. patching security vulnerabilities), it is not possible for DG to provide an opt-in. In these cases, DG will notify customers of the upcoming change through each customer's preferred method of communication, as well as the Deepgram Changelog. These communications will describe what changes, if any, to expect on the client side. Deepgram support, [support@deepgram.com](mailto:deepgram.comnull), is always available to help walk customers through new feature purpose and configuration, as well as resolve any issues that arise when upgrading.\n\n## **Upcoming January Releases**\n\n### **Improved Spanish Support**\n\n`[language](https://developers.deepgram.com/api-reference/speech-recognition-api)=es` Accuracy improvements and an increased model vocabulary for our Spanish language support. Additionally, we've updated our Spanish support to incorporate our latest improvements to punctuation (`punctuate=true`). To use the latest Spanish support, pass the `language=es` as an API parameter in your request. If you do not wish to use the latest version, be sure to specify the desired version as `version={desired_version}`. Note that Spanish support is only available with Deepgram's General Model (`model=general`).  \n\n### **Improved Turkish Support**\n\n`[language](https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeAudio/properties/language)=tr` Accuracy improvements and an increased model vocabulary for our Turkish language support. Additionally, we've updated our Turkish support to incorporate our latest improvements to punctuation (`punctuate=true`). To use the latest Turkish support, pass the `language=tr` as an API parameter in your request. If you do not wish to use the latest version, be sure to specify the desired version as `version={desired_version}`. Note that Turkish support is only available with Deepgram's General Model (`model=general`).  \n\n### **Improved Meeting Model**\n\n`[model](https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeAudio/properties/model)=meeting` Accuracy improvements and an increased model vocabulary for our base meeting model. Additionally, we've updated our meeting model to incorporate our latest improvements to punctuation (`punctuate=true`). To use the latest meeting model, pass the `model=meeting` as an API parameter in your request. If you do not wish to use the latest version, be sure to specify the desired version as `version={desired_version}`.  \n\n### **Improved Keyword Boosting**\n\n`[keywords](https://developers.deepgram.com/api-reference/speech-recognition-api)={keyword}` We are releasing an updated keyword boosting feature (`keywords={keyword}`) with improved support for out of vocabulary words.  This feature is useful in two situations: (1) recognizing words in domain with a large vocabulary of domain-specific jargon and (2) a client application that depends on recognizing names, brands, or other relatively rare keywords with high accuracy. We recommend using this feature in combination with a custom trained model from Deepgram. Deepgram will customize a model for your domain, ensuring that the vocabulary of the model fits your domain. Keyword Boosting allows customers to add additional words rapidly (i.e. new brands, new menu items, etc) to augment a custom model's domain-focused vocabulary.\n\n### **Improved General Model**\n\n`[model](https://developers.deepgram.com/api-reference/speech-recognition-api#operation/transcribeAudio/properties/model)=general` We are also releasing accuracy improvements and an increased model vocabulary to our general model. Additionally, we've updated our general model to incorporate our latest improvements to punctuation (`punctuate=true`). The general model is Deepgram's default model selection, but can also be called by, passing `model=general` as an API parameter in your request. If you do not wish to use the latest version, be sure to specify the desired version as `version={desired_version}`.   [View the Deepgram Documentation](https://developers.deepgram.com/)";
						}
						async function compiledContent$r() {
							return load$r().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$r() {
							return (await import('./chunks/index.da1ff51e.mjs'));
						}
						function Content$r(...args) {
							return load$r().then((m) => m.default(...args));
						}
						Content$r.isAstroComponentFactory = true;
						function getHeadings$r() {
							return load$r().then((m) => m.metadata.headings);
						}
						function getHeaders$r() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$r().then((m) => m.metadata.headings);
						}

const __vite_glob_0_249 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$r,
  file: file$r,
  url: url$r,
  rawContent: rawContent$r,
  compiledContent: compiledContent$r,
  default: load$r,
  Content: Content$r,
  getHeadings: getHeadings$r,
  getHeaders: getHeaders$r
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$q = {"title":"Use OpenAI Whisper Speech Recognition with the Deepgram API","description":"OpenAI's Whisper speech recognition model is now available to anyone who wants to use it and hosted by Deepgram. Test the model today!","date":"2022-09-22T21:39:04.411Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1663882926/blog/use-openai-whisper-speech-recognition-with-the-deepgram-api/open-ai-whisper-w-deepgram-code-blog_2x_kldwgp.jpg","authors":["scott-stephenson"],"category":"ai-and-engineering","tags":["whisper","machine-learning"],"shorturls":{"share":"https://dpgr.am/f4e76b9","twitter":"https://dpgr.am/e7c9e87","linkedin":"https://dpgr.am/525dce8","reddit":"https://dpgr.am/dde3c8d","facebook":"https://dpgr.am/8888e60"}};
						const file$q = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/use-openai-whisper-speech-recognition-with-the-deepgram-api/index.md";
						const url$q = undefined;
						function rawContent$q() {
							return "\nYesterday was a big day for voice intelligence as OpenAI released [Whisper](https://github.com/openai/whisper), a general-purpose speech recognition model. We’ve gotten several questions about what this means for the future of Voice AI and companies like Deepgram. We’ve also been humored by posts like this:\n\n![Image of tweet from @LewisNWatson saying \"Deepgram bricking it rn with whisper\"](https://res.cloudinary.com/deepgram/image/upload/v1663883725/blog/use-openai-whisper-speech-recognition-with-the-deepgram-api/1572720107775995904_gvzzzg.jpg)\n\nDeepgram has been doing end-to-end deep learning for speech for over 4 years in production, so we feel uniquely positioned to understand the enormous amount of work it takes to bring something like Whisper to fruition. Congratulations from our team to theirs!\n\nOpenAI chooses to work on some of the most challenging, and promising, foundational aspects of artificial intelligence (AI). We’re thrilled that they’ve expanded into voice. Their entry will only bring even more excitement to the voice revolution. We share the opinion that voice intelligence is one of the most important areas of research in AI and will unlock extraordinary opportunities for AI applications across myriad use cases. In fact, we see this every day as we support our global customers in building voice intelligence products.\n\nProducing transcripts from voice data is just scratching the surface of what humanity should be accomplishing. We, and others in the voice community, are working towards a future where machines gather just as much from a conversation as humans can. Nonetheless, we need to celebrate every accomplishment along the way and support each other where possible.\n\nAs our team, like so many others around the world, tried Whisper for the first time yesterday we thought it would be a great thing if people could use a hosted version. Deepgram’s speech API already hosts some of the most accurate and performant speech recognition models in the world, so we figured what’s one more?\n\nToday, we’re making Whisper available to anyone who wants to use it, hosted by Deepgram. Users that [sign up to use Deepgram](https://console.deepgram.com/signup?jump=demo\\&f-whisper=true) will find Whisper available as an additional model to use among our world-class language and use case models. Alternatively, anyone can access the Whisper model programmatically via a hosted API — no sign-up required.\n\nTo test it quickly, run this command:\n\n```shell\ncurl --request POST \\\n  --url 'https://api.deepgram.com/v1/listen?model=whisper' \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n\t\"url\": \"https://static.deepgram.com/examples/Bueller-Life-moves-pretty-fast.wav\"\n}'\n```\n\nSo, what does this mean? Deepgram, voice tech, and the voice community are just getting started. Look out for trillion-dollar companies in a decade.\n\n";
						}
						async function compiledContent$q() {
							return load$q().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$q() {
							return (await import('./chunks/index.6a04c158.mjs'));
						}
						function Content$q(...args) {
							return load$q().then((m) => m.default(...args));
						}
						Content$q.isAstroComponentFactory = true;
						function getHeadings$q() {
							return load$q().then((m) => m.metadata.headings);
						}
						function getHeaders$q() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$q().then((m) => m.metadata.headings);
						}

const __vite_glob_0_250 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$q,
  file: file$q,
  url: url$q,
  rawContent: rawContent$q,
  compiledContent: compiledContent$q,
  default: load$q,
  Content: Content$q,
  getHeadings: getHeadings$q,
  getHeaders: getHeaders$q
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$p = {"title":"Voice Control Your Browser with Stëmm","description":"Leveraging both the Deepgram API & Chrome APIs into a Chrome extension that lets users control the browser hands-free with voice commands. Learn how we did it here.","date":"2022-03-07T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1646056012/blog/2022/03/voice-control-browser-stemm/stemm.jpg","authors":["kevin-lewis"],"category":"project-showcase","tags":["nodejs","hackathon"],"seo":{"title":"Voice Control Your Browser with Stëmm","description":"Leveraging both the Deepgram API & Chrome APIs into a Chrome extension that lets users control the browser hands-free with voice commands. Learn how we did it here."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454056/blog/voice-control-browser-stemm/ograph.png"},"shorturls":{"share":"https://dpgr.am/af2c780","twitter":"https://dpgr.am/7bd62ea","linkedin":"https://dpgr.am/57dae70","reddit":"https://dpgr.am/6b556e8","facebook":"https://dpgr.am/f8fce58"}};
						const file$p = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/voice-control-browser-stemm/index.md";
						const url$p = undefined;
						function rawContent$p() {
							return "\r\nBack in January, we supported Hack Cambridge - a 24-hour student hackathon. The team behind Stëmm wanted to bring voice control to one of the most used applications globally - Google Chrome. I sat down with [Benedek Der](https://www.linkedin.com/in/benedek-dér-b8b410200/), [Bianca Sandu](https://www.linkedin.com/in/bianca-sandu-89b364202), [Julius Weisser](https://www.linkedin.com/in/julius-weisser-4895a61b8), and [Siddharth Srivastava](http://siddharthsrivastava0501.github.io) to ask them about their project.\r\n\r\nThe team behind Stëmm all study Computer Science at the University of Warwick, are friends, and most of them are also flatmates. While Hack Cambridge was their first in-person hackathon, at Hack Duke in October 2021, they built a Chrome extension that identified COVID facts in a webpage.\r\n\r\nMost of the team met up a week before Hack Cambridge to start brainstorming ideas, not aware that themes would be announced on the morning. They marched down to the venue, electronics kit in hand, and realized they would need to rethink their game plan as soon as the opening ceremony took place.\r\n\r\n## The Project\r\n\r\nFortunately, some of the team saw our live demo at the event that highlighted how easy it is to [get started with Deepgram's Speech Recognition API in the browser](https://blog.deepgram.com/live-transcription-mic-browser/). While they had to decide which sponsored challenge categories they would incorporate into their project, the team \"instantly recognized the vast potential the Deepgram API gives developers by allowing us to use speech recognition in innovative ways within our projects\" says Sid.\r\n\r\nAfter bouncing around ideas, they chose to expand their knowledge from October's event. They landed on what would become Stëmm - the aim was to build a browser interface for users with motor disabilities. The team leveraged both the Deepgram API and Chrome API into a Chrome extension that, once installed and given microphone permissions, lets users control Chrome hands-free with voice commands like \"chrome, open tab,\" \"chrome, search for recipes,\" and \"chrome, add bookmark.\"\r\n\r\n<YouTube id=\"8w6rmlqOW6o\"></YouTube>\r\n\r\n## Command and Control\r\n\r\nThis use case category is very familiar to us at Deepgram - and we call it \"command and control,\" which allows voice control of systems. Using Deepgram's [keywords](https://developers.deepgram.com/documentation/features/keywords/) and [search](https://developers.deepgram.com/documentation/features/search/) features, along with custom processing, you can build something similar in just a few lines of code.\r\n\r\nWe've seen it used in web applications, as an interface for games, and dedicated devices.\r\n\r\n## The Hours Tick By\r\n\r\nAs you might imagine, Google has a strict set of security provisions for extensions, and during the hackathon this became the main challenge to overcome. I remember having multiple conversations with the Stëmm team over several hours and wondering if they'd be able to overcome the blockers and get their project working, especially given the vague error messages they were battling. Thankfully, they managed to work out the right configuration that allowed their extension to operate.\r\n\r\nOnce the extension could access a user's microphone and get transcripts from Deepgram, the result used a language processing algorithm built by Benedek & Bianca to identify the commands in the recorded text, and by integrating these commands with the Chrome developer tools, it executes them to control the browser.\r\n\r\nThe extension is still somewhat limited in terms of commands, but the team directly welcomes contributions to their project repository to add new features. You can find setup and contribution guidelines [directly on GitHub](https://github.com/siddharthsrivastava0501/hackcambridge-2022).\r\n\r\n";
						}
						async function compiledContent$p() {
							return load$p().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$p() {
							return (await import('./chunks/index.b11c6e2d.mjs'));
						}
						function Content$p(...args) {
							return load$p().then((m) => m.default(...args));
						}
						Content$p.isAstroComponentFactory = true;
						function getHeadings$p() {
							return load$p().then((m) => m.metadata.headings);
						}
						function getHeaders$p() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$p().then((m) => m.metadata.headings);
						}

const __vite_glob_0_251 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$p,
  file: file$p,
  url: url$p,
  rawContent: rawContent$p,
  compiledContent: compiledContent$p,
  default: load$p,
  Content: Content$p,
  getHeadings: getHeadings$p,
  getHeaders: getHeaders$p
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$o = {"title":"How to Use Voice to Control Music with Python and Deepgram","description":"This tutorial will use Python and the Deepgram API speech-to-text audio transcription to play voice-controlled music with the piano.","date":"2022-08-18T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1660765739/blog/2022/08/voice-controlled-music-with-python/2208-How-to-Use-Voice-to-Control-Music-with-Python-and-Deepgram-blog%402x.jpg","authors":["tonya-sims"],"category":"tutorial","tags":["python"],"seo":{"title":"How to Use Voice to Control Music with Python and Deepgram","description":"This tutorial will use Python and the Deepgram API speech-to-text audio transcription to play voice-controlled music with the piano."},"shorturls":{"share":"https://dpgr.am/3eb1ffd","twitter":"https://dpgr.am/568a128","linkedin":"https://dpgr.am/b4afffb","reddit":"https://dpgr.am/345ca5a","facebook":"https://dpgr.am/67e6c9c"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454126/blog/voice-controlled-music-with-python/ograph.png"}};
						const file$o = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/voice-controlled-music-with-python/index.md";
						const url$o = undefined;
						function rawContent$o() {
							return "\r\nMove over Beethoven. This tutorial will use Python and the Deepgram API speech-to-text audio transcription to play a piano with your voice. The song we’ll play is the first few phrases of [Lady Gaga’s Bad Romance](https://youtu.be/-bsMuWw-v6c). It’s a simple piece in C Major, meaning no flats and sharps! We’ll only use pitches C, D, E, F, G, A, and B, and no black keys. What a beautiful chance for someone learning how to play the piano without a keyboard, tapping into the power of voice to play music!\r\n\r\nAfter running the project, we'll see the GIF below when running the project as a PyGame application. A window will appear, and the piano will play the song. We'll hear the notes, which also light up on the keyboard.\r\n\r\n![Python and Deepgram API playing voice-controlled music with the piano](https://res.cloudinary.com/deepgram/image/upload/v1659451110/blog/2022/08/voice-controlled-music-with-python/python-voice-controlled-music.gif)\r\n\r\nLet’s get started!\r\n\r\n## What We’ll Need to Play Voice-Controlled Music Using AI\r\n\r\nThis project requires macOS but is also possible with a Windows or Linux machine. We’ll also use Python 3.10 and other tools like FluidSynth and Deepgram Python SDK speech-to-text audio transcription.\r\n\r\n### FluidSynth\r\n\r\nWe need to install [FluidSynth](https://www.fluidsynth.org/), a free, open-source MIDI software synthesizer that creates sound in digital format, usually for music. **MIDI** or **Musical Instrument Digital Interface** is a protocol that allows musical gear like computers, software, and instruments to communicate with one another. **FluidSynth** uses **SoundFont** files to generate audio. These files have samples of musical instruments like a piano that play MIDI files.\r\n\r\nThere are various options to install FluidSynth on a Mac. In this tutorial, we’ll use [Homebrew](https://brew.sh/) for the installation. After installing Homebrew, run this command anywhere in the terminal:\r\n\r\n    brew install fluidsynth\r\n\r\nNow that FluidSynth is installed, let’s get our Deepgram API Key.\r\n\r\n### Deepgram API Key\r\n\r\nWe need to grab a [Deepgram API Key from the console](https://console.deepgram.com/signup?jump=keys). It’s effortless to sign up and create an API Key here. Deepgram is an AI automated speech recognition voice-to-text company that allows us to build applications that transcribe speech-to-text. We’ll use Deepgram’s Python SDK and the [Numerals feature](https://developers.deepgram.com/documentation/features/numerals/), which converts a number from written format to numerical format. For example, if we say the number “three”, it would appear in our transcript as “3”.\r\n\r\nOne of the many reasons to choose Deepgram over other providers is that we build better voice applications with faster, more accurate transcription through AI Speech Recognition. We offer real-time transcription and pre-recorded speech-to-text. The latter allows uploading a file that contains audio voice data for transcribing.\r\n\r\nNow that we have our Deepgram API Key let’s set up our Python AI piano project so we can start making music!\r\n\r\n## Create a Python Virtual Environment\r\n\r\nMake a Python directory called `play-piano` to hold our project. Inside of it, create a new file called `piano-with-deepgram.py`, which will have our main code for the project.\r\n\r\nWe need to create a virtual environment and activate it so we can `pip` install our Python packages. We have a more in-depth article about virtual environments on our Deepgram Developer [blog](https://blog.deepgram.com/python-virtual-environments/).\r\n\r\nActivate the virtual environment after it’s created and install the following Python packages from the terminal.\r\n\r\n    pip install deepgram-sdk\r\n    pip install python-dotenv\r\n    pip install mingus\r\n    pip install pygame\r\n    pip install sounddevice\r\n    pip install scipy\r\n\r\nLet’s go through each of the Python packages.\r\n\r\n*   `deepgram-sdk` is the Deepgram Python SDK installation that allows us to transcribe speech audio, or voice, to a text transcript.\r\n*   `python-dotenv` helps us work with environment variables and our Deepgram API KEY, which we’ll pull from the `.env` file.\r\n*   `mingus` is a package for Python used by programmers and musicians to make and play music.\r\n*   `pygame` is an open-sourced Python engine to help us make games or other multimedia applications.\r\n*   `sounddevice` helps get audio from our device’s microphone and records it as a NumPy array.\r\n*   `scipy` helps writes the NumPy array into a WAV file.\r\n\r\nWe need to download a few files, including [**keys.png**](https://github.com/bspaans/python-mingus/blob/master/mingus_examples/pygame-piano/keys.png), which is the image of the piano GUI. The other file we need is the **Yamaha-Grand-ios-v1.2** from [this site](https://sites.google.com/site/soundfonts4u/). A SoundFont contains a sample of musical instruments; in our case, we’ll need a piano sound.\r\n\r\n## The Code to Play Voice-Controlled Music with Python and AI\r\n\r\nWe’ll only cover the Deepgram code in this section but will provide the entire code for the project at the end of this post.\r\n\r\n```python\r\nfile_name = input(\"Name the output WAV file: \")\r\n\r\nAUDIO_FILE = file_name\r\n\r\nfs = 44100\r\nduration = 30.0\r\n\r\n\r\ndef record_song_with_voice():\r\n   print(\"Recording.....\")\r\n   record_voice = sd.rec(int(duration * fs) , samplerate = fs , channels = 1)\r\n   sd.wait()\r\n   write(AUDIO_FILE, fs,record_voice)\r\n   print(\"Finished.....Please check your output file\")\r\n\r\nasync def get_deepgram_transcript():\r\n   deepgram = Deepgram(os.getenv(\"DEEPGRAM_API_KEY\"))\r\n  \r\n   record_song_with_voice() \r\n   with open(AUDIO_FILE, \"rb\") as audio:\r\n       source = {\"buffer\": audio, \"mimetype\": \"audio/wav\"}\r\n       response = await deepgram.transcription.prerecorded(source, {\"punctuate\": True, \"numerals\": True})\r\n  \r\n   return response\r\n\r\nasync def get_note_data():\r\n   note_dictonary = {\r\n          '1': 'C',\r\n          '2': 'D',\r\n          '3': 'E',\r\n          '4': 'F',\r\n          '5': 'G',\r\n          '6': 'A',\r\n          '7': 'B'\r\n  }\r\n \r\n   get_numbers = await get_deepgram_transcript()\r\n   data = []\r\n   if 'results' in get_numbers:\r\n       data = get_numbers['results']['channels'][0]['alternatives'][0]['words']\r\n  \r\n   return [note_dictonary [x['word']] for x in data]\r\n\r\ndata = asyncio.run(get_note_data())\r\n```\r\n\r\n## Deepgram Python Code Explanation\r\n\r\nThis line of code prompts the user to create a name of the audio file so that the file will save in `.wav` format:\r\n\r\n```python\r\nfile_name = input(\"Name the output WAV file: \")\r\n```\r\n\r\nOnce the file is created the function `record_song_with_voice` gets called inside the `get_deepgram_transcript` method.\r\n\r\n```python\r\ndef record_song_with_voice():\r\n   print(\"Recording.....\")\r\n   record_voice = sd.rec(int(duration * fs) , samplerate = fs , channels = 1)\r\n   sd.wait()\r\n   write(AUDIO_FILE, fs,record_voice)\r\n   print(\"Finished.....Please check your output file\")\r\n```\r\n\r\nInside the `record_song_with_voice` function, this line records the audio.\r\n\r\n```python\r\n record_voice = sd.rec(int(duration * fs) , samplerate = fs , channels = 1)\r\n```\r\n\r\nWhere `duration` is the number of seconds it takes to record an audio file, and `fs` represents the sampling frequency. We set both of these as constants near the top of the code.\r\n\r\nThen we write the voice recording to an audio file using the `.write()` method. That line of code looks like this:\r\n\r\n```python\r\n   write(AUDIO_FILE, fs,record_voice)\r\n```\r\n\r\nOnce the file is done writing, this message will print to the terminal `”Finished.....Please check your output file\"`, which means the recording is complete.\r\n\r\nThe function `get_deepgram_transcript` is where most of the magic happens. Let’s walk through the code.\r\n\r\n```python\r\nasync def get_deepgram_transcript():\r\n   deepgram = Deepgram(os.getenv(\"DEEPGRAM_API_KEY\"))\r\n  \r\n   record_song_with_voice() \r\n\r\n   with open(AUDIO_FILE, \"rb\") as audio:\r\n       source = {\"buffer\": audio, \"mimetype\": \"audio/wav\"}\r\n       response = await deepgram.transcription.prerecorded(source, {\"punctuate\": True, \"numerals\": True})\r\n  \r\n   return response\r\n```\r\n\r\nHere we initialize the Deepgram Python SDK. That’s why it’s essential to grab a [Deepgram API Key from the console](https://console.deepgram.com/signup?jump=keys).\r\n\r\n```python\r\ndeepgram = Deepgram(os.getenv(\"DEEPGRAM_API_KEY\"))\r\n```\r\n\r\nWe store our Deepgram API Key in a `.env` file like so:\r\n\r\n```python\r\nDEEPGRAM_API_KEY=\"abc123\"\r\n```\r\n\r\nThe `abc123` represents the API Key Deepgram assigns us.\r\n\r\nNext, we call the external function `record_song_with_voice()`, which allows us to record our voice and create a `.wav` file that will pass into Deepgram as pre-recorded audio.\r\n\r\nFinally, we open the newly created audio file in binary format for reading. We provide key/values pairs for `buffer` and a `mimetype` using a Python dictionary. The buffer’s value is `audio`, the object we assigned it in this line `with open(AUDIO_FILE, \"rb\") as audio:` The mimetype value is `audio/wav`, which is the file format we’re using, which one of 40+ different file formats that Deepgram supports. We then call Deepgram and perform a pre-recorded transcription in this line: `response = await deepgram.transcription.prerecorded(source, {\"punctuate\": True, \"numerals\": True})`. We pass in the `numerals` parameter so that when we say a number, it will process in numeric form.\r\n\r\n```python\r\n with open(AUDIO_FILE, \"rb\") as audio:\r\n       source = {\"buffer\": audio, \"mimetype\": \"audio/wav\"}\r\n       response = await deepgram.transcription.prerecorded(source, {\"punctuate\": True, \"numerals\": True})\r\n  \r\n   return response\r\n```\r\n\r\nThe last bit of code to review is the `get_note_data` function, doing precisely that: getting the note data.\r\n\r\n```python\r\nasync def get_note_data():\r\n   note_dictonary = {\r\n          '1': 'C',\r\n          '2': 'D',\r\n          '3': 'E',\r\n          '4': 'F',\r\n          '5': 'G',\r\n          '6': 'A',\r\n          '7': 'B'\r\n  }\r\n \r\n   get_numbers = await get_deepgram_transcript()\r\n   data = []\r\n   if 'results' in get_numbers:\r\n       data = get_numbers['results']['channels'][0]['alternatives'][0]['words']\r\n  \r\n   return [note_dictonary [x['word']] for x in data]\r\n\r\ndata = asyncio.run(get_note_data())\r\n```\r\n\r\nWe have a Python dictionary with keys from ‘1’ to ‘7’ corresponding to every note in the C Major scale. For example, when we say the number `1` that plays the note `C`, saying the number `2` will play the ‘D’ note, and so on:\r\n\r\n```python\r\n   note_dictonary = {\r\n          '1': 'C',\r\n          '2': 'D',\r\n          '3': 'E',\r\n          '4': 'F',\r\n          '5': 'G',\r\n          '6': 'A',\r\n          '7': 'B'\r\n  }\r\n```\r\n\r\nHere’s how that would look on a piano. Each note in C Major is labeled, and located above is a corresponding number. The numbers 1 - 7 are critical, representing a single note in our melody.\r\n\r\n![Piano Keys with Deepgram API to play voice-controlled music with Python](https://res.cloudinary.com/deepgram/image/upload/v1659450623/blog/2022/08/voice-controlled-music-with-python/Musical-Keyboard-Python-Deepgram.png)\r\n\r\nNext, we get the numerals from the Deepgram pre-recorded transcript `get_numbers = await get_deepgram_transcript()`.\r\n\r\nWe then create an empty list called `data` and check if there are any `results` in the parsed response we get back from Deepgram. If results exist, we get that result and store it in `data`:\r\n\r\n```python\r\n   data = []\r\n   if 'results' in get_numbers:\r\n       data = get_numbers['results']['channels'][0]['alternatives'][0]['words']\r\n```\r\n\r\nExample output may look like the below, depending on which song we create.\r\n\r\n```\r\n    [\r\n    {'word': '1', 'start': 2.0552316, 'end': 2.4942129, 'confidence': 0.99902344, 'punctuated_word': '1'}, \r\n    {'word': '4', 'start': 2.8533795, 'end': 3.172639, 'confidence': 0.9980469, 'punctuated_word': '4'}, \r\n    {'word': '3', 'start': 3.6116204, 'end': 4.1116204, 'confidence': 0.9975586, 'punctuated_word': '3'}\r\n    ]\r\n```\r\n\r\nWe notice that the `word` key in the above response correlates to a numeral we speak into the microphone when recording the song.\r\n\r\nWe can now create a new list that maps each numeral to a note on the piano, using a list comprehension `return [note_dictonary [x['word']] for x in data]`.\r\n\r\nTo run the project, we’ll need all the code. See the end of this post.\r\n\r\nThen in our terminal, we can run the project by typing:\r\n\r\n```\r\npython3 piano-with-deepgram.py\r\n```\r\nNow, use our voice to say the following numerals, which correspond to piano notes, to play the first few phrases from Lady Gaga’s song Bad Romance:\r\n\r\n`12314 3333211 12314 3333211`\r\n\r\n## Next Steps to Extend the Voice-Controlled Python AI Music Example\r\n\r\nCongratulations on getting to the end of the tutorial! We encourage you to try and extend the project to do the following:\r\n\r\n*   Play around with the code to play songs in different octaves\r\n*   Play voice-controlled music that has flats and sharps\r\n*   Tweak the code to play voice-controlled music using whole notes and half notes\r\n\r\nWhen you have your new masterpiece, please send us a Tweet at [@DeepgramAI](https://twitter.com/DeepgramAI) and showcase your work!\r\n\r\n## The Entire Python Code for the Voice-Controlled Music Example\r\n\r\n```python\r\n# -*- coding: utf-8 -*-\r\n\r\nfrom pygame.locals import *\r\nfrom mingus.core import notes, chords\r\nfrom mingus.containers import *\r\nfrom mingus.midi import fluidsynth\r\nfrom os import sys\r\nfrom scipy.io.wavfile import write\r\nfrom deepgram import Deepgram\r\nfrom dotenv import load_dotenv\r\nimport asyncio, json\r\nimport pygame\r\nimport os\r\nimport time\r\nimport sounddevice as sd\r\n\r\n\r\nload_dotenv()\r\nfile_name = input(\"Name the output WAV file: \")\r\n\r\n# Audio File with song\r\nAUDIO_FILE = file_name\r\nSF2 = \"soundfont.sf2\"\r\nOCTAVES = 5 # number of octaves to show\r\nLOWEST = 2 # lowest octave to show\r\nFADEOUT = 0.25 # 1.0 # coloration fadeout time (1 tick = 0.001)\r\nWHITE_KEY = 0\r\nBLACK_KEY = 1\r\n\r\nWHITE_KEYS = [\r\n  \"C\",\r\n  \"D\",\r\n  \"E\",\r\n  \"F\",\r\n  \"G\",\r\n  \"A\",\r\n  \"B\",\r\n]\r\n\r\nBLACK_KEYS = [\"C#\", \"D#\", \"F#\", \"G#\", \"A#\"]\r\n\r\nfs = 44100\r\nduration = 30.0\r\n\r\ndef record_song_with_voice():\r\n   print(\"Recording.....\")\r\n   record_voice = sd.rec(int(duration * fs) , samplerate = fs , channels = 1)\r\n   sd.wait()\r\n   write(AUDIO_FILE, fs,record_voice)\r\n   print(\"Finished.....Please check your output file\")\r\n\r\nasync def get_deepgram_transcript():\r\n   # Initializes the Deepgram SDK\r\n   deepgram = Deepgram(os.getenv(\"DEEPGRAM_API_KEY\"))\r\n  \r\n   # call the external function\r\n   record_song_with_voice()\r\n   # Open the audio file\r\n   with open(AUDIO_FILE, \"rb\") as audio:\r\n       # ...or replace mimetype as appropriate\r\n       source = {\"buffer\": audio, \"mimetype\": \"audio/wav\"}\r\n       response = await deepgram.transcription.prerecorded(source, {\"punctuate\": True, \"numerals\": True})\r\n  \r\n   return response\r\n\r\ndef load_img(name):\r\n  \"\"\"Load image and return an image object\"\"\"\r\n  fullname = name\r\n  try:\r\n      image = pygame.image.load(fullname)\r\n      if image.get_alpha() is None:\r\n          image = image.convert()\r\n      else:\r\n          image = image.convert_alpha()\r\n  except pygame.error as message:\r\n      print(\"Error: couldn't load image: \", fullname)\r\n      raise SystemExit(message)\r\n  return (image, image.get_rect())\r\nif not fluidsynth.init(SF2):\r\n  print(\"Couldn't load soundfont\", SF2)\r\n  sys.exit(1)\r\n  \r\npygame.init()\r\npygame.font.init()\r\nfont = pygame.font.SysFont(\"monospace\", 12)\r\nscreen = pygame.display.set_mode((640, 480))\r\n(key_graphic, kgrect) = load_img(\"keys.png\")\r\n(width, height) = (kgrect.width, kgrect.height)\r\nwhite_key_width = width / 7\r\n\r\n# Reset display to wrap around the keyboard image\r\npygame.display.set_mode((OCTAVES * width, height + 20))\r\npygame.display.set_caption(\"mingus piano\")\r\noctave = 4\r\nchannel = 8\r\n\r\n# pressed is a surface that is used to show where a key has been pressed\r\npressed = pygame.Surface((white_key_width, height))\r\npressed.fill((0, 230, 0))\r\n\r\n# text is the surface displaying the determined chord\r\ntext = pygame.Surface((width * OCTAVES, 20))\r\ntext.fill((255, 255, 255))\r\n\r\nplaying_w = [] # white keys being played right now\r\nplaying_b = [] # black keys being played right now\r\nquit = False\r\ntick = 0.0\r\n\r\ndef play_note(note):\r\n  \"\"\"play_note determines the coordinates of a note on the keyboard image\r\n  and sends a request to play the note to the fluidsynth server\"\"\"\r\n  global text\r\n  octave_offset = (note.octave - LOWEST) * width\r\n  if note.name in WHITE_KEYS:\r\n      # Getting the x coordinate of a white key can be done automatically\r\n      w = WHITE_KEYS.index(note.name) * white_key_width\r\n      w = w + octave_offset\r\n      # Add a list containing the x coordinate, the tick at the current time\r\n      # and of course the note itself to playing_w\r\n      playing_w.append([w, tick, note])\r\n  else:\r\n      # For black keys I hard coded the x coordinates. It's ugly.\r\n      i = BLACK_KEYS.index(note.name)\r\n      if i == 0:\r\n          w = 18\r\n      elif i == 1:\r\n          w = 58\r\n      elif i == 2:\r\n          w = 115\r\n      elif i == 3:\r\n          w = 151\r\n      else:\r\n          w = 187\r\n      w = w + octave_offset\r\n      playing_b.append([w, tick, note])\r\n  # To find out what sort of chord is being played we have to look at both the\r\n  # white and black keys, obviously:\r\n  notes = playing_w + playing_b\r\n  notes.sort()\r\n  notenames = []\r\n  for n in notes:\r\n      notenames.append(n[2].name)\r\n  # Determine the chord\r\n  det = chords.determine(notenames)\r\n  if det != []:\r\n      det = det[0]\r\n  else:\r\n      det = \"\"\r\n  # And render it onto the text surface\r\n  t = font.render(det, 2, (0, 0, 0))\r\n  text.fill((255, 255, 255))\r\n  text.blit(t, (0, 0))\r\n  # Play the note\r\n  fluidsynth.play_Note(note, channel, 100)\r\n  time.sleep(0.50)\r\n\r\nasync def get_note_data():\r\n   note_dictonary = {\r\n          '1': 'C',\r\n          '2': 'D',\r\n          '3': 'E',\r\n          '4': 'F',\r\n          '5': 'G',\r\n          '6': 'A',\r\n          '7': 'B'\r\n  }\r\n \r\n   get_numbers = await get_deepgram_transcript()\r\n   data = []\r\n   if 'results' in get_numbers:\r\n       data = get_numbers['results']['channels'][0]['alternatives'][0]['words']\r\n\r\n  \r\n   return [note_dictonary [x['word']] for x in data]\r\ndata = asyncio.run(get_note_data())\r\ni = 0\r\n\r\nwhile i < len(data):\r\n   # Blit the picture of one octave OCTAVES times.\r\n   for x in range(OCTAVES):\r\n       screen.blit(key_graphic, (x * width, 0))\r\n   # Blit the text surface\r\n   screen.blit(text, (0, height))\r\n  # Check all the white keys\r\n   for note in playing_w:\r\n      diff = tick - note[1]\r\n      # If a is past its prime, remove it, otherwise blit the pressed surface\r\n      # with a 'cool' fading effect.\r\n      if diff > FADEOUT:\r\n          fluidsynth.stop_Note(note[2], channel)\r\n          playing_w.remove(note)\r\n      else:\r\n          pressed.fill((0, ((FADEOUT - diff) / FADEOUT) * 255, 124))\r\n          screen.blit(pressed, (note[0], 0), None, pygame.BLEND_SUB)\r\n   if tick > i/4:\r\n       play_note(Note(data[i], octave))\r\n       i += 1\r\n      # if i == len(data):\r\n      # i = 0\r\n  \r\n \r\n   pygame.display.update()\r\n   tick += 0.005 # or 0.001 or 0.0001\r\n```\r\n\r\n";
						}
						async function compiledContent$o() {
							return load$o().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$o() {
							return (await import('./chunks/index.07ad7868.mjs'));
						}
						function Content$o(...args) {
							return load$o().then((m) => m.default(...args));
						}
						Content$o.isAstroComponentFactory = true;
						function getHeadings$o() {
							return load$o().then((m) => m.metadata.headings);
						}
						function getHeaders$o() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$o().then((m) => m.metadata.headings);
						}

const __vite_glob_0_252 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$o,
  file: file$o,
  url: url$o,
  rawContent: rawContent$o,
  compiledContent: compiledContent$o,
  default: load$o,
  Content: Content$o,
  getHeadings: getHeadings$o,
  getHeaders: getHeaders$o
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$n = {"title":"Voice in Healthcare- Dr. Yared Alemu, CEO, TQIntelligence- Project Voice X","description":"Voice in Healthcare presented by Dr. Yared Alemu, CEO of TQIntelligence, presented on day one of Project Voice X. ","date":"2021-12-09T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981395/blog/voice-in-healthcare-dr-yared-alemu-ceo-tqintelligence-project-voice-x/proj-voice-x-session-dr-yared-alemu-blog-thumb-554.png","authors":["claudia-ring"],"category":"speech-trends","tags":["healthcare","project-voice-x"],"seo":{"title":"Voice in Healthcare- Dr. Yared Alemu, CEO, TQIntelligence- Project Voice X","description":"Voice in Healthcare presented by Dr. Yared Alemu, CEO of TQIntelligence, presented on day one of Project Voice X. "},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981395/blog/voice-in-healthcare-dr-yared-alemu-ceo-tqintelligence-project-voice-x/proj-voice-x-session-dr-yared-alemu-blog-thumb-554.png"},"shorturls":{"share":"https://dpgr.am/e1144ff","twitter":"https://dpgr.am/b716182","linkedin":"https://dpgr.am/a3d61de","reddit":"https://dpgr.am/419114a","facebook":"https://dpgr.am/d2f1fc6"}};
						const file$n = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/voice-in-healthcare-dr-yared-alemu-ceo-tqintelligence-project-voice-x/index.md";
						const url$n = undefined;
						function rawContent$n() {
							return "*This is the transcript for \"Voice in Healthcare,\" presented by Dr. Yared Alemu, CEO at TQIntelligence, presented on day one of Project Voice X.* *The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was **94% accurate.**  Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram's API.  If you want to see if Deepgram is right for your use case, [contact us](https://deepgram.com/contact-us/).*\n\n\\[Yared Alemu:] Hi. Good afternoon, everybody. It's a a bit unfair to go after Henry. That's... I should have been the warm-up part, and then and then he should go. So I'm Yared Alemu. This is not this is not by design. So so if you guys are impressed about the the wording underneath, just... I'm not sure what happened. So I'm a psychologist by training, and we are a start-up at Georgia Tech at the Advanced Technology Development Center, and our work is partly as... is funded by the grant from National Science Foundation. That's phase one and phase two.\n\nSo we're hoping that by the time we finish, we will have some of the tax money, that tune of about one point five million dollars. So... but we're very... you know, we're taking care of your money, so no worries. We're not we're not wasting your... it's just a... we're also partly funded by Google and other... our active collaborators. So one of the things that have a pretty dramatic impact on on all human beings, right, all included, is things that happen very early in life.\n\nAnd so a while ago, the CDC and Kaiser had a bunch of data, and the... so they've done this long, continual studies, right, and and and looked at other things that happen early in life. We... it could be trauma, but it is a more expanded version of trauma because it includes emotional, physical, sexual abuse, includes neglect, parental, substance abuse, and so so a whole range of them. Do they have any impact, the person that's transitioning into or not transitioning to becoming an adult?\n\nThe answer is not only us, but it's devastating. Yeah. So more especially that if you score... you guys can go to our website. If you score four or above, right, the consequences include a twenty years reduction of your life expectancy in addition to multiple mental health as well as other chronic diseases. Right? And there is a reason why. And the reason is trauma is not just an emotional experience. Trauma is primarily a physiological experience. There is a net changes in a number of your organs that directly negatively impacts not just who you are, but also who you becoming. It's it's very difficult for kids who've been through trauma to transition into becoming a a productive member of society. Right? But because we can't see it... right? So so perhaps some sort of an injury that has impacted me from working, if it is visible, people could empathize with that. But if I have an injury, right, the injury is invisible, it it is on multiple of my organs, right, it is... you know, for a group of kids, that becomes... they're externalizing the suffering. Right? So they share that with you. Right? And and and they share that freely. Right? So if you're in a classroom, I'm sure you recognize who the... who they are. They have contact with, you know, the legal system, substance abuse. And then there's another group of kids. They're internalizing it. Right? So that becomes depression, anxiety, self-harm behavior, suicidality.\n\nAnd so so so this is something that, in terms of the cost... right? California decided to account for what is the cost to our system as for... when I say system, the state of California, the cost of ace. Right? The cost is about more than a hundred billion dollars in two thousand thirteen. Hundred billion dollars. Right? So that cost is health care. Right? That cost is incarceration. That cost is emergency room visits. That cost was child protective services. So not addressing this, right, mean, the... I... you know, I can't... I grew up poor. Right? I I don't know... I would rather not been born where I was born. Right? This is not a choice. There's nothing outside of being poor. I mean, people sometimes romanticize and write songs and write books about, you know you know, that made me a better person. No. It's not... you know?\n\nI could have been a a much better person if I did not grew up poor. Right? So what are those things that come with being poor is is that's kinda, passive, traumatic experiences, sometimes, active trauma... traumatic experiences. They will add up and doing a number on a person. I am a byproduct of excellent mental health services. Right? Got lucky from the get-go.\n\nBut there is a group of people, right, there's a group of kids that that is not the case. So there's about forty five million kids that are currently having their health insurance through Medicaid. Right? Forty five million kids. So to qualify for that, right, you have to make, you know, as a family of two, twelve, thirteen thousand dollars or less. So you know what it comes with that kind of income. Right? Not much good happens.\n\nAmong those, there's five million of them, right, receiving mental health services currently, five million, at the tune of two hundred fifty billion dollars a year. That is one of the most expensive services for any person in this country. When I say expensive, right, the money it takes to educate a child per year, right, the amount Blue Cross Blue Shield or these private companies spent per patient per year, if you take all those things, this is the most expensive and the least effective.\n\nThere is absolutely no data right now that, despite this level of investment, there are improvement in outcomes. It is absolutely ridiculous that you pour into that kind of money into a system and you're not asking for data whether the chart is actually... when I say benefit... kids have three jobs. I have two kids. Right? They try not to do those three job, but have three jobs. One is school. Right? Everybody doesn't to be... doesn't have to be an A/B student, but they're the best of their abilities. Right? Two is the relationship in the community, school. Right? And then three is at home, their relationship with their... other members with the family. Three. Right? So when we say that improving outcomes, we're not inventing something special. We're saying they can be able to do these three things. Right? Or or or they will do better on these three things six months from now if I spent, right, five, six thousand dollars on treatment. Right? So so so the the problem what we have and the why that is, right, is we don't really have we don't really have an objective way of measuring and measuring quickly the severity. Right?\n\nWe don't really have an effective way to be able to track outcomes. Right? When I say track outcomes, I'm not talking about this pre-imposed, some fake stuff that these providers do. I'm talking about systematically tracking whether someone is getting better or not. Right? And then and then the other one is changing the model of treatment from episodic. Right? I'm gonna see you today. Until I see you next time. God help you. Right? As if like somehow in between, right, nothing happens. So we don't really have a good way of monitoring when they're not... when not not seen by treatment providers. Right?\n\nThose three things primarily account for why we have significant disparities in mental health treatment. Not just for kids from low-income communities, but people who receiving mental services at all. So what we're doing is... what we've been doing is taking about a forty five to ninety second voice sample and and being able to detect, right, being able to identify the severity, and being able to track whether someone is getting better or not. And so we have pilot sites in three different places currently. Hundred percent of the kids that are now pilot sites are kids from low-income communities. We just started a pilot site in North Carolina with the most severe patient population. These are kids that are... just came out of the hospital, right, trying to keep them from going back to the hospital because that's what happens when you when you don't have good outpatient services.\n\nPeople show up in emergency rooms asking for medication refills. Right? People show up in patient hospitalizations because there's nobody else to be able to monitor their suicide behaviors. Right? They're seeing four, five, six times a week by provider. And so we're collecting data. When we collect the voice sample, this is kinda what makes us different than most other voice companies. For every voice sample we collect, we know the diagnosis. We know what their medication is on. We know for how long they've been in treatment, and then we collect our own data, right, including collecting the ace, the PHQ nine, and another one, the SFSS. Right?\n\nWhenever you work with kids, you have to ask the parents how the hell they're doing. Right? If you don't ask the parents... so so so the... if you look at the research, there's about a thirty two percent agreement between a mentalist provider and a parent on the perception of how bad the problem is. Just thirty two percent. Right? It's very difficult to make progress working with kids without closing that gap, right, closing the gap between the perception between the parent and the provider. So that's why we intentionally select an instruments that allow us to engage parents. Right? Just because you're poor doesn't mean that that you're not gonna be able to engage them in treatment.\n\nThere is no reason, right, for... even if they don't read. Right? Because we have... we... we're in some rural areas in Georgia. Right? The level of deprivation and poverty in those areas are ridiculous. Very limited connectivity. Right? The idea that telemental health services using Zoom is gonna reach there is very unlikely. Right? 'Cause Zoom is not really intended to be able to, you know, be used in those settings that require this... you know, a whole set of these connectivity issues.\n\nSo so we have to... so so all that is intended all that is intended is to be able to build our model, right, and and build it from the ground up, because there's no clinical voice samples for just young people in general. Right? So we have adult voice samples, you know, be... but in terms of having voice samples for this this specific population, it does not exist. So what we do, we collect them through... having all these, we individually label the voice samples. And then we have outside anonymous people that that level the voice samples. These are psychologists. Right?\n\nAnd then we have somebody else that does what they call inter-rater reliability, which means our... between these three or four people that are leveling this voice sample, what the what's the level of agreement? Right? If it is one, that means perfect. Right? And then if it's not any... anywhere about seven... in a point seven or less, we put that voice on on a side. If it's it's point seven or above, but voice sample is included in terms of developing your model. Right? So so it's a... it's data intensive. It's resource intensive. Right? And and it's not quite friendly for for investors to be engaged in in a process that is... so you deal with a high risk. Right?\n\nSo the the area of speech emotion recognition, it's... as as as an emerging science, it's a high risk. Working with and developing solutions for poor people is considered a high risk. Right? But the issue is the... if you really wanna make an impact and go where the money is, right, you go to those areas. Right? Mean... so there's... nobody pays more for mental health than Medicaid, by far combined. Right? So there is... you know, we're not as greedy as any other start-up, but we also see an opportunity to be able to impact an area that attracts very limited innovation. So so if you're thinking about this area, please jump in because, you know, it's not... there's no... we don't see anything getting better for the foreseeable future. Right?\n\nWe don't see a highly trained group of psychotherapist, right, decided to enter this area. Right? And, by and large, part of the reason there are disparity is because the people who come in in those areas to provide services are new therapists that just graduating. Right? They don't even know what they're going. You don't let anyone who doesn't have his license, right, provide any services for you or for you child. Don't. I mean, you know, you don't go to someone who just graduated from from medical school, right, to get a prescription for medication. They have to go to residency.\n\nI mean, there's a whole part of things that are necessary before they're capable of making a decision on your behalf. Mental health should not be any different. Right? People providing services in the publicly funded system, by and large, are unlicensed. Right? They're in our head because they get a lot of hours. Right? So two or three years, they get the license, and they're out the door. So so so so technology, in this case, is is... that we wanna... this is kinda been telling and augmentation that we we cannot solve the problem of people coming in that are unlicensed. But what we can do is we can come up with solutions, in this case, algorithms that allows them to be able to identify, identify it early, right, and then be able to come up with a treatment plan and then follow up to see whether that treatment... all that is is is based on data. Right? So I think I have one more minute. I'm done. Thank you very much, everybody.";
						}
						async function compiledContent$n() {
							return load$n().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$n() {
							return (await import('./chunks/index.c68f373c.mjs'));
						}
						function Content$n(...args) {
							return load$n().then((m) => m.default(...args));
						}
						Content$n.isAstroComponentFactory = true;
						function getHeadings$n() {
							return load$n().then((m) => m.metadata.headings);
						}
						function getHeaders$n() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$n().then((m) => m.metadata.headings);
						}

const __vite_glob_0_253 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$n,
  file: file$n,
  url: url$n,
  rawContent: rawContent$n,
  compiledContent: compiledContent$n,
  default: load$n,
  Content: Content$n,
  getHeadings: getHeadings$n,
  getHeaders: getHeaders$n
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$m = {"title":"Voice in Healthcare - Henry OConnell, CEO, Canary Speech - Project Voice X","description":"Voice in Healthcare  presented by Henry OConnell, CEO of Canary Speech, presented on day one of Project Voice X. ","date":"2021-12-09T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981397/blog/voice-in-healthcare-henry-oconnell-ceo-canary-speech-project-voice-x/proj-voice-x-session-henry-oconnell-blog-thumb-554.png","authors":["claudia-ring"],"category":"speech-trends","tags":["healthcare","project-voice-x"],"seo":{"title":"Voice in Healthcare - Henry OConnell, CEO, Canary Speech - Project Voice X","description":"Voice in Healthcare  presented by Henry OConnell, CEO of Canary Speech, presented on day one of Project Voice X. "},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981397/blog/voice-in-healthcare-henry-oconnell-ceo-canary-speech-project-voice-x/proj-voice-x-session-henry-oconnell-blog-thumb-554.png"},"shorturls":{"share":"https://dpgr.am/ba1a147","twitter":"https://dpgr.am/a63a517","linkedin":"https://dpgr.am/4a4bf4a","reddit":"https://dpgr.am/888a4ec","facebook":"https://dpgr.am/12d0b9a"}};
						const file$m = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/voice-in-healthcare-henry-oconnell-ceo-canary-speech-project-voice-x/index.md";
						const url$m = undefined;
						function rawContent$m() {
							return "*This is the transcript for \"Voice in Healthcare,\" presented by Henry O'Connell, CEO at Canary Speech, presented on day one of Project Voice X.* *The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was **94% accurate.**  Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram's API.  If you want to see if Deepgram is right for your use case, [contact us](https://deepgram.com/contact-us/).* \n\n\\[Henry O'Connell:] First off, I'm I'm grateful to be here. I hope I have a chance to... can you... you can hear me. Right? Ok. I hope I have a chance to speak with many of you. Canary Speech was started a little over five years ago. Historically, Jeff and I, the cofounder, Jeff Adams and myself met thirty eight years ago. I was at the National Institutes of Health doing research in neurological disease, and Jeff was at an institution we're not supposed to name, building models to decode spy messages coming across during the cold war. Jeff's career was much more interesting, so I'm gonna tell you about it. Jeff went on after that government job to work with Ray Kurzweil.\n\nAt the time, natural language processing did not exist. Ray was interested in building it. And Jeff built the first commercial NLP, and then built the basis in commercial... commercialized Dragon NaturallySpeaking. He worked with Nuance for a number of years, building some of their core products, and then went on to build the core products that we use every day on our cell phones for speech to text. At the time, about nine or ten years ago, Amazon was interested in building two products. One never saw the light of day, and the other one is probably the most successful speech product ever launched. It's the Amazon Alexa product. They simply bought the company Jeff was in.\n\nJeff Adams, seventeen speech and language scientist, and Jeff O'Neil, the patent lawyer, went on to form the core that built the Alexa product far-field speech. Approximately, a thousand patents were prosecuted, and the team graduated into about a hundred and fifty individuals in speech and language. Shortly after that, about a year after that, Jeff and I got together to create Canary Speech specifically to build the next generation of technology that could commercialize in the health care space.\n\nSo our our goal was simple. We really wanted to advance speech and language in such a way that it could be commercialized in a practical sense in the health care market, providing actionable information to clinical teams in the process of doing a diagnosis. Over this period of time Jeff and I founded it, we've been issued eight patents. We have six pending patents. We have patents in Japan, in Europe, and here. On our core patents, and believe me, there are thousands of speech and language patents. On our core patents in all three jurisdictions, we received a hundred percent of our claims. We believed the approach previously would never be commercialized in this space, not in the clinical space providing real-time actionable information that was accurate enough to contribute to diagnosis.\n\nWe're operating today in over a dozen hospitals, including in Mandarin, Japanese, we're working in both England and in Ireland and in... and in the in the United States in in English. We have models built for depression, anxiety, stress, tiredness, a range of cognitive functions from cognitive, mild cognitive impairment, and Alzheimer's. Our studies in Alzheimer's and cognitive function are are on all major continents today. We're functioning uncognitive in China, Japan, and in Europe. Hold the mic higher? You mean nothing I've said has been heard? \n\n\\[SPEAKER 2:] No. \n\n\\[Henry O'Connell:] Could you have waved at me earlier? Thank you. I apologize. I've never ever been told I talked too soft. We recently... this year, we added several key people to our team. So we added a Chief Technology Officer for the last ten years. He was in the senior technology position at Nuance. We brought on Gavin, who's in our Dublin office now. Gavin for the last ten years was with Amazon. Caitlin came on about two years ago and has a dozen years of experience in health care. Our scientific team is run by Namhee and by Samuel. And then we just recently brought in, today actually was his first day, David.\n\nWe're currently processing millions of datasets a month. Our our rollout in Japan included ten million individuals. We're analyzing those individuals for both stress and cognitive function in an annual health care call, backing up and and replacing the MMSI, the GAD seven, the \\[unsure], and several other tests that took twenty five minutes to do. And we're running those tests in less than two minutes from the same sample and measuring that entire range of test.\n\nWhat we wanted to do with the technology was to streamline the information that contributes to patient diagnosis. Diagnosis are not done by tests. Diagnosis are done by clinical teams after they have considered the information they're provided. We're part of that information stream. We wanted to reduce readmissions in hospitals. Well over half of the applications we have are post-discharge applications, both for hospitals and in telemedicine. We wanted to provide immediate actionable information that could be used to assess an individual's health condition and provide clinical guidance for their welfare. From the point of which we capture an audio to the time in which it returns to device is less than two seconds.\n\nWe're processing more than twelve million data points a minute. We do that in near real time through our system and return that information to device. So if an individual is in a telemedicine call, they're getting guidance from us while they're in the call. We do not identify at the word level. Everything we're doing is at subword level. We take a model that we've built for depression in English. It has a performance level of better than eighty percent in Japanese.\n\nWe then tweak it, qualify it, retrain it, and deploy it in Japanese. We did the same thing from there to Mandarin Chinese. This is just a diagram of kind of the flow. So today, we're commercialized in English, Japanese, and and Mandarin. We'll commercialize later this year in in Spanish. Our models are language agnostic, so our commercialization in various languages really has to do with deploying our tools in the language of the user. So we generally are b to b. The b to c or or b to client or b to partner engagements that we have really are for validating new models. So when we're working with a with a a large hospital partner, we're validating and and peer reviewing the models that we have. And also building those into the process and flow that makes them useful for the clinical teams. If you go on Apple or Android, you'll find you'll find Canary Speech research app. It's not deployed to to the public. We only deploy it within a partnership.\n\nThere will, over the next couple of months, be a couple more apps. We can, in real time, measure both stress, anxiety, depression kind... we don't call it mood because we're actually measuring stress and anxiety. But we also have a new model for energy. So imagine I went to our scientist and I asked them if they knew the cartoon for the the series Winnie the Pooh, and they told me no. They were born and raised in Korea. And and I said, let me send you some videos. So I sent them videos, and we got back together the next day. And I said, the Winnie the Pooh series has been so popular because it represents such a wide range of human emotions.\n\nYou could think of it, you know, between Eeyore and Tigger and Piglet, all the different emotions that it demonstrates. I said, when you're talking to me, you're aware that I'm excited about what I'm doing. I don't say to you I'm excited about what we're doing. There's a musical element of speech that's lost when you do ASR that conveys a whole range of human emotions and condition.\n\nAmong the primary dataset you apply these tools, ASR is wonderful, then you get a bunch of words at the top. Those words represent the spoken language, but not necessarily the energy or the emotion that was in that when the individual spoke it. What we're doing is is mining the primary data layer for all of those elements and many other things. But imagine you... many of you have families.\n\nMy family is five children, my wife and I, too many animals to speak about, and and I really, truly mean that we live on large acreage. We have horses and goats and chickens and things that I don't wanna mention beyond that. But my five kids are are old enough now. They're married, and they're out of home. When they were in the house, and they would come home from from school after practice, whatever the sport was, their gait across the room would indicate to me much about how the day went. When they turned and looked at me, I could gather from their facial expressions whether it was a good day or a bad day. \n\nAll of us do this, and all of us all of us have had this done for us. They're out of home now. They're all married. They're having their own families, young children, my wife and I, our grandchildren. When I call my daughter within moments, and I I could promise you it's irritatingly accurate, I'll say to her, what's up? I know whether her day is a good day or a bad day or she's anxious or she's sad or she's nervous or she's angry. And it has nothing to do with the word she has spoken. And we know that. Doctors know that. Their whole lives center around doing that. The elements that tell us that are also part of the most complex motor function the human body produces called speech.\n\nI believe that speech is more complex than all other data produced by the human body, except the genome itself. In the twelve million data points that we analyze every minute, it's deep in information. And we're only scratching the surface of that. Let me give you some practical examples, and I'll finish up. I don't wanna talk about any of this. So Hackensack Meridian is one of the hospitals we were working with. We currently have five ongoing validations with them. I just wanna talk about one of them because it's not one that I would have expected to be on the top of the list. Congestive heart failure.\n\nSo Hackensack has between thirty five and forty thousand congestive heart failure patients annually. They have about a twenty one percent return. About sixty percent of those don't make it. They lost nineteen point two million dollars last year in insurance payables because of the twenty one percent return. When I talk with Elliot Frank, who is in charge of the organization, what he said to me was, Henry, there's nothing good about this. It's not good for the hospital. It's not good for our patients. It's not good for our staff.\n\nIt's... there's nothing good about it. What they do is they post discharge, they're calling these these patients twice a week for the first month, and then once a week thereafter. They ask them twelve questions. We took the twelve questions and we integrated them into our app. Our app is not just HIPAA compliant, which is important, but our app is externally audited for vulnerability penetration CIS and SOC two type one. And this week, it's being submitted to the FDA after six months of discussions with them for five ten k clearance.\n\nWhat we did was put the twelve questions into the app. The telephone call can be made from a tablet by a clinical partner, a nurse, or or a similar individual to, let's say, Mary at home. Mary answers the phone, and she's now a component of a clinical call, a HIPAA-compliant environment. In addition to the twelve questions, during the conversation, we provide information on stress, anxiety, depression, tiredness, pulmonary sounds, and changes in vocal patterns, all of that in real time in the background. Changes in vocal patterns are an indicator of a coronary heart disease three days in the future. They're frequently observed. I promise you that clinical team is listening for that.\n\nBut nursing might be making fifty calls a month to fifty different people. It's hard for her to determine or anyone if there's been vocal pattern changes from call one to call two to call three. Our models are perfect at it. They're not even good. They're perfect at it. And we can give her real-time real-time warnings that vocal patterns in Mary's language have changed. In addition to that, we're listening to pulmonary sounds. Changes in pulmonary sounds can be indicative of a pulmonary tract infection like pneumonia. So in real time, we're not only telling her that that sounds are different. We're telling her how different they are from the first call. Because maybe Mary has other problems. Maybe she smoked for fifty years. I don't know. But we're looking at deltas from first to second to third call, and we're doing that in real time with them.\n\nThe goal is to reduce return to admissions initially to fifteen and then to ten and then to drop it from there. So in in very many real ways, voice is being applied to augment the dataset that our clinical teams have to serve the patient population to improve the quality of life and quality of care they receive. I wanna finish up here. We're deploying, as you can imagine, in telemedicine as well for a whole range of different types of applications. Our belief has been that in order to make this a practical solution for the health care environment, it had to be specific. It had to be accurate. It had to be fast real time.\n\nAnd it had to provide actionable information that could make a difference in the treatment of patients. The best way to do that was to partner with literally dozens of health care facilities around the world. We're fortunate that when we talk with groups like this, voice is understood to be a valuable dataset that they can implement in the... their day-to-day treatment of patients. I wanna thank you for your time. I'm certainly here through Wednesday. If anybody would like to talk, I... and I'm terrible at it. If you approach me, I'll probably keep you longer than you wanna stay. Just wink or something, and I'll shut up and let you go. So thank you very much, and please take care. Thank you very much.";
						}
						async function compiledContent$m() {
							return load$m().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$m() {
							return (await import('./chunks/index.3aeffe28.mjs'));
						}
						function Content$m(...args) {
							return load$m().then((m) => m.default(...args));
						}
						Content$m.isAstroComponentFactory = true;
						function getHeadings$m() {
							return load$m().then((m) => m.metadata.headings);
						}
						function getHeaders$m() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$m().then((m) => m.metadata.headings);
						}

const __vite_glob_0_254 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$m,
  file: file$m,
  url: url$m,
  rawContent: rawContent$m,
  compiledContent: compiledContent$m,
  default: load$m,
  Content: Content$m,
  getHeadings: getHeadings$m,
  getHeaders: getHeaders$m
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$l = {"title":"Voice Technology and the Changing Landscape of Customer Experience","description":"Customer experience is a major differentiator for companies, and voice is providing new ways to drive incredible customer interactions.","date":"2022-02-08T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981407/blog/voice-technology-customer-experience/voice-tech-and-changing-landscape-of-CX-thumb-554x.png","authors":["chris-doty"],"category":"speech-trends","tags":["voice-strategy","voice-tech"],"seo":{"title":"Voice Technology and the Changing Landscape of Customer Experience","description":"Customer experience is a major differentiator for companies, and voice is providing new ways to drive incredible customer interactions."},"shorturls":{"share":"https://dpgr.am/7989905","twitter":"https://dpgr.am/7f075ac","linkedin":"https://dpgr.am/9f5e1b3","reddit":"https://dpgr.am/adbdc69","facebook":"https://dpgr.am/b307a50"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981407/blog/voice-technology-customer-experience/voice-tech-and-changing-landscape-of-CX-thumb-554x.png"}};
						const file$l = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/voice-technology-customer-experience/index.md";
						const url$l = undefined;
						function rawContent$l() {
							return "\r\nCustomer experience is a major differentiator for companies. And with the rise of voice technology, businesses are gaining an improved ability to better understand customer needs and sentiment. As customer conversations are analyzed, companies will be able to more quickly and easily identify appropriate actions to take to improve customer experience. As Amy Brown, founder and CEO of [Authenticx](https://authenticx.com/), said in the recent webinar [**The Importance of Voice Technology for Customer Experiences**](https://offers.deepgram.com/importance-of-voice-technology-for-customer-experiences-on-demand), her company was founded to \"help humans understand humans\" with technology, including voice technology. This is ultimately the goal of any implementation of voice tech in the realm of customer experience: to empower your employees to better understand your customers and support them. Let's take a look at four of the ways that voice technologies are changing the landscape of customer experience, now and in the coming years.\r\n\r\n## 4 Ways that Voice Tech is Improving Customer Experience\r\n\r\nThere are a range of possible impacts that voice can have on customer experience, depending on where a company is at in adopting these technologies, the abilities of the particular tools that a company is using, and what the company's overall goals are.\r\n\r\n### 1\\. Turning Audio into Text\r\n\r\nOne of the primary ways that voice technology is impacting customer experience is by providing new avenues to analyze customer conversations. Audio data is notoriously difficult to deal with; if you want to extract insights for your organization, you need to turn that audio into text. Through [automatic speech recognition](https://blog.deepgram.com/what-is-asr/) (ASR), businesses can create transcripts of spoken conversations in order to get insight into customer interactions. Transcripts create a searchable database of conversations that can be mined for information to improve products, services, and customer support. This text data can also be used for things like internal machine learning projects.\r\n\r\n<WhitepaperPromo whitepaper=\"latest\"></WhitepaperPromo>\r\n\r\n\r\n\r\n### 2\\. Real-time Listening\r\n\r\nAnother way that voice technology is impacting customer experience is by giving businesses the ability to listen to and understand customer conversations in real-time. This allows companies to address customer concerns and questions as they are happening. Real-time listening can be used to, for example, help agents on the phone easily find the right information quickly, or suggest marketing assets to sales team members at the right time in their conversations with prospects. As customer service becomes more [conversational](https://deepgram.com/solutions/voicebots/), businesses will be better equipped to provide excellent customer support by using real-time listening to understand what's being discussed and empower agents to more quickly and easily respond.\r\n\r\n### 3\\. Understanding Emotion, Not Just Words\r\n\r\nBy understanding customer emotions and needs, businesses can take actions that improve customer experience. Businesses can do this by finding ways to understand customer emotion, not just the words their customers say. [Emotion recognition](https://blog.deepgram.com/sentiment-analysis-emotion-regulation-difference/), for example, aims to determine how a speaker feels emotionally, rather than what it is they're saying. Emotion recognition can be further augmented by [sentiment analysis](https://blog.deepgram.com/sentiment-analysis-emotion-regulation-difference/), which seeks to understand whether a person feels positively or negatively about a topic. Most sentiment analysis tools rely only on text, but in the future, voice tech will help businesses look at factors such as tone, pitch, and intensity in order to gauge how a customer feels about a product or service and to conduct emotion recognition. This information can then be used to help businesses improve customer interactions, customer service, and the products and services that they offer.\r\n\r\n### 4\\. Virtual Assistants\r\n\r\nVirtual assistants are another way that voice technology is impacting customer experience. With virtual assistants, businesses are able to provide a personal customer experience through automated customer support. This can be done by answering customer questions and providing help with simple tasks such as reordering products, making reservations, and changing addresses. Although the current generation of virtual assistants can be a pain to deal with-if you've ever shouted, \"Agent!\" over and over into your phone, you know what I mean-the insights that can be derived from smart voice tech can help eliminate these problems, driving smoother experiences that let customers solve their own problems. This helps keep customers happy while letting customer service agents focus on the complicated, high-value cases that AI isn't as good at handling. Virtual assistants are also able to learn about customer preferences over time, which allows them to provide an even more personalized customer experience, building customer loyalty over time.\r\n\r\n## Looking Ahead\r\n\r\nAs customer service becomes more conversational and voice technology adoption continues to grow, businesses will be better equipped to provide excellent customer support. The future of customer experience is looking bright, and with the help of voice technology, we can expect even more improvements in the years to come.  If you'd like to learn more about how voice tech is changing the face of customer experience, register for our upcoming webinar, [The Future of Voice Technology to Improve the Customer Experience](https://offers.deepgram.com/the-future-of-voice-technology-in-customer-experience), to get expert insights into where the field of customer experience is going, and how voice technology is going to be a major part of that future.\r\n";
						}
						async function compiledContent$l() {
							return load$l().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$l() {
							return (await import('./chunks/index.c9d65b76.mjs'));
						}
						function Content$l(...args) {
							return load$l().then((m) => m.default(...args));
						}
						Content$l.isAstroComponentFactory = true;
						function getHeadings$l() {
							return load$l().then((m) => m.metadata.headings);
						}
						function getHeaders$l() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$l().then((m) => m.metadata.headings);
						}

const __vite_glob_0_255 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$l,
  file: file$l,
  url: url$l,
  rawContent: rawContent$l,
  compiledContent: compiledContent$l,
  default: load$l,
  Content: Content$l,
  getHeadings: getHeadings$l,
  getHeaders: getHeaders$l
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$k = {"title":"Voicebots Will Enhance Your Life (Not Destroy It)","description":"Voicebots are getting better and are moving to more human-like conversations instead of command and control. Learn from the experts at the cutting edge of voicebots.","date":"2021-07-16T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981379/blog/voicebots-will-enhance-your-life-not-destroy-it/voicebots-enhance-life%402x.jpg","authors":["keith-lam"],"category":"speech-trends","tags":["conversational-ai","nlp","voicebots"],"seo":{"title":"Voicebots Will Enhance Your Life (Not Destroy It)","description":"Voicebots are getting better and are moving to more human-like conversations instead of command and control. Learn from the experts at the cutting edge of voicebots."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981379/blog/voicebots-will-enhance-your-life-not-destroy-it/voicebots-enhance-life%402x.jpg"},"shorturls":{"share":"https://dpgr.am/8317646","twitter":"https://dpgr.am/35ef0b4","linkedin":"https://dpgr.am/999c225","reddit":"https://dpgr.am/f97324a","facebook":"https://dpgr.am/023130c"}};
						const file$k = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/voicebots-will-enhance-your-life-not-destroy-it/index.md";
						const url$k = undefined;
						function rawContent$k() {
							return "We have been lucky to have two interesting and informative panel discussions on the evolution, current state, and future predictions of Conversational AI voicebots. \n\n* [What makes a Great Conversational AI Experience](https://offers.deepgram.com/what-makes-a-great-conversational-ai-experience-webinar-on-demand)\n* [Evolution of Voicebots: From Command and Control to Human-Like Conversations](https://offers.deepgram.com/evolution-of-voicebots-panel-webinar-on-demand)\n\nOur panelists included some of the pioneers of voicebots like Al Lindsay, former VP that lead the technical creation of Amazon Alexa, and companies that are creating a more human-like voicebot experience: Bitext, Elerian AI, OneReach AI, Uniphore, and Valyant AI. Our discussions were not about the Command and Respond type smart speakers (Amazon Alexa, Siri, and Google) but on applications that sound, feel, and respond like speaking with a human; where you feel you can have a more natural conversation with the machine.  One case study example was a customer that continually said \"Thank You\" to the AI voicebot because the interaction was so human-like. Some interesting takeaways discussed:\n\n* Implementing a human-like voicebot is difficult and you have to be committed\n* Currently, there is no unified voicebot that can speak on any topic, in all dialects, and in all languages, so each voicebot must be built for a specific use case and domain.\n* The foundation of human-like voicebots is having a great speech-to-text application.\n* Conversational AI voicebots cannot replace humans and will enhance their jobs\n\n## **Commitment to Voicebots**\n\nPlug and play voicebots don't exist yet as each voicebot must be trained to the domain and use case of the organization.  For example, you can't use a voicebot trained for banking in England for food ordering in the U.S.  The voicebot will not understand the accents, dialects, and terminology of the customer nor can it respond correctly.  And you may want your text-to-speech engine to have a British accent instead of an American one.  Remember, these conversational AI voicebots are not simple apps giving directions where it only has to respond with  20 different sentences.  Kevin Fredrick, the Managing Partner of OneReach.ai, expressed this best when he said, \"Building a Conversational AI voicebot is like planning to summit a mountain.  Those who are looking for an 'easy button' get frustrated and quit.  The ones who think it will be too hard, don't ever start.  It is the ones who know the challenge is worth it and have the right partners and use the right tools who make the summit.\"  Although it is not easy, the rewards and return on investment when done correctly are worth it.  Better customer satisfaction with less waiting on hold, faster answers to customer's questions, more opportunities for add-on sales, and freeing your human agents for more difficult tasks are some of the outsized benefits.\n\n## **A Unified Voicebot**\n\nUnlike the movies, there is no Conversational AI voicebot that knows everything.  If you have seen the movie, \"HER,\" Samantha may be the ultimate personal voicebot. Unfortunately, even with cloud computing, big data, and ultrafast connections, we cannot put all knowledge into one voicebot.  For some voicebot companies, that may be the ultimate goal.  Think of a voicebot that you can have a conversation with about finance and soccer and it remembers the previous conversations you have had and refers back to them.  It can combine what it learned from current events, previous conversations with others, and provide you recommendations and responses. Currently, you need one voicebot trained for one domain and use case.  But, as Antonio Valderrabanos, CEO of Bitext indicated, you may be able to combine a multitude of voicebots to get larger ranges of knowledge and conversations.  So, how do we get there?  Our experts think it will take both a large breakthrough innovation and a bunch of smaller innovations along the entire voicebot workflow from speech recognition to text to speech to create the unified voicebot.  ​​So, when you find yourself getting upset with Alexa or Siri, remember they are still a long way off from Samantha. They're only capable of so much.\n\n## **Great Speech to Text is the Foundation**\n\nDion Millson, CEO of Elerian AI summed it up best when he said, \"For Conversational AI voicebots, it all starts off with speech recognition, if you don't understand what the person said and transcribe it to text accurately, you are not in the game. Unfortunately, the general ASR models standardize around 70% accuracy, and it is just not good enough to respond to a caller with real-time accuracy and relevance. Our partnership with Deepgram and their models in conjunction with our internal models that are trained on case-specific data get well over 90% accuracy.\"  He further said that some words are much more important than others in a specific use case.  For his banking customers, the account numbers, phone numbers, and government ID numbers are vitally important for the voicebot to provide the right response.  You cannot be 70% accurate on these keywords, you need to be closer to 100% correct or the whole system fails.  Inaccurate transcriptions sent to the artificial intelligence knowledge base will lead to incorrect responses or a \"please repeat that \" request.  The foundation must be a highly accurate speech recognition solution that can be trained on the keywords for that use case.\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-how-deepgram-works\"></WhitepaperPromo>\n\n## **Enhancing Humans**\n\nYes, robotics and machines have replaced humans in some roles, mostly in manufacturing, when they are doing one task or more repetitive tasks.  For Conversational AI voicebots, we asked our panel if humans will be replaced by voicebots.  Our answer was a resounding NO.  Jason Curran, Head of Engineering at Valyant AI, a voicebot company focused on food orders, sees their voicebot as enhancing the drive-thru experience for the human clerk.  Now, the clerk can focus on making sure the order is correct and providing great customer service instead of having to type in orders. Jason believes that his voicebots help a clerk's job satisfaction by eliminating the more mundane and repetitive tasks. This holds true for Elerian AI and Uniphore's voicebots as well. They help contact centers increase their productivity and satisfaction of their support and sales staff by removing them from answer the same questions over and over again like, \"What is my balance?\", \"How do you turn on the router?\", \"When will my order be sent?\", or \"How much is the warranty plan?\". Now, the staff can work on problem-solving more challenging issues or working on selling a higher value product; i.e. they can use their minds instead of saying repeating things by rote.   Current voicebots are not going to replace humans on the phone or drive-thru speakers anytime soon, nor will they be an all-knowing entity.  There is still a ways to go before we have a Samantha. For the full conversations, you can view the on-demand below:\n\n* [What makes a Great Conversational AI Experience](https://offers.deepgram.com/what-makes-a-great-conversational-ai-experience-webinar-on-demand)\n* [Evolution of Voicebots: From Command and Control to Human-Like Conversations](https://offers.deepgram.com/evolution-of-voicebots-panel-webinar-on-demand)";
						}
						async function compiledContent$k() {
							return load$k().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$k() {
							return (await import('./chunks/index.b0d8215f.mjs'));
						}
						function Content$k(...args) {
							return load$k().then((m) => m.default(...args));
						}
						Content$k.isAstroComponentFactory = true;
						function getHeadings$k() {
							return load$k().then((m) => m.metadata.headings);
						}
						function getHeaders$k() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$k().then((m) => m.metadata.headings);
						}

const __vite_glob_0_256 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$k,
  file: file$k,
  url: url$k,
  rawContent: rawContent$k,
  compiledContent: compiledContent$k,
  default: load$k,
  Content: Content$k,
  getHeadings: getHeadings$k,
  getHeaders: getHeaders$k
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$j = {"title":"We Raised $25 Million from Tiger Global and Others to Unlock the Power of Voice Data and Fuel the World’s Big Ideas","description":"Deepgrams excited to announce a $25 million Series B, led by Tiger Global, to ensure we can continue the momentum built in 2020. Learn more.","date":"2021-02-03T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981360/blog/we-raised-25-million/we-raised-25M%402x.png","authors":["scott-stephenson"],"category":"dg-insider","tags":["machine-learning","voice-strategy"],"seo":{"title":"We Raised $25 Million from Tiger Global and Others to Unlock the Power of Voice Data and Fuel the World’s Big Ideas","description":"Deepgrams excited to announce a $25 million Series B, led by Tiger Global, to ensure we can continue the momentum built in 2020. Learn more."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981360/blog/we-raised-25-million/we-raised-25M%402x.png"},"shorturls":{"share":"https://dpgr.am/127b460","twitter":"https://dpgr.am/cccd3d6","linkedin":"https://dpgr.am/9336669","reddit":"https://dpgr.am/02c25cb","facebook":"https://dpgr.am/4e9095b"}};
						const file$j = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/we-raised-25-million/index.md";
						const url$j = undefined;
						function rawContent$j() {
							return "Over the past year, we have seen the speech recognition market evolve like never before. When we first announced our [Series A](https://blog.deepgram.com/deepgram-series-a/) in March 2020, enterprises were starting to recognize the impact a tailored approach to speech could have on their business. Yet, there was no \"race to space\" moment driving companies to adopt a new solution, especially when their existing provider was working \"fine.\" That quickly changed when COVID-19 hit. Companies were at an inflection point and forced to fast track digital transformation initiatives, compressing years of well-thought-out plans into mere months, and quickly transitioning teams to a remote workforce. The pandemic exposed the cracks in the foundation-while many enterprises were loosely prepared to adopt a remote workforce, the technology in play was not adequate. Voice enabled technology for example, which was considered a \"nice to have\", quickly became a necessity for many businesses. **At Deepgram, we take our role in this evolution very seriously. We are on a mission to set the standard for how to build voice enabled applications, and help enterprises differentiate themselves.** \n\n<WhitepaperPromo whitepaper=\"latest\"></WhitepaperPromo>\n\nBig data and cloud computing has allowed us to collect massive amounts of customer and employee data from emails, forms, websites, apps, chat, and SMS. This structured data is what companies can currently see and use, and it's just the tip of the iceberg. 90% of all data in the world is unstructured-this includes audio, images and video not easily put into databases. We believe that voice data presents businesses with a significant opportunity to gain more insight into business, customers and markets, and we are eager to help enterprises unlock the potential hidden within their voice data. With that, **we are excited to announce that we have raised a $25 million Series B led by Tiger Global with participation from Citi Ventures, Wing VC, SAP.io and NVIDIA Inception GPU Ventures to ensure we can continue the momentum built in 2020.** \n\n*\"Voice is the largest treasure trove of enterprise data waiting to be unlocked. Deepgram is modernizing speech recognition for the modern enterprise, making it fast and simple to unearth valuable data from any conversation, meeting or customer interaction,\" Zach DeWitt, Partner at Wing VC. \"Today's speech recognition technology is designed for simple queries, think your Alexa or Siri, but enterprise speech is more complicated. You have multiple people speaking over each other with industry-specific jargon. Deepgram's end-to-end deep learning approach is revolutionizing what data can be extracted from voice, providing enterprise customers unparalleled accuracy and scalability.\"*\n\n## Now is the Time for Tailored Speech\n\nAt Deepgram, we have focused our efforts on reinventing [Automatic Speech Recognition (ASR)](https://deepgram.com/product/overview/) because the old way just doesn't work anymore. Speech is messy! Think about the endless video calls we have been on the past year-you have multiple employees, often speaking at the same time, with various background noise and poor microphones to account for. All of these factors significantly impact the quality of the voice recording, which ultimately impacts the types of insights that you can gather from your transcription. With advancements in Artificial Intelligence (AI) and Natural Language Processing (NLP), the ability to deliver highly accurate transcriptions in real-time is finally achievable. We train our [speech models](https://deepgram.com/product/train/) to learn and adapt under complex, real-world scenarios, taking into account customers' unique vocabularies, accents, product names, and background noise. \n\nThis enables us to provide our customers with transcriptions that are over 95% accurate, without having to compromise on any other dimension, meaning they can spend less time sifting through audio files and transcriptions to gather actionable insights. This new funding will allow us to continue to strengthen our lead over competitors in key market differentiators like accuracy, speed, scale, and affordability. We have spent the last year investing in key capabilities across data acquisition, labeling, model training, our Application Programming Interface (API) and we're ready to scale. \n\nThis new funding will support our efforts to deliver higher accuracy, improved reliability, real-time speeds and massive scale at an affordable price for our customers. In the next year, we will also grow our team even more and deliver an easy self-service experience for our customers. Our hope is to make it easier than ever for enterprises and software companies to use Deepgram to quickly build new voice-enabled experiences, easily integrating voice into their existing business processes.\n\n*\"Deepgram is one of the early members of Inception, NVIDIA's virtual acceleration platform for AI startups,\" said Jeff Herbst, Vice President of Business Development and Head of NVIDIA Inception GPU Ventures. \"They've made significant contributions to the field of conversational intelligence, particularly as it relates to GPU-accelerated automated speech recognition, and we're delighted to extend our relationship with an additional financial investment.\"*\n\n*\"Almost every company is becoming a software company, and all companies need to listen and respond to their customers and employees. Deepgram enables enterprises to build new and compelling voice experiences for our increasingly digital world,\" said John Curtius, Partner at Tiger Global. \"We look forward to partnering with Scott and the entire Deepgram team as the company continues to grow and unlock speech for all enterprises.\"* \n\nThis funding comes on the heels of an extremely busy, transformative year for the Deepgram team. We welcomed 47 new employees in 2020 including several executive team members, growing our headcount 5x, processed more than 100 billion spoken words this year (that's equivalent to a few millennia), launched a new training capability, [Deepgram AutoML](https://offers.deepgram.com/whats-new-auto-ml-on-demand), to further streamline AI model development and helped customers realize the untapped potential hidden within their audio data. \n\n*\"Deepgram's ability to create custom voice-recognition models made the decision to bring the team on as a technology partner a no-brainer for us,\" said Adam Settle, Vice President of Product at [Sharpen](https://offers.deepgram.com/sharpen-case-study). \"The ASR platform pushes its models to perform under complex, real-life conditions, which helps our customers achieve vastly improved accuracy rates without compromising on the accuracy or speed of the transcription. Plus, with the speech recognition element of our platform squared away, our data science and development teams can focus on the tools and analytics that our customers require to be successful.\"* \n\nWe're excited about this next chapter of Deepgram's story and can't wait to share the future milestone moments that we accomplish with our customers along the way. With today's advancements in technology, voice has the ability to become the next programmable interface, and through Deepgram, enterprises have the power to use speech data to power the next set of big ideas and experiences. Voice is the future of the enterprise, and we're looking forward to helping companies capitalize on their audio data.\n";
						}
						async function compiledContent$j() {
							return load$j().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$j() {
							return (await import('./chunks/index.b8a64bd5.mjs'));
						}
						function Content$j(...args) {
							return load$j().then((m) => m.default(...args));
						}
						Content$j.isAstroComponentFactory = true;
						function getHeadings$j() {
							return load$j().then((m) => m.metadata.headings);
						}
						function getHeaders$j() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$j().then((m) => m.metadata.headings);
						}

const __vite_glob_0_257 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$j,
  file: file$j,
  url: url$j,
  rawContent: rawContent$j,
  compiledContent: compiledContent$j,
  default: load$j,
  Content: Content$j,
  getHeadings: getHeadings$j,
  getHeaders: getHeaders$j
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$i = {"title":"What Are the Top Mistakes in Deep Learning? — AI Show","description":"In this episode of the AI Show, we explore some of the most common deep learning mistakes.","date":"2018-12-07T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1662069470/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/placeholder-post-image%402x.jpg","authors":["morris-gevirtz"],"category":"ai-and-engineering","tags":["deep-learning"],"seo":{"title":"What Are the Top Mistakes in Deep Learning? — AI Show","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1662069470/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/placeholder-post-image%402x.jpg"},"shorturls":{"share":"https://dpgr.am/655d533","twitter":"https://dpgr.am/1fd794b","linkedin":"https://dpgr.am/8abb8be","reddit":"https://dpgr.am/6f2d31f","facebook":"https://dpgr.am/9c3f4af"}};
						const file$i = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-are-the-top-mistakes-in-deep-learning-ai-show/index.md";
						const url$i = undefined;
						function rawContent$i() {
							return "**Scott:** Welcome to the AI Show. Today we're asking the question: What are the top mistakes in deep learning?\n\n**Susan:** We've got huge ones! We make them all the time.\n\n**Scott:** The mistake is the rule in deep learning. You make nine mistakes and one maybe good move.\n\n**Susan:** How are you gonna find new things if you don't make mistakes?\n\n**Scott:** It's frontier tech, right?\n\n**Susan:** There's just so many fun pitfalls you can fall into that everybody's fallen into.\n\n**Scott:** What do you think the big areas are?\n\n**Susan:** I think that there's really two major areas. There's just analyzing the problem; the basics of analyzing whatever problem you're going down and the pitfalls around there. Then there's the model and data. If you don't analyze your problem correctly then it really doesn't matter what you do with the model and data. You've fallen off the tracks. Once you finally got a good understanding you can get down there and fall into brand new pitfalls. Have you fallen into any analysis problems there, Scott?\n\n**Scott:** I'd say there's too many to talk about, but yes. Analyze the problem, make sure your data is good, these are all good traps. Make sure your model is actually a model that we'd actually be able to perform the task you care about and then in the end you actually have to train it too. Training has its own history, as well. Probably I can name some things that I've done that are not good. Some of it just when you're doing deep learning you're doing programming. Programming is hard. Copy pasting, you're reusing old code, you're iterating as you go along and you're using the problems and errors that you're getting to help you along until you finally get water through the pipes.\n\n**Susan:** That's really important: water through the pipes.\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976785/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/business-equipment-factory-357440.jpg)\n\n**Scott:** But usually once you first get water through the pipes and you get your first result everything is totally wrong anyway, so you're backtracking. Okay, where did that mistake come in?\n\n**Susan:** Comes out more mud than water.\n\n**Scott:** How do you get the pieces to fit together in the model at all? How do you get even a few pieces of data into the right form to be jammed into the model? Then the model finally has to output something and its output doesn't make any sense at all. But biggest thing is trying to learn an impossible task.\n\n**Susan:** I've done that.\r\n## Don't try to do something that is impossible.\r\n\n**Susan:** I've a fairly good rule of thumb for that one. If you as a human looking at your training data can't learn the task, then I can pretty well guarantee you that no matter how good of a model builder you are it's probably not gonna be able to learn that task.\n\n**Scott:** Even if you think you can learn it, but if you ask somebody else and they're like, no that's a totally different thing. This is something like summarizing a scientific article and its jammed full of information. People would summarize it in different ways. Maybe that's not an impossible task, but that's a very, very, very hard task.\n\n**Susan:** The big point here is: pick battles that are winnable. Make sure you're doing the steps necessary to figure out that this is a winnable battle. Don't let yourself believe that magic black box that is deep learning will be able to learn anything so long as you throw in enough data at it.\r\n\n**Scott:** There's so many ways that it can go wrong. You don't want the number one reason of just this isn't even possible to take you down. Pick something that maybe a human could do. This is a good rule of thumb. If a human could do it with maybe a second worth of thinking and come out with a reliable answer every time, maybe that's something a machine could do.\n\n**Susan:** The second half of that is truly important. A reliable answer. What does that mean? An objective answer. Something that if you had ten humans and you asked them the same question, you would get probably nine of them at least agreeing. Because if you have five of them saying one thing and five of them saying another thing, even if they're super positive that's the right answer, a machine's gonna have a very hard time figuring out what side of those five to be on.\n\n**Scott:** Yeah ask ten people what five times five is. Okay, maybe you can teach a machine how to do that. We know that for sure. But ask them what the meaning of life is or who won the argument.\n\n**Susan:** Was that dress blue or was that dress ... Well maybe that one a machine could figure out perfectly.\n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976785/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/The_Dress_-viral_phenomenon-.png)\r\n\n## Trying deep learning first.\r\n\n**Susan:** Deep learning thinks it's everything.\n\n**Scott:** Pump the brakes here a little bit.\n\n**Susan:** What is the simple approach? Also, you'd be surprised, just the most basic of algorithms can solve some really, really hard problems, or what we think of as hard problems. Just add it all up and find the mean and suddenly you got 90% of your answer in a lot of cases. Don't skip to the most complex possible huge-chain answer that you can right off the bat.\n\n**Scott:** You have to load up a ton of context and tools and the know-how in your brain and time and the iteration cycle on deep learning is so long that it's hard. You could be doing something with just regular stats or traditional tree based machine learning technique or something that would go through the data and you'd have a result in five minutes. That often is better than waiting two days for deep learning. You want to get on that track, like the five minutes.\n\n**Susan:** Even then, just doing a simple least squares fit. I've done that so many times and that is an amazingly great, quick way. Say, hey, this is a tackle-able problem. Also, sometimes the answer is right where you need it to be.\r\n## Don't throw away your standard stats and machine learning techniques.\r\n\n**Scott:** Go to those first. If you think this isn't performing as well as it should and:\r\n*   I've turned lots of knobs,\n*   I have a ton of data,\r\n*   I have the computing power,\r\n*   I have the time to go through and try to figure this out\r\nMaybe try deep learning then. And it's not impossible. What about data?\n\n**Susan:** I'm gonna tell an embarrassing story. I'm going to hope most people have had the same embarrassing story. I started playing around with deep learning probably just for fun. Not deep learning, just machine learning in general, about 10 years ago, 15 years ago. I was playing around with little things and then I hit the stock market like everybody does.\n\n**Scott:** Yeah, first thing. Oh, machine learning, here we come!\n\n**Susan:** I'm gonna predict the stock market! I started building my little model and man, it was accurate.\n\n**Scott:** Super accurate?\n\n**Susan:** It was crazy accurate. I was like, I'm gonna be a millionaire! What did I not do properly?\n\n**Validation versus training.**\r\n## Validation versus training\r\n\n**Scott:** What you're trying to do is set up the problem, so I'm gonna give you this data and you have to learn from it, but then I'm gonna hold this other data out, which is true data, again, but the model's never seen it, and now I'm gonna show it to you and see how correct you are. Well, if you don't do that properly, then you really fool yourself.\n\n**Susan:** You can delude yourself so amazingly well. And the great thing about this particular trap is that there's so many variations to this trap. Let's go to the audiobook world and you're trying to do something like speech recognition. You have 500 audiobooks and 18 people reading those audiobooks. You could split your data one of two ways: off of audiobook or off of people.\n\n**Scott:** Well there's 500 audiobooks, so ...\n\n**Susan:** I can guarantee you if you do it off of audiobook you won't be able to use it in the real world. There's 500 audiobooks, but now you suddenly look a lot better than you actually are because you got the same person in your train versus validates.\n\n**Scott:** It learned how that person spoke in the other books and even though it hadn't heard that exact audio, it's heard that voice before and it's learned a lot form that voice and so it can do a good job on it.\n\n**Susan:** It's not just, hey: \"I'm gonna randomly select.\" I have unfortunately learned this lesson on several times in my life. You want to know how the model performs in the wild. Real, wild data. This is a hard thing to just keep track of, especially in deep learning because it takes so long to do your experiments. It's easy to say what if I use this? What if I do this? What if I look over here? If you train just once on one of your validation data sets, you have a big problem. You can't undo that. If you've been training your model for the last three months and you then spend a day training on your validation data set, you might as well throw that validation data set away, put it in your training data set, or start over on the model.\n\n**Susan:** Hopefully you had a checkpoint before you did that.\n\n**Scott:** Or go back. Go back into a checkpoint.\n\n**Susan:** You contaminate that model and you are done. And it can happen so many ways you don't even realize it. You get a new data set you didn't realize was actually derived data set and it has different IDs associated with the thing it was derived from and suddenly, wow, I'm doing really well on this, except for it's not real.\n\n**Scott:** Training a model is not a reversible process. There's no undo button.\n\n**Susan:** It's very incredibly easy to mess up your various sets of data, so treat them wisely. But, you know what's another great way to mess up training versus validate?\n\n**Scott:** What?\r\n## Overfit your hyper parameters.\r\n\n**Scott:** Oh yeah, for sure. You're always picking how wide should it be, how deep should it be, how many input this do I need, how many that? What should my learning rate be? And all these different hyper parameters and then you try to use it on some wild data and things aren't working. Why aren't they working?\n\n**Susan:** What happened? I ran 10,000 tests, each one tweaking it to be the best possible result on my validation. Why isn't that matching reality?\n\n**Scott:** You're training yourself a little bit here. It isn't that the model got adjusted so much, it's that you are thinking: \"Oh, what if I tune these knobs a little bit?\" Now it does really well on your validation data set, but if you give it some real wild data, it doesn't do so well.\n\n**Susan:** It's hard to grasp at, but the first time you really truly run into it and smacks you in the face, you definitely feel it hard. It's like that big gap between validation and reality is huge. There's still even more ways you can mess up your validation set. We could talk about this forever.\n\n**Scott:** There's a good way to combat this, which is you have your large training data set, then you have a validation data set, then you have another validation data set, then another validation data set, then you at least have these backups. Hey, you can test over here, test over here, test over here, get your hyper parameters where you think you need them, and then test them on these other data sets that are maybe they're small too like your validation data set, but at least it's giving you a little cross check.\n\n**Susan:** Do what you can. Also, on that note, grooming these sets as time goes on is important. Just like you're talking about setting aside a secret or whatever you wanna call these additional data sets.\n\n**Scott:** Test validation secret.\n\n**Susan:** There's a lot of different ways to split these up, but if you're getting data over time, make sure that you keep on adding to these sets over time, because data changes over time. The English language changes over time. All it takes is one popular new song and people shift their language a little bit. We keep going back to the language world because that's what we think about a lot, but this is also images. Just think about how cameras have changed, just the quality settings and all that. Your average distribution of colors are probably changing a little bit.\n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976786/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/english-evolution_.jpg)\n\n_Here we see how one verse of the Old Testament changed over time. By comparing the Old English (Anglo Saxon) and Middle English (Wycliffe's) versions you can see quite a bit of sound change as well as grammatical change. The King James version is radically different, showing the changes in culture that led to serious change in interpretation. The older two versions reflect a closer translation from the Latin bible._\n\n**Scott:** If you take a picture on an iPhone now versus eight years ago it's gonna look vastly different.\n\n**Susan:** Or the self driving world. If you did nothing but from the 80s, a whole bunch of video from the 80s, cars would've changed a lot, clothes would've changed a lot. Do you think that would all affect how well you can recognize everything out there? All this is talking about how you manage your data and the disconnect between the reality of the data you're training on and the current real world production environment you're gonna be on. That's the big mistake that a lot of people run into. You've been in this little locked in world and then suddenly reality is somehow different. That's because you're training and your validate probably aren't as good as representing the real world as you thought.\n\n**Scott:** Some other good ones are you have to choose your hyper parameters, which is how big is your network, how deep is it, how wide is it? What learning rate are you going to use, et cetera? On the topic of learning rate, if you chose too high of a learning rate, your gradients get huge.\r\n\r\n## Choosing the wrong learning rate\n**Susan:** Blow your weights.\n\n**Scott:** It tries to learn too much too fast and it learns nothing.\n\n**Susan:** There's so much that goes into hand tweaking all of that stuff. One of the first things that you need to do is do a quick exploration. Whatever mechanism that you do for that grid searcher or whatever, try to find those big hyperparameter sweet spots, but then at the same time, you've gotta fight that problem of overfitting your hyper parameters. It's a real challenge and it becomes a real pitfall that you do things like wrong learning rates and then you restart and this and that and the other, then suddenly at the end of it you have a set of hyper parameters that seem to fit and you've gotten into that overfit world. Minimize how often you change it.\n\n**Scott:** Try to get into a good region and then don't change it too much and then rely on your data to do the talking of shaping where the model should go. You could have too low of a learning rate too, where it just takes way too long. We've talked about this before, where you're patience limited in deep learning because things take a while, there's a lot of parameters, it uses a lot of computational power, and if you're training models for days and it's just only getting better very slowly, maybe your learning rate needs to be jacked up again.\n\n**Scott:** Again, that's part of that search.\n\n**Susan:** That gets into one that I've re-learned recently, which is people pay a lot of attention to data, rightfully so. A lot of attention to model structure, rightfully so, but less attention to the optimizer that they use.\n\n**Scott:** I completely agree.\n\n**Susan:** There's just an epic difference out there based off that. It's hard to even begin with how important that selection is you forget about.\n\n**Scott:** Heuristically, it's like, if I know what my error is and I know the errors I've been making in the past, how far should I project where I should go? You can think about this as a human too. If you're walking around in the day and you keep hearing the same wrong thing, over and over, and you're like, maybe I really need to go look into that and adjust it or something. It's weighting all of that. How much of the past should I keep for now and use in the future? How much should I throw away? There's a momentum side to that too. \n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976787/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/conifer-dawn-daylight-167698.jpg)\n\n**Susan:** One way to think of it is this; going back to the classic forest problem. Dense forest and how do I get to low ground? The optimizer's are like how fast you should be walking at any one given time?\n\n**Scott:** If it was all curvy right there, maybe you should do something else. But there are different programs there, basically, or different rules you should follow and that's what picking your optimizer is.\n\n**Susan:** And how it adjusts that rate of walking as you go along. Those are big, huge, massive things there. What about the bottom end of a model? What kind of output errors have you run into here, Scott?\n\n**Scott:** A lot.\n\n**Susan:** I can give you my favorite one. If you got one right off the top, go for it.\n\n**Scott:** Not outputting good results is the biggest. Outputting all zeros when you're loss function is all screwed up or whatever.\n\n**Susan:** My personal favorite is outputting something that's useful in a training set, but not useful in the real world. Forgetting the difference between training and production. When you look at code and when you look at training models and stuff like that, you're going to take this thing, you're gonna wrap it in a loss function, you're going to do all these different things. I remember building my first models, when I actually tried to apply them to a real world problem, I realized that was completely not useful. The training set that I had been training against and the outputs I had been using there were giving me these great numbers of loss.\n\n**Scott:** 99% accuracy.\n\n**Susan:** And accuracy and I was like, oh, and I was tweaking the various setting and all that stuff. Then I disconnected the model from all that apparatus and tried to do something with it.\n\n**Scott:** Yeah, feed it something real.\n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976788/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/box-1.jpg)\n\n_[Black box problem](https://en.wikipedia.org/wiki/Black_box)_\n\n**Susan:** Suddenly I had no idea what it was actually doing. It just had this black box of data in, black box of data out, these metrics around it. You've always, always gotta keep in your mind that the whole point is not the training set, the whole point is not the accuracy number, the whole point is not all this apparatus over there, it's when you finally get to production. What is that output supposed to look like and how is it useful?\n\n**Susan:** If you can't take whatever output it's got coming out there and make it useful, then you've gotta address that right away. It's like the water through the pipes that we were talking about there, 'cause water through the pipes on the training side and also water through the pipes on the production side.\n\n**Scott:** Does it get sensible outputs in production-land as well?\n\n**Susan:** Are you really able to take that 10,000 classes and do something with it or should have you stripped it down to those five because that's all you cared about in production?\n\n**Scott:** I think some real nuts and bolts ones could help too. Like normalizing your input data.\n\n**Susan:** Oh, yeah.\r\n\r\n## Not normalizing your input data\n**Scott:** What is normalizing your input data mean? It's if you have an image, for example, and each pixel value goes from zero to 10,000, what scale should you keep it at? Should it be zero to 10,000 or should it be some other number? Typically machine learning algorithms are tuned to work well when you're talking about negative one to one or zero to one, or something like that. You should normalize your data so that zero to 10,000 now is like zero to one, or something like that.\n\n**Susan:** It's staggering what normalization can do for you. You just think about humans. Think about looking around your environment. The first physical step of light going into your body goes through a normalization layer and that is what's your eye doing right now when it's stares into a light versus stares into a dark room?\n\n**Scott:** It's getting smaller and bigger.\n\n**Susan:** It shows you how important that step is that you physically do that. You do that with hearing too. It's a loud environment, what do we do? We put earmuffs on. We're trying to get all the sound normalized.\n\n**Scott:** Or it's loud but it doesn't seem that loud. Hearing is logarithmic. There's tons of energy being pumped into your ear but it's only a little bit louder. One more on hearing. You've got two ears and the ability to swivel your head, when you listen to something, when you really wanna listen to something, you actually swivel your head and that helps your hearing. That physical motion of adjusting the two microphones in your body to be better aligned with [the source makes a big different to understanding](https://www.youtube.com/watch?v=Oai7HUqncAA&t=185). Don't forget that lesson when you're in the machine learning world. Normalize your data, don't forget those initial purely simple algorithmic filters you could apply to data beforehand.\n\n**Scott:** We haven't discussed all that much, but essentially if you're Amazon Alexa or Google Home, or something like that, they usually have seven or eight microphones on them. But humans do pretty good with just two.\n\n**Susan:** Sure, however humans have the ability to turn their heads.\n\n**Scott:** Yeah we can move around. You have these infinite amount of microphones.\n\n**Susan:** Yeah. We've got a lot of great things there.\r\n\r\n## More examples of common pitfalls\r\n\n\n\n**Scott:** You have a deep learning model and in there are tons of parameters. Those parameters are initialized initially to what? What do I mean by initialize? I mean you fill up a bunch of matrices with numbers, what do you fill them with? Zeros? Is that a good answer?\n\n**Susan:** Bad answer.\n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976789/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/Wolfsgrube.jpg)\n\n_[Pitfalls](https://en.wikipedia.org/wiki/Trapping_pit) were one method of hunting animals and defeating enemies in war. Now it's just a metaphor._\n\n**Scott:** Very bad answer.\n\n**Susan:** Very bad answer. As soon as you get to two layers everything goes out the window.\n\n**Scott:** Do you fill them with all the same number? One?\n\n**Susan:** Nope, it can't be the same number. Even just pure randomness is not good. We're not gonna get into the math and why, but look up some algorithms for how to fill up your parameters.\n\n**Scott:** You want it round and peaked with some variance.\n\n**Susan:** Based off of size, based off of number parameters.\n\n**Scott:** But random still, but off a distribution.\n\n**Susan:** Randomness is such an amazingly strong tool in the world. It's just crazy and that one example shows that you don't randomize things that literally will not learn.\n\n**Scott:** You can stitch together networks or loss functions or pieces of code that other people have written and you use part of it, but you use it incorrectly. In other words, it should have a soft max that it's run through before it gets put in there, or it shouldn't. Do you strip those pieces off or not? This is an impedance mismatch.\n\n**Susan:** To me this happens mostly going back to the training environment versus the production environment. You see this happen a lot. This is a basic one, luckily you fix it pretty quick just by changing that little piece of code. In training you may or may not need something at the end of your model that you'll strip off for production. Keeping that in mind will help save some debugging steps. Ask yourself: Is this really what I'm gonna use for production or was this here just for training? Forgetting that you've got dropout set wrong, or something like that.\n\n**Scott:** You want it turned off for the evaluation inference probably.\n\n**Susan:** You need to make sure your model's set for the right environment.\n\n**Scott:** Drop out shuts off part of the deep learning brain basically. It's like oh, don't you think you might want that in there?\n\n**Susan:** To get it in there if you've got a bunch of parameters. You can then randomly select a few of them to just ignore basically. The important thing about this is it tries to get the same level of output signal, so when you go to the production environment, you want those neurons to be there. You want those parameters to be useful, but if they were all turned on, suddenly your signal goes up, say, by 25% if you've got 25% drop out on there. You treat the evaluation side different than the train side. Some pretty important things to remember otherwise it's just not as good as it could be.\n\n<iframe src=\"https://www.youtube.com/embed/KcrAkPNB8jc\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\n_If you train your model based on speaker (narrator) then you have a narrator detector. You probably don't want that._\n\n**Scott:** One of my favorite things to do though is overfitting. Which is normally a bad thing, but when you're in the water through the pipes stage, where hey, I've finally got my data going, I finally got my model built, I finally got some kind of output and I can verify that it's coming now. Now, take a really small training data set, maybe just one example or maybe 100, or maybe eight or whatever and train the model and make sure that you can very quickly get 100% accurate.\n\n**Susan:** You wanna see that curve of learning.\n\n**Scott:** Your error just drops like a rock because the model now has so many parameters it's able to just focus on getting those correct. All that it's doing at that point is memorizing, but if it can't memorize those eight things, it's probably not gonna do anything else.\n\n**Susan:** That helps you prove that your loss function is working. It helps you prove you have some inputs and outputs there, at least in the right realm. It doesn't prove that you will have a good model structure.\n\n**Scott:** No, you could have an awful model structure, but at least it has some hope.\n\n**Susan:** That's your water through the pipes basic all the plumbing is involved here is working here. There's a ton of common, big, huge pitfalls. What's is the biggest one overall?\n\n**Scott:** For me, I think the biggest one is like deep learning and hey, you can do anything. And it's like, okay cool, not anything, but there are a lot of things that you can do that you couldn't do before. Or it can do it better or something like that.\r\n\r\nThey think, \"I'm gonna go to TensorFlow and then I'm gonna solve my problem.\" If that's your thought process, you gotta back up a little bit and think we need to take baby steps here because you're not going to just go download a model or an example or train it with a couple days of toying around and get a real production thing done. It's just not gonna happen.\r\n\r\nEven from the engineering standpoint, you're not gonna do that. From the model building standpoint, you're not gonna get there basically unless it's something super simple you could've done with normal stats or machine learning.\r\n\r\n## The process side of the house is to me the biggest problem here.\r\n\r\n\n**Susan:** Did you analyze the problem correctly from the start? This is any engineering problem on the planet.\n\n**Scott:** This takes budgeting the appropriate amount of time and the appropriate amount of time is at least days, and probably weeks, to get a good look at that problem. It might be months.\n\n**Susan:** That might actually be the biggest pitfall in deep learning, is the assumption that it'll only be a day or two.\n\n**Scott:** Yeah. Yeah. A day or two minimum. That happens frequently.\n\n**Susan:** Oh, that's no problem. I'll have something in a week.\n\n**Scott:** Yeah, \"Oh weird. It's not doing what I thought it would do. But what if I just do this little trick?\" Two days later: \"Huh, okay,\" and then eight days later, \"Huh.\"\n\n**Susan:** It's a big time thing, that's for sure.\r";
						}
						async function compiledContent$i() {
							return load$i().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$i() {
							return (await import('./chunks/index.c16fdf55.mjs'));
						}
						function Content$i(...args) {
							return load$i().then((m) => m.default(...args));
						}
						Content$i.isAstroComponentFactory = true;
						function getHeadings$i() {
							return load$i().then((m) => m.metadata.headings);
						}
						function getHeaders$i() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$i().then((m) => m.metadata.headings);
						}

const __vite_glob_0_258 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$i,
  file: file$i,
  url: url$i,
  rawContent: rawContent$i,
  compiledContent: compiledContent$i,
  default: load$i,
  Content: Content$i,
  getHeadings: getHeadings$i,
  getHeaders: getHeaders$i
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$h = {"title":"As the Crow Flies — What Does It Mean?","description":"Why are idioms used and where does the as the crow flies idiom come from?","date":"2019-02-14T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981347/blog/what-does-as-the-crow-flies-mean/as-the-crow-flies%402x.jpg","authors":["morris-gevirtz"],"category":"linguistics","tags":["education","language"],"seo":{"title":"As the Crow Flies — What Does It Mean?","description":"Why are idioms used and where does the as the crow flies idiom come from?"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981347/blog/what-does-as-the-crow-flies-mean/as-the-crow-flies%402x.jpg"},"shorturls":{"share":"https://dpgr.am/bdad6db","twitter":"https://dpgr.am/2a61b50","linkedin":"https://dpgr.am/e03598e","reddit":"https://dpgr.am/8465909","facebook":"https://dpgr.am/e840451"}};
						const file$h = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-does-as-the-crow-flies-mean/index.md";
						const url$h = undefined;
						function rawContent$h() {
							return "![map](https://res.cloudinary.com/deepgram/image/upload/v1661976819/blog/what-does-as-the-crow-flies-mean/Screen-Shot-2019-02-11-at-5.04.43-PM.png)\n\n*By road, the shortest road route is in fact 141.685 miles, and **as the crow flies** it is 120.495 miles. One assumes, that this is from the pinnacle of one capitol dome the other. [Source here.](https://www.freemaptools.com/how-far-is-it-between.htm)*\n\nAs the crow flies means \"in a straight line\" or moving about from point a to point b without deviation for obstructions. It is a fanciful saying that reminds us that obstacles often lie between us and our destinations and that birds are free to move without impediment. Like so many sayings, it's origins are obscure. Folk etymology attributes the saying to unlikely Viking tradition and to the book of Genesis. In truth, it seems that the Scots were among the earliest to use the phrase in English (or in a language geographically close to English speakers). A Scots English phrase \"the crow road\" predates the phrase \"as the crow flies.\" The latter phrase appears in English in 1767. The modern American English phrase seems to have been made popular by Charles Dickens who used it in his book Oliver Twist. The term \"crow road,\" incidentally, but not surprisingly, is the title of a book by Scottish author Iain Banks. It was turned into a TV mini-series. Neither the book nor the TV show seem to have anything to do with travel, crows, or birds for that matter.\n\n## English is not Alone\n\nIt is worth noting that this saying is one that exists in multiple languages.\n\n* **French:** à vol d'oiseau\n* **Turkish:** Kuş uçuşuyla\n\nNeither of the above phrases mention crows specifically, but both translate roughly as: \"as the bird flies.\" Similar phrases probably exist in other languages as well. What is not clear-and here is a standard linguistics question-is **if it is one language that came up with the phrase** which has now been translated into others, or **if different cultures came up with the idea separately**? In many languages the same concept is expressed with a phrase meaning \"in a straight line.\" This is the case in Spanish and German, for example.\n\n* **Spanish:** en línea recta\n* **German:** Luftlinienentfernung\n\n![canal](https://res.cloudinary.com/deepgram/image/upload/v1661976820/blog/what-does-as-the-crow-flies-mean/3330472560_9981b433d7_o.jpg)\n\n*A canal somewhere in Togo. It is approximately 140 miles **as the crow flies** from the eastern border of Togo to the western border of Nigeria. [photo credit](https://www.flickr.com/photos/attawayjl/3330472560/in/photolist-65ixsG-nG4ruZ-pnfhTN-83NHu9-99S7dk-21thCX5-6H2Dun-5dM6PH-iNPTLy-4d8ov3-9LQbGK-2cCFUZ5-djztHb-q5PDfZ-hUi55H-rKeLsa-aEenUv-7jLggZ-r8Kimy-DiR8rk-b675hT-JzzU6H-hhJVnE-8M6N49-2cd88mR-dY7gWE-dPurPF-EisYZu-8gRCaZ-5RaVMt-8MmHJi-9RmhLF-vqrvaM-bsQrbb-prWZdz-2diU3f9-dKWnrX-aVUzTH-992mMi-7A8fpX-496oEb-2djWyR6-97o5nt-6TyiE1-2o88rs-a4Lert-nutk2i-5NBAM7-cohUGA-28V6wvL)*\n\nBy comparison, in Nembe, a language from Nigeria, the same idea of \"as the crow flies\" is communicated with the phrase: *Ongo dug* which roughly translates as \"take the canal\". Perhaps the \"crow road\" the Scots spoke of in the 18th century is nothing more than a canal! In any case, English may be richer than most languages; when it comes to flight-related \"move in a straight line\" animal metaphors, we have two! To make a beeline for something roughly means to move directly toward some item, usually a desirable one. Why it is that these animals have been chosen as the biological embodiments of linear movement, it may never be known. We might have chosen ¨as shooting star falls.¨ \n\n<WhitepaperPromo whitepaper=\"latest\"></WhitepaperPromo>\n\n## What Are Idioms?\n\nIdioms are phrases in language that are used to convey emotions about certain topics. They are fascinating to everyday language speakers and linguists for a couple of reasons. They exist in a space between grammar and single words-this means that they somewhat conform to grammatical rules, yet have to be learned phrase by phrase just like most words need to be learned. Idioms also preserve in them the history of how and where they came to be. Certain idioms haven't left their villages in rural England. Others are universal. Variations in one idiom tell us about where they have been in time and place. They remind us of times long-gone and places near and far-when and where the world was different but people were the same.";
						}
						async function compiledContent$h() {
							return load$h().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$h() {
							return (await import('./chunks/index.7156df4b.mjs'));
						}
						function Content$h(...args) {
							return load$h().then((m) => m.default(...args));
						}
						Content$h.isAstroComponentFactory = true;
						function getHeadings$h() {
							return load$h().then((m) => m.metadata.headings);
						}
						function getHeaders$h() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$h().then((m) => m.metadata.headings);
						}

const __vite_glob_0_259 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$h,
  file: file$h,
  url: url$h,
  rawContent: rawContent$h,
  compiledContent: compiledContent$h,
  default: load$h,
  Content: Content$h,
  getHeadings: getHeadings$h,
  getHeaders: getHeaders$h
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$g = {"title":"Under the Weather — What Does it Mean?","description":"Where does the phrase \"under the weather\" come from? Find out here!","date":"2019-01-08T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981953/blog/what-does-it-mean-to-be-under-the-weather/placeholder-post-image%402x.jpg","authors":["morris-gevirtz"],"category":"linguistics","tags":["education","language"],"seo":{"title":"Under the Weather — What Does it Mean?","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981953/blog/what-does-it-mean-to-be-under-the-weather/placeholder-post-image%402x.jpg"},"shorturls":{"share":"https://dpgr.am/ad4b98f","twitter":"https://dpgr.am/8e092ec","linkedin":"https://dpgr.am/12f4966","reddit":"https://dpgr.am/9c7fae0","facebook":"https://dpgr.am/738d63f"}};
						const file$g = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-does-it-mean-to-be-under-the-weather/index.md";
						const url$g = undefined;
						function rawContent$g() {
							return "\r\nWe know this means to be sick or ill, but why?\r\n\r\n\r\n\r\n\r\n\r\n![](https://res.cloudinary.com/deepgram/image/upload/v1661976797/blog/what-does-it-mean-to-be-under-the-weather/U.S.S.-Seminole--Thursday--November-07--1861---Shi.jpg)\r\n\r\n<div>_A picture the ship's log from the [U.S.S. Seminole](https://en.wikipedia.org/wiki/USS_Seminole_(1859)), Thursday, November 07, 1861._</div>\r\n\r\n1.  Back in the sailing days, captains kept logs of all the activity on the boat, including who was ill. When the number of sick sailors exceeded the space allotted in the ship's log for sick sailors their names were written under the weather section of the log.\r\n2.  In the days of sailing and even in the golden days of steam powered ships, passengers would often get seasick. The ill would be advised to sleep off their nausea under the ship's weather rail. This is the below-deck area on the side of the boat that faces the wind.\r\n3.  Ill passengers and sailors alike would go below deck, out of the weather and under the topsides of the ship. Perhaps the notion of out of the elements and underneath were joined.\r\n\r\n\r\n\r\n\r\n\r\nLike so many set phrases in English, to be under the weather is a sailing term. There are several conjectures as how it came about. Here are three: It is neat to point out that if one or one's machines are \"ship-shape\" they are in good condition. Though this is not the literal opposite of \"under the weather\" it is a contrasting phrase. You can't be ship-shape if you are under the weather.\r\n\r\n* * *\r\n\r\n### What Are Idioms?\r\n\r\nIdioms are phrases in language that are used to convey emotions about certain topics. They are fascinating to everyday language speakers and linguists for a couple of reasons. They exist in a space between grammar and single words-this means that they somewhat conform to grammatical rules, yet have to be learned phrase by phrase just like most words need to be learned. Idioms also preserve in them the history of how and where they came to be. Certain idioms haven't left their villages in rural England. Others are universal. Variations in one idiom tell us about where they have been in time and place. They remind us of times long-gone and places near and far-when and where the world was different but people were the same.\r\n";
						}
						async function compiledContent$g() {
							return load$g().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$g() {
							return (await import('./chunks/index.2a6b537b.mjs'));
						}
						function Content$g(...args) {
							return load$g().then((m) => m.default(...args));
						}
						Content$g.isAstroComponentFactory = true;
						function getHeadings$g() {
							return load$g().then((m) => m.metadata.headings);
						}
						function getHeaders$g() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$g().then((m) => m.metadata.headings);
						}

const __vite_glob_0_260 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$g,
  file: file$g,
  url: url$g,
  rawContent: rawContent$g,
  compiledContent: compiledContent$g,
  default: load$g,
  Content: Content$g,
  getHeadings: getHeadings$g,
  getHeaders: getHeaders$g
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$f = {"title":"That’s the Way the Cookie Crumbles — What Does it Mean?","description":"What does \"that's the way the cookie crumbles\" mean? Find out here!","date":"2019-01-28T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981341/blog/what-does-thats-the-way-the-cookie-crumbles-mean/the-way-the-cookie-crumbles%402x.jpg","authors":["morris-gevirtz"],"category":"linguistics","tags":["language"],"seo":{"title":"That’s the Way the Cookie Crumbles — What Does it Mean?","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981341/blog/what-does-thats-the-way-the-cookie-crumbles-mean/the-way-the-cookie-crumbles%402x.jpg"},"shorturls":{"share":"https://dpgr.am/c75cdb3","twitter":"https://dpgr.am/84926e5","linkedin":"https://dpgr.am/d5e69d2","reddit":"https://dpgr.am/2160c8e","facebook":"https://dpgr.am/17ae8dc"}};
						const file$f = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-does-thats-the-way-the-cookie-crumbles-mean/index.md";
						const url$f = undefined;
						function rawContent$f() {
							return "This is a wonderfully idiomatic way to affirm the undeniable fact that there are many things in life over which we have no control. Synonyms include such affirmations as: \"That's the way life is\" and \"what can you do about it? Often: \"whatchagonnado?\"\n\n> Example: \"I got fired for stealing staplers from the stationary closet. I guess that's the way the cookie crumbles.\"\n\nThe phrase in question seems to have appeared in the English vernacular in the early 20th century. Like so many idiomatic phrases its origins are obscure. The music duo Billy & Lillie helped popularize the phrase with their 1960 single: \"That's The Way The Cookie Crumbles (Ah So)\" - evidencing the fascinating truth that Rock music has helped shape English.\n\n ![billy](https://res.cloudinary.com/deepgram/image/upload/v1661976805/blog/what-does-thats-the-way-the-cookie-crumbles-mean/billy-lillie-ultra-original-sweden-ep_1_61e940590d.jpg) \n\nIt is, however, worthwhile to look at the origins of the word \"cookie.\" Finding out just how many English words are Dutch in origin would surprise many. The word cookie, is one such word. It is related to the word \"cake\" which itself is a word the English speakers of the early middle ages borrowed from the Vikings. Both the word \"cake\" and the word \"cookie\" are derivatives of an older Germanic word meaning a flat, mostly unleavened, baked or fried dough food item. However, naming things-the act of saying this is this, and that is that, is powerful. For example, [in the UK \"cookies\" have VAT](https://www.crunch.co.uk/knowledge/tax/cake-or-biscuit-vats-the-difference/) on them, but \"cakes\" do not.\n\n ![jaffa](https://res.cloudinary.com/deepgram/image/upload/v1661976806/blog/what-does-thats-the-way-the-cookie-crumbles-mean/jaffa.jpg) \n\n*Is it a cake? is it a cookie? Lingustically they're arguably the same, but as far as the taxman is concerned, they're a cake-in the UK.*\n\nCookies and all baked goods changed in character after the 1860s when pelleted yeasts where invented. Prior to this date, and especially before the 17th century, it was hard (if not impossible) to make sweet AND leavened pastries. Much of the calories eaten in the Western world, were made of breads that looked nothing like our breads.\n\n## What Are Idioms?\n\nIdioms are phrases in language that are used to convey emotions about certain topics. They are fascinating to everyday language speakers and linguists for a couple of reasons. They exist in a space between grammar and single words-this means that they somewhat conform to grammatical rules, yet have to be learned phrase by phrase just like most words need to be learned. Idioms also preserve in them the history of how and where they came to be. Certain idioms haven't left their villages in rural England. Others are universal. Variations in one idiom tell us about where they have been in time and place. They remind us of times long-gone and places near and far-when and where the world was different but people were the same. Header image: Grace Coolidge enjoys a girl scout cookie in 1923.";
						}
						async function compiledContent$f() {
							return load$f().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$f() {
							return (await import('./chunks/index.d057e066.mjs'));
						}
						function Content$f(...args) {
							return load$f().then((m) => m.default(...args));
						}
						Content$f.isAstroComponentFactory = true;
						function getHeadings$f() {
							return load$f().then((m) => m.metadata.headings);
						}
						function getHeaders$f() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$f().then((m) => m.metadata.headings);
						}

const __vite_glob_0_261 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$f,
  file: file$f,
  url: url$f,
  rawContent: rawContent$f,
  compiledContent: compiledContent$f,
  default: load$f,
  Content: Content$f,
  getHeadings: getHeadings$f,
  getHeaders: getHeaders$f
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$e = {"title":"What Does the AI Dystopia Look Like? — AI Show","description":"Is an AI dystopia coming? If so, what will it look like? Have a listen to this episode of the AI Show.","date":"2019-02-01T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981345/blog/what-does-the-ai-dystopia-look-like-ai-show-2/what-will-ai-dystopia-look-like%402x.jpg","authors":["morris-gevirtz"],"category":"ai-and-engineering","tags":["deep-learning","machine-learning"],"seo":{"title":"What Does the AI Dystopia Look Like? — AI Show","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981345/blog/what-does-the-ai-dystopia-look-like-ai-show-2/what-will-ai-dystopia-look-like%402x.jpg"},"shorturls":{"share":"https://dpgr.am/18715f2","twitter":"https://dpgr.am/84d89dd","linkedin":"https://dpgr.am/02f3c73","reddit":"https://dpgr.am/ba0efa7","facebook":"https://dpgr.am/103b135"}};
						const file$e = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-does-the-ai-dystopia-look-like-ai-show-2/index.md";
						const url$e = undefined;
						function rawContent$e() {
							return "<iframe src=\"https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/570095520&color=%23ff5500&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&show_teaser=true\" width=\"100%\" height=\"166\" frameborder=\"no\" scrolling=\"no\"></iframe>\n\n**Scott:** Welcome to the AI Show. Today, we're asking the question: What does the AI dystopia look like?\n\n**Susan:** Oh, man, we are going down the tubes. It's going to be terrible.\n\n**Scott:** Let's take it to negative town. The world is over.\n\n**Susan:** The world, as we knew it, ends basically every year and a half as the next revolution hits, but this is the last one.\n\n**Scott:** There's a law against that, isn't there?\n\n**Susan:** There is?\n\n**Scott:** Everything has a life expectancy about twice what its current age is, but until it abruptly dies.\n\n**Susan:** Oh, yeah, that's cool. I've got to look that one up.\n\n**Scott:** Yeah, so the abrupt death is coming. Everything looks like we're going to live twice as long.\n\n**Susan:** Well, exactly. Well, everybody keeps saying, \"Hey, the pendulum's going to swing back, and technology is going to help us more than hurt.\"\n\n**Scott:** Hey, we're still alive, right?\n\n**Susan:** That's all true up until the very last time. Then, that last time, people are like, \"Well, I guess it didn't swing back that time.\"\n\n**Susan:** Man, there's a lot to be pessimistic about.\n\n## What's the first thing that's going to go?\n\n**Susan:** Oh, the first thing is privacy.\n\n**Scott:** Privacy's number one?\n\n**Susan:** Everybody knows that their privacy is not nearly what it may have been in the past.\n\n**Scott:** What is privacy anyway?\n\n![bigbro](https://res.cloudinary.com/deepgram/image/upload/v1661976812/blog/what-does-the-ai-dystopia-look-like-ai-show-2/1984_by_iskallvinter_d15owbe.jpg)\n\n*Photo credit [iskallvinter](https://www.deviantart.com/iskallvinter/art/1984-70025882)*\n\n**Susan:** Now, we're going to take the loss of privacy to the next level. Not only will they have your data, but they'll have the computing power and the algorithms to actually do something with it.\n\n**Scott:** This isn't like drones looking in your bedroom window.\n\n**Susan:** Who cares how many data points you have, if you can't actually make sense of them? But you know, we can actually listen to every single phone call and record it. But, if you can't actually do anything with all that audio, who cares?\n\n**Susan:** Now we can take all the surveillance cameras, and we can analyze video.\n\n**Susan:** We can put together your entire human history by taking pictures from 4000 different things.\n\n**Scott:** Your browser history.\n\n**Susan:** The smarts are finally there to analyze multiple terabytes of data and come up with you.\n\n**Scott:** Does Google become a state-owned company?\n\n**Susan:** Well, no, Google owns the state.\n\n**Scott:** They become the state. Google's the state.\n\n**Susan:** The United States of Google?\n\n**Scott:** The United States of Google!\n\n**Susan:** How about you, Scott? Where do you think the dystopian future of AI goes?\n\n**Scott:** Well, I think we should bring this to self-driving cars. Every inch of your life is known.\n\n**Susan:** Is this the trolley problem?\n\n**Scott:** You're going to start driving around, or you're not driving around anymore. You've got mainly machines that drive for you.\n\n**Susan:** Of course.\n\n**Scott:** But now, what are they going to do? They're going to drive you around and take you by the billboard that's also AI powered.\n\n**Susan:** I love it! It's like why are we taking this route? Oh, what's that billboard there?\n\n**Scott:** Everybody always has the Uber driver experience. \"Why are we going this way?\" Well, this is going to have a monetary reason behind it.\n\n**Susan:** From upfront a voice says: \"This is faster. Trust me.\" Why are we stopped in the middle of nowhere, with nothing but advertisement around me?\n\n**Scott:** Is the AI that drives the car like a humanoid? It turns around. \"Trust me. It's faster. (robotic voice)\" Yeah.\n\n**Susan:** Oh, well, there's a privacy bent to this, too. Just think of it.\n\n**Scott:** No more sexual relations in the back of the Uber?\n\n**Susan:** No, it's creepy if you did, but no. Now, every last inch of your entire life is ... Your position is known. You get in the car, and it knows where you're at, but more than that, it's just a big data collection device. It's built for it. All those LIDARS are constantly going, scanning every single thing around them, all that stuff.\n\n**Scott:** Before, it just knew your position. Now it knows your total surroundings.\n\n**Susan:** The natural outcropping of self-driving car technology is really fantastic image recognition and classification. Not only is it going around recording every last square inch of visual detail, but man, it's saying, \"That is a flower pot from Ikea.\"\n\n**Susan:** $12.99.\n\n**Scott:** You could buy this.\n\n**Susan:** As you're in the car, you're sitting here. You're buying stuff.\n\n**Susan:** As you're in your car, you're looking around over there, and a little advertisement pops up. It's like, \"You can also have this flowerpot for $9.99. Just press here.\" It sees your eyes, and there you go. The attention economy.\n\n**Scott:** Well, have you ever seen the show Black Mirror?\n\n\n![mirror](https://res.cloudinary.com/deepgram/image/upload/v1661976813/blog/what-does-the-ai-dystopia-look-like-ai-show-2/download-1.jpg)\n\n*\"Fifteen Million Merits\" Series 1, episode 2 of Black Mirror.*\n\n**Susan:** First of all, if you see the first episode... Go to the second or third episode. The first one will just...\n\n**Scott:** Yeah, don't watch the first.\n\n**Scott:** Yeah, but the attention economy is discussed there in one of the episodes a little bit, right?\n\n**Susan:** There's an episode where they're on bikes, just pedaling aimlessly, just to pay off their credits, and they're being forced to watch TV advertisements. Oh my. That's dystopian.\n\n**Scott:** If they start to fall asleep, it'll jolt them back awake, because AI's watching them. It knows what they're looking at, what they're taking in.\n\n**Susan:** No! Yeah. They're forced to watch a commercial, and it knows whether they're paying attention or not.\n\n**Scott:** You can skip the commercial, but you'll have to pay.\n\n**Susan:** How evil is that? I've got my headphones on now, and a commercial comes on, I just go like that, but then the commercial will pause, and you put them back on, and it's right back there where it was at.\n\n**Scott:** Right back there.\n\n**Susan:** I was like, what? You're just like, oh, I'm going to get smart and switch channels, and it's still there-the commercial does not go away!\n\n**Scott:** Yeah, it's the same one.\n\n**Susan:** Until you listen to that commercial and actually pay attention.\n\n**Scott:** Then you give in. You're like, okay, my AI overlord.\n\n**Susan:** Oh, whew. That is is tremendous.\n\n## Are they going to take our jobs?\n\n**Susan:** If you drive something, you can forget about it. Cars, trucks, planes, which ... Man, get in an aircraft. There's no cockpit.\n\n**Scott:** This is the easiest job ever, right? Why not have an AI pilot?\n\n**Susan:** Yeah, exactly, and by the way most accidents happen up in front, just saying. Maybe it'll be a lot safer, not that airlines are less safe right now. The autonomous revolution. I mean, drones are going to start delivering your food.\n\n**Scott:** They could poison you. Or they just selectively, \"Oh, you're not a Trump supporter? Your food is going to be cold.\"\n\n## Technology is high-tech mediocrity\n\n**Susan:** I was just thinking the whole drone revolution there, delivery revolution, suddenly you're starting to get the seconds stuff. You've got to pay for Prime to get the fresh eggs, right? These are still technically good, but they're one day away from expiration. It's like, magically, all the food that gets delivered to your house, unless you pay the premium, is one day away from expiration.\n\n**Scott:** But it's been managed very well, like the warehouse is near you. It's been stocked just for you, because they know.\n\n**Susan:** Oh yeah, yeah. It knows exactly that cutting point, you know? We call this Susan's Law here.\n\n**Scott:** Susan's Law.\n\n**Susan:** Susan's Law: Technology allows you to make something just good enough. The better your technology is, the finer you can cut that line to be just good enough for the customer that they'll pay for it ... I think we've seen this.\n\n**Scott:** Absolutely.\n\n**Susan:** As technology's gotten more and more capable, we've gotten to the ... We're always on the verge of saying, \"This is so bad! If one more thing happened, I'd get rid of it. If just one more thing ...\"\n\n**Scott:** Yep, but you won't. You just keep paying.\n\n**Susan:** You won't, because It'll be so perfectly honed to you, that you'll never be actually happy. You'll be on the verge of so unhappy that you'll get rid of it, but you won't actually get rid of it.\n\n**Scott:** AI will optimize the frustrations. AI knows your dreams, and it's going to make sure that you never achieve them, but you're going to be very, very close, always.\n\n**Scott:** Just one more little bit, that's it. You're so close.\n\n**Susan:** You will be addicted to things, because of AI, that are ridiculous. It's like they will have honed the rewards system on whatever to be, well, if you just click this one more time-\n\n**Scott:** Just one more, just one more, what's the big deal.\n\n**Susan:** Just one more ... Eight hours later, you're sitting there in a pile of filth, and you're like, just one more click, and I'll go to sleep.\n\n**Scott:** Well, that might already happen on YouTube. Doctors and pilots are now unemployable.\n\n**Scott:** Yep, what about doctors?\n\n![doc](https://res.cloudinary.com/deepgram/image/upload/v1661976814/blog/what-does-the-ai-dystopia-look-like-ai-show-2/hungry-will-work-for_.jpg)\n\n**Susan:** Oh, doctors, geez ... Not to offend any doctors out there.\n\n**Scott:** You're gone. You're a goner.\n\n**Susan:** A subjective opinion is probably not a good one.\n\n**Scott:** Are you saying doctors are subjective?\n\n**Susan:** Sometimes, sometimes. They're professionals. They're well-trained, but they're still people.\n\n**Scott:** Yeah, they get tired. They're trying to get their Medicare bill/Medicaid bill paid.\n\n**Susan:** After you've seen the 90th simple cold come in and act like they're about to die of the plague and ask for all the wrong medicines, and you have to tell them, \"It's just a cold. Drink some fluids. Get some sleep. Tomorrow, you'll feel fine.\" AI will take over that.\n\n**Scott:** I mean, saying that's kind of an easy job?\n\n**Susan:** I'll say this: that we train doctors and pilots for that last little one percent. AI's going to cut that down to a half a percent, and then a quarter of a percent. Take away that 99, that big huge grunt of stuff that is all normal, right? We understand the things that they go through. Here's the lifecycle of the flu. Here's the whatever. Here's the things to look for. It is this.\n\n**Scott:** Doctors, all your jobs, they're gone.\n\n**Susan:** Doctors, pilots.\n\n**Scott:** Pilots gone.\n\n**Susan:** Oh, did you see the ticket bot?\n\n**Scott:** Ticket bot?\n\n**Susan:** Trying to get you out of tickets, law programs.\n\n**Scott:** Well, now it's an arms race, right? To give you tickets and to get you out of tickets. Funding the AI technology boom. The war on tickets. Old laws become asphyxiating.\n\n**Susan:** Another interesting thing is, as technology gets better we're able to enforce laws that when they were put in place they were never intended to be enforced at that level. Just think about speeding tickets and the like. The idea of speeding and the resources put into catching people and all that were from before we had technology - like cameras and stuff like that. The laws were put in place back then.\n\n![cops](https://res.cloudinary.com/deepgram/image/upload/v1661976815/blog/what-does-the-ai-dystopia-look-like-ai-show-2/lexus_police_edition_by_eun_su_d4xr1fd-1.jpg)\n\n*Photo credit: [Eun-su](https://www.deviantart.com/eun-su/art/Lexus-Police-Edition-298553593)*\n\n**Scott:** Yeah, a while ago.\n\n**Susan:** Now, we get better and better and easier and easier enforcement, and we enforce disproportionately to how the laws were initially put in place.\n\n**Susan:** This enforcement allows for new realms of ... I don't want to say abusing the law, but making it very easy to be in violation of the law and get caught, to the detriment of society, you know?\n\n**Scott:** It might go sour, if you can be caught for everything.\n\n**Susan:** Literally. I mean, you walk out the door and you get in your car, you've probably broken four laws..\n\n**Scott:** Maybe the laws will get better defined now. Couldn't that be a good thing? No, probably not. That's not going to happen.\n\n**Susan:** An AI-enforced legal system, oh man!\n\n**Scott:** Things might go quicker. No, they'll just frustrate you to extract more money from you.\n\n**Susan:** Pretty much, yeah.\n\n**Scott:** Isn't that the Government's job, roughly, to protect you just enough to extract value out of you?\n\n**Susan:** Just enough? Ooh, I think we're in a different territory there.\n\n**Scott:** Well, they're investing a lot in AI.\n\n**Susan:** Maybe I want some AI in my government.\n\n## Creatives join doctors and pilots in the homeless shelters\n\n**Scott:** Well, for now, until they get too efficient. Creative jobs, writers, anything like that? What's that? What's going to happen to them?\n\n**Susan:** Well, I mean, doesn't Facebook already have like a snippet writer for articles?\n\n**Scott:** Yeah, rather than a clickbait title, how about a little summary? Here's two sentences that, hey, if you just knew this one weird trick about swimming pools...\n\n**Susan:** There's been a lot of research about actually generating well-formed text, well-formed software. We're seeing code assistance inside of, again, Facebook, trying to write little snippets of where things might be going wrong. We're seeing ... Coders, look out. Writing copy, actually taking and summarizing articles, these are all areas where AI is making real progress. I mean, let's be honest here. It's not there yet, but does that mean six months from now, a year, five years?\n\n**Scott:** Give it a couple years, stir the pot.\n\n**Susan:** Even if it's ... We'll give it 10 years away, 10 years away, taking away what we thought was deeply creative work? That is a staggering thought right there, you know?\n\n**Scott:** Yeah.\n\n**Susan:** That's deeply into interesting job territory there.\n\n**Scott:** You mean, pop music is formulaic?\n\n**Susan:** Oh, geez, of course. Pop music ... Oh man, we've got to write a...\n\n**Scott:** A pop music bot?\n\n**Susan:** A pop music bot.\n\n**Scott:** I love it.\n\n**Susan:** Well, we've got to give it a cool, cool name, too, like Electon, elector ... I don't know. Let's see what. I'm going to make up a word and see how it gets transcribed.\n\n**Scott:** Yeah, there you go.\n\n**Susan:** That's what it'll be, Eclector.\n\n**Scott:** DeadElectron.\n\n**Susan:** Yeah, DeadElectron! I like it, DeadElectron.\n\n<iframe src=\"https://www.youtube.com/embed/gA03iyI3yEA\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\n*This company really does make AI music.*\n\n**Scott:** There we go, yeah. Is anything going to get better? Is my life going to be easy now, because AI dictates my life? Are you going to get the Jetson's finger cramp from pressing the button?\n\n**Susan:** From pressing the button? It was a long day at work! My finger is so cramped from pushing that button all day long!\n\n**Scott:** Now you can have a to-do list, generated from your friendly AI, telling you what to do.\n\n**Susan:** That's a cool thing. Tell me another good thing, Scott?\n\n**Scott:** Yeah, do you think that that's actually a good thing, though?\n\n**Susan:** Of course, it's going to be shaping the way that you're going to do your day. I mean, yeah, AI is saying, hey, you had these four tasks, and this fifth one that you don't quite remember me putting on there for you.\n\n**Scott:** You're supposed to do this. You're supposed to walk by this billboard.\n\n**Susan:** I don't really remember saying I had to do this.\n\n**Scott:** You're supposed to buy the no-egg mayo, okay?\n\n**Susan:** We're basically going to be well-trained humans. I mean, really, people want to be lazy, right?\n\n**Scott:** It's the human condition.\n\n**Susan:** It's the human condition. If you take that cognitive load off, it's like, \"Yours; you can take it.\" Then, you just follow along like the herd. As long as it's not too far away from what you expect, you're like, ah, whatever. Here's the list of things I'm supposed to do today to be a functioning adult. Oh, on there is buy such-and-such a brand mayonnaise. Oh, of course, that's what I wanted, click.\n\n## What about AI dating?\n\n**Susan:** What do you think about AI dating? Will your soulmate be picked out?\n\n**Scott:** For you, yeah, so you have my AI talk to your AI. They'll figure things out.\n\n**Susan:** Ooh, they can go on a pre-date.\n\n![dating](https://res.cloudinary.com/deepgram/image/upload/v1661976816/blog/what-does-the-ai-dystopia-look-like-ai-show-2/dating-bots.jpg)\n\n*The press seem to believe that we will be dating robots, when indeed, it's the robots who'll date in our stead-proxy dating.*\n\n**Scott:** Yeah, a pre-date to figure out if the ... It's like blind dating, but you blame the AI now, if it doesn't go well.\n\n**Susan:** In a blink of an eye, will the AIs go through the whole thing, the whole date a couple times?\n\n**Scott:** The whole thing. Yeah, a second later, they decided.\n\n**Susan:** They'll decide to shack up together. They'll get married. They'll have little AI children. They'll have some messy fights. Then you'll decide to get divorced, and it'll all break up, and by the end of it, it comes back to you, and it says, \"Stay away from this one.\"\n\n**Scott:** It happens in the span of a second.\n\n**Susan:** It's not going to end well.\n\n**Scott:** It's going to take all the spice out of life.\n\n**Susan:** Yeah, but what if it comes back and says, \"This is the one.\" Now, you sit down and you're like, \"My AI says you're the one.\" The other one says, \"Yeah, my AI says you're the one,\" so how should we act at that point.\n\n**Scott:** Now you're entitled, right? You're stuck with me. We have to be together.\n\n**Susan:** We have to be together. Should I even try?\n\n**Scott:** You just start letting it go right then, right? There's no salad for dinner. It's two T-bone steaks and some Indian food afterward.\n\n**Susan:** Your first meal, you're sitting there, you eat, you belch, you undo the top button. It's like, it doesn't matter. You're my one. You're not going to leave.\n\n**Scott:** It's decided.\n\n**Susan:** The AIs have determined what's going to go on here.\n\n**Susan:** Wait. Let me check how many children we're supposed to have. Oh, two. AI is your new boss\n\n**Scott:** They'll be watching you at work, too. It's like this is some little Santa stuff here.\n\n**Susan:** Do you think they're going to be watching at work? What are they going to do?\n\n**Scott:** Yeah, performance measurement, man. It's like the thing that everybody gets advertised to on Instagram now, that's about slouching. You tape it to your back, and it tells you if you're leaned over, and it zaps you, right? Now, at work, it'll be cameras watching you.\n\n**Susan:** Everywhere.\n\n> \"They'll be looking at your email. Have you sent enough emails today? Have you attended enough meetings? Have you spoken enough words to your coworkers? Have you said enough buzzwords?\"\n\n**Susan:** A performance review will come into your email, completely crafted by some sort of machine learning algorithm.\n\n**Scott:** Every day.\n\n**Susan:** It'll be brutal. Here are the 10 things you did well, and here's the 10 things you need to work on, you know?\n\n**Susan:** The 10 up at the top- They're kind of fluff.\n\n**Scott:** It's like we have to give you a sandwich.\n\n**Susan:** Yeah, we've got to make you think you're any good but...\n\n**Scott:** A perfectly crafted visual representation of your day to make you react and like, damn, I've got to work harder.\n\n**Susan:** Here's the box you need to be in. If you're not in this box, you are a problem.\n\n**Susan:** There's huge promise in that. Don't get me wrong, but there are dangers of fitting too much to the mean there.\n\n## The machines use us for only one thing\n\n**Scott:** What's the value of a human anymore? I mean, this is just going to be The Matrix soon, where we're just a power source for AI overlords. What's the deal?\n\n**Susan:** I never quite got the power source of the Matrix anyway.\n\n**Scott:** I don't know the power source thing either, but some other thing ... I don't know.\n\n**Susan:** That was weird.\n\n**Scott:** Are we actually going to be more creative?\n\n**Susan:** Oh, is only creativity left?\n\n![imagine](https://res.cloudinary.com/deepgram/image/upload/v1661976817/blog/what-does-the-ai-dystopia-look-like-ai-show-2/04-11_Robots_repost-unsplash-web.jpg)\n\n*Source: [Jehyun Sung](https://unsplash.com/photos/xdEeLyK4iBo)*\n\n**Scott:** Our only job ... Yeah, that's it, and you take in all the inputs and you're like, \"No, it should be this way.\"\n\n**Susan:** You do that one creative thing a year. That's it. There's this exact one moment where you add randomness to the system.\n\n**Scott:** Exactly.\n\n**Susan:** You do some irrational thing.\n\n**Scott:** \"No, it shouldn't be this way!\"\n\n**Susan:** Suddenly, you get $100,000 because that was your job. That one creative thing.\n\n**Scott:** We needed that. We needed that random thought there. Yeah, everything ... We're too logical.\n\n**Susan:** Thanks, you're our random number generator.\n\n**Scott:** Yeah. What ... We already know the answer.\n\n**Scott:** Random number generator.\n\n**Susan:** They don't need us for power. They need us for random number generation.\n\n**Scott:** We figured it out, yeah, yeah. Well, this is the value of children, right?\n\n**Susan:** Talk about randomness. You never thought the things that could happen would happen. Yeah, 2:00 in the morning, what is that sound? Why is there paint everywhere? Please, please, I told you, not the cat! I'm sorry, is that what we were talking about, Scott?\n\n**Scott:** We'll just all be children in the AI world now. You know, just banging pots and pans together.\n\n**Susan:** I'm looking forward to that.\n\n**Scott:** Yeah, it sounds like a pretty sweet existence, right?\n\n**Susan:** Nap time, you know?\n\n**Scott:** With milk, warm milk, and you have your blanket.\n\n**Susan:** It's going to be awesome. Yeah, the lights slowly go down.\n\n![nap](https://res.cloudinary.com/deepgram/image/upload/v1661976818/blog/what-does-the-ai-dystopia-look-like-ai-show-2/napercise.jpg)\n\n*[\"Napercise\"](https://www.forbes.com/sites/brucelee/2017/04/29/napercise-why-nap-for-free-when-you-can-pay-for-it/#68e889164be4), as David Lloyd Clubs in London calls it, is not far off. Soon all humans will do is be randomly creative and drink juice from sippy cups.*\n\n**Scott:** Is this something you did at school back in the day? This is something that they do at school now in California, at least. You have your own blanket. You get your milk. You take a nap.\n\n**Susan:** I love it.\n\n**Scott:** You're like 10.\n\n**Susan:** Whoa, 10?\n\n**Scott:** I mean, 9, 8-\n\n**Susan:** Great. I remember doing it from kindergarten.\n\n**Scott:** Yeah, I know. I remember it way back. Did you have your special mat?\n\n**Susan:** The most valuable skills from kindergarten ... We completely forget those. Everybody should have nap time.\n\n**Scott:** Well, because we get trained to be more like robots, but now the robots are finally going to take over the rightful owner of those tasks, and now we just get to be children all the time.\n\n## Your insurance premiums go up\n\n**Susan:** They're going to tell you when you're not doing the optimal thing each day, right? This is when you should be napping. Oh, man, oh, you didn't nap. For that day, your insurance premium went up by a dollar. You are not living the optimally healthy life.\n\n**Scott:** Yeah, for randomness. They need you to be healthy for the randomness.\n\n**Susan:** Yeah, but I'm just saying that AIs are going to come in and judge every second of your life, and you're going to be charged more based off of you not living the right way. That beer you wanted? It didn't just cost a couple bucks. It also got reported back to your insurance company.\n\n**Scott:** Well, maybe human lifespan doesn't need to be as long, now, because we lose our mojo by the time we're 30, 20, 15, you know?\n\n**Scott:** It's like, eh, screw you, after a while.\n\n**Susan:** Ooh, it could be like Logan's Run. At 25, you're dead-only they trick us. They say, \"Your brain is being uploaded into the cloud. In reality, no.\n\n**Scott:** Bye, everybody. I can't wait to see all my ancestors.\n\n**Susan:** Yeah, some quick little chatbot has put up that fake that says it's you for about a day, until everybody forgets that you exist.\n\n**Scott:** That's true. Let's say, a week, right? Yeah.\n\n**Susan:** A week, for a week your loved ones are typing to you.\n\n**Scott:** \"Oh, look at you. Hey ...\"\n\n**Susan:** \"Oh, it's so great in the cloud.\" They say, \"Yes, you will love it here.\" A week later, \"Got to go now. There's so many exciting things. I can't pay attention to chat anymore.\"\n\n**Scott:** Yeah, \"I can't wait to see you.\"\n\n**Susan:** Then erase that from the system.\n\n**Scott:** Then they ghost you, yeah. All right, well, we have some worst case scenario. Maybe in the future, we'll have some best case scenario. I mean, the best?\n\n**Susan:** Yeah.\n\n**Scott:** At least something.\n\n**Susan:** Do you think [AI could actually be good for the world?](https://youtu.be/pE5-K7gg0kE)\n\n**Scott:** Nah, I don't think so.\n\n**Susan:** You know what? I have a sneaking suspicion that for everything we said was bad, there might be a couple good things.\n\n**Scott:** Are there some good things?\n\n**Susan:** There just might be.";
						}
						async function compiledContent$e() {
							return load$e().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$e() {
							return (await import('./chunks/index.0c75d62a.mjs'));
						}
						function Content$e(...args) {
							return load$e().then((m) => m.default(...args));
						}
						Content$e.isAstroComponentFactory = true;
						function getHeadings$e() {
							return load$e().then((m) => m.metadata.headings);
						}
						function getHeaders$e() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$e().then((m) => m.metadata.headings);
						}

const __vite_glob_0_262 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$e,
  file: file$e,
  url: url$e,
  rawContent: rawContent$e,
  compiledContent: compiledContent$e,
  default: load$e,
  Content: Content$e,
  getHeadings: getHeadings$e,
  getHeaders: getHeaders$e
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$d = {"title":"What is ASR?","description":"ASR is a technology that process audio data (phone calls, voice searches, podcasts, etc.) into a format computers can understand.","date":"2018-10-16T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981317/blog/what-is-asr/what-is-asr%402x.jpg","authors":["morris-gevirtz"],"category":"ai-and-engineering","tags":["machine-learning","deep-learning","voice-tech"],"seo":{"title":"What is ASR?","description":"ASR is a technology that process audio data (phone calls, voice searches, podcasts, etc.) into a format computers can understand."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981317/blog/what-is-asr/what-is-asr%402x.jpg"},"shorturls":{"share":"https://dpgr.am/fd26d4f","twitter":"https://dpgr.am/e00b23a","linkedin":"https://dpgr.am/db02f18","reddit":"https://dpgr.am/0c68606","facebook":"https://dpgr.am/a328c4e"}};
						const file$d = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-is-asr/index.md";
						const url$d = undefined;
						function rawContent$d() {
							return "Automatic Speech Recognition, also known as ASR or Voice Recognition, is a term you've heard a lot in recent years. In a sentence,\n\n> ASR is series of technologies used to automatically process audio data (phone calls, voice searches on your phone, podcasts, etc.) into a format computers can understand. Often readable text, it is a necessary first step to figuring out what cool information is hiding in voice recordings.\n\nSince its fledgeling beginnings in the 1950s, ASR has come a long way-once an ARPA (DARPA) project in the 70's, then a frustrating dictation product in the 90's to a buzzword today. \n\n![alt](https://res.cloudinary.com/deepgram/image/upload/v1661976380/blog/what-is-asr/Sputnik_asm-2.jpg)\n\n*News of the successful launch of the first artificial satellite [\"Sputnik\"](https://en.wikipedia.org/wiki/Sputnik_1#/media/File:Sputnik_asm.jpg) by the USSR (1957) spurred President Eisenhower to create ARPA (Advanced Research Projects Agency) in 1958. This agency is responsible for the creation of some minor 20th century inventions such as the computer mouse and the Internet, as well as every Tom Clancy plot ever.*\n\nSince the advent of products like Siri, Alexa, and Google Home there has been a lot of excitement around the term. This is because [ASR technologies are the driving force behind a wide array of modern technologies](https://blog.deepgram.com/what-makes-alexa-siri-terminator-and-hal-tick/). If you have a smartphone -66% of living humans do (and at least one [Rhesus monkey](https://www.youtube.com/watch?v=YQW2mNcZT5o)), then you have ASR technologies at your fingertips. And it's used by 83% of business to transcribe speech, according to respondents in [The State of Voice 2022](https://deepgram.com/state-of-voice-technology-2022/). However, like so many oft-thrown-about terms, ASR seems to remain a nebulous term.\n\n* What is ASR really?\n* What does ASR do?\n* How can I use ASR?\n* What are the limitations of ASR?\n* Why should I care about it?\n\nOur goal in this and subsequent articles is to answer some of these questions and, perhaps, ask a few questions of our own.\n\n## What is ASR?\n\nWell, as you may have gathered from the first sentence above, ASR stands for **Automatic Speech Recognition**. The purpose of ASR is to convert audio data into data formats that data scientists use to get actionable insights for business, industry and academia. Most often, that data format is a readable transcript. Sounds simple, and in principle it is. Let's unpack the three words behind ASR so we can make more sense of what is going on:\n\n* The term **`automatic`** makes reference that after a certain point, machines are doing some human task ***without any*** human intervention. Speech data in, machine-readable data out.\n* The term **`speech`** tells us that we are working with audio data -technically ***any audio data***. These include noisy customer recordings from an angry customer calling from a 16 lane highway in Los Angeles, to super-crisp, extra bass-y podcast audio.\n* The term **`recognition`** tells us that our goal here is to convert audio into ***a format*** that computers can understand (often a text a transcript). In order to do neat things with audio data, such trigger a command to buy something online (think: OK Google) or figure out what sort of phone sales interactions lead to better sales numbers, you need to convert audio data into a parsable data format for machines (and humans) to analyze.\n\nHowever, **not all ASR is made equal.** There are many approaches to converting speech into text-some better than others....\n\n## 3 Ways to Do Automatic Speech Recognition\n\nThere are essentially three ways to do automatic speech recognition:\n\n1. **The old way**\n2. **The Frankenstein approach**\n3. **The new way (End-to-End Deep Learning)**\n\nASR technologies began development in the 1950 and 1960s, when researchers made hard-wired (vacuum tubes, resistors, transistors and solder) systems that could recognize **individual words** -not sentences or phrases. That technology, as you might imagine, is essentially obsolete (I do suspect, however, that those people who are into vacuum tube amps, might still be into it).\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-make-application-voice-ready\"></WhitepaperPromo>\n\n### 1. The Old Way\n\nIn the 1970's, with ARPA funding, a team at [Carnegie Melon University](https://asa.scitation.org/doi/abs/10.1121/1.381666?journalCode=jas) developed technology that could produce transcripts from context-specific speech, e.g. voice-controlled versions of chess, chart-plotting for GIS and navigation and office-environment document management. Briefly, in that approach (revolutionary at the time) audio was converted into machine-readable text in a series of logical steps: load audio, reduce noise, cut up into meaningful sounds, statistically guess the sounds, statistically combine said sounds to guess the words, compare resulting options, output a best guess. In the 1990s, this approach (with improvements) allowed Nuance to make a piece of software (Dragon Translate) that was not too bad at transcribing single speakers when they spoke clearly in low-noise environments. Unfortunately, it took many hours to train the software to transcribe what one person said -again, assuming perfect acoustic surroundings -like the back of my closet, where all the clothes muffle any noise, especially my gentle cries of frustration. Products like these had one major limitation: they could only reliably convert speech to text for one person. This is because **no two people speak alike**. Among any two speakers there are variations in pronunciation, tone, word-choice, grammar choice, even amount of lung pressure, that from a mathematical perspective (computers speak math) make what they say completely different-*even if it's the same to you and me*. In fact, even if the same person utters a sentence twice, the sounds when recorded and measured are mathematically different! \n\n![alt](https://res.cloudinary.com/deepgram/image/upload/v1661976381/blog/what-is-asr/Butterfly_v4-1.png)\n\n*These are two spectrograms of two people saying the same word: **butterfly.** Spectrograms are one way to visualize audio data. As you can see, these two spectrograms are very different from one another. Pay special attention to the slopes of the darker lines and their relative shapes. Same word to our human, meat-based brains, two mathematical realities for silicon brains!*\n\nThough unable to transcribe the utterances of multiple speakers, these ASR-based, personal transcription tools and products were revolutionary and had legitimate business uses.\n\n### 2. Franken-ASR\n\nIn the mid-2000s, companies like Nuance, Google and Amazon figured out that they could make the old-school 1970s approach to ASR work for multiple speakers and better in noisy environments. To do this, these big companies replaced a small part of their speech recognition contraption with a new gizmo: neural networks. Rather than having to train ASR to understand one individual, these franken-ASRs were able to understand multiple speakers decently well - a considerable feat given the acoustic and mathematical realities of spoken language (see above). The reason that this is possible is because these neural-network algorithms can \"learn on their own\" when given certain stimuli. In the case of stimuli for ASR, that stimulus is painstakingly human-transcribed audio data. As a result, some pretty neat products were made possible. However, **the downside is that by slapping a neural network on top of the older machinery (recall, based on techniques from the 1970s), results in bulky, complex and resource hungry machines like the DeLorean from Back-to-the-Future** or my college bicycle: a franken-bike that worked, when the tides and winds were just right, usually, except when it didn't. (note: just cause you add a Mr. Fusion to your time-machine/bicycle, it does not mean that the other parts will work any better). \n\n![alt](https://res.cloudinary.com/deepgram/image/upload/v1661976382/blog/what-is-asr/Frankenstein-s_monster_-Boris_Karloff--1.png)\n\n*No picture of my franken-bike available as it is still classified. Above, [Boris Karloff](https://en.wikipedia.org/wiki/Boris_Karloff) portrays Frankenstein's monster in the 1931 Hollywood classic.*\n\nWhile clunky, the hybrid approach to ASR developed in the mid-2000s, works well enough for certain applications, after all, you don't expect to solve any real data questions with Siri. Philosophical ones however...\n\n### 3. End-to-End Deep Learning\n\nThe newest approach, **end-to-end deep learning ASR**, leverages the power of neural nets and scraps the clunky 1970's approach. Essentially, this new approach allows you to do something that was not possible even 2 years ago: [to quickly train the ASR to recognize dialects, accents and industry-specific word sets and to do so accurately](https://deepgram.com/product/train/). Think of this as a purpose-built, Mr. Fusion bicycle, no rusty bike-frames or ill-fated auto brands. The magic behind this depends on a few things: break-through math from the 80's, computing power/technology from the mid 2010s, big data, and the ability to rapidly innovate. This last idea is important. Being able to try new architectures, technologies and approaches is critical. Consider something known as the Concorde syndrome: if a company university (or any entity) has invested a lot of time and money in a huge machine that works more or less, they won't have much motivation to start from scratch even *if doing so makes engineering/financial/logical sense.* \n\n![alt](https://res.cloudinary.com/deepgram/image/upload/v1661976383/blog/what-is-asr/concorde.jpg)\n\n*The [Concorde](https://picryl.com/media/concorde-british-french-supersonic-transport-airplane-db633e) project, first devised before humans walked on the moon, and completed long after, wound up costing 20 times the projected amount. In 2003, when it made its last commercial flight, a seat on the plane cost about $5,500 ($7,300 in 2018 dollars).*\n\nWith modern approaches to ASR (specifically end-to-end deep learning approaches) we can build ASR systems that allow us to make highly accurate transcripts of audio data that have specialized words, accents, noise conditions etc., but with a technology that scales well and is always evolving. For example, modern ASR systems are able to further overcome noise and speaker variability issues by taking linguistic context into account. This means that computers are learning to distinguish the meaning of the word \"tree\" when found in conversations about family versus ones about botany. Legacy ASR systems built using the hybrid, franken-ASR are designed to deal with \"general\" audio, not specialized audio for industry, business or even academic purposes. In other words, they provide generalized speech recognition and cannot realistically be trained to improve on your speech data.\n\n## Questions to be answered...\n\nHopefully, by now, you have had a few of your questions answered. Most likely you also have other questions burning in your mind. For example, you might have gathered that ASR-in some form or another-is behind products like OK Google and Siri, but you may wonder:\n\n* Where else can I find ASR?\n* What can you do with a good transcript of audio data?\n* What sorts of secrets lie hidden in all that talk?\n* [How do I evaluate speech recognition providers?](https://blog.deepgram.com/how-to-test-automatic-speech-recognition-asr-providers-for-your-business/)\n* What are the steps to test ASR providers?\n\n**ASR is the technology that, deployed intelligently, creates highly accurate transcripts and opens the door to insights that lead to lower costs, higher revenue and academic discovery.**";
						}
						async function compiledContent$d() {
							return load$d().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$d() {
							return (await import('./chunks/index.09866bca.mjs'));
						}
						function Content$d(...args) {
							return load$d().then((m) => m.default(...args));
						}
						Content$d.isAstroComponentFactory = true;
						function getHeadings$d() {
							return load$d().then((m) => m.metadata.headings);
						}
						function getHeaders$d() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$d().then((m) => m.metadata.headings);
						}

const __vite_glob_0_263 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$d,
  file: file$d,
  url: url$d,
  rawContent: rawContent$d,
  compiledContent: compiledContent$d,
  default: load$d,
  Content: Content$d,
  getHeadings: getHeadings$d,
  getHeaders: getHeaders$d
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$c = {"title":"What is Automatic Speech Recognition: Past, Present, and Future? ebook","description":"Learn about automatic speech recognition (ASR), the 3 different methods out there and why End to End Deep Learning is the future.","date":"2021-05-03T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981369/blog/what-is-automatic-speech-recognition-past-present-and-future-ebook/what-is-asr-ebook%402x.jpg","authors":["shae-burnette"],"category":"speech-trends","tags":["voice-tech"],"seo":{"title":"What is Automatic Speech Recognition: Past, Present, and Future? ebook","description":"Learn about automatic speech recognition (ASR), the 3 different methods out there and why End to End Deep Learning is the future."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981369/blog/what-is-automatic-speech-recognition-past-present-and-future-ebook/what-is-asr-ebook%402x.jpg"},"shorturls":{"share":"https://dpgr.am/27dc32a","twitter":"https://dpgr.am/3d6b9d1","linkedin":"https://dpgr.am/7c49eac","reddit":"https://dpgr.am/245db6c","facebook":"https://dpgr.am/6a7a2e6"}};
						const file$c = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-is-automatic-speech-recognition-past-present-and-future-ebook/index.md";
						const url$c = undefined;
						function rawContent$c() {
							return "Automatic Speech Recognition (ASR) is a powerful speech to text solution that can help you get the most out of your audio data. Want to know more? Enter [Automatic Speech Recognition: Past, Present, and Future](https://offers.deepgram.com/what-is-asr-ebook) ebook. A quick yet deep dive into ASR. You'll find out what ASR really is, why it is so important, how exactly it works, and where it's headed.\n\n**What is ASR?** First, the [ebook](https://offers.deepgram.com/what-is-asr-ebook) explains what ASR stands for, and the powerful methods and goals hidden in those three deceptively simple words. It then highlights the difference between structured and unstructured data, and gives you insight into how ASR enables companies to **extract value from the 90% of unstructured data** that would otherwise languish un-researchable in storage.\n\n**Why do you need it?** You might say, I don't deal with audio data.  But what would you gain if you could data mine this conversational data? We list eight revenue generating and cost saving benefits from implementing ASR, and show the almost unlimited potential for ASR to be an important part of your organizational strategy. In the [2021 State of ASR](https://deepgram.com/state-of-asr-report/)report by Opus Research, **over 85% or organizations say speech recognition is \"important\" or \"very important\" to future business strategy.**\n\n**How do the three types of ASR work?** There are three types of ASR: the Traditional Method (or Tri-Gram Model,) The Hybrid Method (which adds neural networks and AI,) and the [End-to-End Deep Learning Method](https://f.hubspotusercontent00.net/hubfs/6890003/Whitepapers/Whitepaper%20How%20Deepgram%20Works%20-%20Aug%202020.pdf?__hstc=70535183.6e87c8bcb0b6e961cd8f3630ef1b0074.1606858477151.1609018533129.1609801437749.38&__hssc=&hsCtaTracking=60727426-f1f6-4ead-a813-3a53ba4335fe%7Cbf87d452-4ef3-49a9-8ce8-611a97830b70). The [ebook](https://offers.deepgram.com/what-is-asr-ebook) lays out clearly the pros and cons of each when it comes to extracting value from conversational data in a world of infinitely diverse voices. Once you understand the differences, pros and cons between these three types of ASR, you will understand why we feel End to End Deep Learning ASR is the best method now and for the future.\n\n**What does the future hold?** Lastly, you will learn where we are on the path to a Unified ASR Solution: full language understanding. And how you can position yourself to be ready for the Unified ASR Solution. The use of ASR to extract value from unstructured data is a fresh and rich frontier, awaiting bold explorers. This [ebook](https://offers.deepgram.com/what-is-asr-ebook) is the map you need to start your journey to harvesting its untold, untapped resources.";
						}
						async function compiledContent$c() {
							return load$c().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$c() {
							return (await import('./chunks/index.237f3456.mjs'));
						}
						function Content$c(...args) {
							return load$c().then((m) => m.default(...args));
						}
						Content$c.isAstroComponentFactory = true;
						function getHeadings$c() {
							return load$c().then((m) => m.metadata.headings);
						}
						function getHeaders$c() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$c().then((m) => m.metadata.headings);
						}

const __vite_glob_0_264 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$c,
  file: file$c,
  url: url$c,
  rawContent: rawContent$c,
  compiledContent: compiledContent$c,
  default: load$c,
  Content: Content$c,
  getHeadings: getHeadings$c,
  getHeaders: getHeaders$c
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$b = {"title":"What is AutoML? How the Technology Paves the Way for the Future of ASR","description":"There is a lot of buzz around the term \"AutoML\" and what it can really do for companies. Learn what it means here.","date":"2020-10-22T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981357/blog/what-is-automl-how-the-technology-paves-the-way-for-the-future-of-asr/what-is-automl%402x.jpg","authors":["katie-byrne"],"category":"ai-and-engineering","tags":["machine-learning","deep-learning"],"seo":{"title":"What is AutoML? How the Technology Paves the Way for the Future of ASR","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981357/blog/what-is-automl-how-the-technology-paves-the-way-for-the-future-of-asr/what-is-automl%402x.jpg"},"shorturls":{"share":"https://dpgr.am/6c64f6c","twitter":"https://dpgr.am/8542cd5","linkedin":"https://dpgr.am/2f06f98","reddit":"https://dpgr.am/e042049","facebook":"https://dpgr.am/6155e95"}};
						const file$b = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-is-automl-how-the-technology-paves-the-way-for-the-future-of-asr/index.md";
						const url$b = undefined;
						function rawContent$b() {
							return "There is a lot of buzz around the term \"AutoML\" and what it can really do for companies. When you hear the term \"AI creating AI\", it's bound to incite a lot of excitement... and curiosity. At its core, AutoML means automated machine learning. It gives enterprises the power to train and deploy thousands of models to fit the needs of various customers and industries through an automated approach. This capability especially benefits speech recognition, which is notoriously time-consuming, complex and expensive. With 73 percent of organizations [expecting to spend more money](https://deepgram.com/state-of-asr-report/) on speech recognition in the next 12 months, it's critical to ensure that the technology is worth the investment and is as efficient as possible. Our team recently announced [Deepgram AutoML](https://blog.deepgram.com/deepgram-pioneers-novel-training-approach-setting-new-standard-for-ai-companies-2/), a new training capability that streamlines AI model development, reducing manual cycles for data scientists and developers, while providing them with the best accuracy on the market. In light of this exciting news, we wanted to provide a deeper dive into what AutoML actually is and how it can unlock value for companies' voice data.\n\n## Background on AutoML\n\nAutoML is a relatively new process that automates iterative tasks of machine learning model development, essentially allowing AI to create the next AI. Typically, when building a machine learning model, a data scientist needs to perform various tasks such as collecting representative data sets, prepping those files, defining model training curriculums, neural network parameters, and much more. Each of these micro-decisions determines a model's effectiveness, however tedious tasks can pull data scientists away from strategic areas that also need their attention, such as ways to reduce bias. AutoML removes tedious steps and allows data scientists to more rapidly iterate on their models, to reach their accuracy and overarching business goals faster.\n\n## How AutoML Unlocks Speech\n\nWhile AutoML has been used in Natural Language Processing (NLP), image recognition and vision for the last few years, Deepgram AutoML is the first time that AutoML has been deployed in automatic speech recognition (ASR). As the world's fastest, most accurate, reliable and scalable speech solution, Deepgram is uniquely positioned to provide easy access to this technology. By leveraging AutoML within ASR, organizations of all sizes can now:\n\n* **Extract even more value from their audio:** Most enterprises have years of recordings, but don't have the insights readily available to unlock value for their business. For example, imagine being able to arm your sales team with knowledge articles dynamically as they are speaking with a customer, or notifying a manager or technical expert to jump on the phone at just the right time to answer a question. By leveraging AutoML, your data science team can develop speech recognition models faster, with higher accuracy than ever before. With accurate, reliable transcriptions delivered quickly to engineering you can build delightful experiences for internal and external customers now-not in 6 or 12 months-that will differentiate you from your competition.\n* **Solve the talent gap:** Data scientists with experience in deep learning are hard to find and hard to keep. By leveraging AutoML, you can reserve precious productivity cycles for data scientists and allow them to focus on the strategic problems and opportunity areas for your business. AutoML enables data scientists to scale the development of models, and mentor less experienced data enthusiasts to implement low code/no code solutions that also contribute to company results.\n* **Pave the way for accurate NLP:** By integrating ASR powered by AutoML, you'll lay the foundation for accurate NLP. If you don't have a reliable foundation, your data classification won't be accurate either, which will cause additional feedback cycles for data scientists, engineers, compliance and QA teams down the road. Once the ASR foundation is set and you no longer have to worry about the quality of the data, you can turn your attention to how to classify the data, and find ways to create a deeper understanding of your customer.\n\nAutoML is an important next step in the evolution of AI and ML and we're excited to offer access to this technology to our customers and partners. To learn more about our AutoML offering, check out our blog post [here](https://blog.deepgram.com/deepgram-pioneers-novel-training-approach-setting-new-standard-for-ai-companies)!";
						}
						async function compiledContent$b() {
							return load$b().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$b() {
							return (await import('./chunks/index.71cdf414.mjs'));
						}
						function Content$b(...args) {
							return load$b().then((m) => m.default(...args));
						}
						Content$b.isAstroComponentFactory = true;
						function getHeadings$b() {
							return load$b().then((m) => m.metadata.headings);
						}
						function getHeaders$b() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$b().then((m) => m.metadata.headings);
						}

const __vite_glob_0_265 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$b,
  file: file$b,
  url: url$b,
  rawContent: rawContent$b,
  compiledContent: compiledContent$b,
  default: load$b,
  Content: Content$b,
  getHeadings: getHeadings$b,
  getHeaders: getHeaders$b
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$a = {"title":"What is Code-Switching? And How Did it Make English?","description":"What is code-switching, why is it an important concept to understand, and how did it make English what it is today? Read on to find out.","date":"2022-10-11T13:42:39.408Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1665495814/2209-codeswitching-featured-1200x630_nukh4d.png","authors":["morris-gevirtz"],"category":"linguistics","tags":["code-switching","english","language"],"seo":{"description":"What is code-switching, why is it an important concept to understand, and how did it make English what it is today? Read on to find out."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1665495814/2209-codeswitching-social-1200x1200_vj8wgr.png","description":"What is code-switching, why is it an important concept to understand, and how did it make English what it is today? Read on to find out."},"shorturls":{"share":"https://dpgr.am/9db52f2","twitter":"https://dpgr.am/2b5e3fb","linkedin":"https://dpgr.am/c96294f","reddit":"https://dpgr.am/5b96df7","facebook":"https://dpgr.am/04a9fb2"}};
						const file$a = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-is-code-switching-and-how-did-it-make-english/index.md";
						const url$a = undefined;
						function rawContent$a() {
							return "You don't know this, but you don't speak English. You actually speak Fronglish—a mix of Anglo-Saxon, Norse, and Old French. You eat in French: pork, beef, poultry are French words. However, you hunt (and slaughter) in English: boar, cow, goose. Surprisingly, while you are born in English, the end of life comes in Viking. Death is taboo in many cultures, and so often the words used to refer to \"the end\" are euphemisms or foreign words. The word \"to die\" is a Norse word borrowing. \n\nIn addition to speaking fluent Fronglish, I also can speak Spanglish. I was raised in a more-or-less bilingual home—my mother is Chilean, my father a New Yorker. As a result, I have an easy time in California, where Spanglish is spoken as commonly as English and Spanish. I can read the New Yorker as well as Revueltas, a Chilean magazine. By contrast, being a Fronglish speaker is not sufficient training to understand French or Norwegian. This is different from being a Spanglish speaker. How do we explain this? \n\nIn this article, we will explore some of the core concepts of what language is and attempt to structure some decidedly complex human social phenomena. While we will be unable to draw hard and fast lines (because they don’t exist), this exercise is crucial for researchers who hope to use AI techniques to build language models. Because AI relies on well-structured, curated data, the definitions (and assumptions) that guide the structuring and curation make difference between success, failure or “[bias](https://blog.deepgram.com/detecting-and-reducing-bias-in-speech-recognition/).”\n\n## What is Code-Switching?\n\nSpanglish, a portmanteau of “Spanish” + “English”, is generally considered code-switching. There are other types, of course, such as Hinglish (Hindi and English), Taglish (Tagalog and English), along with many other mixes. Each of these is different from each other in many ways, linguistic and cultural. But they do have a few things in common. \n\nOn its face, code-switching is a straight-forward communicative behavior: two or more people speak and, when they speak, they use two or more languages. This would be enough of a definition to disentangle Fronglish from Spanglish, if there existed a reliable definition for the concept “a language.” Sadly, [the concept of language is not well defined either](https://deepgram.com/blog/difference-between-language-dialect/).\\\n\\\nFor example, according to 19th & 20th century nationalists, Norwegian and Danish are separate languages, yet I am told that the language of Madrid is the same language as the language of Santiago de Chile. Likewise, the Turkish of the hills of Rize is somehow Turkish, but the language spoken in the capital of Azerbaijan is not Turkish. I think more than one Istanbulite would agree that it is often easier to understand the Azerice of Azerbaijan than the “[Rizece](https://www.youtube.com/watch?v=ChnRgKF9L0s)” of Rize!\n\nThe simple definition of code-switching suggests that an Azerbaijani person who speaks their native tongue in Istanbul is somehow engaged in code-switching rather than speaking in an odd or cool accent. If you consider Azerice and Türkçe to be dialects of one another, then the Azerbaijani living in Istanbul now just speaks Turkish, albeit with a different accent. Here we see that because we don’t have clear lines between “language” and “dialect” we have a hard time telling how many languages are being spoken at one time.\\\n\\\nIn his analysis of the state of Yiddish in the world, Max Weinreich is famously quoted as saying: \"A language is a dialect with an army and navy.\" This pithy quote helps explain why the Danes and Norwegians don't speak the same language and yet the hill-folk of Rize and the pampered urbanites of Bebek in Istanbul somehow do. In the 19th century, nationalists worked hard to differentiate the ex-Viking dialects and drew a hard line where there wasn’t one before.\n\nWe need to put boundaries somewhere. Let's imagine that two languages are different so long as they are not mutually intelligible. \n\n“Code-switching as a communicative behavior wherein 2 or more speakers are able to speak while mixing two languages.”\\\n\\\nGreat, we solved it. No need to think any further.\\\n\\\nBut wait, If you go to a taco truck and say, \"Can I have three birria tacos and a tamarind Jarrito?\" does this count as code-switching? The easy answer is no, but a better answer is, it depends. To understand why it depends, we have to ask ourselves what borrowed words are and why people do the code-switching thing.\n\nThis second definition belies a few fascinating notions. \n\n1. This definition does not account for word borrowings.\n2. For it to work, we must assume that the 2+ speakers have reasonably comparable understandings of the entirety of the code-switched languages. \n\nLet's unpack these concepts.\n\n## Loanwords Aren’t Code-Switching\n\nUsing words from a different culture in your speech does not necessarily entail that you are switching back and forth between languages. When you order a [taco](http://etimologias.dechile.net/?taco.n), refer to the [zeitgeist](https://www.etymonline.com/word/zeitgeist#etymonline_v_5006), or even say the word [Saturday](https://www.etymonline.com/word/Saturday#etymonline_v_22771), you are not engaging in code-switching despite the fact that none of those words is of English origin. No language is pure in any sense of the word. Words, grammar, ideas, technology, art, just about every element in any one culture has been inherited over the millennia through contact with “different” people.\n\nPeople come into contact with each other for different reasons, for different durations of time and as a result, cultural contact affects language and culture in unpredictable ways. Trade, war, migration, and invasion are four of the myriad reasons why large groups of people leave their home and travel to distant lands and interact with each other.\n\n![](https://res.cloudinary.com/deepgram/image/upload/v1665500852/victor-ajayi-RUWOIbf01HY-unsplash_wq5mdf.jpg)\n\nIn the Middle Ages, European traders traveled along long difficult roads and plied treacherous waters looking for things to trade. Deals were made, and traders returned with useful and beautiful things. \n\nThe European traders who did business with Turks, Indians, Persians, Morroccans, etc. often learned local tongues. When they returned home and spoke of the exotic places and things they had seen, they used their own vernaculars (or Latin), inserting foreign words where needed. While the traders came by these ideas and terms through long-term or iterative contact with “foreign” peoples, the majority of their countrymen never even left their village when they learned the exotic words. \n\nFor example, sometime in the Middle Ages Germans became acquainted with Chinese apples \"Apfelsine\" (Apfel = apple, sine = Chinese). In the modern day we can interpret this as: \"Chinese apple\". You call these fruits “oranges”. Many languages got the names for \"the orange\" through imitation of pronunciation. We got that word from “norange” a French interpretation of an Italian word which came by way of Persians or Arabs. Trace it all the way back and you learn that the fruit and the name came from Tamil lands in southern India where it originally meant “fragrant fruit” நரந்தம். \n\nIn this way, ordering a birria taco, or even a coffee (from Arabic قهوة “kahwa”) in the US does not (usually) represent a code-switching event. We must accept the fact that no culture is “pure” and that most of the goods and ideas we have today were developed by foreigners (from other lands and other times). \n\n## So Where Does Code-Switching Come From?\n\nTrade and war can bring very different people into contact and, as a result, new words are often borrowed. However, trade and war don’t always result in prolonged, multigenerational contact between cultures. In scenarios where people do stay, make livelihoods for themselves and, most importantly, reproduce, multigenerational contact leads to very complex cultural power dynamics.\n\nFor example, the Anglo-Saxons learned the words beef, pork, and poultry as part of their slavery to the Norman kings. These were the terms for the expensive protein, the food of the kings. Cow, pig, and goose, the names for the animals hunted and raised for the benefit of the elites, remained English. In this scenario, (Norman) French was the language of the court and the nobility, Latin the language of the church and international diplomacy. The language of the peasant classes remained predominantly Anglo-Saxon and its dialects (which were complex in-and-of-themselves, in part due to the Viking invasions). The farther you were from the court and the clergy, the less likely it was that you spoke French. \\\n\\\nIn the movie Braveheart, the plucky rebel William Wallace impresses the princess and audiences with his ability to speak French and Latin, a sign that he was educated and a worthy general not just a peasant. Presumably, the historic William Wallace spoke Scots, some version of Middle English, and, according to Mr. Gibson, Latin and French. At that time, it would be doubtful that the princess would have known much (Middle) English (or Scots), unless of course her wet-nurse had taught her.\n\n![](https://lh4.googleusercontent.com/dKyqTEvhiHIO_YNxdkifQb6XeGsIOShDu3k30tSYgy7HFOeJk5tSWaaUoe7wdgP_jyh3OOBFCxmqajCSyFL0dNy97tUxzIOsaH47b567K-EAswj5bmmsuEqI-ohRtJlRmO1VkP0YMTm1txcpJtHgyUhuagjv3Irs8z_QuCvD2XmFYD71ljY7HF25Bg)\\\nWhen William Wallace was alive, London-based nobility would speak French natively and were schooled in Latin. The servant staff would likely have been familiar with French, and the higher the servant’s post, the better their French would be. At some point in the social hierarchy, there were people who spoke good French and good local Anglo-language, too. What language is the right language for any task is a matter of complex, situation-specific social dynamics.\n\nA significant portion of the American workforce was born and raised in a Spanish speaking home. For example, according to the State of California ([pdf](https://www.labormarketinfo.edd.ca.gov/specialreports/CA_Employment_Summary_Table.pdf)), ~38% of the labor force is “Hispanic” (including me). However, a large fraction of these Spanish speakers who live and work in the US immigrated to the US as adults. According to the US government, [in 2018 nearly 11,000,000 Latin Americans (most, but not all Spanish speakers) moved to the US](https://www.migrationpolicy.org/article/mexican-immigrants-united-states-2019?gclid=Cj0KCQjwpeaYBhDXARIsAEzItbE8VTJATsZ4ea8GiggmwUB7uyfVcjQ9mRotb1CgIDajCtEEK7HxfVoaAnTrEALw_wcB) (not all stay). What happens when more than 30% of your labor force speaks Spanish? This is a classic population-level language-contact situation.\\\n\\\nNow ask yourself, what would life be like in California if you grew up here but never learned more than 100 words in English (presumably for basic trade/survival purposes)? Instead you chose (or were forced to choose) to speak Spanish almost exclusively. I’d say it could become a rather hard situation. By contrast a very large number of Californians never learn Spanish! Some only speak English, while others do learn additional tongues at home including languages such as: Hindi, Gujarati, Farsi, Mandarin, Vietnamese, etc. Some, of course, don’t speak English or Spanish as they come from other regions of the world.*\n\nToday, people who grow up in California all learn some dialect of English. English is the prestige language of the state and the US government. Which dialect of English you speak and what other non-English languages you learn depends on who your parents are and where in the state you grew up. If you sit outside the right coffee shop in the Bay Area, or you visit the right bar at the right time, you will hear people speaking in English and Spanish at the same time switching back and forth. Actually, I also get to hear people code-switching between Spanish and Nahuatl, but that is a whole ‘nother topic!\\\n\\\nSitting by our code-switching coffee sippers, you may hear full sentences in Spanish with the occasional English word and then a full sentence in English with a Spanish word thrown in. You may hear a sentence start in Spanish but end in an English saying. Unless you two are familiar with both languages, especially the sub-dialects being used, you’d be lost in the conversation. The people who talk like this can get by in each of these languages alone.\\\n\\\nOne person may be a Californio who grew up in ol’ San José. She, let’s call her Azeli, could do business in Spanish, but not super comfortably. Linguists call this kind of speaker “a heritage speaker.” Azeli probably speaks Spanish at home in this conversation and will use sayings, catch phrases, verbs, nouns and quips that all Californios use. Seated with her is Miguel, who came to the states when he was 18 and who speaks really good English. He reads novels in English and Spanish and so he represents the “ideal” foreign language learner. Our third interlocutor is David, who has been in California for 15 years. His English is not perfect, far from it. But he can get along, and do business with people who speak English well, but that’s because he works as a day laborer and does not need a large vocabulary or complex grammar.\\\n\\\nThese folks’ lingua franca is best described as [Spanglish](https://www.youtube.com/watch?v=nYMnNlfSMC0). The issues they face and care about require knowledge of both languages since they live in a world where English is the prestige language and Spanish is a widely spoken language and the language of their families, friends and ancestors. Their lives are not lived “wholly” in one language or another.\n\nThe choice as to what words and grammar and sayings are uttered in one language or another is made no different than how monolinguals make choices. The difference, as stated before, is that people who are acculturated into more than one linguistic tradition can, when in the presence of similarly acculturated people, make use of all the culture, all the symbolism, or language held in common.\n\nThis same phenomenon is why English is so full of sailing terminology today. English speakers were part of a culture where sailing was a prominent aspect of their lives. Great empires and great fortunes were made by means of sailing. “Try a new tack”, “[feeling under the weather](https://blog.deepgram.com/what-does-it-mean-to-be-under-the-weather/)”, “give leeway”, “hand over fist” are all examples of sailing-specific language used metaphorically in English today. Instead of a difference in “language” the source of “switching” is related to a way of life with its own vocabulary and grammar.\n\nWhen people come together and share ideas, they just use the language, phrases, and metaphors they have in common. The fact that some metaphors come from a linguistic tradition that is so radically different from yours that you call it “a foreign language” is purely an accident of history like everything else is. Consider that the term “give some leeway” would be meaningless to you if you had not inherited an English that had been washed in sailing culture for 400 years. \n\n## How Code-Switching Helped Create English\n\nBy the late 14th century, the Norman nobility in London had been intermarrying with local pre-conquest nobility and as a result had begun to speak some English (Middle English), the language of Chaucer. As the Anglo-Normans gained power French remained the prestige language. But as the years wore on, the value of Anglo-Saxon gained increased and they code-switched with the fancy French and Latin, but used the structure of their “native” tongue. \n\nThe plebeians probably spoke a less Frenchy form of Middle English but whatever they spoke was not fit to print. Only the language of the fancy people was immortalized on sheep’s skins—parchment. That simplified Anglo-Saxon, full of French terms, was the prestige language of the capital. The farther you got from the throne, the less Frenchy was the Anglo-Saxon you heard. The power of this particular code-switched language became cemented when [John Wycliffe used it to translate the Bible](https://textusreceptusbibles.com/Wycliffe/17/1).\\\n\\\nDo realize that to the average peasant, the fancy code-switched language in Wycliffe’s bible was as odd as the English-filled Hinglish of fancy Prithviraj Road sounds to farmers in Chhattisgarhi. Wycliffe could have chosen a “purer” form of Anglo-Saxon still spoken all over the island, or Gaelic, but instead he chose the vernacular of the (rich) people who commissioned the work. For example, in Wycliff’s bible the word the old Anglo-Saxon [costnung](https://en.wiktionary.org/wiki/costning) was replaced with the Frenchy “temptation.”\n\nThis form of “English” became the prestige language in the British Isles whether or not you spoke Gaelic, Scots, French or anything else. In the 19th and 20th centuries, when the central government ponied up for public education, the language taught was this odd but highly codified mix of languages, not the ancient (and long-forgotten) Anglo-Saxon or foreign French (still a prestige language taught to the rich). What may have begun as code-switching in the market and the English court ended up as a new, highly codified language of a nation state.\n\n## Code-Switching: Normal for Humans, Hard for AI\n\nCode-switching is a communicative behavior in which two or more speakers are able to communicate more fully by using words, set phrases, grammar, and other cultural elements from different languages. In many code-switching situations, speakers have at their disposal something like 2x the synonyms and grammatical structures than do the speakers of any of the languages used in code-switching which means that there is choice in what gets said and how. Depending on unpredictable sociological factors, code-switching can lead to language change, even to new languages. Most of the time, code-switching is simply a more complete way to share ideas with others when you have more than one language in common.\n\nThe notion that all participants need to be well versed in the languages/cultures involved in code-switching is important as it has serious implications for AI. Back in 1100, you could not have understood fancy code-switched Anglo-Saxon/Old French unless you knew both languages. Likewise, today, to create automatic speech recognition for Spanglish or Hinglish, it is not enough to “mix” two “pure” datasets and train a model. This suggests that somehow code-switched speech is more than the sum of the component languages. \n\nAs a result, you have to treat code-switched speech like its own language or dialect. If you have read this essay you won’t find this assertion to be very surprising. In our next article we will look at some data science-based approaches to drawing a line between code-switching and other forms of language switching. \n\n### Additional Reading\n\n* [Indigenous languages of the Americas at Wikipedia](https://en.wikipedia.org/wiki/Indigenous_languages_of_the_Americas)\n* [The Anglo-Saxon Bible](https://www.bible-researcher.com/anglosaxon.html)\n* [The Lord’s Prayer over time](https://www.wtamu.edu/~mjacobsen/lp.htm)\n\n\\-- \n\n\\* It is important to note that of the dozens of languages that were spoken in what today is California in 1700, only a few have survived to the present day as spoken tongues. According to Wikipedia, the native California tongue with the most speakers is [Yurok with around 400 native speakers alive today](https://en.wikipedia.org/wiki/Yurok_language). The military invasions of the 18th and 19th century brought new diseases and a new political order to the western part of North America. The social value of knowing Spanish, then English increased greatly at the expense of the languages that had been spoken in the region for hundreds of years prior to the invasions.";
						}
						async function compiledContent$a() {
							return load$a().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$a() {
							return (await import('./chunks/index.a5a3f70f.mjs'));
						}
						function Content$a(...args) {
							return load$a().then((m) => m.default(...args));
						}
						Content$a.isAstroComponentFactory = true;
						function getHeadings$a() {
							return load$a().then((m) => m.metadata.headings);
						}
						function getHeaders$a() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$a().then((m) => m.metadata.headings);
						}

const __vite_glob_0_266 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$a,
  file: file$a,
  url: url$a,
  rawContent: rawContent$a,
  compiledContent: compiledContent$a,
  default: load$a,
  Content: Content$a,
  getHeadings: getHeadings$a,
  getHeaders: getHeaders$a
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$9 = {"title":"What is DevRel anyway: A Deepgram Approach to Developer Relations","description":"Developer Relations can mean many things. Keep reading for a high-level understanding of what a DevRel team does and a close-up of how our Deepgram team works.","date":"2022-04-18T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1649423317/blog/2022/04/what-is-devrel-a-deepgram-approach/DevRel-at-DG%402x.jpg","authors":["bekah-hawrot-weigel"],"category":"devlife","tags":["technical-writing"],"seo":{"title":"What is DevRel anyway: A Deepgram Approach to Developer Relations","description":"Developer Relations can mean many things. Keep reading for a high-level understanding of what a DevRel team does and a close-up of how our Deepgram team works."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661454076/blog/what-is-devrel-a-deepgram-approach/ograph.png"},"shorturls":{"share":"https://dpgr.am/1e7b253","twitter":"https://dpgr.am/133fbb5","linkedin":"https://dpgr.am/8c7f3f0","reddit":"https://dpgr.am/7e80e1c","facebook":"https://dpgr.am/e5819e2"}};
						const file$9 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-is-devrel-a-deepgram-approach/index.md";
						const url$9 = undefined;
						function rawContent$9() {
							return "There’s been a developer relations (DevRel) explosion over the last couple of years, and the pandemic has forced many teams to redefine what their DevRel teams are doing and how they’re doing it. Many teams who have spent most of their time doing conference talks and in-person interactions have been forced to pivot their focus to online content creation, including talks, and exploring new modes of engagement like voice chats. Before we jump into the [Deepgram DevRel team](https://blog.deepgram.com/categories/devlife/), let’s take a look at some approaches to developer relations.\n\n## What is DevRel?\n\nFirst, let’s give DevRel a definition. This is a little harder than it seems because everyone defines it differently. **Developer Relations is an interdisciplinary field in technology that focuses on creating, nurturing, and supporting the relationship between a company or product and developers or developer teams.** The interdisciplinary nature of the work means that it will often overlap other departments, such as marketing, product, sales, and education, for example, and may even fall under the direction of one of those departments.\n\n### DevRel Approaches\n\nOne thing is for sure, not all developer relation teams will look the same. Each team's approach will depend on what department the team falls under and who leads the team. Some examples of different approaches include:\n\n1. Content creation, including writing and audio-visual\n2. Events, whether through sponsorships, hosting, or education opportunities\n3. Partnerships and feedback loops\n4. Developer Experience and documentation\n5. Community Building\n\n## Deepgram DevRel\n\nAt Deepgram, our DevRel team frequently collaborates, provides each other with feedback, and works together to support developers in the wider tech industry, whether through the content we create, 1:1 support, or general involvement in communities and events. We all also create awareness of Deepgram and highlight the awesome projects we see in the community that use Deepgram, like [this post on creating automatic blog posts from videos](https://dev.to/karinakato/create-automatic-blog-posts-from-videos-1c6i)\n\nTogether, our team takes a three-pillar approach:\n\n* Community\n* Education\n* Experience\n\n### Community\n\nCommunity is both the tech community at large and the community we’re creating at Deepgram. Our community approach includes collaborating with existing communities, attending third-party initiatives like events, and creating our own events like livestreams and Twitter Spaces. Another part of this is creating documentation for community events as well as structures and processes to help ensure a safe and inclusive community space.\n\n### Education\n\nMost of our written and audio-visual content creation falls under this category. We create educational content through our blog posts, videos, livestreams, demos, workshops, and internal education. Our education focus includes internal efforts within Deepgram and the external content you see.\n\n### Experience\n\nBy a strict definition, our engineers who work on maintaining our [Deepgram Docs site](https://developers.deepgram.com/), [SDKs](https://developers.deepgram.com/sdks-tools/), and tooling fall under the pillar of Experience. However, it’s worth noting that everything we do could fall under the Experience pillar. How do our community members feel after attending our event? Have we created clear content that provides a good learning experience? We value providing a good experience for developers in all spaces.\n\nOur team of seven consists of three Developer Experience Engineers, two Developer Advocates, one Technical Community Builder, and our Head of Developer Relations. As a team, we don't report to any other department. Rather, we openly communicate with other departments and determine the best focus for our team.\n\nOur team members are involved in each of the pillars and we value the support and perspectives we bring to the team. There is some specialization, with the Developer Experience team concentrating on the Experience pillar. The Developer Advocates and Technical Community Builder concentrate on our efforts within the Community and Education pillars, but we make a continual and deliberate decision to create space for discussions and decisions to be made together on all three pillars. We all come from diverse backgrounds and have different experiences that enrich and inform what we’re working on to make it better.\n\nAs a DevRel team, we want our interactions to build up the developers we’re engaging with--whether through our site and tools, content, or other interactions. We want developers to dream big, and we want to help make those dreams happen. We’re here to help developers become better developers.\n\nIf you have questions or want to hear more about starting in DevRel, you can check our upcoming Twitter Space or let us know what questions you have about DevRel in general or how our team works together: [@DeepgramA](https://twitter.com/DeepgramAI)";
						}
						async function compiledContent$9() {
							return load$9().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$9() {
							return (await import('./chunks/index.791fc3f3.mjs'));
						}
						function Content$9(...args) {
							return load$9().then((m) => m.default(...args));
						}
						Content$9.isAstroComponentFactory = true;
						function getHeadings$9() {
							return load$9().then((m) => m.metadata.headings);
						}
						function getHeaders$9() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$9().then((m) => m.metadata.headings);
						}

const __vite_glob_0_267 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$9,
  file: file$9,
  url: url$9,
  rawContent: rawContent$9,
  compiledContent: compiledContent$9,
  default: load$9,
  Content: Content$9,
  getHeadings: getHeadings$9,
  getHeaders: getHeaders$9
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$8 = {"title":"What is Speaker Diarization?","description":"Want to learn more about what speaker diarization is and how it works? We've got you—this post has everything you need to know.","date":"2022-08-16T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981430/blog/what-is-speaker-diarization/what-is-speaker-diarization-thumb-554x220-1.png","authors":["keith-lam"],"category":"ai-and-engineering","tags":["deep-learning","diarization"],"seo":{"title":"What is Speaker Diarization?","description":"Want to learn more about what speaker diarization is and how it works? Weve got you—this post has everything you need to know."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981430/blog/what-is-speaker-diarization/what-is-speaker-diarization-thumb-554x220-1.png"},"shorturls":{"share":"https://dpgr.am/5c88637","twitter":"https://dpgr.am/8dab2fc","linkedin":"https://dpgr.am/3d52c5d","reddit":"https://dpgr.am/aa2ece3","facebook":"https://dpgr.am/b33a9a4"}};
						const file$8 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-is-speaker-diarization/index.md";
						const url$8 = undefined;
						function rawContent$8() {
							return "Speaker diarization is a process of separating individual speakers in an audio stream so that, in the automatic speech recognition (ASR) transcript, each speaker's utterances are separated. Each speaker is separated by their unique audio characteristics and their utterances are bucketed together. This type of feature can also be called speaker labels or speaker change detection. Customers who use audio with multiple speakers and want transcripts to appear in a more readable format often use speaker diarization.  Example without speaker diarization:\n\n> Hello, and thank you for calling premier phone service. Please be aware that this call may be recorded for quality and training purposes. My name is Beth, and I will be assisting you today. How are you doing? Not too bad. How are you today? I'm doing well. Thank you. May I please have your name? My name is Blake.\n\nWith speaker diarization:\n\n> \\[Speaker:0] Hello, and thank you for calling premier phone service. Please be aware that this call may be recorded for quality and training purposes.\n>\n> \\[Speaker:0] My name is Beth, and I will be assisting you today. How are you doing?\n>\n> \\[Speaker:1] Not too bad. How are you today?\n>\n> \\[Speaker:0] I'm doing well. Thank you. May I please have your name?\n>\n> \\[Speaker:1] My name is Blake.\n\nThe outputs from Deepgram's API, with diarized text, can then be used to build downstream workflows. Speaker diarization is different from channel diarization, where each channel in a multi-channel audio stream is separated; i.e., channel 1 is speaker 1 and channel 2 is speaker 2. Channel diarization can be used for one-to-one phone calls, where there is only one speaker per channel. When there are multiple speakers per channel, such as in the recording of a meeting, speaker diarization is needed to separate the speakers.\n\n## How does Speaker Diarization Work?\n\nSpeaker diarization is generally broken down into four major subtasks:\n\n1. **Detection** - Find regions of audio that contain speech as opposed to silence or noise.\n2. **Segmentation** - Divide and separate those detected regions into smaller audio sections.\n3. **Representation** - Use a discriminative vector to represent those segments.\n4. **Attribution** - Add a speaker label to each segment based on its discriminative representation.\n\nDiarization systems can include additional subtasks. For a [true end-to-end AI diarization system](https://blog.deepgram.com/deep-learning-speech-recognition/), one or more of these subtasks may be joined together to improve efficiency. Let's dig a bit deeper into what these subtasks accomplish for speaker diarization.\n\n**Detection** is often accomplished by a Voice Activity Detection (VAD) model, which determines if a region of audio contains voice activity (which includes but is not limited to speech) or not. For a more precise outcome, Deepgram leverages the millisecond-level word timings that come with our ASR transcripts. This gives us very accurate regions in time where we are confident that there is speech.\n\n**Segmentation** is often done uniformly, using a very small window of a few hundred milliseconds or a slightly longer sliding window. Small windows are used to ensure that segments contain a single speaker, but smaller segments produce less informative representations; i.e., it is also hard for people to decide who's talking from a very short clip. So, instead of relying on fixed windowing, we use a neural model to produce segments based on speaker changes.\n\n**Representation** of a segment usually means embedding it. Statistical representations like i-vectors have been broadly surpassed by embeddings like d- or x-vectors that are produced by a neural model trained to distinguish between different speakers.\n\n**Attribution** is approached in many different ways and is an active area of research in the field. Notable approaches include the Spectral and Agglomerative Hierarchical Clustering algorithms, Variational Bayes inference algorithms, and various trained neural architectures. Our approach successively refines an initial, density-based clustering result to produce accurate and reliable attributions.\n\n## Why is Speaker Diarization Used?\n\nSpeaker diarization is used to increase transcript readability and better understand what a conversation is about. Speaker diarization can help extract important points or action items from the conversation and identify who said what. It also helps to identify how many speakers were on the audio.  Some examples uses are when reviewing a post-call sales meeting and you need to know did if the customer agreed to the business terms or if the salesperson just say they did. Who gave the final buying decision? For [real-time educational use](https://blog.deepgram.com/top-7-uses-speech-to-text-education/), caption diarization would help online students better understand who said what in the classroom. Was it the professor or a student?\n\n## What Are the Use Cases for Speaker Diarization?\n\nAs we mentioned above, creating readable transcripts is one major use, but other use cases for diarization include:\n\n* **Audio/Video/Podcast management** - Speaker separated transcripts or captions allow easier searches for company/product attribution and better understanding of viewers or listeners.\n* **Compliance** - Determining that a customer agreed to certain business terms in a multi-person meeting.\n* **Conversational AI** - A food ordering voicebot trying to determine who is placing the food order when there are multiple adults and children talking.\n* **Education** - Transcribing a student question and answer session to parse out the answers given by the professor or the students.\n* **Health** - Separate patient and doctor comments for both in-person appointments and phone consultations.\n* **Law enforcement** - Parsing who said what in body cam footage or other recordings.\n* **Recruiting** - Tracking recruiter and candidate conversations for compliance, bias issues, and review.\n* **Sales Enablement** - Tracking who said what in a sales meeting and coaching the salesperson on what to say and when to keep quiet.\n* **Speaker Analysis** - Track current and previous comments from a certain speaker during meetings or [track talk time during a phone call](https://developers.deepgram.com/use-cases/talk-time-analytics/).\n\n## What Are the Metrics for Speaker Diarization?\n\nThe main metric used for speaker diarization in the business world is the accuracy of identifying the individual speakers or \"who spoke what\". Most of the measures in academia are measures of \"who spoke when\". We believe the best way to measure speaker diarization improvement is to measure time base confusion error rate (tCER) and time based time based diarization error rate (tDER). \n\n**Time-based Confusion Error Rate**  (tCER)  = confusion time / total reference and model speech time \n\n**Time-based Diarization Error Rate**  (tDER) = false alarm time + missed detection time + confusion time / total reference and model speech time \n\n![](https://res.cloudinary.com/deepgram/image/upload/v1661976860/blog/what-is-speaker-diarization/speaker-diarization-blog.gif) **Key** M = Missed model, F = False alarm, C = Confusion \n\ntCER is based on how much time the diarization identifies the wrong speaker over the total time of audio with speech. The smaller the CER the better the diarization. If there are four speakers and the diarization process has a CER of 10% then for one hour of spoken audio, it misidentified speakers for 6 minutes.  A tCER of less than 10% would be considered very good. However, this measurement is not weighted by the number of speakers or other measures, so you can have a 10% tCER result with identifying one speaker on a two-speaker call when one speaker dominates the conversation for 90% of the time and the secondary speaker only speaks 10%. Deepgram's testing consists of audio with widely varying durations and speaker counts. The other metric is tDER which adds to tCER by including false alarm time (time the model thinks someone is talking when there is just noise or silence) and missed detection time (time where there is speech but the model does not pick it up as speech). tDER is a more standard measure and can provide some more insights into model performance.\n\n## Speaker Diarization with Deepgram vs. Others\n\nNow that we understand how diarization works and how accuracy and errors are assessed, it's important to understand that there are varying capabilities to the diarization features that different ASR providers offer. Deepgram has the following benefits:\n\n* **No need to specify the number of speakers** in the audio. Some ASR providers require you to input the number of speakers in the audio before processing. Deepgram separates the different speakers without any human intervention.\n* **No cap on the number of speakers in the audio**. We have seen very high accuracy of speaker identification on audio with 16+ speakers. Other ASR providers may only be able to perform speaker diarization on 4 or less speakers on one channel.\n* **Supports** **[any language](https://deepgram.com/product/languages/)** **Deepgram transcribes.** Our speaker diarization is language agnostic. Many providers only offer speaker diarization on English or a handful of other languages, which limits your growth.\n* **Supports both pre-recorded or real-time streaming audio in the cloud or on-prem**. While other ASR providers can only perform speaker diarization on pre-recorded audio, Deepgram can do both real-time and pre-recorded audio due to our parallel processing and fast AI model architecture.\n\nThe [full documentation and implementation guides](https://developers.deepgram.com/documentation/features/diarize/) are available so you can immediately try out our diarization features on our [Console](https://console.deepgram.com/).\n\n## Wrapping Up\n\nIf you're looking to get started with a speech-to-text solution, [feel free to contact us](https://deepgram.com/contact-us/) and we'll be happy to discuss your use case and help you get started. Or, you can [sign up for Console for free and get $150 in credits](https://console.deepgram.com/signup) to give Deepgram a try.\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-how-deepgram-works\"></WhitepaperPromo>";
						}
						async function compiledContent$8() {
							return load$8().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$8() {
							return (await import('./chunks/index.56f1f925.mjs'));
						}
						function Content$8(...args) {
							return load$8().then((m) => m.default(...args));
						}
						Content$8.isAstroComponentFactory = true;
						function getHeadings$8() {
							return load$8().then((m) => m.metadata.headings);
						}
						function getHeaders$8() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$8().then((m) => m.metadata.headings);
						}

const __vite_glob_0_268 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$8,
  file: file$8,
  url: url$8,
  rawContent: rawContent$8,
  compiledContent: compiledContent$8,
  default: load$8,
  Content: Content$8,
  getHeadings: getHeadings$8,
  getHeaders: getHeaders$8
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$7 = {"title":"What is the Most Important Channel to Engage Your Customers On?","description":"The 2018 Talkdesk Opentalk conference made patent one fact: A.I. will revolutionize customer service. Learn how here.","date":"2018-12-11T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1662069472/blog/what-is-the-most-important-channel-to-engage-your-customers-on/placeholder-post-image%402x.jpg","authors":["morris-gevirtz"],"category":"speech-trends","tags":["voice-strategy"],"seo":{"title":"What is the Most Important Channel to Engage Your Customers On?","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1662069472/blog/what-is-the-most-important-channel-to-engage-your-customers-on/placeholder-post-image%402x.jpg"},"shorturls":{"share":"https://dpgr.am/b7484fd","twitter":"https://dpgr.am/7efb009","linkedin":"https://dpgr.am/a2dbb8d","reddit":"https://dpgr.am/9ab6a68","facebook":"https://dpgr.am/dcc327c"}};
						const file$7 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-is-the-most-important-channel-to-engage-your-customers-on/index.md";
						const url$7 = undefined;
						function rawContent$7() {
							return "At the Talkdesk Opentalk 2018 Conference, Deepgram had the opportunity to talk to leaders about customer care.\n\n<iframe src=\"https://www.youtube.com/embed/LX4PlTxK5P8\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\nThe 2018 Talkdesk Opentalk conference made patent one fact: <mark>A.I. will revolutionize customer service.</mark> We at Deepgram were curious to get the pulse on what people thought of voice, CX and A.I. We interviewed fellow attendees and speakers at the 2018 Opentalk conference and discovered a few things.\n\n1. A.I. is series of technologies that will help surface cohesive insights in a multichannel world.\n2. [Customer-centric](https://blog.deepgram.com/how-to-become-a-customer-centric-organization/) companies know how to strategically use their voice data to understand their customers better.\n3. Voice is most powerful and information channel.\n4. A.I. technologies will transform voice, allowing teams to get more from [the voice data they already have](https://blog.deepgram.com/five-ways-to-use-speech-recognition-apis-to-empower-your-business/).\n\nTo hear more about cutting-edge industry trends, check out some [more footage from the conference](https://blog.deepgram.com/how-can-companies-extract-more-value-out-of-voice/).";
						}
						async function compiledContent$7() {
							return load$7().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$7() {
							return (await import('./chunks/index.8a162da0.mjs'));
						}
						function Content$7(...args) {
							return load$7().then((m) => m.default(...args));
						}
						Content$7.isAstroComponentFactory = true;
						function getHeadings$7() {
							return load$7().then((m) => m.metadata.headings);
						}
						function getHeaders$7() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$7().then((m) => m.metadata.headings);
						}

const __vite_glob_0_269 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$7,
  file: file$7,
  url: url$7,
  rawContent: rawContent$7,
  compiledContent: compiledContent$7,
  default: load$7,
  Content: Content$7,
  getHeadings: getHeadings$7,
  getHeaders: getHeaders$7
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$6 = {"title":"What is Word Error Rate (WER)?","description":"Learn all about what word error rate is, what it means, and how it's calculated here.","date":"2018-12-04T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981330/blog/what-is-word-error-rate/what-is-wer%402x.jpg","authors":["morris-gevirtz"],"category":"ai-and-engineering","tags":["word-error-rate"],"seo":{"title":"What is Word Error Rate (WER)?","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981330/blog/what-is-word-error-rate/what-is-wer%402x.jpg"},"shorturls":{"share":"https://dpgr.am/3587a50","twitter":"https://dpgr.am/24510e0","linkedin":"https://dpgr.am/62e4f0e","reddit":"https://dpgr.am/0675328","facebook":"https://dpgr.am/9e5fe1f"}};
						const file$6 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-is-word-error-rate/index.md";
						const url$6 = undefined;
						function rawContent$6() {
							return "Word Error Rate (WER) is a common metric used to compare the accuracy of the transcripts produced by speech recognition APIs. Speech recognition APIs are used to surface actionable insights from large volumes of audio data in addition to powering robust IVRs and voice-command-enabled devices such as the Amazon Echo. Product developers and data scientists can choose from many speech recognition APIs. How are they to judge which will be a good fit for their application? When evaluating speech recognition APIs, the first metric they'll consider is likely to be WER. However, a metric has no value unless we understand what it tells us. Let's break down WER to find out what it means and how useful a metric it is.\n\n## How to Calculate WER\n\nWord error rate is the most common metric used today to evaluate the effectiveness of an [automatic speech recognition system (ASR)](https://blog.deepgram.com/what-is-asr/). It is simply calculated as: \n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976771/blog/what-is-word-error-rate/wer-1.jpg)\n\n**S** stands for substitutions (replacing a word). **I** stands for insertions (inserting a word). **D** stands for deletions (omitting a word). **N** is the number of words that were actually said *Note: WER will be calculated incorrectly if you forget to normalize capitalization, punctuation, numbers, etc. across all transcripts*\n\nImagine you are using speech recognition to find out why customers are calling. You have thousands of hours of calls, and you want to automatically categorize the calls. On playback, one such call starts as follows: \n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976772/blog/what-is-word-error-rate/creditcardcall-1.jpg)\n\nHowever, when the machine transcribes this same call, the output may look like this: \n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976773/blog/what-is-word-error-rate/transcript_creditcardcall-1.jpg)\n\n If you compare this transcript with the one above, it's clear that the machine's one has problems. Let's analyze them in terms of our WER formula.\n\n1. In line one, we see that the word \"Upsilon\", has been interpreted as \"up silent\". We will say that this represents (1) **substitution**-a wrong word in place of the correct word-and (1) **insertion**-adding of a word that was never said.\n2. On line two, we have (1) **substitution:** \"brat\" instead of \"Pratt.\"\n3. On line three we have (1) **substitution:** \"designed\" instead of \"declined.\"\n4. On line four we have (2) **substitutions:** \"cart\" instead of \"card\" and \"because\" instead of cause. On this line we also have (1) **deletion:** the word \"it\" is gone.\n\nThe original phone call contained 36 words. With a total of 9 errors, the WER comes out to 25%.\n\n## What WER Really Means\n\nHow well a speech recognition API can transcribe audio depends on a number of factors, which we will discuss below. Before we do, we must ask ourselves the most important question one can ask when assessing a speech recognition system: \"is the transcript usable for my purposes?\" Let's consider our example. This is a good transcription if you are trying to figure whether customers are calling to solve issues with credit cards or debit cards. However, if your goal is to figure why out each person called your call center (to deal with a declined card, lost card, etc.) then this phone call would likely get mis-sorted. **This is because the system did not properly transcribe a keyword: \"declined.\"** <mark>When a speech recognition API fails to recognize words important to your analysis, it is not good enough-no matter what the WER is.</mark> Word error rate, as a metric, does not give us any information about how the errors will affect usability for users. As you can see, you the human can read the flawed transcript and still manage to figure out the problem. The only piece of information you might have trouble reconstructing is the name and location of the gas station. Unfamiliar proper names are troublesome for both humans and machines.\n\n## A Low WER can be Deceptive-Depending on the Data\n\nDepending on the data we want to look at, even low word error rate transcripts may prove less useful than expected. For example, notice how on line 4, \"'cause\" was transcribed as \"because\" and the object pronoun \"it\" was omitted. These two errors may not matter, especially if your goal is to find out why customers are calling. If the speech recognition API had not made these errors, we would have a 19.4% WER-almost as good as it gets for general, off-the-shelf speech recognition. But, as you can see, a low(er) error rate does not necessarily translate into a more useful transcript. This is because adverbs like \"because\" and object pronouns like \"it\" are not of much interest to us in this case.\n\n> \"You can have two systems with similar accuracy rates that produce wildly differently transcripts in terms of understandability. You can have two different systems that are similar in terms of accuracy but maybe one handles particular vocabulary that's germane to your application better than the other. There's more than just accuracy at the heart of it.\"\n>\n> —Klint Kanopka Stanford Ph.D. Researcher\n\nWhile WER is a good, first-blush metric for comparing the accuracy of speech recognition APIs, it is by no means the only metric which you should consider. **Importantly, you should understand how the speech recognition API will deal with *your* data. What words will it transcribe with ease? What words will give it trouble?** What words matter to you? In our example, the words \"declined\" and \"credit card\" are likely the ones we want to get right every time.\n\n## What Affects the Word Error Rate?\n\nA 25% word error rate is about average for \"off the shelf\" speech recognition APIs like Amazon, Google, IBM Watson, and Nuance. The more technical, the more industry-specific, the more \"accented\" and the more noisy your speech data is, the less likely that a general speech recognition API (or humans) will do as well.\n\n### Technical and Industry-specific Language\n\nThere's a reason that human transcriptionists charge more for technical or industry-specific language. It simply takes more time and effort for human brains to reliably recognize niche terms. The language that is normal to call center manager, a lawyer or a business executive is rare elsewhere. As a result, speech recognition systems trained on \"average\" data also struggle with more specialized words. As you'd guess, the technical language was created for a reason and accordingly, it's the language that businesses care the most about.\n\n### Accented Language\n\nAccent is a highly relative, very human concept. This author has a strong accent in Dublin, but almost none in New York. The speech recognition systems built by large companies such as Google, Nuance and IBM are very familiar with the sort of English one hears on TV: \"general American English\" and RP (received pronunciation-the form of British English spoken by the Queen, the BBC and Oxford graduates). They are not necessarily familiar with the \"real\" English spoken in Palo Alto, CA; Athens, Georgia; New Dehli, India or Edinburgh, Scotland. However, companies are interested in the \"real\" language since a very tiny subset of their employees are TV anchors. \n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976774/blog/what-is-word-error-rate/raghu-nayyar-501556-unsplash-1.jpg)\n\n*In New Delhi-English is spoken natively and non-natively by a large percentage of the population. Photo Credit: [Raghu Nayyar](https://unsplash.com/photos/EpAq2EE-shg).* \n\nTherefore, if your data has a wider variety of accents (it almost certainly does), or is limited to a set of accents not well represented in the data used to create general speech recognition APIs, then you probably need a [custom-built model](https://deepgram.com/product/train/) to really get a good look at your data.\n\n### Noisy Data\n\nWouldn't it be nice if everyone who called us to do business did so from a sound studio? Wouldn't you love that crisp, bassy, noise-free audio? Better yet, how about they didn't call us over the phone, since VoIP and traditional phone systems compress audio, cut off many frequencies and add noise artifacts? The real world is full of noise. Phone calls are inherently bad quality, people call while rushing to work, or walking by a seemingly endless line of jackhammers, fire engines and screaming 4-month-olds.\n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976774/blog/what-is-word-error-rate/baby-firetruck-jackhammer.jpg)\n\nSomehow, human transcribers do okay with such noisy data, and speech recognition APIs, if properly trained, can do okay too. However, as you can imagine, when companies advertise super-low word error rates, these are not the WERs they get when transcribing audio captured at Iron Maiden concerts held in the middle of 16 lane interstate highways.\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-how-deepgram-works\"></WhitepaperPromo>\n\n## Choosing an Speech Recognition API\n\nSpeech recognition APIs are fantastic tools that allow us to look into vast amounts of audio data in order to learn meaningful things about our customers, our operations and the world in general. WER is one metric that allows us to compare speech recognition APIs. However, as it is the case in any science, there is no one \"best\" metric.\n\nI like analogies, so here is one: Asking which is the best metric to judge the quality of a bicycle could end in disaster. If your say \"weight is the best metric, the lighter the better,\" then people like me who use their bikes to carry heavy groceries, 2 months of laundry and the occasional 2x4 would be in trouble. If you said \"the number of pannier racks on a bike\" is a good metric, then Tour de France cyclists would become a lot more winded, faster. All in all, you need to choose what's right for you. \n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976775/blog/what-is-word-error-rate/derek-thomson-271991-unsplash.jpg)\n\n*This bike is a robust touring bike with pannier racks-great for shopping and 10,000 mile tours, bad for Tour de France. Photo credit: [Derek Thomson](https://unsplash.com/photos/AJ-7QpXV9U4)*\n\nWhen you want to decide which speech recognition API to use, ask yourself:\n\n* Are there particular audio types that you need the speech recognition API to perform well on (phone call, TV, radio, meetings)?\n* Are there certain words or accents that the speech recognition API should do well on?\n* Can you [customize the API](https://deepgram.com/product/train/) to perform better on your data?\n\nFor more, check out our [step by step guide on how to evaluate an ASR provider](https://blog.deepgram.com/how-to-test-automatic-speech-recognition-asr-providers-for-your-business/) or [have us evaluate](https://deepgram.com/contact-us) the ASR provider for you.";
						}
						async function compiledContent$6() {
							return load$6().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$6() {
							return (await import('./chunks/index.ab8326af.mjs'));
						}
						function Content$6(...args) {
							return load$6().then((m) => m.default(...args));
						}
						Content$6.isAstroComponentFactory = true;
						function getHeadings$6() {
							return load$6().then((m) => m.metadata.headings);
						}
						function getHeaders$6() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$6().then((m) => m.metadata.headings);
						}

const __vite_glob_0_270 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$6,
  file: file$6,
  url: url$6,
  rawContent: rawContent$6,
  compiledContent: compiledContent$6,
  default: load$6,
  Content: Content$6,
  getHeadings: getHeadings$6,
  getHeaders: getHeaders$6
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$5 = {"title":"What Makes a Great Conversational AI Experience?","description":"Find out what experts in Speech to Text, Natural Language Understanding and Conversational AI have to say about the creating great voicebots.","date":"2021-06-14T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981375/blog/what-makes-a-great-conversational-ai-experience/what-makes-great-convo-ai-experience%402x.jpg","authors":["keith-lam"],"category":"speech-trends","tags":["conversational-ai","nlp","nlu"],"seo":{"title":"What Makes a Great Conversational AI Experience?","description":"Find out what experts in Speech to Text, Natural Language Understanding and Conversational AI have to say about the creating great voicebots."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981375/blog/what-makes-a-great-conversational-ai-experience/what-makes-great-convo-ai-experience%402x.jpg"},"shorturls":{"share":"https://dpgr.am/433491f","twitter":"https://dpgr.am/fbc016b","linkedin":"https://dpgr.am/1cf38f4","reddit":"https://dpgr.am/4b93fd5","facebook":"https://dpgr.am/9763498"}};
						const file$5 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-makes-a-great-conversational-ai-experience/index.md";
						const url$5 = undefined;
						function rawContent$5() {
							return "You are not alone if you have ever experienced a poor interaction with a voicebot. Perhaps you heard, \"Sorry, I didn't get that. Can you repeat your request?\" or have been transferred to a human agent after a simple question. Unfortunately, these experiences are common across various industries, resulting in a negative customer experience and ultimately cause churn.  When you look at the leading edge of Conversational AI, what is currently possible?  Well, we are getting much closer to having voicebots that converse like humans, but only on specific subjects or use cases.  There is still not a voicebot that you can universally converse on everything or be a real personal assistant.\n\n## **The need to be persistent**\n\nSo what are the obstacles to human-like conversational AI?  Many technical and data obstacles exist to reach this goal.  Kevin Fredrick, Managing Partner of [OneReach.ai](https://onereach.ai/), expressed this best when he said, \"Building a Conversational AI voicebot is like planning to summit a mountain.  Those who are looking for an 'easy button' get frustrated and quit.  The ones who think it will be too hard, don't ever start.  It is the ones who know the challenge is worth it and have the right partners and use the right tools who make the summit.\" There are still technical challenges we continue to overcome including transcription speed and accuracy optimization, better Natural Language Processing (NLP) and Natural Language Understanding (NLU), improved human-like text-to-speech engines, and tighter integrations between all the parts of this workflow, but we see the path to reaching this summit.\n\n## **Lack of training data**\n\nOn the data side, Antonio Valderrabanos, CEO of [Bitext](https://www.bitext.com/), indicated that the availability of data for AI model training and evaluation is one of the main challenges for creating a voicebot for all languages, accents, dialects, and use cases. Do we have the audio and text data in these accents, dialects, and use cases to train that AI model? This data currently does not exist in the public domain or even in the private domain, so this training data must be built in a scalable way. Valderrabanos believes we can get there but there needs to be automated methods for data generation, for both training and evaluation.\n\n## **Why is it harder to create voicebots vs. chatbots?**\n\nAs Adam Sypniewski, CTO of [Deepgram](https://deepgram.com/) noted, there is no plug-and-play with voicebots.  You can't just unplug the chat and install automatic speech recognition into the Conversational AI workflow and expect it to work.  Texting and chatting can be looked at as one-dimensional while speech is multidimensional.  You have different tones of voice meaning different things.  You have pauses in a conversation, which may or may not mean you are done speaking.  You have different words that all mean the same thing; like \"Yes\", \"Yeah\", \"Sure\" or \"Uh-huh\".  Wait, did he say \"Uh-huh\" as meaning non-commital or as meaning affirmative?  This is just English.  What about English as a second language speaker accents or different languages with different expressions?  Simple transcripts from an automatic speech recognition (ASR) system will not pick up these differences and they may not get the words correct.  Yeah, there is no easy button but these cutting-edge companies are climbing that mountain.\n\n## **Light at the end of the tunnel**\n\nYes, there is no easy button to a great overall Conversational AI voicebot but very good single-use case voicebots are available now.  Our experts all agree we are going to see a big evolution in this space in the next two to three years that will evolve to that universal voicebot or personal assistant that you can speak with as a friend.\n\n## **Want to hear from the experts**\n\nOur on-demand webinar with [Bitext](https://www.bitext.com/), [OneReach.ai](https://onereach.ai/), and [Deepgram](https://www.deepgram.com/) discusses why customers are rejecting simple IVRs, chatbots, and menu-driven voicebots to embrace more human-like bots that understand the customer intent and respond correctly to meet customer needs.  They also discuss what you need to consider in creating that great voicebot. [View the on-demand webinar](https://offers.deepgram.com/what-makes-a-great-conversational-ai-experience-webinar-on-demand).\n\n<WhitepaperPromo whitepaper=\"deepgram-whitepaper-how-deepgram-works\"></WhitepaperPromo>\n\n## **Questions from the audience**\n\n**1.  Any specific deployed examples of successful voice-based IVR.** Measuring the success of a voice-based IVR boils down to three things:\n\n1. Adoption - Are people using it?\n2. Satisfaction - Are you enhancing relationships with users?\n3. Task completion - Are you providing value?\n\nSuccessful IVRs have less to do with the specific tools being used, and more to do with the outcome that those tools provide. A leading consumer brand was retiring a fictitious marketing character, and through this campaign to retire it, they created an experience that allowed fans to call and talk to the character. Designing for the nuances of voice conversations is always challenging, but even more so when designing for a conversation with a fictitious character. The average call for this experience was around 7 minutes long, and this goes to show that when the design is well-thought-out, you can create experiences that are really engaging and fun.\n\n**2. Often, people have to struggle to understand second language speakers (accents, grammar, misused idioms, etc.). Now does NLU/P handle L2 speakers?** The key is having training data that reflects the language specificities of L2 speakers. L2 is not harder to handle than L1, it's a different language accent or variety. Natural Language Generation (NLG) or automatic data generation is the solution to dealing with these and other language variants.\n\n**3. What are your thoughts on Spotify's new patent in emotional speech recognition to choose music for the user based on how they say things?** Detecting sentiment, emotion, or speaker attributes like age are interesting areas of research both for their technical merits, but also for the ethics surrounding training and using features like that. This is a necessary field though because truly understanding intent in a conversational setting will require at least some understanding of emotional cues and guesses about speaker attributes. Much of natural conversation can't be captured merely as words so ignoring intonation, inflection, speaker context and the emotion of what was said removes much of the meaning and intent.  There are many challenges here that need to be solved. In training data alone there are large issues. For example, one person's 'rudeness' is another's 'assertive'. The same person may give changing estimates of emotion for the same example based purely on their own current emotional state. Data like this is both hard to gather and hard to train with and that is just the beginning of a pipeline that would deliver real value in a production setting.  We are just at the beginning of these technologies. The next few years will see the first real trailblazing, successful, implementations and, more important than the applications they are a part of, the first few implementations will start to formalize the problems. That will allow follow on work that will bring this truly to the mainstream.\n\n**4. Hi guys, in what you describe (esp. Adam), isn't the way we architecture the conversational systems in separated silos (ASR->NLU->NLG->TTS) one of the main issues?** Right now the problem is too big to bite off in one single system. Going from audio to action is just too big of a problem which means we have to break it into smaller, solvable, pieces. Getting those pieces right at a high level is crucial to the success of a project. Luckily we are seeing rapid improvements in all parts of the pipeline and, as a consequence, really starting to understand what needs to be passed between each of them to effectively handle the problem. As these technologies evolve and mature the roles and connections between these pieces will likely shift and blur more to allow each to get access to the information it needs. We are also likely to see some of these pieces merge or change function radically as the problem clarifies and the solutions become more capable. So, yes, the current silos are a major part of the problem, but they are enabling a rapid evolution in the space that is leading to rapid improvement.\n\n**5. Where can someone go to learn more about predictive analytics?** The founder and chief technologist of OneReach.ai is writing a book called *[Age of Invisible Machines](https://onereach.ai/#bots)* that explores how interfaces are disappearing and conversation is taking over. The book goes into some incredible detail regarding the big ideas, such as predictive analytics, but also explains how to think practically and apply academic research to the real world. In addition, many vendors in the space would be happy to set up an executive review to do a deep dive on how they think about specific topics. These will be the people who are really thinking about the topics and have important best practices to share, so I would recommend that as well.\n\n**6. Most chatbots today are task-oriented. How can one make bots that are able to work with/detect things like soft skill etc. so as to make it more like humans?** The current answer may be the integration of different voicebots, one for handling content (the classical bot we know now) and one for handling soft skills (tone, politeness, use of colloquial language...)  Will we get to a universal conversational voicebots that can speak on many topics and handle all soft skills?  Someday but we may need more optimization in hardware processing in addition to optimization on the overall Conversational AI pipeline.";
						}
						async function compiledContent$5() {
							return load$5().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$5() {
							return (await import('./chunks/index.ea5e6b68.mjs'));
						}
						function Content$5(...args) {
							return load$5().then((m) => m.default(...args));
						}
						Content$5.isAstroComponentFactory = true;
						function getHeadings$5() {
							return load$5().then((m) => m.metadata.headings);
						}
						function getHeaders$5() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$5().then((m) => m.metadata.headings);
						}

const __vite_glob_0_271 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$5,
  file: file$5,
  url: url$5,
  rawContent: rawContent$5,
  compiledContent: compiledContent$5,
  default: load$5,
  Content: Content$5,
  getHeadings: getHeadings$5,
  getHeaders: getHeaders$5
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$4 = {"title":"What Makes Your Voice Uniquely Yours?","description":"Why is your voice so unique? Is it all physical features or environmental influences. Read more about what makes your voice yours.","date":"2021-11-23T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981387/blog/what-makes-your-voice-uniquely-yours/what-makes-your-voice-yours-blog-thumb-554x220%402x.png","authors":["morris-gevirtz"],"category":"linguistics","tags":["language"],"seo":{"title":"What Makes Your Voice Uniquely Yours?","description":"Why is your voice so unique? Is it all physical features or environmental influences. Read more about what makes your voice yours."},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981387/blog/what-makes-your-voice-uniquely-yours/what-makes-your-voice-yours-blog-thumb-554x220%402x.png"},"shorturls":{"share":"https://dpgr.am/cbb6941","twitter":"https://dpgr.am/dbcad77","linkedin":"https://dpgr.am/5fe6ea9","reddit":"https://dpgr.am/fe1d16f","facebook":"https://dpgr.am/cbfeacd"}};
						const file$4 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-makes-your-voice-uniquely-yours/index.md";
						const url$4 = undefined;
						function rawContent$4() {
							return "What is it that makes your voice uniquely yours? Dozens upon dozens of people can recognize your voice. They can even recognize you over the phone - sometimes even if you have a cold! What makes your voice yours can be described as a function of the shape and size of the breathing/speech tube we call \"the vocal tract.\"\n\nLike most people who speak a \"spoken\" language (i.e. not a sign language), I \"speak\" by coordinating muscles in my chest, throat, and mouth. The tongue, lips, and larynx all provide me with the muscular system to say things. Lots of things. If I say things right, people like me, I get others to help me, and sometimes I even get free chocolate. If I say things wrong, however...\n\nHow do we form words? Each vocal articulation that results in a sound or word or \"tone of voice\" is a poorly understood set of neural impulses sent to muscles (such as the styloglossus at the base of the tongue). fMRI and \"brain scans\" are tools that have helped us slowly cast light on the neurology of articulatory phonetics. It turns out that we can think of language change *as small changes in timing and strength of neural impulses, motivated by changes in how people relate to one another* . A timing difference there, copied by generations of friends and foes, converted Latin 'aqua' into Spanish 'agua'; Old English 'myrgð' into English 'mirth'.\n\nIt turns out that these small changes in the timing of various vocal tract muscles are caused by more than random chance. We put meaning into our word choices, or choice of tone, or funny little modifications on words. The phrases \"What's up brother?\" and \"What's up bruh?\" are not the same thing! Our patterned word choices say something about who we are, what community we belong to (or wish to), and what we do for a living. We recognize that people \"self-organize\" into goal-oriented speech communities.\n\nA dialect can describe the \"linguistic and cultural uniqueness\" of a community made up of people who have lived and worked together for generations. They had to use language to achieve their goals such as making food, arguing about land use, finding a lifemate, seeking fortune. In working together (and sometimes fighting) for generations, unique ways of speaking evolved.\n\nScientifically, what is a dialect? The variables of dialect exist in the realms of \"word choice\", \"syntax\", \"intonation,\" \"turns-of-phrase\", \"pronunciation\", even \"dress\" and \"gesture.\" If you take dialect and you distill it down to some sort of simpler symbolic representation of the \"intentions\" in speech, striping out the \"pronunciation\" and \"intonation\" and other articulatory features that define dialect, **you would have something called \"writing**.\" Writing can be used to mask a lot of what people call dialect --but not everything. If you speak, read and write American English fluently, you can read Scottish news easier than you can understand it when spoken. But there will be \"funny\" words, odd place names, turns of phrase...  Dialect is more than just \"accent\".\n\nDialect can be thought of as a \"tighter\" agreement between speakers on the meaning and use of words than the group called \"language\". Dialect is a result of cooperation, hard work, and big dreams. The social and environmental circumstances under which we learn our languages cause the fossilization of the \"meanings\" of words in grammar. People use language to put images and ideas in their heads. When you know people better, at the same dialect level, sharing ideas and images is easier.\n\nHow do words get their fixed meaning? That depends on the communicative needs of the community. For example, 2000 years ago hardy Britons and Vikings decided that [\"starboard\"](https://www.etymonline.com/search?q=starboard) and [\"laderboard](https://www.etymonline.com/word/larboard)\" were good enough terms to get the job done. Later, some more Latin-friendly folks decided that \"port\" is better than \"laderboard.\" The survival-oriented sailor learned quickly. Language, is a survival tool and dialects represent team-specific sets of these tools. When we speak in high-value settings we make careful word choices. Each choice is made to achieve some communicative goal.\n\nWhen folks are aligned on goals, word meanings --and pronunciations-- crystalize. That's why the worlds of sailing, physics, music, dance, war, etc. all have technical terms and ideas that seem hard to learn at first. Those words, grammar, and pronunciations evolved and survived because they helped community members get the job done. **The outcome of success is specialized communication.**\n\nBusinesses and academic disciplines exist through time like generations of people. They too have high-value language that members must learn to use well in order to get the job done. Even today, Apple users talk differently than Android users. Tech support centers know this, so they staff differently because Apple and Android have their own speech communities, dialects. Each company and industry has its technical terms, key phrases, ways of speaking that evolved organically to get engineers, marketers, salespeople, customers on the same page.\n\nSo what makes your voice, your voice?  Your voice is a representation of where you have been and where you want to go. This can be thought of in two ways: the genetic and the memetic. Your capacity to speak a language—the specific articulations and underlying symbolisms you CAN and do give thoughts is made possible by your genes.\n\nMemes (in the Dawkins sense) are the sum total of specific articulations and language patterns that make your ideas possible: your choice of words, your choice of metaphors represents thousands if not tens of thousands of years of the human story. You use the genetic and memetic language palate to convey meaning and extract meaning from the world around you.\n\nIn sum, your voice is everything that has ever been in human history, and everything that you want to be.";
						}
						async function compiledContent$4() {
							return load$4().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$4() {
							return (await import('./chunks/index.bb64d7e6.mjs'));
						}
						function Content$4(...args) {
							return load$4().then((m) => m.default(...args));
						}
						Content$4.isAstroComponentFactory = true;
						function getHeadings$4() {
							return load$4().then((m) => m.metadata.headings);
						}
						function getHeaders$4() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$4().then((m) => m.metadata.headings);
						}

const __vite_glob_0_272 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$4,
  file: file$4,
  url: url$4,
  rawContent: rawContent$4,
  compiledContent: compiledContent$4,
  default: load$4,
  Content: Content$4,
  getHeadings: getHeadings$4,
  getHeaders: getHeaders$4
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$3 = {"title":"What's the Best Infrastructure for Machine Learning — AI Show","description":"Today we ask the question: what's the best infrastructure for machine learning?","date":"2018-11-30T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981333/blog/whats-the-best-infrastructure-for-machine-learning-ai-show/whats-best-infrastructure-for-ml-blog-thumb%402x.jpg","authors":["morris-gevirtz"],"category":"ai-and-engineering","tags":["deep-learning"],"seo":{"title":"Whats the Best Infrastructure for Machine Learning — AI Show","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981333/blog/whats-the-best-infrastructure-for-machine-learning-ai-show/whats-best-infrastructure-for-ml-blog-thumb%402x.jpg"},"shorturls":{"share":"https://dpgr.am/6b9cf9b","twitter":"https://dpgr.am/cd16a23","linkedin":"https://dpgr.am/6fb6384","reddit":"https://dpgr.am/dfeffaf","facebook":"https://dpgr.am/26d3712"}};
						const file$3 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/whats-the-best-infrastructure-for-machine-learning-ai-show/index.md";
						const url$3 = undefined;
						function rawContent$3() {
							return "**Scott:** Welcome to the AI Show. Today we ask the question: What's the best infrastructure for machine learning?\n\n**Susan:** We have seen a revolution recently and we can point to one of the big enablers, which has been GPUs. We've talked about this a whole lot. GPUs have not always been here.\n\n**Scott:** Machine learning has been going on for a while. GPUs haven't been the end-all-be-all over all of time.\n\n**Susan:** Exactly! Back in the '50s you think they had GPUs to crank on? No, they didn't.\n\n**Scott:** They barely had CPUs to crank on. I don't think they had that really. They had some kind of calculator type machines.\n\n**Susan:** The roots of machine learning really goes back to the '50s. A lot of great experiments on computers of the day started back there. Minsky wrote his great article describing all these problems in the '50s.\n\n## What were the architectures through the ages?\n\n**Scott:** You have tubes, and huge rooms.\n\n**Susan:** A series of tubes.\n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976780/blog/whats-the-best-infrastructure-for-machine-learning-ai-show/IBM-Electronic-Data-Processing-Machine.jpg)\n\n**Scott:** Vacuum tubes filling rooms, generating lots of heat, back in the '50s, '60s, and maybe into the '70s.\n\n**Susan:** All the experiments were done on mainframes, on these single big computers that filled a room.\n\n**Scott:** What's a mainframe?\n\n**Susan:** A centralized single big computer where you may have had multiple terminals plugged into it. As time went on, they had terminal access.\n\n**Scott:** This is like racks for one computer.\n\n**Susan:** One big university may have that one mainframe that your PhD candidates could steal time on.\n\n**Scott:** You could wait for a little computing time on.\n\n**Susan:** And get their 16k program to run on.\n\n**Scott:** On punch cards that they lined up.\n\n**Susan:** Then as the years roll on, these things get bigger, and bigger, and bigger. And we finally see the real split happen in say the '80s, and what happened in the '80s, Scott?\n\n**Scott:** Microprocessors, CPUs, the PC really. You could spend $5000 bucks, which was a lot of money at the time, maybe 25K equivalent now, but you could spend that amount of money and now you have your own computer, which was unimaginable.\n\n**Susan:** At that point I think what you see-from a history of machine learning-is you see quite a few new things come out. It wasn't just that thinking changed and computation became available in the form of the PC. You're not gonna be able to do the stuff the mainframe could, but you could at least try out a lot of ideas rapidly.\n\n**Rapid prototyping is huge in every single industry, and so you see big things.** You see a lot of big things in reinforcement learning coming out of the 80s. You see a lot of basic stuff being laid down.\n\n**Scott:** The groundwork is being laid then.\n\n**Susan:** But even then the computers still don't have what it takes to power, the ML systems of today. You see as the architectures get more RAM, more compute, you see a steady trickle of more and more come out from the 80s, through the 90s, and then the 2000s hit, and the world changed.\n\n**Scott:** The Pentium 1, Pentium 2, Pentium 3, that kind of happened throughout the 90s. CPUs got bigger, better, faster, a little more memory, the hard drives got bigger, clock speeds got faster, and it sort of famously, Moore's law-that stuff happened. But then into the late 90s, mid 90s you have graphics processing units, or GPUs that start to become popular. And then mid 2000s is when NVIDIA (who is actually an investor in Deepgram) started doing Cuda. Which is very big ... Cuda. If you don't know what it is, it made it so that a normal programmer could access a GPU and use it to perform tasks in a way that they're used to.\n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976781/blog/whats-the-best-infrastructure-for-machine-learning-ai-show/Moore-s_Law_Transistor_Count_1971-2016.png) *Moore's Law-named after the the famed Intel co-founder Gordon Moore-is an observation that the number of transistors on integrated circuits has doubled every year since their inception. Has that [doubling of transistor number come to an end?](https://www.technologyreview.com/s/601441/moores-law-is-dead-now-what/)*\n\n**Susan:** C-like interface.\n\n## It's not just hardware, infrastructure also means software\n\n**Scott:** You make a nice interface, that's ike C, which is a programming language that many programmers know, but then it actually runs on the GPU. Usually what this means is very fast speed-ups for a specific tasks. Those specific tasks are like matrix operations, and things that are [massively parallel](https://en.wikipedia.org/wiki/Parallel_computing).\n\n**Susan:** During that period you still don't see, what we see today, but that's probably when the big research academics started taking notice. One starts seeing the beginnings of papers talking about GPUs and how they can speed up a machine. It had been done in the 90s, using GPU-like processors, it had been done through the 2000s, but it just never caught because everything single time it was hard. It's really hard. It's really specialized. They changed the hardware out and all your stuff breaks.\n\n**Scott:** You had to be a machine learning expert, a data expert, a software expert, and a hardware expert in order to do any of it.\n\n**Susan:** And at a certain number of experts you break. I ran out of fingers to be an expert on.\n\n**Scott:** Yeah. Too many experts.\n\n**Susan:** Cuda is just a game changer, and by the late 2000s, early 2010s, we are starting to see an explosion of machine learning stuff coming out. Just like when the PC came out, you no longer needed this huge server of a thousand cores to be able to do anything in a reasonable amount of time. Now you can go to your desktop, bust out a reasonably priced graphics card, and you can do amazing things. You can start recognizing cats, and once you can recognize cats...\n\n**Scott:** Well people are excited then.\n\n**Susan:** You're excited then.\n\n**Scott:** Yeah, you identify the cat neuron.\n\n**Susan:** And you can do a rapid prototyping. You've got a crazy idea, you don't have to write up a 15 page report on what you think you might gain out of this so that you can borrow a bunch of time on a huge cluster.\n\n**Scott:** You just go do it, and maybe a day or 2 later you got the answer.\n\n**Susan:** Yeah, and it either works or it doesn't. It's enough to go forward on.\n\n**Susan:** And so you just see this massive explosion around you.\n\n**Scott:** We're in particular talking about the [deep learning side](https://en.wikipedia.org/wiki/Deep_learning) of machine learning but there was machine learning going on before deep learning, that's true.\n\n**Susan:** Deep learning is a little subset of machine learning.\n\n**Scott:** Not little.\n\n**Susan:** It's a big subset, yeah.\n\n**Scott:** There's a lot of hype, a lot of money dumped into it, but a lot of promising things, especially on the perception side. Like, for example, being able to see or understand the are the sorts of things that deep learning is pretty successful at. But there are other types like [decision trees](https://en.wikipedia.org/wiki/Decision_tree), and boosted decision trees, and other machine learning types that have been around for, the 80s or earlier, that you could do on CPUs, and actually are very fast. You can train them, they're quick. You can do tests, you can actually dissect them, and figure out why they made their decisions in a lot of ways. But, they usually aren't as good. Then people started to get really excited about GPUs and deep learning because once you throw enough data, and compute, and deep learning-ness at something, it starts to work really well.\n\n**Susan:** Figure out your [loss function](https://en.wikipedia.org/wiki/Loss_function), get some basic structure going.\n\n**Scott:** Yeah get some basic structure going, then it's like \"Wait a minute, things are starting to work really well!\" and this is like late 2000s, early 2010s.\n\n**Susan:** You see it on image recognition. You see the accuracy becoming amazing, error rates dropping like crazy.\n\nIt's not that suddenly brand new ideas magically appeared out of nowhere. It's the ideas that everybody wanted to do, it just took them a long time to do. They were finally able to rip through it.\n\n**Scott:** This is why the infrastructure is so important, you finally got all your stuff together, your hard drives are big enough, your memory is large enough, your memory bandwidth is fast enough, your computation speed is fast enough, the number of flops that you can do is enough.\n\n**Susan:** And the tools. This is again is the big thing, like Cuda-it has enabled a huge set of tools.\n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976782/blog/whats-the-best-infrastructure-for-machine-learning-ai-show/Screen-Shot-2018-12-11-at-12.19.09-PM.png) *[Cuda](https://devblogs.nvidia.com/easy-introduction-cuda-c-and-c/) was a key element in the deep learning revolution.*\n\n**Scott:** There are layers upon layers here where there's a GPU that has hardware stuff going on inside it, but then that is abstracted away by Cuda, but then Cuda is abstracted away by other deep learning frameworks as well.\n\n**Susan:** That just enables the next round, and the next round, and the next round.\n\n**Scott:** It takes a cognitive load off the person who's trying to develop something. When you're going to develop something you say: I'm gonna spin up this thing, I don't even necessarily need to know how it works under the hood, but I can make it do stuff.\n\n**Susan:** It's one less expert finger you need to have. I used to have all 10 expert fingers, and then slowly your strip them away.\n\n**Scott:** Well now you can make progress in the one or two areas.\n\n**Susan:** You can delve deep into, \"I just don't care as much about those other things, I wanna care about this problem right here.\" So yeah, it's been an amazing roller coaster ride.\n\n**Scott:** This is like the training side though, right?\n\n**Susan:** You're right, the training side has been here, production side has not. There's not a tremendous production push on the GPU side in a lot of areas until recently.\n\n**Scott:** Until very recently.\n\n**Susan:** But that's part of the shift we're starting to see.\n\n**Scott:** I think it's the inference side-which is like the production serving of these models-that lags behind because the training has to happen, your models have to work, you have to get comfortable with how they work, and then you think \"Oh, maybe I should turn this into a product,\" and it takes a little while.\n\n**Susan:** That was up till the early 2010s, mid 2010s. Now we're in the later 2010s and what are we seeing? I think we're seeing another architecture shift. What do you think?\n\n## Are we seeing another architecture shift?\n\n**Scott:** Well I think the AI chip, and iPhone, people have heard about that, where you can use ML Kit to do stuff on the iPhone and it's not using the CPU anymore, or it's using a lot less.\n\n**Susan:** They hope. Apple opened it up to other developers before. I think this was all enclosed and only they had access to it. Now they've opened it up to developers.\n\n**Scott:** Any developer could go and try to use this chip to do something, and have it be.\n\n**Susan:** That's the trend. First we had no specialized processing, and then we found the graphics world does a lot of what we need. Those big matrix math applications.\n\n## Why are GPUs so good?\n\nSusan: GPUs do the same operations that it takes to take a 3D scene, and rotate it around, and do all these special effects there.\n\n**Scott:** If you're gaming and the guy shoots this, or the plane flies here, this all has to be generated in a virtual world, and there are all the polygons, and points, but then it has to be projected onto your 2D screen.\n\n**Susan:** Yep, exactly. Those same calculations, that matrix math, is also at the core of most deep learning at the moment. Deep learning has jumped, and adapted itself towards that GPU world that was in the 3D rendering world.\n\n**Scott:** Scientists were using CPUs, and they're sitting there waiting, patiently tapping their fingers, saying \"This is taking a long time.\" And, they look out and say, \"What else is going on in the world? Is there anything fast out there? Is there anything that's massively parallel by any chance, because the operations that we're doing could be very parallelized.\" Then they see GPUs: \"Wait a minute, we should be using those.\" It takes awhile to sort of migrate over but once you see the first little nibbles of it actually working, then the flood gates open.\n\n**Susan:** The first time you go and say, well let's see what happens here - 70 times faster than what I was doing before - I think we've got something we wanna spend more time on. This is only gonna get better.\n\n**Scott:** The concept for deep learning and machine learning scientists that are trying to figure out a solution to a problem, you're mostly patience limited. You can only wait a few days or maybe weeks if you're really patient, or maybe months if you're really sure it's gonna turn into something good, but you can't wait that long to get an answer to your experiment. So any speed-up you can get, even 2X you're happy about, 10X you're really happy about. But 20X, 50X, 70X on a GPU, you're just like \"What? This enables so much more!\"\n\n**Susan:** It wasn't just one order of magnitude better, it's multiple orders of magnitude better.\n\n**That brings the point though, when machine learning came to the GPU, it wasn't the GPU came to machine learning.** I think now what we're seeing is, with these specialized integrated circuits, we're seeing hardware is coming to machine, learning for the first time.\n\nIn the past, machine learning adapted to the hardware, now the hardware we're starting the hardware adapt to the machine learning. That's the revolution that's going on right now.\n\nEverybody knows it, but not too many people are really stating that seed change. Your phone's got specialized logic in it, where we're starting to see chip manufacturers really make this a major design priority, as opposed to some little secondary thing. That's a big deal.\n\n**Scott:** It's hard, it's a big expense. It's a big gamble, to say \"Hey, I'm going to build a specialized chip for this thing over here. I'm gonna take 3 years and develop it, and spend many millions of dollars. The ground might shift underneath me, and because machine learning, deep learning is moving at such a fast pace, that maybe 3 years from now the thing I'm building isn't even gonna matter.\"\n\n**Susan:** Maybe.\n\n**Scott:** People have seen over the past 5 years or so, like well [CNNs](https://en.wikipedia.org/wiki/Convolutional_neural_network) are a thing, Convolutional Neural Networks, Recurrent Neural Networks are a thing. For the most part, like these ML Kit type chips really focus on the CNN, making that good, because it's like image processing from like Image Net, and running those things. These typical tasks, yeah I can build those, and I can make them fast, and I can accelerate them here rather than doing it on a CPU, or having to put a GPU, kind of the middle version of it in there. It's not a CPU, GPUs a lot faster, maybe uses some power, but then I can put it on this special chip and actually get a result out of it. People have actually-people, meaning companies like Apple-have actually put their money down and said, okay we're gonna go after that.\n\n**Susan:** It shows maturity, and it shows that we have enough examples floating around of what it takes to do this stuff, to understand how we need to access memory that's different than how you do it with that generic GPU. I shouldn't say generic GPU but the specialized GPU.\n\n**Scott:** It feels generic now, cause they work. Things have been working for years now, but if you went back 5 years ago you'd be like \"Whoa, it's all kind of, is this actually going to work?\" 5 years later we're like \"Yeah, yeah.\"\n\n**Susan:** It's out of the box now.\n\n**Scott:** Fine, you just buy one, you pop it in.\n\n**Susan:** We now have a much better understanding of the specialized hardware environment that machine learning, deep neural networks, and that kind of world, is imposing on this hardware, and you know, the money's there, the knowledge is there, there's still risk. I don't think anyone doubts that we're gonna see major, major shifts in how machine learning works. However, these are the first stabs, and the first stabs are making huge improvements. Low powered machine learning chips sitting in your iPhone enabling amazing new things. Can you do real-time voice, can you do real-time image recognition, the whole augmented reality world has gotta be just having a field day with the ability to do real time image recognition with these things. It's going to enable a powerful new class of applications, it already has enabled powerful new classes of applications. That's direct result of making those specialized hardware bets.\n\n**Susan:** This is just a natural progression, we saw this in like hard drives back in the old days. The CPU literally would have to tell them to move exactly one place or another. Then you've got controllers that started taking that logic off, and then you have whole IO subsystems, and then specialized, and specialized, and specialized.You get big gains from that but you had to have that knowledge base beforehand to figure out how to specialize correctly.\n\n## Where are the gains?\n\n**Scott:** Not everything is happening on your mobile phone though. You're not using training networks there. It might just be the inference, and it's doing inference only for you locally. Add it up over everybody's mobile phone, maybe that's a lot of inference power, but still it's not going to do other things like cloud based or in your browser when you're searching a web page. These things are all places where the computation can happen. The CPU on your laptop has a certain amount of power, and computational ability, and memory available to it, and a computer in the cloud usually has more. Then on your phone it's less.\n\n**Susan:** It is interesting that we're seeing a lot of great tools starting to hit on the browser front. You know, JavaScript APIs for machine learning are coming out, and when I say for machine learning, like you said it's more for the inference side.\n\n**Susan:** You're gonna do your heavy lifting on a box that's got some nice requirements and all this stuff.\n\n**Scott:** Yeah, you do your training on a server.\n\n**Susan:** And then you make sure it can run on the browser.\n\n**Scott:** Yep, and then maybe you shrink it down.\n\n**Susan:** If you're doing it locally in a browser, you probably only need to recognize 1 stream of audio. You only need to recognize one image. You're not doing 10,000 streams simultaneously. But, it's just amazing to see how quickly we've gone, and how much we're now specializing, and how big. This is making hardware changes. It's carving out a niche in architectures, which is a pretty big deal.\n\n**Scott:** If your phone is trying to do some inference on a chip then it has to go out and access a database on a server somewhere else in order to do the inference-it doesn't make that much sense. What it would rather do probably is just ship off the image, have that handled somewhere else, and then get the answer back.\n\n**Susan:** That's a great point. You wanna put the processing power where the data is at. Because, even though we got big pipes, they may not be perfect. So you see that trend: we're pushing tools on to the phones. There's one area that is growing in leaps and bounds right now that's a big big deal, and that's the self driving car world: autonomous vehicles.\n\n## You can't process that on the cloud.\n\n**Scott:** Yeah you don't want that. If your internet connection goes down, you don't want your car to stop driving.\n\n**Susan:** Yeah, it's like, I'm sorry we can't drive you to grandma's house.\n\n**Scott:** The buffering graphic comes up.\n\n**Susan:** But there's a turn, please connect it! There's a turn!!\n\n\\***\\*Scott:** Buffering, buffering.\n\n**Susan:** Please stand by.\n\n**Susan:** Just like on the phone front, you're really starting to see specialized hardware come out, I mean big time. NVIDIA got a great platform for that. Trying to standardize hardware and all that for doing those big problems locally.\n\n**Scott:** Because you have tons of sensors and you need to have a rack to mount everything to, and so it's like just get this thing, and then now you essentially have an autonomous car. This is still more like a development kit though, right? This isn't like go buy this, put it on your car, now your car's gonna go drive itself.\n\n**Susan:** This is like you're playing your PC, it doesn't mean that you can do anything with it, it just means that you've got the compute necessary to do cool things with it.\n\n**Scott:** You've got the hardware. Still a lot of massaging that needs to happen.\n\n**Susan:** The point is that we're specializing hardware for the machine learning applications, and no longer are we kind of begging, borrowing, stealing, from the graphics world. And the graphics world's like: \"We'll make a little concession for you.\" Now it's full blown, big deal, making specialized stuff for it which is a huge change. It's a huge big deal. But so is this what we're gonna see? Where are we at and are we on the right path?\n\n## Is this what we're gonna see 10 years from now?\n\n**Scott:** I think the time scale of 10 years, maybe there could be a change but that's a long time away.\n\n**Susan:** Yeah, in internet years. In learning years.\n\n**Scott:** In deep learning years. That's a long, long time away. But I think talking about the different types of devices matters. So mobile, personal computer, and server, that matters. If you're talking about training your inference, I think it matters. It's going to be hard to get away from GPUs in the server domain just because power isn't so much of an issue there, and you really want raw high functioning power that is very agile, so you can do your experiments. If you're going to have a really hard coded AI training chip, then you better not change your model very much.\n\n**Susan:** You better not bring in brand new, crazy concepts. You better not bring in brand new ideas. You better stick to large numbers times large numbers plus large numbers.\n\n**Scott:** Exactly, and you can train in your image net model really fast, fine, right?\n\n**Susan:** That's so 2000s.\n\n**Scott:** But if you're trying to do something more complicated, then you're probably not going to be able to. Like time delaying your neurons or something, that's not gonna happen on that chip. You probably can do it in a more flexible architecture. Maybe it'll be easier to do it in CPUs, hard and time consuming to train, but then morph it to a GPU-land where it can actually grow up a bit there, and then maybe you ship it off to that AI chip.\n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976783/blog/whats-the-best-infrastructure-for-machine-learning-ai-show/AI-chip-2.jpg)\n\n**Susan:** I totally one hundred percent agree with you, and for those who don't know, Scott and I have had this kind of running debate which goes: \"What is the future of machine learning like?\" One of my first shots across a bow with Scott was, matrix math, 10,000 wide whatever. I think that we're gonna see much incredibly more complex and fine-grained structures come out-ones that are not as compatible with today's efficient, and large matrix multiplication.\n\n**Scott:** Right now, the idea is: Take whatever problem you have and think really hard so you can shoehorn it into doing a matrix multiplication. If you can do that then it'll probably be fast enough in order to train, and you can see the result.\n\n**Susan:** We've taken a brand new big, high resource area of all this compute that GPUs have given us and we've adapted to that. Now that we're seeing the hardware adapt to us, I think we're going to see an explosion of new architecture ideas. There's a lot more revived interest in and thinking about the basic structure: thinking about the basic neurons going back to biology again, and pulling in new things like time delays. Of course there's pulling in brand new types of ways of connecting them. Again going down to single connections as opposed to thinking, \"Oh I've got this 1024 layer, fully connected layer to it, now it's a 1024 to fully connect the layer, just because it's efficient and fits right in perfectly memory wise and all these different things.\" But nothing about the problem said a 1024. How do you appropriately connect these things?\n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976784/blog/whats-the-best-infrastructure-for-machine-learning-ai-show/Artificial_neural_network.jpg)\n\n*This is a representation of a generic neural network. Note the connections between the first layer of three nodes, and the second layer of four.* \n\n**Susan:** Now that we're merging back and we're having hardware starting to adapt to us, I think that we're going to start seeing that the current situation is an iteration number 1. By the time we get to iteration number 3 we'll be looking back at the first stuff and see that it can't run anything. This would be because we've been able to explore these crazy amazing, brand new, out there, fun, architectures.\n\n**Scott:** The backwards compatibility isn't going to be that good.\n\n**Susan:** You see this throughout. You make this blind stab into the future, and especially with a field that's changing so rapidly, and has so much, so far to go. It is an impossible task to pick it perfectly. It's amazing. It truly is amazing that we've had as much longevity as we've had with the tool kits, and I think that speaks to the massive, massive power gain that we got from GPUs. It's just such massive resource that we got, that we have kept using it, and not shifted it dramatically so far.\n\n**Scott:** But it speaks to how large the problem is too. Hey we still use 'ye old internet too', and it's like, well things get better but it's still using similar protocols that we just kind of beefed up over time. Yeah, it's frustrating it doesn't work as perfectly as you would expect, but hey, it works. Of course, you can make advancements with it, and a similar thing will probably happen over the next 5 or 10 years with deep learning. We have all these tools, we have all this established literature, and tribal knowledge about how this stuff works, and we keep getting more and more results. But eventually you'll hit some diminishing returns. We will need to make a jump to something else.\n\n**Susan:** It's an exciting time to be here, to see this stuff. Just seeing all the new worlds we can work in. The idea that you can really apply these tools in the browser, you can apply them on your phone, you can apply them on an internet of things device, you can apply them on something rolling down the road at 100 miles an hour. I don't drive at 100 miles an hour, honest.\n\n**Scott:** I think, you're going to see it deployed in those areas, server, personal computer, on your phone you're already seeing it happen, but you're gonna see it just integrated more and more, and the inference especially. Training not so much, that's still gonna stay in the servers for awhile. But you know, people discuss about federated training, right like using your mobile phone or your CPU or something to train a model, but like very slowly, locally, but then aggregating all the results on a server. That might be a thing, but has not been a thing even though people have talked about it for a few years.\n\n**Susan:** This is a little off-the-infrastructure-side, but these pieces of infrastructure, enabling rapid prototyping, we get breakthroughs every once in awhile that radically change the playing field. Often what we find is we were doing something incredibly inefficiently before, but because we didn't have the resources to just do that thing fast enough to try the next thing, and next thing, and next thing, it was just a huge roadblock. Had we known this one weird trick.\n\n**Susan:** We could have done the things in the past.\n\n**Scott:** The cost is discovering the trick.\n\n**Susan:** And once you find that one weird trick. And so it's entirely possible that, or entirely probable that the next sets of revolutions will come as we find that one weird trick, and it changes everything we know about how networks work, and the best way to plug them together. We've got 5 100ths of the memory, and a thousand times the speed, just because of that one weird trick.\n\n**Scott:** You already see some of the GPU side morphing a little bit, saying hey we're going to include FP16 training, which is like a lower precision training so it can train faster but while giving up precision. You see tensor cores, that are a more specialized unit instead of just the standard processing unit that they use in the GPUs, like, \"Hey here's one specialized just for machine learning,\" but it's still pretty general. You know you see the hardware changing on the GPU side as well, just to kind of get a little closer to that specialization but without losing the flexibility.\n\n**Susan:** Yeah, like I said, we start off adapting to the hardware, and now we're seeing the hardware adapt to us, and it's the circle of computer life.\n\n**Scott:** The circle of life. They eat each other too and they recycle the silicone.";
						}
						async function compiledContent$3() {
							return load$3().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$3() {
							return (await import('./chunks/index.0584ce64.mjs'));
						}
						function Content$3(...args) {
							return load$3().then((m) => m.default(...args));
						}
						Content$3.isAstroComponentFactory = true;
						function getHeadings$3() {
							return load$3().then((m) => m.metadata.headings);
						}
						function getHeaders$3() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$3().then((m) => m.metadata.headings);
						}

const __vite_glob_0_273 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$3,
  file: file$3,
  url: url$3,
  rawContent: rawContent$3,
  compiledContent: compiledContent$3,
  default: load$3,
  Content: Content$3,
  getHeadings: getHeadings$3,
  getHeaders: getHeaders$3
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$2 = {"title":"Why Does Your Speech Recognition Need Context?","description":"Learn what context means in speech recognition, and why it's so important.","date":"2018-12-01T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1662069465/blog/why-does-your-speech-recognition-need-context/placeholder-post-image%402x.jpg","authors":["morris-gevirtz"],"category":"ai-and-engineering","tags":["machine-learning","voice-tech"],"seo":{"title":"Why Does Your Speech Recognition Need Context?","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1662069465/blog/why-does-your-speech-recognition-need-context/placeholder-post-image%402x.jpg"},"shorturls":{"share":"https://dpgr.am/57466dd","twitter":"https://dpgr.am/fdd6127","linkedin":"https://dpgr.am/8e14aa4","reddit":"https://dpgr.am/ec104f3","facebook":"https://dpgr.am/6526000"}};
						const file$2 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/why-does-your-speech-recognition-need-context/index.md";
						const url$2 = undefined;
						function rawContent$2() {
							return "<iframe src=\"https://www.youtube.com/embed/04YXLTnafTc\" width=\"600\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\nAlexa and Siri are great and all, but they often make funny, if not serious mistakes. Why is this?\n\n![](https://res.cloudinary.com/deepgram/image/upload/v1661976769/blog/why-does-your-speech-recognition-need-context/Screen-Shot-2018-11-19-at-4.19.58-PM-1.png)\n\n\n\nI conducted an experiment to measure how important context is in understanding audio-especially when the audio is noisy. As humans we tend to enter conversations with some knowledge of what the topic is, which allows us to follow along. The speech recognition APIs which power Google Assistant, Siri, and Amazon Alexa, do not have this context. **As far as they know, every time you speak to them is like the first time you have ever spoken to them.** Suspecting this might be the reason they often misunderstand things which they shouldn't, I decided to test this:\n\n## The Experiment\n\nI found an interesting image and recorded myself saying a sentence about it. I chose one of these images and went out to the street to ask people to listen to the recording, first without the visual stimulus, then with it. Both times, I asked people to tell me what they heard in order to see what effect seeing the image had on their ability to understand what was said. Without the context, people said some crazy things-their grammar, word choice and logic was awful. But, when they had some notion as to what the sentence could be about, they performed significantly better (despite the noisy audio).\n\n## The Take-Home\n\nWhat is true of the human brain, in this case, is also true of speech recognition APIs. <mark>When you use a speech recognition API trained on your specific voice data, it's context gets narrowed down to your world, limiting and focusing the words it can choose from.</mark> The result is much more accurate, and usable, transcription.\n\n### Try the Experiment Yourself\n\n<iframe src=\"https://www.youtube.com/embed/IyqLOIDLZnQ\" width=\"600\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>";
						}
						async function compiledContent$2() {
							return load$2().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$2() {
							return (await import('./chunks/index.0c92ddd5.mjs'));
						}
						function Content$2(...args) {
							return load$2().then((m) => m.default(...args));
						}
						Content$2.isAstroComponentFactory = true;
						function getHeadings$2() {
							return load$2().then((m) => m.metadata.headings);
						}
						function getHeaders$2() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$2().then((m) => m.metadata.headings);
						}

const __vite_glob_0_274 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$2,
  file: file$2,
  url: url$2,
  rawContent: rawContent$2,
  compiledContent: compiledContent$2,
  default: load$2,
  Content: Content$2,
  getHeadings: getHeadings$2,
  getHeaders: getHeaders$2
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter$1 = {"title":"Why Enterprise Audio Requirements are More “Nuanced” at Real-time Speeds","description":"Review of Nuance and Deepgram for Real-Time Speech to Text Transcriptions","date":"2021-02-16T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981361/blog/why-enterprise-audio-requirements-are-more-nuanced-at-real-time-speeds/enterprise-audio-reqs-more-nuanced%402x.jpg","authors":["keith-lam"],"category":"speech-trends","tags":["education"],"seo":{"title":"Why Enterprise Audio Requirements are More “Nuanced” at Real-time Speeds","description":"Review of Nuance and Deepgram for Real-Time Speech to Text Transcriptions"},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981361/blog/why-enterprise-audio-requirements-are-more-nuanced-at-real-time-speeds/enterprise-audio-reqs-more-nuanced%402x.jpg"},"shorturls":{"share":"https://dpgr.am/bb88bde","twitter":"https://dpgr.am/fe888c1","linkedin":"https://dpgr.am/da9bdf3","reddit":"https://dpgr.am/f02c6da","facebook":"https://dpgr.am/79ef36a"}};
						const file$1 = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/why-enterprise-audio-requirements-are-more-nuanced-at-real-time-speeds/index.md";
						const url$1 = undefined;
						function rawContent$1() {
							return "[Nuance](https://deepgram.com/compare-nuance-dragon-speech-recognition-alternatives/) recently acquired Saykara, a mobile speech recognition technology provider to expand their medical transcription business. This acquisition is one of many major investments and acquisitions in the Natural Language Processing (NLP) and Customer Service space that we are monitoring at Deepgram. Due to the recent acquisition of Saykara, we thought it would be a good time to review Nuance speech recognition capabilities and why customers creating real-time experiences should consider alternatives. Enterprise audio is more nuanced than it may seem - pun intended! \n\n## **Leader of previous speech tech solutions** \n\nNuance is a great brand in the speech recognition business and has been around for over 30 years.  They have gobbled up smaller speech recognition businesses, including Saykara just recently to expand their medical transcription business.  If you need speech to text transcription especially in the medical setting, they will always be on the list to evaluate.  To be honest, our most recent survey indicates good satisfaction with Nuance.\n\n## **Core architecture has remained unchanged** \n\nNuance does a good job at speech to text transcription using their 1970's legacy speech model, called the Hidden Markov Model or [tri-gram model](https://deepgram.com/product/overview/).  They have added some AI and keyword libraries to their models to improve their accuracy but technically they need to sacrifice transcription speed for this accuracy.  So, for non-real time transcriptions, like medical transcriptions for medical records, they do an admirable job.  They can add hundreds of medical specific terms, acronyms, and drug names to make their model more accurate but it slows down their transcriptions. Deepgram does not use this legacy [tri-gram model](https://deepgram.com/product/overview/). We built our speech recognition solution from scratch using a completely different architecture. Deepgram uses an end to end Deep Learning Neural Network, which in simple terms means we perform audio to text transcription in one AI-enabled step and we can continually improve our accuracy with more data at the same transcription speed.  Due to our architecture, customers do not have to compromise accuracy vs. speed, speed vs. costs or cost vs. scalability. Our tests with their speech recognition engine shows they can transcribe 1 hour of normal speech data (500 MB with one CPU/GPU) in 1 hour.  While, Deepgram can transcribe the same 1 hour in 30 seconds. Check out this [demo](https://drive.google.com/file/d/1OylFXC4siC9PKlIg6ybcJ80MrLOE54FR/view) of Deepgram speed compared to Google, and this [demo](https://www.youtube.com/watch?v=3gv8lbbuY-Q&t=2s) of Deepgram scale.\n\n## **Enabling real-time AI is Deepgram's forte**\n\nWhen we talk about real-time AI for [Conversational AI virtual agents](https://deepgram.com/solutions/voicebots/), sales or support [agent enablement](https://deepgram.com/solutions/contact-centers/), or real-time [compliance monitoring](https://deepgram.com/solutions/finance/), you need both millisecond speed and high accuracy.  Customers do not want to wait for the virtual agent to transcribe what you said, send that data to the AI engine, get a response and then turn the response from text to speech.  Any lag in that process would cause customer dissatisfaction.  Worst is if the response is incorrect or the virtual agent needs to ask the customer to repeat what they said, poor transcription accuracy.  For real-time streaming, our AI Speech Platform transcription lag is under 300 milliseconds.\n\n## **Compare us**\n\nYou know I'm biased so [do a comparison yourself](https://deepgram.com/asr-comparison/) or we can do a [comparison](https://offers.deepgram.com/nuance-speech-assessment) for you. [Get your comparison](https://offers.deepgram.com/nuance-speech-assessment)";
						}
						async function compiledContent$1() {
							return load$1().then((m) => m.compiledContent());
						}

						// Deferred
						async function load$1() {
							return (await import('./chunks/index.65c8a32d.mjs'));
						}
						function Content$1(...args) {
							return load$1().then((m) => m.default(...args));
						}
						Content$1.isAstroComponentFactory = true;
						function getHeadings$1() {
							return load$1().then((m) => m.metadata.headings);
						}
						function getHeaders$1() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load$1().then((m) => m.metadata.headings);
						}

const __vite_glob_0_275 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter: frontmatter$1,
  file: file$1,
  url: url$1,
  rawContent: rawContent$1,
  compiledContent: compiledContent$1,
  default: load$1,
  Content: Content$1,
  getHeadings: getHeadings$1,
  getHeaders: getHeaders$1
}, Symbol.toStringTag, { value: 'Module' }));

// Static
						const frontmatter = {"title":"Why IoT Means Speech Recognition","description":"Voice assistants and smart speakers are at the forefront of the Internet of Things—and they're powered by speech recognition.","date":"2018-11-29T00:00:00.000Z","cover":"https://res.cloudinary.com/deepgram/image/upload/v1661981329/blog/why-iot-means-speech-recognition/why-IOT-means-speech-rec%402x.jpg","authors":["morris-gevirtz"],"category":"speech-trends","tags":["voice-tech"],"seo":{"title":"Why IoT Means Speech Recognition","description":""},"og":{"image":"https://res.cloudinary.com/deepgram/image/upload/v1661981329/blog/why-iot-means-speech-recognition/why-IOT-means-speech-rec%402x.jpg"},"shorturls":{"share":"https://dpgr.am/1d1cea7","twitter":"https://dpgr.am/453e052","linkedin":"https://dpgr.am/be5817c","reddit":"https://dpgr.am/a5bbccb","facebook":"https://dpgr.am/1567b21"}};
						const file = "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/why-iot-means-speech-recognition/index.md";
						const url = undefined;
						function rawContent() {
							return "People now use their voice to control an increasing number of their devices-from TVs to cars, and everything in between. The technology that makes voice control possible is speech recognition - technology that transforms your verbal commands into executable actions via an API.\n\n## How do Speech Recognition APIs work?\n\nWhen you talk to your speech recognition-enabled device-Google assistant, Siri, or your stove-you first have to wake the device by saying (or yelling) a keyword like \"OK Google.\" These devices have a little bit of local hardware that is constantly listening for a specific wake word. Once it hears that, it really starts listening until you stop speaking. At this point, the device sends your voice clip to a speech recognition API in the cloud. There, the API does its best to convert your verbal command into text. The service running the speech recognition API then looks at the transcript and reacts accordingly. Maybe it talks to a music service and plays a song for you, maybe it offers you a choice of paper towels, or maybe it dims the lights. The ability to control the world using our voices is a game-changer in our relationship with the products, machines and services that are part of our modern life. Here are some examples:\n\n### Voice Assistants\n\nThere are probably a couple dozen voice assistants available on the market today. Google Assistant, Siri, and Cortana are three of the better known ones. These are speech recognition API-powered apps that make our lives easier, essentially acting as concierges and note-takers. Need to schedule a phone call with your mother at 2pm, Wednesday May 15th 2024? No problem, just ask Siri to do so. Need to call a Lyft to take you to your meeting with an executive? No problem, ask Google. When we use voice assistants, we the consumer have a sense that the speech app itself is the scheduling dates, ordering food, playing music, etc. However, that is not quite the case. Speech recognition APIs are a bit of cloud software that engineers can add to their products similarly to how you can add apps like Uber to your phone. That speech recognition API is what allows you to interact with other technologies or applications via voice. Speech recognition APIs can be worked into almost any app or device to allow people to interact with technology in a more human way using voice, rather than fiddling with mice, keyboards, and touch screens.\n\n### Smart Speakers\n\n![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976769/blog/why-iot-means-speech-recognition/rahul-chakraborty-556155-unsplash.jpg)\n\nThe Amazon Echo and Google Home are two renowned examples of smart speakers-voice recognition-enabled devices. These devices are screenless extensions of the digital voice assistants that \"live\" in your phone, except with better sound and [Space Odyssey 2001](https://blog.deepgram.com/what-makes-alexa-siri-terminator-and-hal-tick/)-like lines. Like the voice assistants on your phone, these two allow you to make appointments, send texts, and, depending on the product, even make purchases. Users make use of them to look up recipes while their hands are covered in chocolate mousse, buy concert tickets while parents clean up said abandoned chocolate mousse, and of course, to turn on music without interrupting more important activities. What makes the consumer experience different for each smart speaker depends on three factors:\n\n1. The accuracy of the speech recognition API running behind the scenes\n2. The suite of apps and services that are integrated with the speaker-e.g. Spotify vs. iTunes, Apple Maps vs. Google Maps.\n3. The quality of the actual speaker.\n\n### Controlling Homes\n\nApple HomeKit, GoogleHome and the Athom Homey are three examples of devices designed to allow humans to talk to their machines, rather than push buttons. Now, the 30 possible functions of the modern home thermostat, as well as the 40 billion possible functions of a modern television set can be controlled by speaking. We can think of these devices as smart speakers more richly integrated into the home. \n\n![ALT](https://res.cloudinary.com/deepgram/image/upload/v1661976770/blog/why-iot-means-speech-recognition/brandon-jacoby-306845-unsplash.jpg)\n\n**Smart home solutions are a transformational technology.** Here's why:\n\n1. They bring all the home machines, i.e. heating, cooling, refrigerator, alarm system, into one ecosystem. Such unification is the sort of disruptive change that makes economic successes.\n2. Smart home solutions connect your home to the cloud, and thus your mobile phone. As a result they make any home part of a village - no matter where you go, you are always close to home.\n\n### Controlling our Cars\n\nSome auto manufacturers have utilized speech recognition APIs to enable next-generation hands-free control of car systems. Until recently, automotive voice command systems received fairly poor reviews. This has now begun to change as auto manufacturers have begun to utilize [more accurate speech recognition APIs.](https://blog.deepgram.com/why-8-year-olds-are-like-asr/) When integrated into the automobile and concomitant apps, speech recognition APIs allow users to safely do hands free navigation, hear and write text messages, make phone calls and use the climate control, audio system and other traditional \"hand-on\" aspects of the auto. Clearly, speech recognition APIs are particularly well suited for implementation in cars. That said, this is a use case where accuracy is as important as it can get. When used at 70 miles an hour, no matter the task, good performance is absolutely critical. As a result, which speech recognition API companies choose matters.\n\n## Voice UI and Speech Recognition APIs are Revolutionary\n\nSpeech recognition APIs make it possible to interact with the machines, apps and devices that we use every day in a more human, user-friendly way. As such, their advances bring about a paradigm shift how we interact with machines. In 1930, no one would have thought of running a tractor with a keyboard. <mark>In 2018, we know we will be running nearly everything with voice.</mark> From this we can gather three key insights:\n\n1. Products will increasingly be designed with voice in mind.\n2. Advertisers will take the [voice channel](https://blog.deepgram.com/how-can-companies-extract-more-value-out-of-voice/)-listening and speaking-into account. Advertising campaigns must be designed to work with this changing, multichannel ecosystem.\n3. As companies design products that rely on more and more accurate speech recognition APIs, product leaders will look to [next-generation automatic speech recognition](https://blog.deepgram.com/deep-learning-speech-recognition/) technologies to reliably build better voice experiences for their users.";
						}
						async function compiledContent() {
							return load().then((m) => m.compiledContent());
						}

						// Deferred
						async function load() {
							return (await import('./chunks/index.9cc5f48e.mjs'));
						}
						function Content(...args) {
							return load().then((m) => m.default(...args));
						}
						Content.isAstroComponentFactory = true;
						function getHeadings() {
							return load().then((m) => m.metadata.headings);
						}
						function getHeaders() {
							console.warn('getHeaders() have been deprecated. Use getHeadings() function instead.');
							return load().then((m) => m.metadata.headings);
						}

const __vite_glob_0_276 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  frontmatter,
  file,
  url,
  rawContent,
  compiledContent,
  default: load,
  Content,
  getHeadings,
  getHeaders
}, Symbol.toStringTag, { value: 'Module' }));

const postImportResult = /* #__PURE__ */ Object.assign({"../content/blog/posts/2021-state-of-automatic-speech-recognition-infographic/index.md": __vite_glob_0_0,"../content/blog/posts/5-ways-understand-voice-of-the-customer-voice-technology/index.md": __vite_glob_0_1,"../content/blog/posts/6-challenges-asr-hindi/index.md": __vite_glob_0_2,"../content/blog/posts/a-conversation-with-asian-american-pacific-islander-deepgrammers/index.md": __vite_glob_0_3,"../content/blog/posts/a-note-to-our-customers-openai-whispers-entrance-into-voice/index.md": __vite_glob_0_4,"../content/blog/posts/a-voice-destin-ation-project-voice-x-2021/index.md": __vite_glob_0_5,"../content/blog/posts/accuracy-matters-improving-speech-recognition-through-data-processes-esteban-gorupicz-ceo-atexto-project-voice-x/index.md": __vite_glob_0_6,"../content/blog/posts/adding-subtitles-to-html-video-element/index.md": __vite_glob_0_7,"../content/blog/posts/ai-show-bias-in-machine-learning/index.md": __vite_glob_0_8,"../content/blog/posts/ai-show-different-types-of-machine-learning/index.md": __vite_glob_0_9,"../content/blog/posts/ai-show-how-do-you-use-a-neural-network-in-your-business/index.md": __vite_glob_0_10,"../content/blog/posts/ai-show-how-will-data-influence-the-future-of-machine-learning/index.md": __vite_glob_0_11,"../content/blog/posts/ai-show-what-does-an-ai-tranformation-look-like/index.md": __vite_glob_0_12,"../content/blog/posts/ai-show-what-does-it-mean-for-a-machine-to-learn/index.md": __vite_glob_0_13,"../content/blog/posts/ai-show-what-will-the-ai-utopia-look-like/index.md": __vite_glob_0_14,"../content/blog/posts/all-about-transcription-for-real-time-audio-streaming/index.md": __vite_glob_0_15,"../content/blog/posts/ar-note-taking-airnote/index.md": __vite_glob_0_16,"../content/blog/posts/asr-important-deaf-hoh-community/index.md": __vite_glob_0_17,"../content/blog/posts/asynchronous-logic-to-write-a-vue-3-and-deepgram-captions-component/index.md": __vite_glob_0_18,"../content/blog/posts/autobubble-youtube-speech-bubbles/index.md": __vite_glob_0_19,"../content/blog/posts/automatic-speech-recognition-education/index.md": __vite_glob_0_20,"../content/blog/posts/automatically-transcribe-summarize-and-send-phone-call-summaries/index.md": __vite_glob_0_21,"../content/blog/posts/automatically-transcribing-podcast-episodes-with-pipedream-and-python/index.md": __vite_glob_0_22,"../content/blog/posts/bekah-joins-deepgram/index.md": __vite_glob_0_23,"../content/blog/posts/best-8-deepgram-projects-hack-cambridge/index.md": __vite_glob_0_24,"../content/blog/posts/best-python-audio-manipulation-tools/index.md": __vite_glob_0_25,"../content/blog/posts/best-speech-recognition-model-business/index.md": __vite_glob_0_26,"../content/blog/posts/best-speech-to-text-apis/index.md": __vite_glob_0_27,"../content/blog/posts/brian-barrow-hello/index.md": __vite_glob_0_28,"../content/blog/posts/build-a-livestream-web-application-vue-and-express-setup/index.md": __vite_glob_0_29,"../content/blog/posts/build-a-livestream-web-application-with-amazon-ivs-and-deepgram/index.md": __vite_glob_0_30,"../content/blog/posts/build-a-presentation-coaching-application-with-recall/index.md": __vite_glob_0_31,"../content/blog/posts/build-a-todo-list-with-pinia-and-vue-3/index.md": __vite_glob_0_32,"../content/blog/posts/build-a-voice-controlled-to-do-list-app-with-deepgram-and-vue-3/index.md": __vite_glob_0_33,"../content/blog/posts/build-npm-packages/index.md": __vite_glob_0_34,"../content/blog/posts/build-with-the-official-deepgram-sdks/index.md": __vite_glob_0_35,"../content/blog/posts/building-404-pages-that-bring-joy/index.md": __vite_glob_0_36,"../content/blog/posts/building-a-conversational-ai-flow-with-deepgram/index.md": __vite_glob_0_37,"../content/blog/posts/building-the-future-of-voice-scott-stephenson-ceo-deepgram-project-voice-x/index.md": __vite_glob_0_38,"../content/blog/posts/celebrating-black-history-month-with-a-vision-of-more-inclusive-speech-recognition/index.md": __vite_glob_0_39,"../content/blog/posts/celebrating-jewish-american-history-month/index.md": __vite_glob_0_40,"../content/blog/posts/censor-profanity-nodejs/index.md": __vite_glob_0_41,"../content/blog/posts/chili-pepper/index.md": __vite_glob_0_42,"../content/blog/posts/chromium-kiosk-pi/index.md": __vite_glob_0_43,"../content/blog/posts/classroom-captioner/index.md": __vite_glob_0_44,"../content/blog/posts/closed-captioning-companies-use-asr/index.md": __vite_glob_0_45,"../content/blog/posts/cloud-to-butt/index.md": __vite_glob_0_46,"../content/blog/posts/coding-website-with-voice/index.md": __vite_glob_0_47,"../content/blog/posts/comic-books-videos-yack/index.md": __vite_glob_0_48,"../content/blog/posts/complete-guide-punctuation-capitalization-speech-to-text/index.md": __vite_glob_0_49,"../content/blog/posts/contact-center-as-a-service-utilize-solutions/index.md": __vite_glob_0_50,"../content/blog/posts/contextual-video-overlay-tomscottplus/index.md": __vite_glob_0_51,"../content/blog/posts/conversational-ai-platforms-utilize-top-asr-tools/index.md": __vite_glob_0_52,"../content/blog/posts/conversational-intelligence-podcast-with-scott-stephenson/index.md": __vite_glob_0_53,"../content/blog/posts/create-readable-transcripts-for-podcasts/index.md": __vite_glob_0_54,"../content/blog/posts/cross-platform-nuget-dotnet/index.md": __vite_glob_0_55,"../content/blog/posts/customer-story-rideshare-smartrhino-deepgram/index.md": __vite_glob_0_56,"../content/blog/posts/customer-story-stanford-moves-education-forward-with-deepgram/index.md": __vite_glob_0_57,"../content/blog/posts/cześć-we’re-releasing-a-base-polish-beta-speech-to-text-language-model/index.md": __vite_glob_0_58,"../content/blog/posts/daily-video-live-transcription/index.md": __vite_glob_0_59,"../content/blog/posts/deep-learning-asr-for-business/index.md": __vite_glob_0_60,"../content/blog/posts/deep-learning-speech-recognition/index.md": __vite_glob_0_61,"../content/blog/posts/deepgram-and-recall-ai-partner-to-make-it-easier-for-developers-to-extract-insights-from-meeting-audio-and-automate-tedious-workflows/index.md": __vite_glob_0_62,"../content/blog/posts/deepgram-announces-unimrcp-integration-to-power-modern-customer-experience/index.md": __vite_glob_0_63,"../content/blog/posts/deepgram-diversity-inclusion/index.md": __vite_glob_0_64,"../content/blog/posts/deepgram-enables-developers-to-build-the-future-of-voice-with-suite-of-new-features-and-10-million-in-free-speech-recognition/index.md": __vite_glob_0_65,"../content/blog/posts/deepgram-enters-strategic-investment-agreement-with-in-q-tel-2/index.md": __vite_glob_0_66,"../content/blog/posts/deepgram-g2-customer-service/index.md": __vite_glob_0_67,"../content/blog/posts/deepgram-g2-review-winter-2022/index.md": __vite_glob_0_68,"../content/blog/posts/deepgram-godot-tutorial/index.md": __vite_glob_0_69,"../content/blog/posts/deepgram-hackathon-recap/index.md": __vite_glob_0_70,"../content/blog/posts/deepgram-is-a-founding-member-of-callminers-open-voice-transcription-standard-ovts/index.md": __vite_glob_0_71,"../content/blog/posts/deepgram-language-speech-models/index.md": __vite_glob_0_72,"../content/blog/posts/deepgram-named-a-high-performer-for-voice-recognition-software-in-g2-fall-report/index.md": __vite_glob_0_73,"../content/blog/posts/deepgram-pioneers-novel-training-approach-setting-new-standard-for-ai-companies-2/index.md": __vite_glob_0_74,"../content/blog/posts/deepgram-projectvoicex-transcription-aicontactcenter-artcoombs/index.md": __vite_glob_0_75,"../content/blog/posts/deepgram-reached-soc-2-type-1-certification/index.md": __vite_glob_0_76,"../content/blog/posts/deepgram-series-a/index.md": __vite_glob_0_77,"../content/blog/posts/deepgram-stepzen-collaboration/index.md": __vite_glob_0_78,"../content/blog/posts/deepgram-summit-speaker-lineup-2021/index.md": __vite_glob_0_79,"../content/blog/posts/deepgram-twilio-streaming-rust/index.md": __vite_glob_0_80,"../content/blog/posts/deepgram-twilio-streaming/index.md": __vite_glob_0_81,"../content/blog/posts/deepgram-unity-tutorial/index.md": __vite_glob_0_82,"../content/blog/posts/deepgram-versus-amazon-google/index.md": __vite_glob_0_83,"../content/blog/posts/deepgram-with-vonage/index.md": __vite_glob_0_84,"../content/blog/posts/deepgrams-speech-to-text-api-number-1-for-developers-g2/index.md": __vite_glob_0_85,"../content/blog/posts/democratizing-speech-analytics-deepgram-callbi/index.md": __vite_glob_0_86,"../content/blog/posts/detect-non-inclusive-language-with-retext-and-node-js/index.md": __vite_glob_0_87,"../content/blog/posts/detecting-and-reducing-bias-in-speech-recognition/index.md": __vite_glob_0_88,"../content/blog/posts/difference-between-language-dialect/index.md": __vite_glob_0_89,"../content/blog/posts/diving-into-vue-3-getting-started/index.md": __vite_glob_0_90,"../content/blog/posts/diving-into-vue-3-methods-watch-and-computed/index.md": __vite_glob_0_91,"../content/blog/posts/diving-into-vue-3-reactivity-api/index.md": __vite_glob_0_92,"../content/blog/posts/diving-into-vue-3-reusability-with-composables/index.md": __vite_glob_0_93,"../content/blog/posts/diving-into-vue-3-setup-function/index.md": __vite_glob_0_94,"../content/blog/posts/do-your-call-transcripts-read-like-mad-libs/index.md": __vite_glob_0_95,"../content/blog/posts/does-unsupervised-learning-create-superior-speech-recognition/index.md": __vite_glob_0_96,"../content/blog/posts/downloading-podcast-transcripts-from-terminal/index.md": __vite_glob_0_97,"../content/blog/posts/draw-with-your-voice-articulate/index.md": __vite_glob_0_98,"../content/blog/posts/embracing-the-diversity-of-spanish/index.md": __vite_glob_0_99,"../content/blog/posts/enhance-audio-with-dolby-and-deepgram/index.md": __vite_glob_0_100,"../content/blog/posts/enhanced-messaging-in-streaming/index.md": __vite_glob_0_101,"../content/blog/posts/everything-you-need-to-know-about-keywords-for-speech-recognition/index.md": __vite_glob_0_102,"../content/blog/posts/exploring-whisper/index.md": __vite_glob_0_103,"../content/blog/posts/fetch-hosted-audio-streams-in-the-browser/index.md": __vite_glob_0_104,"../content/blog/posts/ffmpeg-beginners/index.md": __vite_glob_0_105,"../content/blog/posts/flutter-speech-to-text-tutorial/index.md": __vite_glob_0_106,"../content/blog/posts/foreign-language-practice-triolingo/index.md": __vite_glob_0_107,"../content/blog/posts/freecodecamp-quote-generator-upgrade/index.md": __vite_glob_0_108,"../content/blog/posts/generate-webvtt-srt-captions-nodejs/index.md": __vite_glob_0_109,"../content/blog/posts/generic-asr-will-never-be-accurate-enough-for-conversational-ai/index.md": __vite_glob_0_110,"../content/blog/posts/getting-started-live-transcription-vue/index.md": __vite_glob_0_111,"../content/blog/posts/getting-started-with-apis/index.md": __vite_glob_0_112,"../content/blog/posts/getting-started-with-json/index.md": __vite_glob_0_113,"../content/blog/posts/getting-started-with-mediastream-api/index.md": __vite_glob_0_114,"../content/blog/posts/getting-started-with-supabase/index.md": __vite_glob_0_115,"../content/blog/posts/google-and-amazon-are-wrong-about-voice/index.md": __vite_glob_0_116,"../content/blog/posts/guide-deepspeech-speech-to-text/index.md": __vite_glob_0_117,"../content/blog/posts/happy-national-native-american-heritage-month/index.md": __vite_glob_0_118,"../content/blog/posts/hell-yes-we-have-sdks-apis-and-docs/index.md": __vite_glob_0_119,"../content/blog/posts/hello-world/index.md": __vite_glob_0_120,"../content/blog/posts/how-ai-is-advancing-the-transcription-process/index.md": __vite_glob_0_121,"../content/blog/posts/how-does-microsofts-purchase-of-nuance-communications-affect-the-market/index.md": __vite_glob_0_122,"../content/blog/posts/how-does-santa-do-it-ai-show/index.md": __vite_glob_0_123,"../content/blog/posts/how-gender-shows-up-in-language/index.md": __vite_glob_0_124,"../content/blog/posts/how-is-machine-learning-or-deep-learning-affecting-science-ai-show/index.md": __vite_glob_0_125,"../content/blog/posts/how-is-todays-ai-boom-different-from-those-of-the-past-ai-show/index.md": __vite_glob_0_126,"../content/blog/posts/how-to-add-speech-recognition-to-your-react-project/index.md": __vite_glob_0_127,"../content/blog/posts/how-to-build-an-openai-whisper-api/index.md": __vite_glob_0_128,"../content/blog/posts/how-to-get-a-job-in-deep-learning/index.md": __vite_glob_0_129,"../content/blog/posts/how-to-monitor-media-mentions-in-podcasts-with-python/index.md": __vite_glob_0_130,"../content/blog/posts/how-to-run-openai-whisper-in-command-line/index.md": __vite_glob_0_131,"../content/blog/posts/how-to-run-openai-whisper-in-google-colab/index.md": __vite_glob_0_132,"../content/blog/posts/how-to-test-automatic-speech-recognition-asr-providers-for-your-business/index.md": __vite_glob_0_133,"../content/blog/posts/how-to-train-baidus-deepspeech-model-with-kur/index.md": __vite_glob_0_134,"../content/blog/posts/how-to-use-whisper-openais-speech-recognition-model-in-1-minute/index.md": __vite_glob_0_135,"../content/blog/posts/how-to-write-vue-3-composables-for-a-third-party-api-integration/index.md": __vite_glob_0_136,"../content/blog/posts/how-voice-technology-creates-accessible-world/index.md": __vite_glob_0_137,"../content/blog/posts/identifying-the-best-agent-to-respond-in-your-ivr-system/index.md": __vite_glob_0_138,"../content/blog/posts/import-a-docker-container-in-python/index.md": __vite_glob_0_139,"../content/blog/posts/improve-ivr-prompts-with-custom-reporting/index.md": __vite_glob_0_140,"../content/blog/posts/introducing-auto-generated-summaries-for-audio-content/index.md": __vite_glob_0_141,"../content/blog/posts/introducing-real-time-streaming-and-solutions-for-conversational-ai-sales-and-support-enablement/index.md": __vite_glob_0_142,"../content/blog/posts/introducing-the-new-deepgram-developer-portal/index.md": __vite_glob_0_143,"../content/blog/posts/introducing-topic-detection-feature/index.md": __vite_glob_0_144,"../content/blog/posts/ios-live-transcription/index.md": __vite_glob_0_145,"../content/blog/posts/is-there-an-asr-gender-gap/index.md": __vite_glob_0_146,"../content/blog/posts/just-released-new-version-of-on-premises/index.md": __vite_glob_0_147,"../content/blog/posts/keywords-vs-search/index.md": __vite_glob_0_148,"../content/blog/posts/live-transcribing-radio-feeds-js/index.md": __vite_glob_0_149,"../content/blog/posts/live-transcription-badge-video/index.md": __vite_glob_0_150,"../content/blog/posts/live-transcription-django/index.md": __vite_glob_0_151,"../content/blog/posts/live-transcription-fastapi/index.md": __vite_glob_0_152,"../content/blog/posts/live-transcription-flask/index.md": __vite_glob_0_153,"../content/blog/posts/live-transcription-mic-browser/index.md": __vite_glob_0_154,"../content/blog/posts/live-transcription-quart/index.md": __vite_glob_0_155,"../content/blog/posts/luke-oliff-joins-deepgram/index.md": __vite_glob_0_156,"../content/blog/posts/machine-learning-for-front-end-developers-get-started-with-tensorflow-js/index.md": __vite_glob_0_157,"../content/blog/posts/making-your-audiovisual-content-accessible/index.md": __vite_glob_0_158,"../content/blog/posts/meet-kevin-lewis/index.md": __vite_glob_0_159,"../content/blog/posts/meet-sandra-rodgers/index.md": __vite_glob_0_160,"../content/blog/posts/meeting-analysis-platforms-automatic-speech-recognition-solutions/index.md": __vite_glob_0_161,"../content/blog/posts/michael-jolley-joins-deepgram/index.md": __vite_glob_0_162,"../content/blog/posts/multichannel-vs-diarization/index.md": __vite_glob_0_163,"../content/blog/posts/natural-language-understanding-nlu-for-audio-requires-a-highly-accurate-and-fast-speech-to-text-foundation/index.md": __vite_glob_0_164,"../content/blog/posts/new-releases-five-new-languages-and-three-new-use-case-speech-models/index.md": __vite_glob_0_165,"../content/blog/posts/new-spanish-and-turkish-language-models-and-updated-general-models/index.md": __vite_glob_0_166,"../content/blog/posts/new-tech-lets-journalists-find-damning-soundbites/index.md": __vite_glob_0_167,"../content/blog/posts/nlp-on-the-edge-voice-ai-and-hardware-robert-daigle-and-andi-huels-lenovo-project-voice-x/index.md": __vite_glob_0_168,"../content/blog/posts/now-available-deepgram-speech-recognition-for-twilio-programmable-voice-api/index.md": __vite_glob_0_169,"../content/blog/posts/npx-script/index.md": __vite_glob_0_170,"../content/blog/posts/nuxt-expand-nested-navigation/index.md": __vite_glob_0_171,"../content/blog/posts/olá-enhanced-portuguese-beta-speech-to-text-language-model-now-available/index.md": __vite_glob_0_172,"../content/blog/posts/open-source-projects-for-hacktoberfest-2022/index.md": __vite_glob_0_173,"../content/blog/posts/opening-keynote-bradley-metrock-ceo-project-voice-project-voice-x/index.md": __vite_glob_0_174,"../content/blog/posts/opening-keynote-jeff-blankenberg-principal-technical-evangelist-amazon-alexa-project-voice-x/index.md": __vite_glob_0_175,"../content/blog/posts/optimizing-your-content/index.md": __vite_glob_0_176,"../content/blog/posts/p5js-deepgram-game/index.md": __vite_glob_0_177,"../content/blog/posts/p5js-game-logic/index.md": __vite_glob_0_178,"../content/blog/posts/p5js-getting-started/index.md": __vite_glob_0_179,"../content/blog/posts/podcast-search-engine/index.md": __vite_glob_0_180,"../content/blog/posts/practice-spelling-bees-hero/index.md": __vite_glob_0_181,"../content/blog/posts/propelled-by-product-customer-and-industry-momentum-deepgram-continues-to-build-the-future-of-speech-recognition/index.md": __vite_glob_0_182,"../content/blog/posts/protecting-api-key/index.md": __vite_glob_0_183,"../content/blog/posts/pycon-deepgram-usecases/index.md": __vite_glob_0_184,"../content/blog/posts/pycon-python-speech-to-text/index.md": __vite_glob_0_185,"../content/blog/posts/python-deepgram-roundup/index.md": __vite_glob_0_186,"../content/blog/posts/python-deepgram-twilio/index.md": __vite_glob_0_187,"../content/blog/posts/python-graphing-transcripts/index.md": __vite_glob_0_188,"../content/blog/posts/python-script-compliance/index.md": __vite_glob_0_189,"../content/blog/posts/python-speech-recognition-locally-torchaudio/index.md": __vite_glob_0_190,"../content/blog/posts/python-talk-time-analytics/index.md": __vite_glob_0_191,"../content/blog/posts/python-virtual-environments/index.md": __vite_glob_0_192,"../content/blog/posts/pytorch-intro-with-torchaudio/index.md": __vite_glob_0_193,"../content/blog/posts/real-time-routing-of-conversational-data-is-table-stakes-for-enterprises/index.md": __vite_glob_0_194,"../content/blog/posts/retail-restaurants-and-travel-shilp-agarwal-ceo-blutag-project-voice-x/index.md": __vite_glob_0_195,"../content/blog/posts/saving-transcripts-from-terminal/index.md": __vite_glob_0_196,"../content/blog/posts/say-what-you-mean-navigating-critical-conversations-scott-sandland-ceo-cyrano-ai-project-voice-x/index.md": __vite_glob_0_197,"../content/blog/posts/scrape-a-website-with-your-voice-using-python/index.md": __vite_glob_0_198,"../content/blog/posts/search-through-sound-finding-phrases-in-audio/index.md": __vite_glob_0_199,"../content/blog/posts/sending-audio-files-to-expressjs-server/index.md": __vite_glob_0_200,"../content/blog/posts/sentiment-analysis-emotion-regulation-difference/index.md": __vite_glob_0_201,"../content/blog/posts/should-ai-be-regulated-ai-show-2/index.md": __vite_glob_0_202,"../content/blog/posts/song-search-js/index.md": __vite_glob_0_203,"../content/blog/posts/sonic-branding-in-the-enterprise-audrey-arbeeny-ceo-audiobrain-project-voice-x/index.md": __vite_glob_0_204,"../content/blog/posts/sparking-the-future-of-conversation-design-braden-ream-ceo-voiceflow-project-voice-x/index.md": __vite_glob_0_205,"../content/blog/posts/speech-to-text-content-moderation-companies/index.md": __vite_glob_0_206,"../content/blog/posts/speech-to-text-model-ukrainian/index.md": __vite_glob_0_207,"../content/blog/posts/state-of-speech-our-new-data-report-reveals-asrs-untapped-potential/index.md": __vite_glob_0_208,"../content/blog/posts/state-of-voice-report-2022/index.md": __vite_glob_0_209,"../content/blog/posts/supabase-authentication-vue/index.md": __vite_glob_0_210,"../content/blog/posts/supabase-podcast-player-vue/index.md": __vite_glob_0_211,"../content/blog/posts/technical-writing-a-beginners-guide/index.md": __vite_glob_0_212,"../content/blog/posts/technical-writing-a-developers-guide-to-storytelling/index.md": __vite_glob_0_213,"../content/blog/posts/technical-writing-accessible-writing-for-developers/index.md": __vite_glob_0_214,"../content/blog/posts/technical-writing-ethics-for-developers/index.md": __vite_glob_0_215,"../content/blog/posts/text-cleaning-asr-turkish/index.md": __vite_glob_0_216,"../content/blog/posts/the-contact-center-of-the-future-with-real-time-ai/index.md": __vite_glob_0_217,"../content/blog/posts/the-evolution-of-conversational-ai-in-the-car-and-beyond-shyamala-prayaga-sr-software-product-manager-ford-project-voice-x/index.md": __vite_glob_0_218,"../content/blog/posts/the-history-of-automatic-speech-recognition/index.md": __vite_glob_0_219,"../content/blog/posts/the-history-of-the-word-hacker-2/index.md": __vite_glob_0_220,"../content/blog/posts/the-importance-of-testing-with-voice-experiences-and-conversational-ai-john-kelvie-ceo-bespoken-project-voice-x/index.md": __vite_glob_0_221,"../content/blog/posts/the-language-of-lgbtq-inclusion-and-allyship/index.md": __vite_glob_0_222,"../content/blog/posts/the-new-age-of-voice-commerce-mike-zagorsek-coo-soundhound-project-voice-x/index.md": __vite_glob_0_223,"../content/blog/posts/the-trouble-with-wer/index.md": __vite_glob_0_224,"../content/blog/posts/the-weak-link-in-your-multichannel-strategy/index.md": __vite_glob_0_225,"../content/blog/posts/tips-on-choosing-a-call-analytics-development-path/index.md": __vite_glob_0_226,"../content/blog/posts/tips-on-choosing-a-conversational-ai-development-path/index.md": __vite_glob_0_227,"../content/blog/posts/tips-on-choosing-a-sales-and-support-enablement-development-path/index.md": __vite_glob_0_228,"../content/blog/posts/tonya-sims-joins-deepgram/index.md": __vite_glob_0_229,"../content/blog/posts/top-3-use-cases-speech-to-text-gaming/index.md": __vite_glob_0_230,"../content/blog/posts/top-6-dutch-asr-challenges/index.md": __vite_glob_0_231,"../content/blog/posts/top-7-uses-speech-to-text-education/index.md": __vite_glob_0_232,"../content/blog/posts/top-six-uses-cases-for-asr-social-media/index.md": __vite_glob_0_233,"../content/blog/posts/topic-detection-with-python/index.md": __vite_glob_0_234,"../content/blog/posts/track-brand-mentions-across-podcast-episodes/index.md": __vite_glob_0_235,"../content/blog/posts/train-a-deep-learning-speech-recognition-model-to-understand-your-voice/index.md": __vite_glob_0_236,"../content/blog/posts/transcribe-google-drive-files-pipedream/index.md": __vite_glob_0_237,"../content/blog/posts/transcribe-phone-calls-with-twilio-functions-and-deepgram/index.md": __vite_glob_0_238,"../content/blog/posts/transcribe-videos-nodejs/index.md": __vite_glob_0_239,"../content/blog/posts/transcribe-youtube-videos-from-terminal/index.md": __vite_glob_0_240,"../content/blog/posts/transcribe-youtube-videos-nodejs/index.md": __vite_glob_0_241,"../content/blog/posts/transcribing-browser-tab-audio-chrome-extensions/index.md": __vite_glob_0_242,"../content/blog/posts/transcription-netlify-functions/index.md": __vite_glob_0_243,"../content/blog/posts/transfer-learning-spanish-portuguese/index.md": __vite_glob_0_244,"../content/blog/posts/translation-itranslate/index.md": __vite_glob_0_245,"../content/blog/posts/tune-in-deepgram-summit-11-18-21/index.md": __vite_glob_0_246,"../content/blog/posts/twilio-crm-log-js/index.md": __vite_glob_0_247,"../content/blog/posts/understanding-webhooks/index.md": __vite_glob_0_248,"../content/blog/posts/upcoming-january-releases/index.md": __vite_glob_0_249,"../content/blog/posts/use-openai-whisper-speech-recognition-with-the-deepgram-api/index.md": __vite_glob_0_250,"../content/blog/posts/voice-control-browser-stemm/index.md": __vite_glob_0_251,"../content/blog/posts/voice-controlled-music-with-python/index.md": __vite_glob_0_252,"../content/blog/posts/voice-in-healthcare-dr-yared-alemu-ceo-tqintelligence-project-voice-x/index.md": __vite_glob_0_253,"../content/blog/posts/voice-in-healthcare-henry-oconnell-ceo-canary-speech-project-voice-x/index.md": __vite_glob_0_254,"../content/blog/posts/voice-technology-customer-experience/index.md": __vite_glob_0_255,"../content/blog/posts/voicebots-will-enhance-your-life-not-destroy-it/index.md": __vite_glob_0_256,"../content/blog/posts/we-raised-25-million/index.md": __vite_glob_0_257,"../content/blog/posts/what-are-the-top-mistakes-in-deep-learning-ai-show/index.md": __vite_glob_0_258,"../content/blog/posts/what-does-as-the-crow-flies-mean/index.md": __vite_glob_0_259,"../content/blog/posts/what-does-it-mean-to-be-under-the-weather/index.md": __vite_glob_0_260,"../content/blog/posts/what-does-thats-the-way-the-cookie-crumbles-mean/index.md": __vite_glob_0_261,"../content/blog/posts/what-does-the-ai-dystopia-look-like-ai-show-2/index.md": __vite_glob_0_262,"../content/blog/posts/what-is-asr/index.md": __vite_glob_0_263,"../content/blog/posts/what-is-automatic-speech-recognition-past-present-and-future-ebook/index.md": __vite_glob_0_264,"../content/blog/posts/what-is-automl-how-the-technology-paves-the-way-for-the-future-of-asr/index.md": __vite_glob_0_265,"../content/blog/posts/what-is-code-switching-and-how-did-it-make-english/index.md": __vite_glob_0_266,"../content/blog/posts/what-is-devrel-a-deepgram-approach/index.md": __vite_glob_0_267,"../content/blog/posts/what-is-speaker-diarization/index.md": __vite_glob_0_268,"../content/blog/posts/what-is-the-most-important-channel-to-engage-your-customers-on/index.md": __vite_glob_0_269,"../content/blog/posts/what-is-word-error-rate/index.md": __vite_glob_0_270,"../content/blog/posts/what-makes-a-great-conversational-ai-experience/index.md": __vite_glob_0_271,"../content/blog/posts/what-makes-your-voice-uniquely-yours/index.md": __vite_glob_0_272,"../content/blog/posts/whats-the-best-infrastructure-for-machine-learning-ai-show/index.md": __vite_glob_0_273,"../content/blog/posts/why-does-your-speech-recognition-need-context/index.md": __vite_glob_0_274,"../content/blog/posts/why-enterprise-audio-requirements-are-more-nuanced-at-real-time-speeds/index.md": __vite_glob_0_275,"../content/blog/posts/why-iot-means-speech-recognition/index.md": __vite_glob_0_276

});
const allPosts = Object.values(postImportResult).sort((a, b) => {
  const aDate = new Date(b.frontmatter.date);
  const bDate = new Date(a.frontmatter.date);
  return aDate.getTime() - bDate.getTime();
}).slice(0, 5);
async function getPosts() {
  const unresolved = allPosts.map(async post => {
    const slug = post.frontmatter.slug || post.file.split("/")[post.file.split("/").length - 2];
    return {
      link: slug,
      title: post.frontmatter.title,
      description: post.frontmatter.description,
      pubDate: post.frontmatter.date,
      customData: `<content:encoded><![CDATA[${await post.compiledContent()}]]></content:encoded>`
    };
  });
  return await Promise.all(unresolved);
}
const get = async () => rss({
  title: "Deepgram Blog",
  site: ({}).URL,
  description: "Deepgram Automatic Speech Recognition helps you build voice applications with better, faster, more economical transcription at scale.",
  items: await getPosts(),
  xmlns: {
    content: "http://purl.org/rss/1.0/modules/content/"
  }
});

const _page5 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  get
}, Symbol.toStringTag, { value: 'Module' }));

const $$Astro$i = createAstro("/Users/sandrarodgers/web-next/blog/src/pages/posts/[...page].astro", "", "file:///Users/sandrarodgers/web-next/blog/");
async function getStaticPaths$2({ paginate }) {
  const sbApi = F();
  const allPosts = await sbApi.getAll("cdn/stories", {
    by_slugs: "blog-posts/*"
  });
  const sortedPosts = allPosts.sort((a, b) => {
    const aDate = new Date(b.content.date);
    const bDate = new Date(a.content.date);
    return aDate.getTime() - bDate.getTime();
  });
  return paginate(sortedPosts, {
    pageSize: 12
  });
}
const $$$1 = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$i, $$props, $$slots);
  Astro2.self = $$$1;
  const { page } = Astro2.props;
  const schema = {
    "@context": "https://schema.org",
    "@type": "BreadcrumbList",
    itemListElement: [
      {
        "@type": "ListItem",
        position: 1,
        name: "Deepgram Home",
        item: "https://deepgram.com"
      },
      {
        "@type": "ListItem",
        position: 2,
        name: "Blog",
        item: Astro2.url.origin
      },
      {
        "@type": "ListItem",
        position: 3,
        name: "All Posts"
      }
    ]
  };
  return renderTemplate`${renderComponent($$result, "Layout", $$Default, {}, { "default": () => renderTemplate`${renderComponent($$result, "PrimarySection", $$PrimarySection, { "class": "pt-[6.75rem] md:pt-[7.03125rem] lg:pt-[7.3125rem] xl:pt-[7.875rem]" }, { "default": () => renderTemplate`${renderComponent($$result, "GeneralPostsPage", $$GeneralPostsPageSB, { "slug": "posts", "posts": page.data, "page": page, "link": "/", "linkText": "Back to blog home", "subtitle": "All posts" })}` })}`, "head:description": () => renderTemplate`${renderComponent($$result, "Description", $$Description, { "slot": "head:description", "name": "description", "content": "All the blog posts on the Deepgram blog." })}`, "head:title": () => renderTemplate`${renderComponent($$result, "Title", $$Title, { "slot": "head:title", "title": "All blog posts - Deepgram Blog \u26A1\uFE0F" })}`, "json:ld": () => renderTemplate`${renderComponent($$result, "JsonLD", $$JsonLD, { "slot": "json:ld", "schema": schema })}`, "og:description": () => renderTemplate`${renderComponent($$result, "Meta", $$Meta, { "slot": "og:description", "property": "og:description", "content": "All the blog posts on the Deepgram blog." })}`, "og:title": () => renderTemplate`${renderComponent($$result, "Meta", $$Meta, { "slot": "og:title", "property": "og:title", "content": "All blog posts - Deepgram Blog \u26A1\uFE0F" })}` })}`;
}, "/Users/sandrarodgers/web-next/blog/src/pages/posts/[...page].astro");

const $$file$3 = "/Users/sandrarodgers/web-next/blog/src/pages/posts/[...page].astro";
const $$url$3 = "/posts/[...page]";

const _page6 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  getStaticPaths: getStaticPaths$2,
  default: $$$1,
  file: $$file$3,
  url: $$url$3
}, Symbol.toStringTag, { value: 'Module' }));

const $$Astro$h = createAstro("/Users/sandrarodgers/web-next/blog/src/components/tags/Hero.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Hero = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$h, $$props, $$slots);
  Astro2.self = $$Hero;
  const { title, svg } = Astro2.props;
  return renderTemplate`${maybeRenderHead($$result)}<div class="grid grid-cols-1 xl:grid-cols-3">
	<h1 class="mb-20 text-white">${title}</h1>
	<div class="hidden md:inline xl:order-last">
		${renderComponent($$result, "RightDecoration", $$RightDecoration, { "icon": "star-left", "class": "text-white w-6 lg:w-8 xl:-mt-10" })}
		${renderComponent($$result, "LeftDecoration", $$LeftDecoration, { "icon": "star-right", "class": "text-white w-5 lg:w-6 mt-4 xl:mt-20" })}
	</div>
	<div>${renderComponent($$result, "Svg", $$Svg, { "class": "opacity-0 -mb-10 h-32 lg:h-auto", "name": "tag-gradient" })}</div>
</div>`;
}, "/Users/sandrarodgers/web-next/blog/src/components/tags/Hero.astro");

const $$Astro$g = createAstro("/Users/sandrarodgers/web-next/blog/src/pages/tags/index.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Index = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$g, $$props, $$slots);
  Astro2.self = $$Index;
  const sbApi = F();
  const allBlogTags = await sbApi.getAll("cdn/tags");
  const schema = {
    "@context": "https://schema.org",
    "@type": "BreadcrumbList",
    itemListElement: [
      {
        "@type": "ListItem",
        position: 1,
        name: "Deepgram Home",
        item: "https://deepgram.com"
      },
      {
        "@type": "ListItem",
        position: 2,
        name: "Blog",
        item: Astro2.url.origin
      },
      {
        "@type": "ListItem",
        position: 3,
        name: "Tags"
      }
    ]
  };
  return renderTemplate`${renderComponent($$result, "Layout", $$Default, {}, { "default": () => renderTemplate`${renderComponent($$result, "ContrastSection", $$ContrastSection, { "contrast": "black", "class": "bg-tag-gradient bg-responsive-svg", "background": "darkCharcoal", "bottomDivider": "eclipse-divider" }, { "default": () => renderTemplate`${renderComponent($$result, "PrimarySection", $$PrimarySection, { "class": "pt-[6.75rem] md:pt-[7.03125rem] lg:pt-[7.3125rem] xl:pt-[7.875rem]" }, { "default": () => renderTemplate`${renderComponent($$result, "Backlink", $$Backlink, { "href": "/posts", "class": "mb-6 md:mb-8 lg:mb-10 xl:mb-12 text-lightIris" }, { "default": () => renderTemplate`All posts` })}${renderComponent($$result, "Hero", $$Hero, { "title": "Tags", "svg": "tag" })}` })}` })}${renderComponent($$result, "PrimarySection", $$PrimarySection, {}, { "default": () => renderTemplate`${renderComponent($$result, "LinkCardListTitle", $$LinkCardListTitle, { "class": "mt-8", "href": "/posts", "title": "Posts", "linkClass": "draw-underline nudge-icon nudge-icon--right ml-auto" })}${maybeRenderHead($$result)}<ul class="grid grid-cols-1 sm:grid-cols-2 md:grid-cols-3 gap-y-8 lg:grid-cols-4 grid-flow-row-dense">
			${allBlogTags.map((tag) => renderTemplate`<li class="">
					<a class="text-cloud inline-flex items-center text-lg"${addAttribute(`/tags/${tag.name}`, "href")}>
						${renderComponent($$result, "Icon", $$Icon, { "class": "h-4 w-4 fill-lightIris", "icon": "tag" })}
						<span class="ml-2">#${tag.name}</span>
					</a>
				</li>`)}
		</ul>` })}`, "head:description": () => renderTemplate`${renderComponent($$result, "Description", $$Description, { "slot": "head:description", "name": "description", "content": "All the tags on the Deepgram blog." })}`, "head:title": () => renderTemplate`${renderComponent($$result, "Title", $$Title, { "slot": "head:title", "title": "All tags - Deepgram Blog \u26A1\uFE0F" })}`, "json:ld": () => renderTemplate`${renderComponent($$result, "JsonLD", $$JsonLD, { "slot": "json:ld", "schema": schema })}`, "og:description": () => renderTemplate`${renderComponent($$result, "Meta", $$Meta, { "slot": "og:description", "property": "og:description", "content": "All the tags on the Deepgram blog." })}`, "og:title": () => renderTemplate`${renderComponent($$result, "Meta", $$Meta, { "slot": "og:title", "property": "og:title", "content": "All tags - Deepgram Blog \u26A1\uFE0F" })}` })}`;
}, "/Users/sandrarodgers/web-next/blog/src/pages/tags/index.astro");

const $$file$2 = "/Users/sandrarodgers/web-next/blog/src/pages/tags/index.astro";
const $$url$2 = "/tags";

const _page7 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  default: $$Index,
  file: $$file$2,
  url: $$url$2
}, Symbol.toStringTag, { value: 'Module' }));

const $$Astro$f = createAstro("/Users/sandrarodgers/web-next/blog/src/pages/tags/[tag]/[...page].astro", "", "file:///Users/sandrarodgers/web-next/blog/");
async function getStaticPaths$1({ paginate }) {
  const sbApi = F();
  const allBlogTags = await sbApi.getAll("cdn/tags");
  const allPosts = await sbApi.getAll("cdn/stories", {
    by_slugs: "blog-posts/*"
  });
  const sortedPosts = allPosts.sort((a, b) => {
    const aDate = new Date(b.content.date);
    const bDate = new Date(a.content.date);
    return aDate.getTime() - bDate.getTime();
  });
  return allBlogTags.map((tag) => {
    const filteredPosts = sortedPosts.filter((post) => {
      if (post.tag_list) {
        return post.tag_list.includes(tag.name);
      }
      return false;
    });
    return paginate(filteredPosts, {
      params: { tag: tag.name },
      pageSize: 12,
      props: { tag: tag.name }
    });
  });
}
const $$ = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$f, $$props, $$slots);
  Astro2.self = $$;
  const { page, tag } = Astro2.props;
  const schema = {
    "@context": "https://schema.org",
    "@type": "BreadcrumbList",
    itemListElement: [
      {
        "@type": "ListItem",
        position: 1,
        name: "Deepgram Home",
        item: "https://deepgram.com"
      },
      {
        "@type": "ListItem",
        position: 2,
        name: "Blog",
        item: Astro2.url.origin
      },
      {
        "@type": "ListItem",
        position: 3,
        name: "Tags",
        item: `${Astro2.url.origin}/tags`
      },
      {
        "@type": "ListItem",
        position: 4,
        name: tag
      }
    ]
  };
  return renderTemplate`${renderComponent($$result, "Layout", $$Default, {}, { "default": () => renderTemplate`${renderComponent($$result, "PrimarySection", $$PrimarySection, { "class": "pt-[6.75rem] md:pt-[7.03125rem] lg:pt-[7.3125rem] xl:pt-[7.875rem]" }, { "default": () => renderTemplate`${renderComponent($$result, "GeneralPostsPage", $$GeneralPostsPageSB, { "slug": `tags/${tag}`, "page": page, "posts": page.data, "link": "/tags", "linkText": "All tags", "subtitle": `All posts tagged with #${tag}` })}` })}`, "json:ld": () => renderTemplate`${renderComponent($$result, "JsonLD", $$JsonLD, { "slot": "json:ld", "schema": schema })}` })}`;
}, "/Users/sandrarodgers/web-next/blog/src/pages/tags/[tag]/[...page].astro");

const $$file$1 = "/Users/sandrarodgers/web-next/blog/src/pages/tags/[tag]/[...page].astro";
const $$url$1 = "/tags/[tag]/[...page]";

const _page8 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  getStaticPaths: getStaticPaths$1,
  default: $$,
  file: $$file$1,
  url: $$url$1
}, Symbol.toStringTag, { value: 'Module' }));

const $$Astro$e = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/layout/HasGutters.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$HasGutters = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$e, $$props, $$slots);
  Astro2.self = $$HasGutters;
  return renderTemplate`${maybeRenderHead($$result)}<div class="grid grid-cols-1 xl:gap-6 xl:grid-cols-6">
	<div>${renderSlot($$result, $$slots["left"])}</div>
	<div class="xl:order-last">${renderSlot($$result, $$slots["right"])}</div>
	<div class="xl:col-span-4">${renderSlot($$result, $$slots["default"])}</div>
</div>`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/layout/HasGutters.astro");

const $$Astro$d = createAstro("/Users/sandrarodgers/web-next/blog/src/components/posts/PostImage.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$PostImage = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$d, $$props, $$slots);
  Astro2.self = $$PostImage;
  const { post } = Astro2.props;
  return renderTemplate`${renderComponent($$result, "ImageSrcSet", $$ImageSrcSet, { "class": "shadow-dg w-full rounded-[1.34px]", "filename": post.content.cover_image.filename, "alt": post.content.title })}`;
}, "/Users/sandrarodgers/web-next/blog/src/components/posts/PostImage.astro");

const $$Astro$c = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/images/AuthorPicture.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$AuthorPicture = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$c, $$props, $$slots);
  Astro2.self = $$AuthorPicture;
  const { author, class: classes } = Astro2.props;
  return renderTemplate`${renderComponent($$result, "ImageSrcSet", $$ImageSrcSet, { "filename": author.content.picture.filename, "class": classes, "alt": `Author photo for ${author.content.title}`, "title": author.content.title })}`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/images/AuthorPicture.astro");

const $$Astro$b = createAstro("/Users/sandrarodgers/web-next/blog/src/components/posts/PostAuthorImages.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$PostAuthorImages = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$b, $$props, $$slots);
  Astro2.self = $$PostAuthorImages;
  const { authors } = Astro2.props;
  return renderTemplate`${authors && renderTemplate`${maybeRenderHead($$result)}<ul class="mr-4">
		${authors.map((author, index, array) => renderTemplate`<li class="-mt-5 first:mt-0 first:ml-0 sm:mt-0 sm:-ml-4 sm:inline">
				${renderComponent($$result, "Link", $$Link$1, { "href": `/authors/${author.slug}`, "class": "relative inline-block" }, { "default": () => renderTemplate`${renderComponent($$result, "AuthorPicture", $$AuthorPicture, { "author": author, "class": "shadow-dg h-14 w-14 rounded-full object-cover" })}` })}
			</li>`)}
	</ul>`}`;
}, "/Users/sandrarodgers/web-next/blog/src/components/posts/PostAuthorImages.astro");

const $$Astro$a = createAstro("/Users/sandrarodgers/web-next/blog/src/components/posts/PostAuthorNames.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$PostAuthorNames = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$a, $$props, $$slots);
  Astro2.self = $$PostAuthorNames;
  const { authors } = Astro2.props;
  return renderTemplate`${authors && renderTemplate`${maybeRenderHead($$result)}<span class="author-names astro-EZMMPKP6">
		${authors.map((author, index, array) => renderTemplate`${renderComponent($$result, "Link", $$Link$1, { "href": `/authors/${author.slug}`, "class": "text-lightIris inline-block astro-EZMMPKP6" }, { "default": () => renderTemplate`${renderComponent($$result, "AuthorName", $$AuthorName, { "author": author, "class": "astro-EZMMPKP6" })}` })}`)}
	</span>`}

`;
}, "/Users/sandrarodgers/web-next/blog/src/components/posts/PostAuthorNames.astro");

const $$Astro$9 = createAstro("/Users/sandrarodgers/web-next/blog/src/components/posts/PostDate.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$PostDate = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$9, $$props, $$slots);
  Astro2.self = $$PostDate;
  const { post } = Astro2.props;
  const date = new Date(post.content.date);
  const formattedDate = date.toLocaleDateString("en-US", {
    year: "numeric",
    month: "long",
    day: "numeric"
  });
  return renderTemplate`${formattedDate}
`;
}, "/Users/sandrarodgers/web-next/blog/src/components/posts/PostDate.astro");

const $$Astro$8 = createAstro("/Users/sandrarodgers/web-next/blog/src/components/posts/PostMetaDivider.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$PostMetaDivider = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$8, $$props, $$slots);
  Astro2.self = $$PostMetaDivider;
  return renderTemplate`${maybeRenderHead($$result)}<span class="hidden md:inline border-black border-l py-1 ml-1 mr-2"></span>`;
}, "/Users/sandrarodgers/web-next/blog/src/components/posts/PostMetaDivider.astro");

const $$Astro$7 = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/strings/CategoryName.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$CategoryName = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$7, $$props, $$slots);
  Astro2.self = $$CategoryName;
  const { category } = Astro2.props;
  return renderTemplate`${category}
`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/strings/CategoryName.astro");

const $$Astro$6 = createAstro("/Users/sandrarodgers/web-next/blog/src/components/posts/PostCategory.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$PostCategory = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$6, $$props, $$slots);
  Astro2.self = $$PostCategory;
  const { category } = Astro2.props;
  return renderTemplate`${maybeRenderHead($$result)}<span>in ${renderComponent($$result, "Link", $$Link$1, { "class": "text-lightIris", "href": `/categories/${category.slug}` }, { "default": () => renderTemplate`${renderComponent($$result, "CategoryName", $$CategoryName, { "category": category.content.title })}` })}</span>`;
}, "/Users/sandrarodgers/web-next/blog/src/components/posts/PostCategory.astro");

const $$Astro$5 = createAstro("/Users/sandrarodgers/web-next/blog/src/components/posts/PostMeta.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$PostMeta = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$5, $$props, $$slots);
  Astro2.self = $$PostMeta;
  const { post, authors, category } = Astro2.props;
  const date = new Date(post.first_published_at);
  date.toLocaleDateString("en-US", {
    year: "numeric",
    month: "long",
    day: "numeric"
  });
  return renderTemplate`${maybeRenderHead($$result)}<div class="flex flex-row items-center my-4 text-cloud">
	${renderComponent($$result, "PostAuthorImages", $$PostAuthorImages, { "authors": authors })}
	<p class="flex flex-col md:flex-none md:inline">
		${renderComponent($$result, "PostAuthorNames", $$PostAuthorNames, { "authors": authors })}${renderComponent($$result, "PostMetaDivider", $$PostMetaDivider, {})}${renderComponent($$result, "PostDate", $$PostDate, { "post": post })}${renderComponent($$result, "PostCategory", $$PostCategory, { "category": category })}
	</p>
</div>`;
}, "/Users/sandrarodgers/web-next/blog/src/components/posts/PostMeta.astro");

const $$Astro$4 = createAstro("/Users/sandrarodgers/web-next/blog/src/components/posts/PostHeading.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$PostHeading = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$4, $$props, $$slots);
  Astro2.self = $$PostHeading;
  const { post } = Astro2.props;
  return renderTemplate`${maybeRenderHead($$result)}<h1 class="small">
	${post.content.title}
</h1>`;
}, "/Users/sandrarodgers/web-next/blog/src/components/posts/PostHeading.astro");

const $$Astro$3 = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/forms/Feedback.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Feedback = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$3, $$props, $$slots);
  Astro2.self = $$Feedback;
  const formUuid = await getUuid();
  const url = Astro2.url;
  return renderTemplate`${maybeRenderHead($$result)}<div x-data="{
    error: null,
    feedback: null,
    email: null,
    thumb: null,
    thanks: false,
    ratePage(e) {
      const form = new FormData(e.target)
      this.thumb = form.get('thumb')
      const body = {
        thumb: form.get('thumb'),
        page: window.location.pathname,
        hookId: form.get('hook'),
        honeypot: form.get('bot-field')
      }
      fetch('/.netlify/functions/forms', {
        method: 'POST',
        body: JSON.stringify(body),
      })
      .then(() => {
        this.feedback = null
      })
      .catch(() => {
        this.error = 'A problem occured submitting your rating.'
      })
    },
    sendFeedback(e) {
      const form = new FormData(e.target)
      this.email = form.get('email')
      this.feedback = form.get('feedback')
      const body = {
        email: form.get('email'),
        feedback: form.get('feedback'),
        page: window.location.pathname,
        hookId: form.get('hook'),
        honeypot: form.get('bot-field')
      }
      fetch('/.netlify/functions/forms', {
        method: 'POST',
        body: JSON.stringify(body),
      })
      .then(() => {
        this.email = null
        this.feedback = null
        this.thumb = null
        this.thanks = true
      })
      .catch(() => {
        this.error = 'A problem occured submitting your feedback.'
      })
    }
  }" class="mt-8 astro-OEM3CM4M">
	<h4 x-show="!thanks" class="astro-OEM3CM4M">Share your feedback</h4>
	<div x-show="error" x-text="error" class="astro-OEM3CM4M"></div>
	<div class="astro-OEM3CM4M">
		<p x-show="!thanks" class="feedback-question text-[#9d9b9b] astro-OEM3CM4M">Was this article useful or interesting to you?</p>
		<div x-show="!thanks" class="thumbs flex items-center justify-start astro-OEM3CM4M">
			<form${addAttribute(`thumbsup-form-${formUuid}`, "id")} :class="thumb === 'up' ? 'active' : ''" class="inline thumb up astro-OEM3CM4M" name="rating" @submit.prevent="ratePage">
				<input${addAttribute(`thumbsup-${formUuid}`, "id")} type="submit" aria-hidden="true" class="hidden astro-OEM3CM4M">
				<label${addAttribute(`thumbsup-${formUuid}`, "for")} class="text-3xl astro-OEM3CM4M">
					${renderComponent($$result, "Svg", $$Svg, { "name": "thumbsup", "class": "my-4 h-8 w-auto fill-[#9d9b9b] astro-OEM3CM4M" })}
				</label>

				<label${addAttribute(`thumbsup-honeypot-${formUuid}`, "for")} class="hidden astro-OEM3CM4M"><input${addAttribute(`thumbsup-honeypot-${formUuid}`, "id")} type="text" aria-label="Don't fill this in if you're a human." name="bot-field" class="astro-OEM3CM4M"> Don't fill this in if you're a human.
				</label>
				<input type="hidden" name="thumb" value="up" class="astro-OEM3CM4M">
				<input type="hidden" name="hook" value="bgv9wf1" class="astro-OEM3CM4M">
				<input type="hidden" name="page"${addAttribute(url, "value")} class="astro-OEM3CM4M">
			</form>
			<form${addAttribute(`thumbsdown-form-${formUuid}`, "id")} :class="thumb === 'down' ? 'active' : ''" class="thumb inline down astro-OEM3CM4M" name="rating" @submit.prevent="ratePage">
				<input${addAttribute(`thumbsdown-${formUuid}`, "id")} type="submit" aria-hidden="true" class="hidden astro-OEM3CM4M">
				<label${addAttribute(`thumbsdown-${formUuid}`, "for")} class="text-3xl astro-OEM3CM4M">
					${renderComponent($$result, "Svg", $$Svg, { "name": "thumbsdown", "class": "mt-4 h-8 w-auto fill-[#9d9b9b] astro-OEM3CM4M" })}
				</label>

				<label${addAttribute(`thumbsdown-honeypot-${formUuid}`, "for")} class="hidden astro-OEM3CM4M"><input${addAttribute(`thumbsdown-honeypot-${formUuid}`, "id")} type="text" aria-label="Don't fill this in if you're a human." name="bot-field" class="astro-OEM3CM4M"> Don't fill this in if you're a human.
				</label>
				<input type="hidden" name="thumb" value="down" class="astro-OEM3CM4M">
				<input type="hidden" name="hook" value="bgv9wf1" class="astro-OEM3CM4M">
			</form>
		</div>
		<form${addAttribute(`feedback-form-${formUuid}`, "id")} x-show="thumb !== null" name="feedback" method="POST" @submit.prevent="sendFeedback" class="astro-OEM3CM4M">
			<div class="mb-4 astro-OEM3CM4M">
				<p x-show="thumb ==='up'" class="astro-OEM3CM4M">Thank you! Can you tell us what you liked about it? (Optional)</p>
				<p x-show="thumb !=='up'" class="astro-OEM3CM4M">Thank you. What could we have done better? (Optional)</p>
				<p class="text-sm astro-OEM3CM4M">
					We may also want to contact you with updates or questions related to your feedback and our product. If don't mind, you can optionally leave your email address along with
					your comments.
				</p>
			</div>
			<div class="input-group input-group--light-bg astro-OEM3CM4M">
				<label${addAttribute(`feedback-message-${formUuid}`, "for")} class="astro-OEM3CM4M">Your message<span class="text-fireEngine astro-OEM3CM4M">*</span></label>
				<textarea${addAttribute(`feedback-message-${formUuid}`, "id")} ref="feedback" name="feedback" class="invalid:outline-fireEngine astro-OEM3CM4M" required placeholder="Share your thoughts" x-model="feedback"></textarea>
			</div>
			<div class="input-group input-group--light-bg astro-OEM3CM4M">
				<label${addAttribute(`feedback-email-${formUuid}`, "for")} class="astro-OEM3CM4M">Your email</label>
				<input${addAttribute(`feedback-email-${formUuid}`, "id")} type="email" name="email" placeholder="you@email.com" x-model="email" class="astro-OEM3CM4M">
			</div>
			<div class="flex mt-4 justify-end astro-OEM3CM4M">
				<button type="submit" aria-label="Send Feedback" title="Send Feedback" class="button button--primary astro-OEM3CM4M">Send Feedback</button>
			</div>

			<label${addAttribute(`feedback-honeypot-${formUuid}`, "for")} class="hidden astro-OEM3CM4M"><input${addAttribute(`feedback-honeypot-${formUuid}`, "id")} type="text" aria-label="Don't fill this in if you're a human." name="bot-field" class="astro-OEM3CM4M"> Don't fill this in if you're a human.
			</label>
			<input type="hidden" name="thumb" value="thumb" class="astro-OEM3CM4M">
			<input type="hidden" name="hook" value="bgvnn9g" class="astro-OEM3CM4M">
		</form>
		<div x-show="thanks" class="px-1 astro-OEM3CM4M">
			<p class="text-watercourse text-2xl font-bold pt-4 pb-2 astro-OEM3CM4M">Thank you!</p>
			<p class="astro-OEM3CM4M">We appreciate your response.</p>
		</div>
	</div>
</div>

`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/forms/Feedback.astro");

const $$Astro$2 = createAstro("/Users/sandrarodgers/web-next/blog/src/shared/components/lists/TagsList.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$TagsList = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$2, $$props, $$slots);
  Astro2.self = $$TagsList;
  F();
  const { tags, link } = Astro2.props;
  const tagsArray = tags ? tags.slice(0, 3) : [];
  return renderTemplate`${maybeRenderHead($$result)}<ul class="pt-3 flex text-sm space-x-2">
	${tags && tagsArray.map((tag) => renderTemplate`<li class="bg-steel px-1 text-mist">
				${link && renderTemplate`${renderComponent($$result, "Link", $$Link$1, { "href": `/tags/${tag}`, "class": "text-cloud" }, { "default": () => renderTemplate`
						#${tag}` })}`}
				${!link && renderTemplate`${renderComponent($$result, "Fragment", Fragment, {}, { "default": () => renderTemplate`#${tag}` })}`}
			</li>`)}
</ul>`;
}, "/Users/sandrarodgers/web-next/blog/src/shared/components/lists/TagsList.astro");

const $$Astro$1 = createAstro("/Users/sandrarodgers/web-next/blog/src/components/posts/RelatedResources.astro", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$RelatedResources = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro$1, $$props, $$slots);
  Astro2.self = $$RelatedResources;
  const sbApi = F();
  const { post } = Astro2.props;
  const { data: relatedPosts } = await sbApi.get("cdn/stories", {
    by_slugs: "blog-posts/*",
    per_page: 4,
    sort_by: "content.date:desc",
    filter_query: {
      category: {
        in: post.content.category
      }
    }
  });
  return renderTemplate`${maybeRenderHead($$result)}<div class="grid grid-cols-1 md:grid-cols-2 xl:grid-cols-4 grid-flow-row-dense justify-items-center gap-4 lg:gap-6 text-left mb-10">
	${relatedPosts.stories.map((p) => renderTemplate`${renderComponent($$result, "BlogCard", $$BlogCardSB, { "post": p })}`)}
</div>`;
}, "/Users/sandrarodgers/web-next/blog/src/components/posts/RelatedResources.astro");

const $$Astro = createAstro("/Users/sandrarodgers/web-next/blog/src/pages/[slug].astro", "", "file:///Users/sandrarodgers/web-next/blog/");
async function getStaticPaths() {
  const sbApi = F();
  const allPosts = await sbApi.getAll("cdn/stories", {
    version: "draft",
    by_slugs: "blog-posts/*"
  });
  const sortedPosts = allPosts.sort((a, b) => {
    const aDate = new Date(b.content.date);
    const bDate = new Date(a.content.date);
    return aDate.getTime() - bDate.getTime();
  });
  return sortedPosts.map((post) => {
    const slug = post.slug;
    return {
      params: {
        slug
      },
      props: {
        post
      }
    };
  });
}
const $$slug = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro, $$props, $$slots);
  Astro2.self = $$slug;
  function snakeCase(heading) {
    return slugify(heading, { lower: true });
  }
  function getHeadingsSB(post2) {
    const richTextSections = post2.content.body.filter((section) => {
      return section.component === "RichTextSection";
    });
    const headings2 = richTextSections.map((section) => {
      return section.content.content.filter((richText) => {
        if (richText.type === "heading") {
          return richText.content[0].text;
        }
      });
    });
    return headings2.flat();
  }
  const sbApi = F();
  const { post } = Astro2.props;
  Astro2.params;
  const headings = await getHeadingsSB(post);
  const postAuthors = post.content.authors;
  const { data: authorsWithContent } = await sbApi.get("cdn/stories", {
    version: "draft",
    by_uuids: postAuthors.toString()
  });
  const { data: categoryData } = await sbApi.get(`cdn/stories/${post.content.category}`, {
    version: "draft",
    find_by: "uuid"
  });
  const postCategory = categoryData.story;
  const seo = {
    title: post.content.seo_title ? post.content.seo_title : post.content.title,
    description: post.content.seo_description ? post.content.seo_description : post.content.description,
    url: post.content.seo && post.content.seo.canonical ? post.content.seo.canonical : Astro2.url
  };
  const og = {
    title: seo.title,
    description: seo.description,
    image: post.content.og && post.content.og.filename !== "" ? post.content.og.filename : post.content.cover_image.filename
  };
  const shorturls = {
    share: post.content.shorturls && post.content.shorturls.share ? post.content.shorturls.share : Astro2.url,
    twitter: post.content.shorturls && post.content.shorturls.twitter ? post.content.shorturls.twitter : Astro2.url,
    linkedin: post.content.shorturls && post.content.shorturls.linkedin ? post.content.shorturls.linkedin : Astro2.url,
    reddit: post.content.shorturls && post.content.shorturls.reddit ? post.content.shorturls.reddit : Astro2.url,
    facebook: post.content.shorturls && post.content.shorturls.facebook ? post.content.shorturls.facebook : Astro2.url,
    fullUrl: Astro2.url
  };
  const schema = [
    {
      "@context": "http://schema.org",
      "@type": "BlogPosting",
      image: og.image,
      url: seo.url,
      headline: seo.title,
      alternativeHeadline: og.title,
      dateCreated: post.created_at,
      datePublished: post.first_published_at,
      dateModified: post.published_at,
      inLanguage: "en-US",
      isFamilyFriendly: "true",
      copyrightYear: new Date().getFullYear(),
      copyrightHolder: "Deepgram",
      contentLocation: {
        "@type": "Place",
        name: "San Francisco, California, 94110, USA"
      },
      author: authorsWithContent.stories.map((a) => ({
        "@type": "Person",
        image: a.picture ? a.picture : null,
        name: a.title,
        jobTitle: a.team ? a.jobtitle ? a.jobtitle : "Deepgram Team" : a.alumni ? "Deepgram Alumni" : "Guest Author",
        url: `${Astro2.url.origin}/authors/${a.slug}`
      })),
      publisher: {
        "@type": "Organization",
        name: "Deepgram",
        url: "https://deepgram.com",
        logo: {
          "@type": "ImageObject",
          url: "https://github.com/deepgram/.github/raw/main/profile/dg-logo.png",
          width: "223",
          height: "28"
        }
      },
      mainEntityOfPage: "True",
      keywords: post.tag_list,
      genre: ["Speech Recognition", "Automatic Speech Recognition", "Speech-to-Text", "API"],
      articleSection: postCategory.content.title
    },
    ...authorsWithContent.stories.map((a) => {
      return {
        "@context": "https://schema.org",
        "@type": "Person",
        image: a.content.picture.filename ? a.content.picture.filename : null,
        name: a.content.title,
        jobTitle: a.content.team === "team" ? "Deepgram Team" : a.content.team === "alum" ? "Deepgram Alumni" : "Guest Author",
        url: `${Astro2.url.origin}/authors/${a.slug}`
      };
    }),
    {
      "@context": "https://schema.org",
      "@type": "BreadcrumbList",
      itemListElement: [
        {
          "@type": "ListItem",
          position: 1,
          name: "Deepgram Home",
          item: "https://deepgram.com"
        },
        {
          "@type": "ListItem",
          position: 2,
          name: "Blog",
          item: Astro2.url.origin
        },
        {
          "@type": "ListItem",
          position: 3,
          name: seo.title
        }
      ]
    }
  ];
  return renderTemplate`${renderComponent($$result, "Layout", $$Default, { "class": "astro-CRCHVEWD" }, { "default": () => renderTemplate`${maybeRenderHead($$result)}<article class="astro-CRCHVEWD">
		${renderComponent($$result, "ContrastSection", $$ContrastSection, { "contrast": "black", "background": "darkCharcoal", "bottomDivider": "eclipse-divider", "bottomOverlay": true, "class": "astro-CRCHVEWD" }, { "default": () => renderTemplate`${renderComponent($$result, "PrimarySection", $$PrimarySection, { "class": "pt-[6.75rem] md:pt-[7.03125rem] lg:pt-[7.3125rem] xl:pt-[7.875rem] astro-CRCHVEWD" }, { "default": () => renderTemplate`${renderComponent($$result, "Backlink", $$Backlink, { "linkClass": "inline-block nudge-icon nudge-icon--left draw-underline underline-right", "href": "/posts", "class": "mb-6 md:mb-8 lg:mb-10 xl:mb-12 astro-CRCHVEWD" }, { "default": () => renderTemplate`<span class="astro-CRCHVEWD">All posts</span>` })}${renderComponent($$result, "HasGutters", $$HasGutters, { "class": "astro-CRCHVEWD" }, { "default": () => renderTemplate`<header class="flex flex-col astro-CRCHVEWD">
						${renderComponent($$result, "PostHeading", $$PostHeading, { "post": post, "class": "astro-CRCHVEWD" })}
						${renderComponent($$result, "PostMeta", $$PostMeta, { "post": post, "authors": authorsWithContent.stories, "category": postCategory, "class": "astro-CRCHVEWD" })}
						${renderComponent($$result, "PostImage", $$PostImage, { "post": post, "class": "astro-CRCHVEWD" })}
					</header>` })}` })}` })}
		${renderComponent($$result, "PrimarySection", $$PrimarySection, { "class": "mt-32 xl:mt-56 astro-CRCHVEWD" }, { "default": () => renderTemplate`${renderComponent($$result, "HasGutters", $$HasGutters, { "class": "astro-CRCHVEWD" }, { "default": () => renderTemplate`<div class="post astro-CRCHVEWD">
					<div class="post-prose markdown-body astro-CRCHVEWD">
						${post.content.body.map((section) => renderTemplate`${renderComponent($$result, "StoryblokComponent", $$StoryblokComponent, { "blok": section, "class": "astro-CRCHVEWD" })}`)}
					</div>
					<p class="italic text-cloud astro-CRCHVEWD">If you have any feedback about this post, or anything else around Deepgram, we'd love to hear from you. Please let us know in our <a href="https://github.com/orgs/deepgram/discussions/categories/feedback" class="astro-CRCHVEWD">GitHub discussions</a>.</p>
					<h4 class="astro-CRCHVEWD">More with these tags:</h4>
					${renderComponent($$result, "TagsList", $$TagsList, { "tags": post.tag_list, "link": true, "class": "astro-CRCHVEWD" })}

					${renderComponent($$result, "Feedback", $$Feedback, { "class": "astro-CRCHVEWD" })}
				</div>`, "right": () => renderTemplate`<div class="mb-5 xl:mb-0 mx-0 md:mx-20 lg:mx-32 xl:mx-0 astro-CRCHVEWD">
					<h5 class="pb-5 astro-CRCHVEWD">${renderComponent($$result, "InlineIcon", $$InlineIcon, { "icon": "share", "class": "astro-CRCHVEWD" }, { "default": () => renderTemplate`Share` })}</h5>
					<div class="flex gap-2 text-2xl astro-CRCHVEWD">
						${renderComponent($$result, "Link", $$Link$1, { "href": `mailto:?body=Check%20out%20this%20awesome%20post%20on%20Deepgram!%0A%0A${encodeURIComponent(post.content.title)}%0A%0A${encodeURIComponent(shorturls.share)}`, "target": "_blank", "rel": "noopener noreferrer", "class": "astro-CRCHVEWD" }, { "default": () => renderTemplate`${renderComponent($$result, "Icon", $$Icon, { "icon": "email", "class": "w-[1em] fill-lightIris astro-CRCHVEWD" })}` })}
						${renderComponent($$result, "Link", $$Link$1, { "href": `https://twitter.com/intent/tweet?text=Check%20out%20this%20awesome%20post%20on%20Deepgram%20%40deepgramai%20%20${encodeURIComponent(shorturls.twitter)}`, "target": "_blank", "rel": "noopener noreferrer", "class": "astro-CRCHVEWD" }, { "default": () => renderTemplate`${renderComponent($$result, "Icon", $$Icon, { "icon": "twitter", "class": "w-[1em] fill-lightIris astro-CRCHVEWD" })}` })}
						${renderComponent($$result, "Link", $$Link$1, { "href": `https://www.linkedin.com/sharing/share-offsite/?url=${encodeURIComponent(shorturls.fullUrl)}&title=${encodeURIComponent(post.content.title)}`, "target": "_blank", "rel": "noopener noreferrer", "class": "astro-CRCHVEWD" }, { "default": () => renderTemplate`${renderComponent($$result, "Icon", $$Icon, { "icon": "linkedin", "class": "w-[1em] fill-lightIris astro-CRCHVEWD" })}` })}
						${renderComponent($$result, "Link", $$Link$1, { "href": `https://www.facebook.com/sharer/sharer.php?u=${encodeURIComponent(shorturls.facebook)}`, "target": "_blank", "rel": "noopener noreferrer", "class": "astro-CRCHVEWD" }, { "default": () => renderTemplate`${renderComponent($$result, "Icon", $$Icon, { "icon": "facebook", "class": "w-[1em] fill-lightIris astro-CRCHVEWD" })}` })}
					</div>

					${headings && headings.length > 0 && renderTemplate`<h5 class="hidden pt-8 pb-5 xl:block astro-CRCHVEWD">In this blog post</h5>`}
					<ul class="hidden xl:block astro-CRCHVEWD">
						${headings && headings.length > 0 && headings.filter((heading) => heading.attrs.level <= 2).map((heading) => renderTemplate`<li class="pb-4 astro-CRCHVEWD">
										${renderComponent($$result, "Link", $$Link$1, { "class": "text-cloud astro-CRCHVEWD", "href": `#${snakeCase(heading.content[0].text)}` }, { "default": () => renderTemplate`${heading.content[0].text}` })}
									</li>`)}
					</ul>
				</div>` })}<div class="mt-20 astro-CRCHVEWD">
				<h3 class="mb-10 text-center astro-CRCHVEWD">Related Resources</h3>
				${renderComponent($$result, "RelatedResources", $$RelatedResources, { "post": post, "category": postCategory, "class": "astro-CRCHVEWD" })}
			</div>` })}
	</article>`, "head": () => renderTemplate`${renderComponent($$result, "Meta", $$Meta, { "slot": "head", "property": "article:published_time", "content": post.first_published_at, "class": "astro-CRCHVEWD" })}${authorsWithContent && authorsWithContent.stories.map((author) => renderTemplate`${renderComponent($$result, "Meta", $$Meta, { "slot": "head", "property": "article:author", "content": author.title, "class": "astro-CRCHVEWD" })}`)}${renderComponent($$result, "Meta", $$Meta, { "slot": "head", "property": "article:section", "content": postCategory.content.title, "class": "astro-CRCHVEWD" })}${post.tag_list && post.tag_list.map((tag) => renderTemplate`${renderComponent($$result, "Meta", $$Meta, { "slot": "head", "property": "article:tag", "content": tag, "class": "astro-CRCHVEWD" })}`)}`, "head:canonical": () => renderTemplate`${renderComponent($$result, "MetaLink", $$Link, { "slot": "head:canonical", "rel": "canonical", "href": seo.url, "class": "astro-CRCHVEWD" })}`, "head:description": () => renderTemplate`${renderComponent($$result, "Description", $$Description, { "slot": "head:description", "name": "description", "content": seo.description, "class": "astro-CRCHVEWD" })}`, "head:title": () => renderTemplate`${renderComponent($$result, "Title", $$Title, { "slot": "head:title", "title": `${seo.title} - Deepgram Blog \u26A1\uFE0F`, "class": "astro-CRCHVEWD" })}`, "json:ld": () => renderTemplate`${renderComponent($$result, "JsonLD", $$JsonLD, { "slot": "json:ld", "schema": schema, "class": "astro-CRCHVEWD" })}`, "og:description": () => renderTemplate`${renderComponent($$result, "Meta", $$Meta, { "slot": "og:description", "property": "og:description", "content": og.description, "class": "astro-CRCHVEWD" })}`, "og:image": () => renderTemplate`${renderComponent($$result, "Meta", $$Meta, { "slot": "og:image", "property": "og:image", "itemprop": "image", "content": og.image, "class": "astro-CRCHVEWD" })}`, "og:image:alt": () => renderTemplate`${renderComponent($$result, "Meta", $$Meta, { "slot": "og:image:alt", "property": "og:image:alt", "content": `Deepgram Blog - A post titled: ${og.title}`, "class": "astro-CRCHVEWD" })}`, "og:image:height": () => renderTemplate`${renderComponent($$result, "Meta", $$Meta, { "slot": "og:image:height", "property": "og:image:height", "content": "762", "class": "astro-CRCHVEWD" })}`, "og:image:width": () => renderTemplate`${renderComponent($$result, "Meta", $$Meta, { "slot": "og:image:width", "property": "og:image:width", "content": "1200", "class": "astro-CRCHVEWD" })}`, "og:title": () => renderTemplate`${renderComponent($$result, "Meta", $$Meta, { "slot": "og:title", "property": "og:title", "content": og.title, "class": "astro-CRCHVEWD" })}`, "og:type": () => renderTemplate`${renderComponent($$result, "Meta", $$Meta, { "slot": "og:type", "property": "og:type", "content": "article", "class": "astro-CRCHVEWD" })}` })}

`;
}, "/Users/sandrarodgers/web-next/blog/src/pages/[slug].astro");

const $$file = "/Users/sandrarodgers/web-next/blog/src/pages/[slug].astro";
const $$url = "/[slug]";

const _page9 = /*#__PURE__*/Object.freeze(/*#__PURE__*/Object.defineProperty({
  __proto__: null,
  getStaticPaths,
  default: $$slug,
  file: $$file,
  url: $$url
}, Symbol.toStringTag, { value: 'Module' }));

const pageMap = new Map([["src/pages/index.astro", _page0],["src/pages/categories/index.astro", _page1],["src/pages/categories/[category]/[...page].astro", _page2],["src/pages/authors/index.astro", _page3],["src/pages/authors/[author]/[...page].astro", _page4],["src/pages/rss.xml.js", _page5],["src/pages/posts/[...page].astro", _page6],["src/pages/tags/index.astro", _page7],["src/pages/tags/[tag]/[...page].astro", _page8],["src/pages/[slug].astro", _page9],]);
const renderers = [Object.assign({"name":"astro:jsx","serverEntrypoint":"astro/jsx/server.js","jsxImportSource":"astro"}, { ssr: server_default }),Object.assign({"name":"@astrojs/vue","clientEntrypoint":"@astrojs/vue/client.js","serverEntrypoint":"@astrojs/vue/server.js"}, { ssr: _renderer1 }),Object.assign({"name":"@astrojs/preact","clientEntrypoint":"@astrojs/preact/client.js","serverEntrypoint":"@astrojs/preact/server.js","jsxImportSource":"preact"}, { ssr: server_default$1 }),];

if (typeof process !== "undefined") {
  if (process.argv.includes("--verbose")) ; else if (process.argv.includes("--silent")) ; else ;
}

const SCRIPT_EXTENSIONS = /* @__PURE__ */ new Set([".js", ".ts"]);
new RegExp(
  `\\.(${Array.from(SCRIPT_EXTENSIONS).map((s) => s.slice(1)).join("|")})($|\\?)`
);

const STYLE_EXTENSIONS = /* @__PURE__ */ new Set([
  ".css",
  ".pcss",
  ".postcss",
  ".scss",
  ".sass",
  ".styl",
  ".stylus",
  ".less"
]);
new RegExp(
  `\\.(${Array.from(STYLE_EXTENSIONS).map((s) => s.slice(1)).join("|")})($|\\?)`
);

function getRouteGenerator(segments, addTrailingSlash) {
  const template = segments.map((segment) => {
    return "/" + segment.map((part) => {
      if (part.spread) {
        return `:${part.content.slice(3)}(.*)?`;
      } else if (part.dynamic) {
        return `:${part.content}`;
      } else {
        return part.content.normalize().replace(/\?/g, "%3F").replace(/#/g, "%23").replace(/%5B/g, "[").replace(/%5D/g, "]").replace(/[.*+?^${}()|[\]\\]/g, "\\$&");
      }
    }).join("");
  }).join("");
  let trailing = "";
  if (addTrailingSlash === "always" && segments.length) {
    trailing = "/";
  }
  const toPath = compile(template + trailing);
  return toPath;
}

function deserializeRouteData(rawRouteData) {
  return {
    route: rawRouteData.route,
    type: rawRouteData.type,
    pattern: new RegExp(rawRouteData.pattern),
    params: rawRouteData.params,
    component: rawRouteData.component,
    generate: getRouteGenerator(rawRouteData.segments, rawRouteData._meta.trailingSlash),
    pathname: rawRouteData.pathname || void 0,
    segments: rawRouteData.segments
  };
}

function deserializeManifest(serializedManifest) {
  const routes = [];
  for (const serializedRoute of serializedManifest.routes) {
    routes.push({
      ...serializedRoute,
      routeData: deserializeRouteData(serializedRoute.routeData)
    });
    const route = serializedRoute;
    route.routeData = deserializeRouteData(serializedRoute.routeData);
  }
  const assets = new Set(serializedManifest.assets);
  return {
    ...serializedManifest,
    assets,
    routes
  };
}

const _manifest = Object.assign(deserializeManifest({"adapterName":"@astrojs/netlify/functions","routes":[{"file":"","links":["assets/_slug_.0da0288b.css","assets/_slug_.13065b2e.css","assets/index.9c23bf80.css","assets/_slug_.c2747b55.css"],"scripts":[{"type":"external","value":"hoisted.35d832ba.js"},{"type":"external","value":"page.a5a3d0ae.js"}],"routeData":{"route":"/","type":"page","pattern":"^\\/$","segments":[],"params":[],"component":"src/pages/index.astro","pathname":"/","_meta":{"trailingSlash":"ignore"}}},{"file":"","links":["assets/_slug_.0da0288b.css","assets/_slug_.13065b2e.css","assets/_slug_.c2747b55.css","assets/index.9c23bf80.css"],"scripts":[{"type":"external","value":"hoisted.35d832ba.js"},{"type":"external","value":"page.a5a3d0ae.js"}],"routeData":{"route":"/categories","type":"page","pattern":"^\\/categories\\/?$","segments":[[{"content":"categories","dynamic":false,"spread":false}]],"params":[],"component":"src/pages/categories/index.astro","pathname":"/categories","_meta":{"trailingSlash":"ignore"}}},{"file":"","links":["assets/_slug_.0da0288b.css","assets/_slug_.13065b2e.css","assets/_slug_.c2747b55.css","assets/_...page_.8ae06e0a.css","assets/index.9c23bf80.css"],"scripts":[{"type":"external","value":"hoisted.35d832ba.js"},{"type":"external","value":"page.a5a3d0ae.js"}],"routeData":{"route":"/categories/[category]/[...page]","type":"page","pattern":"^\\/categories\\/([^/]+?)(?:\\/(.*?))?\\/?$","segments":[[{"content":"categories","dynamic":false,"spread":false}],[{"content":"category","dynamic":true,"spread":false}],[{"content":"...page","dynamic":true,"spread":true}]],"params":["category","...page"],"component":"src/pages/categories/[category]/[...page].astro","_meta":{"trailingSlash":"ignore"}}},{"file":"","links":["assets/_slug_.13065b2e.css","assets/_slug_.c2747b55.css","assets/_slug_.0da0288b.css","assets/_slug_.99ec8802.css"],"scripts":[{"type":"external","value":"hoisted.35d832ba.js"},{"type":"external","value":"page.a5a3d0ae.js"}],"routeData":{"route":"/authors","type":"page","pattern":"^\\/authors\\/?$","segments":[[{"content":"authors","dynamic":false,"spread":false}]],"params":[],"component":"src/pages/authors/index.astro","pathname":"/authors","_meta":{"trailingSlash":"ignore"}}},{"file":"","links":["assets/_slug_.0da0288b.css","assets/_slug_.13065b2e.css","assets/_slug_.c2747b55.css","assets/_...page_.8ae06e0a.css","assets/index.9c23bf80.css","assets/_...page_.6c29be62.css"],"scripts":[{"type":"external","value":"hoisted.35d832ba.js"},{"type":"external","value":"page.a5a3d0ae.js"}],"routeData":{"route":"/authors/[author]/[...page]","type":"page","pattern":"^\\/authors\\/([^/]+?)(?:\\/(.*?))?\\/?$","segments":[[{"content":"authors","dynamic":false,"spread":false}],[{"content":"author","dynamic":true,"spread":false}],[{"content":"...page","dynamic":true,"spread":true}]],"params":["author","...page"],"component":"src/pages/authors/[author]/[...page].astro","_meta":{"trailingSlash":"ignore"}}},{"file":"","links":[],"scripts":[{"type":"external","value":"page.a5a3d0ae.js"}],"routeData":{"route":"/rss.xml","type":"endpoint","pattern":"^\\/rss\\.xml$","segments":[[{"content":"rss.xml","dynamic":false,"spread":false}]],"params":[],"component":"src/pages/rss.xml.js","pathname":"/rss.xml","_meta":{"trailingSlash":"ignore"}}},{"file":"","links":["assets/_slug_.0da0288b.css","assets/_slug_.13065b2e.css","assets/_slug_.c2747b55.css","assets/_...page_.8ae06e0a.css","assets/index.9c23bf80.css"],"scripts":[{"type":"external","value":"hoisted.35d832ba.js"},{"type":"external","value":"page.a5a3d0ae.js"}],"routeData":{"route":"/posts/[...page]","type":"page","pattern":"^\\/posts(?:\\/(.*?))?\\/?$","segments":[[{"content":"posts","dynamic":false,"spread":false}],[{"content":"...page","dynamic":true,"spread":true}]],"params":["...page"],"component":"src/pages/posts/[...page].astro","_meta":{"trailingSlash":"ignore"}}},{"file":"","links":["assets/_slug_.0da0288b.css","assets/_slug_.13065b2e.css","assets/_slug_.c2747b55.css"],"scripts":[{"type":"external","value":"hoisted.35d832ba.js"},{"type":"external","value":"page.a5a3d0ae.js"}],"routeData":{"route":"/tags","type":"page","pattern":"^\\/tags\\/?$","segments":[[{"content":"tags","dynamic":false,"spread":false}]],"params":[],"component":"src/pages/tags/index.astro","pathname":"/tags","_meta":{"trailingSlash":"ignore"}}},{"file":"","links":["assets/_slug_.0da0288b.css","assets/_slug_.13065b2e.css","assets/_slug_.c2747b55.css","assets/_...page_.8ae06e0a.css","assets/index.9c23bf80.css"],"scripts":[{"type":"external","value":"hoisted.35d832ba.js"},{"type":"external","value":"page.a5a3d0ae.js"}],"routeData":{"route":"/tags/[tag]/[...page]","type":"page","pattern":"^\\/tags\\/([^/]+?)(?:\\/(.*?))?\\/?$","segments":[[{"content":"tags","dynamic":false,"spread":false}],[{"content":"tag","dynamic":true,"spread":false}],[{"content":"...page","dynamic":true,"spread":true}]],"params":["tag","...page"],"component":"src/pages/tags/[tag]/[...page].astro","_meta":{"trailingSlash":"ignore"}}},{"file":"","links":["assets/_slug_.13065b2e.css","assets/_slug_.3ebf720f.css","assets/index.9c23bf80.css","assets/_slug_.c2747b55.css","assets/_slug_.0da0288b.css","assets/_slug_.99ec8802.css"],"scripts":[{"type":"external","value":"hoisted.d8f274fa.js"},{"type":"external","value":"page.a5a3d0ae.js"}],"routeData":{"route":"/[slug]","type":"page","pattern":"^\\/([^/]+?)\\/?$","segments":[[{"content":"slug","dynamic":true,"spread":false}]],"params":["slug"],"component":"src/pages/[slug].astro","_meta":{"trailingSlash":"ignore"}}}],"base":"/","markdown":{"drafts":false,"syntaxHighlight":"shiki","shikiConfig":{"langs":[],"theme":"github-dark","wrap":false},"remarkPlugins":[],"rehypePlugins":[],"remarkRehype":{},"extendDefaultPlugins":false,"isAstroFlavoredMd":true,"isExperimentalContentCollections":false,"contentDir":"file:///Users/sandrarodgers/web-next/blog/src/content/"},"pageMap":null,"renderers":[],"entryModules":{"\u0000@astrojs-ssr-virtual-entry":"entry.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/2021-state-of-automatic-speech-recognition-infographic/index.md":"chunks/index.d424c8ac.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/5-ways-understand-voice-of-the-customer-voice-technology/index.md":"chunks/index.dc74e25d.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/6-challenges-asr-hindi/index.md":"chunks/index.2fafce21.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/a-conversation-with-asian-american-pacific-islander-deepgrammers/index.md":"chunks/index.1e829fa0.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/a-note-to-our-customers-openai-whispers-entrance-into-voice/index.md":"chunks/index.63d15a25.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/a-voice-destin-ation-project-voice-x-2021/index.md":"chunks/index.32fed3f0.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/accuracy-matters-improving-speech-recognition-through-data-processes-esteban-gorupicz-ceo-atexto-project-voice-x/index.md":"chunks/index.c768d039.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/adding-subtitles-to-html-video-element/index.md":"chunks/index.0c16eb41.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/ai-show-bias-in-machine-learning/index.md":"chunks/index.8d9c18b4.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/ai-show-different-types-of-machine-learning/index.md":"chunks/index.e3d6f8e8.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/ai-show-how-do-you-use-a-neural-network-in-your-business/index.md":"chunks/index.c7a3eb62.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/ai-show-how-will-data-influence-the-future-of-machine-learning/index.md":"chunks/index.32419930.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/ai-show-what-does-an-ai-tranformation-look-like/index.md":"chunks/index.c738848c.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/ai-show-what-does-it-mean-for-a-machine-to-learn/index.md":"chunks/index.3dbf031e.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/ai-show-what-will-the-ai-utopia-look-like/index.md":"chunks/index.f2f2eac9.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/all-about-transcription-for-real-time-audio-streaming/index.md":"chunks/index.4cdd4bb1.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/ar-note-taking-airnote/index.md":"chunks/index.874fad87.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/asr-important-deaf-hoh-community/index.md":"chunks/index.02b91561.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/asynchronous-logic-to-write-a-vue-3-and-deepgram-captions-component/index.md":"chunks/index.aaf5f3fb.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/autobubble-youtube-speech-bubbles/index.md":"chunks/index.b4737962.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/automatic-speech-recognition-education/index.md":"chunks/index.8453b92a.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/automatically-transcribe-summarize-and-send-phone-call-summaries/index.md":"chunks/index.bb2804e6.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/automatically-transcribing-podcast-episodes-with-pipedream-and-python/index.md":"chunks/index.2018308e.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/bekah-joins-deepgram/index.md":"chunks/index.826cc687.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/best-8-deepgram-projects-hack-cambridge/index.md":"chunks/index.c7485c26.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/best-python-audio-manipulation-tools/index.md":"chunks/index.10c57d2a.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/best-speech-recognition-model-business/index.md":"chunks/index.6347321a.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/best-speech-to-text-apis/index.md":"chunks/index.b6635dd0.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/brian-barrow-hello/index.md":"chunks/index.c12426e0.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/build-a-livestream-web-application-vue-and-express-setup/index.md":"chunks/index.f9ebd95f.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/build-a-livestream-web-application-with-amazon-ivs-and-deepgram/index.md":"chunks/index.6e32bfe0.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/build-a-presentation-coaching-application-with-recall/index.md":"chunks/index.59973521.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/build-a-todo-list-with-pinia-and-vue-3/index.md":"chunks/index.8e05243f.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/build-a-voice-controlled-to-do-list-app-with-deepgram-and-vue-3/index.md":"chunks/index.5e33c272.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/build-npm-packages/index.md":"chunks/index.2f1130f2.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/build-with-the-official-deepgram-sdks/index.md":"chunks/index.bc21e34d.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/building-404-pages-that-bring-joy/index.md":"chunks/index.45d03faf.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/building-a-conversational-ai-flow-with-deepgram/index.md":"chunks/index.b479022c.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/building-the-future-of-voice-scott-stephenson-ceo-deepgram-project-voice-x/index.md":"chunks/index.719ef3f0.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/celebrating-black-history-month-with-a-vision-of-more-inclusive-speech-recognition/index.md":"chunks/index.cfaacbeb.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/celebrating-jewish-american-history-month/index.md":"chunks/index.794093e4.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/censor-profanity-nodejs/index.md":"chunks/index.acc94734.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/chili-pepper/index.md":"chunks/index.5a3e411d.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/chromium-kiosk-pi/index.md":"chunks/index.c7702c1a.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/classroom-captioner/index.md":"chunks/index.99585ee5.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/closed-captioning-companies-use-asr/index.md":"chunks/index.54bc8990.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/cloud-to-butt/index.md":"chunks/index.00e880b8.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/coding-website-with-voice/index.md":"chunks/index.8701314c.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/comic-books-videos-yack/index.md":"chunks/index.4393d450.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/complete-guide-punctuation-capitalization-speech-to-text/index.md":"chunks/index.c2918103.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/contact-center-as-a-service-utilize-solutions/index.md":"chunks/index.efec33c8.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/contextual-video-overlay-tomscottplus/index.md":"chunks/index.afad21a8.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/conversational-ai-platforms-utilize-top-asr-tools/index.md":"chunks/index.7b29711d.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/conversational-intelligence-podcast-with-scott-stephenson/index.md":"chunks/index.6a163209.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/create-readable-transcripts-for-podcasts/index.md":"chunks/index.4f65c9c9.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/cross-platform-nuget-dotnet/index.md":"chunks/index.6a8cc963.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/customer-story-rideshare-smartrhino-deepgram/index.md":"chunks/index.b8135a43.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/customer-story-stanford-moves-education-forward-with-deepgram/index.md":"chunks/index.7c3436e9.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/cześć-we’re-releasing-a-base-polish-beta-speech-to-text-language-model/index.md":"chunks/index.d0cbc64a.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/daily-video-live-transcription/index.md":"chunks/index.67cfacfd.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deep-learning-asr-for-business/index.md":"chunks/index.6aa32753.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deep-learning-speech-recognition/index.md":"chunks/index.5a21666a.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-and-recall-ai-partner-to-make-it-easier-for-developers-to-extract-insights-from-meeting-audio-and-automate-tedious-workflows/index.md":"chunks/index.1a1fff1e.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-announces-unimrcp-integration-to-power-modern-customer-experience/index.md":"chunks/index.1b4c6513.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-diversity-inclusion/index.md":"chunks/index.d76136ab.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-enables-developers-to-build-the-future-of-voice-with-suite-of-new-features-and-10-million-in-free-speech-recognition/index.md":"chunks/index.a2740b03.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-enters-strategic-investment-agreement-with-in-q-tel-2/index.md":"chunks/index.f54e6f9c.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-g2-customer-service/index.md":"chunks/index.bfe34c26.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-g2-review-winter-2022/index.md":"chunks/index.60affbdc.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-godot-tutorial/index.md":"chunks/index.b9c40b02.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-hackathon-recap/index.md":"chunks/index.08237a88.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-is-a-founding-member-of-callminers-open-voice-transcription-standard-ovts/index.md":"chunks/index.6068e62f.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-language-speech-models/index.md":"chunks/index.a26245fa.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-named-a-high-performer-for-voice-recognition-software-in-g2-fall-report/index.md":"chunks/index.cd3bcc45.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-pioneers-novel-training-approach-setting-new-standard-for-ai-companies-2/index.md":"chunks/index.1f225a58.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-projectvoicex-transcription-aicontactcenter-artcoombs/index.md":"chunks/index.b7fc4a23.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-reached-soc-2-type-1-certification/index.md":"chunks/index.176aa9ab.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-series-a/index.md":"chunks/index.0915c09f.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-stepzen-collaboration/index.md":"chunks/index.13d06666.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-summit-speaker-lineup-2021/index.md":"chunks/index.0bf00a68.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-twilio-streaming-rust/index.md":"chunks/index.fed0ba3a.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-twilio-streaming/index.md":"chunks/index.2d907542.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-unity-tutorial/index.md":"chunks/index.36620639.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-versus-amazon-google/index.md":"chunks/index.069870be.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgram-with-vonage/index.md":"chunks/index.91b032e9.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/deepgrams-speech-to-text-api-number-1-for-developers-g2/index.md":"chunks/index.9b7b7102.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/democratizing-speech-analytics-deepgram-callbi/index.md":"chunks/index.49db92f1.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/detect-non-inclusive-language-with-retext-and-node-js/index.md":"chunks/index.78193d99.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/detecting-and-reducing-bias-in-speech-recognition/index.md":"chunks/index.07c01acc.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/difference-between-language-dialect/index.md":"chunks/index.0bc6aaf1.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/diving-into-vue-3-getting-started/index.md":"chunks/index.13570bbe.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/diving-into-vue-3-methods-watch-and-computed/index.md":"chunks/index.da763143.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/diving-into-vue-3-reactivity-api/index.md":"chunks/index.598e8629.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/diving-into-vue-3-reusability-with-composables/index.md":"chunks/index.defbd073.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/diving-into-vue-3-setup-function/index.md":"chunks/index.aeb0a7b1.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/do-your-call-transcripts-read-like-mad-libs/index.md":"chunks/index.d8699b92.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/does-unsupervised-learning-create-superior-speech-recognition/index.md":"chunks/index.9e44a129.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/downloading-podcast-transcripts-from-terminal/index.md":"chunks/index.9a507d17.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/draw-with-your-voice-articulate/index.md":"chunks/index.514a57f9.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/embracing-the-diversity-of-spanish/index.md":"chunks/index.5d3a058e.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/enhance-audio-with-dolby-and-deepgram/index.md":"chunks/index.9f380d83.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/enhanced-messaging-in-streaming/index.md":"chunks/index.cb5844a9.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/everything-you-need-to-know-about-keywords-for-speech-recognition/index.md":"chunks/index.3e17fbb3.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/exploring-whisper/index.md":"chunks/index.735dfce6.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/fetch-hosted-audio-streams-in-the-browser/index.md":"chunks/index.16eeb84e.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/ffmpeg-beginners/index.md":"chunks/index.c6b0eb33.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/flutter-speech-to-text-tutorial/index.md":"chunks/index.df704e62.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/foreign-language-practice-triolingo/index.md":"chunks/index.5ee30198.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/freecodecamp-quote-generator-upgrade/index.md":"chunks/index.1c5f1abb.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/generate-webvtt-srt-captions-nodejs/index.md":"chunks/index.833ac165.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/generic-asr-will-never-be-accurate-enough-for-conversational-ai/index.md":"chunks/index.d56a07a2.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/getting-started-live-transcription-vue/index.md":"chunks/index.ae317751.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/getting-started-with-apis/index.md":"chunks/index.7cab7294.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/getting-started-with-json/index.md":"chunks/index.21104e44.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/getting-started-with-mediastream-api/index.md":"chunks/index.49d8aac9.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/getting-started-with-supabase/index.md":"chunks/index.69e7a1e4.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/google-and-amazon-are-wrong-about-voice/index.md":"chunks/index.79cea011.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/guide-deepspeech-speech-to-text/index.md":"chunks/index.f6dd0fc5.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/happy-national-native-american-heritage-month/index.md":"chunks/index.a7710019.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/hell-yes-we-have-sdks-apis-and-docs/index.md":"chunks/index.218b7afa.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/hello-world/index.md":"chunks/index.fb8076b5.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-ai-is-advancing-the-transcription-process/index.md":"chunks/index.7328ad8f.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-does-microsofts-purchase-of-nuance-communications-affect-the-market/index.md":"chunks/index.fd821c2c.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-does-santa-do-it-ai-show/index.md":"chunks/index.28307eeb.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-gender-shows-up-in-language/index.md":"chunks/index.8f00078a.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-is-machine-learning-or-deep-learning-affecting-science-ai-show/index.md":"chunks/index.87326f68.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-is-todays-ai-boom-different-from-those-of-the-past-ai-show/index.md":"chunks/index.b89851b9.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-to-add-speech-recognition-to-your-react-project/index.md":"chunks/index.012820a0.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-to-build-an-openai-whisper-api/index.md":"chunks/index.c2a1226b.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-to-get-a-job-in-deep-learning/index.md":"chunks/index.2a5a74b9.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-to-monitor-media-mentions-in-podcasts-with-python/index.md":"chunks/index.57256b74.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-to-run-openai-whisper-in-command-line/index.md":"chunks/index.eb91dff4.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-to-run-openai-whisper-in-google-colab/index.md":"chunks/index.e6debbe8.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-to-test-automatic-speech-recognition-asr-providers-for-your-business/index.md":"chunks/index.27e95695.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-to-train-baidus-deepspeech-model-with-kur/index.md":"chunks/index.1d62ef99.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-to-use-whisper-openais-speech-recognition-model-in-1-minute/index.md":"chunks/index.7f186e08.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-to-write-vue-3-composables-for-a-third-party-api-integration/index.md":"chunks/index.c585014d.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-voice-technology-creates-accessible-world/index.md":"chunks/index.6b0290e3.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/identifying-the-best-agent-to-respond-in-your-ivr-system/index.md":"chunks/index.86259ec7.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/import-a-docker-container-in-python/index.md":"chunks/index.6dfe662b.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/improve-ivr-prompts-with-custom-reporting/index.md":"chunks/index.365c7b69.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/introducing-auto-generated-summaries-for-audio-content/index.md":"chunks/index.2dc0b049.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/introducing-real-time-streaming-and-solutions-for-conversational-ai-sales-and-support-enablement/index.md":"chunks/index.719256f1.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/introducing-the-new-deepgram-developer-portal/index.md":"chunks/index.6fd27d0e.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/introducing-topic-detection-feature/index.md":"chunks/index.4ba6c57c.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/ios-live-transcription/index.md":"chunks/index.69795ed5.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/is-there-an-asr-gender-gap/index.md":"chunks/index.defd71e5.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/just-released-new-version-of-on-premises/index.md":"chunks/index.275bae8b.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/keywords-vs-search/index.md":"chunks/index.3b85b6df.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/live-transcribing-radio-feeds-js/index.md":"chunks/index.5f4b6bee.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/live-transcription-badge-video/index.md":"chunks/index.5ce84362.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/live-transcription-django/index.md":"chunks/index.802bb13a.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/live-transcription-fastapi/index.md":"chunks/index.b2b8b909.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/live-transcription-flask/index.md":"chunks/index.8ec90829.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/live-transcription-mic-browser/index.md":"chunks/index.a6cba6fd.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/live-transcription-quart/index.md":"chunks/index.f4f56f34.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/luke-oliff-joins-deepgram/index.md":"chunks/index.fe780025.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/machine-learning-for-front-end-developers-get-started-with-tensorflow-js/index.md":"chunks/index.0d54207d.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/making-your-audiovisual-content-accessible/index.md":"chunks/index.cfee7cfb.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/meet-kevin-lewis/index.md":"chunks/index.75394f1f.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/meet-sandra-rodgers/index.md":"chunks/index.c140444a.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/meeting-analysis-platforms-automatic-speech-recognition-solutions/index.md":"chunks/index.eeec98bb.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/michael-jolley-joins-deepgram/index.md":"chunks/index.0d5d9eac.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/multichannel-vs-diarization/index.md":"chunks/index.d303ca6c.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/natural-language-understanding-nlu-for-audio-requires-a-highly-accurate-and-fast-speech-to-text-foundation/index.md":"chunks/index.2e7fdc0f.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/new-releases-five-new-languages-and-three-new-use-case-speech-models/index.md":"chunks/index.112b5d50.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/new-spanish-and-turkish-language-models-and-updated-general-models/index.md":"chunks/index.193fc9f1.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/new-tech-lets-journalists-find-damning-soundbites/index.md":"chunks/index.d4579f17.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/nlp-on-the-edge-voice-ai-and-hardware-robert-daigle-and-andi-huels-lenovo-project-voice-x/index.md":"chunks/index.da8c71f6.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/now-available-deepgram-speech-recognition-for-twilio-programmable-voice-api/index.md":"chunks/index.9d8888bc.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/npx-script/index.md":"chunks/index.74065a5e.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/nuxt-expand-nested-navigation/index.md":"chunks/index.3b580d2c.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/olá-enhanced-portuguese-beta-speech-to-text-language-model-now-available/index.md":"chunks/index.6b42db62.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/open-source-projects-for-hacktoberfest-2022/index.md":"chunks/index.deaa2b8c.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/opening-keynote-bradley-metrock-ceo-project-voice-project-voice-x/index.md":"chunks/index.5a6b6cd9.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/opening-keynote-jeff-blankenberg-principal-technical-evangelist-amazon-alexa-project-voice-x/index.md":"chunks/index.f9cfd764.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/optimizing-your-content/index.md":"chunks/index.63a21ba8.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/p5js-deepgram-game/index.md":"chunks/index.54265ca5.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/p5js-game-logic/index.md":"chunks/index.141effbf.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/p5js-getting-started/index.md":"chunks/index.bb385126.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/podcast-search-engine/index.md":"chunks/index.9c6e7678.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/practice-spelling-bees-hero/index.md":"chunks/index.b34990f6.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/propelled-by-product-customer-and-industry-momentum-deepgram-continues-to-build-the-future-of-speech-recognition/index.md":"chunks/index.873b851f.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/protecting-api-key/index.md":"chunks/index.64ea3fd1.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/pycon-deepgram-usecases/index.md":"chunks/index.59ec67f8.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/pycon-python-speech-to-text/index.md":"chunks/index.95377e09.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/python-deepgram-roundup/index.md":"chunks/index.c8c62373.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/python-deepgram-twilio/index.md":"chunks/index.a2ce9b44.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/python-graphing-transcripts/index.md":"chunks/index.eed0826c.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/python-script-compliance/index.md":"chunks/index.7c1e3278.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/python-speech-recognition-locally-torchaudio/index.md":"chunks/index.d5d61744.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/python-talk-time-analytics/index.md":"chunks/index.9a369e8e.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/python-virtual-environments/index.md":"chunks/index.f08c2069.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/pytorch-intro-with-torchaudio/index.md":"chunks/index.00273b50.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/real-time-routing-of-conversational-data-is-table-stakes-for-enterprises/index.md":"chunks/index.7615ed3b.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/retail-restaurants-and-travel-shilp-agarwal-ceo-blutag-project-voice-x/index.md":"chunks/index.e8ecc739.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/saving-transcripts-from-terminal/index.md":"chunks/index.dcd3325e.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/say-what-you-mean-navigating-critical-conversations-scott-sandland-ceo-cyrano-ai-project-voice-x/index.md":"chunks/index.e193a275.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/scrape-a-website-with-your-voice-using-python/index.md":"chunks/index.1f45620b.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/search-through-sound-finding-phrases-in-audio/index.md":"chunks/index.d2eb5e44.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/sending-audio-files-to-expressjs-server/index.md":"chunks/index.f475d69f.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/sentiment-analysis-emotion-regulation-difference/index.md":"chunks/index.5e071724.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/should-ai-be-regulated-ai-show-2/index.md":"chunks/index.c2950650.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/song-search-js/index.md":"chunks/index.dfe89bf2.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/sonic-branding-in-the-enterprise-audrey-arbeeny-ceo-audiobrain-project-voice-x/index.md":"chunks/index.25966ffd.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/sparking-the-future-of-conversation-design-braden-ream-ceo-voiceflow-project-voice-x/index.md":"chunks/index.f254ff9f.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/speech-to-text-content-moderation-companies/index.md":"chunks/index.0d1b84fc.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/speech-to-text-model-ukrainian/index.md":"chunks/index.151ff6c7.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/state-of-speech-our-new-data-report-reveals-asrs-untapped-potential/index.md":"chunks/index.3fb87040.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/state-of-voice-report-2022/index.md":"chunks/index.c9bd8745.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/supabase-authentication-vue/index.md":"chunks/index.63a71f7f.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/supabase-podcast-player-vue/index.md":"chunks/index.2c7e077e.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/technical-writing-a-beginners-guide/index.md":"chunks/index.6b968521.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/technical-writing-a-developers-guide-to-storytelling/index.md":"chunks/index.9d51169d.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/technical-writing-accessible-writing-for-developers/index.md":"chunks/index.df480aad.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/technical-writing-ethics-for-developers/index.md":"chunks/index.907802bd.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/text-cleaning-asr-turkish/index.md":"chunks/index.ddf0b7ee.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/the-contact-center-of-the-future-with-real-time-ai/index.md":"chunks/index.465e06e1.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/the-evolution-of-conversational-ai-in-the-car-and-beyond-shyamala-prayaga-sr-software-product-manager-ford-project-voice-x/index.md":"chunks/index.6fe7b5c3.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/the-history-of-automatic-speech-recognition/index.md":"chunks/index.4a3896b3.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/the-history-of-the-word-hacker-2/index.md":"chunks/index.2a9713ab.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/the-importance-of-testing-with-voice-experiences-and-conversational-ai-john-kelvie-ceo-bespoken-project-voice-x/index.md":"chunks/index.6fcb8ef8.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/the-language-of-lgbtq-inclusion-and-allyship/index.md":"chunks/index.a554faab.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/the-new-age-of-voice-commerce-mike-zagorsek-coo-soundhound-project-voice-x/index.md":"chunks/index.453220ed.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/the-trouble-with-wer/index.md":"chunks/index.50d41eb2.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/the-weak-link-in-your-multichannel-strategy/index.md":"chunks/index.d8b04256.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/tips-on-choosing-a-call-analytics-development-path/index.md":"chunks/index.5607fac3.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/tips-on-choosing-a-conversational-ai-development-path/index.md":"chunks/index.9e7e6d84.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/tips-on-choosing-a-sales-and-support-enablement-development-path/index.md":"chunks/index.91cab9da.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/tonya-sims-joins-deepgram/index.md":"chunks/index.15e9f355.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/top-3-use-cases-speech-to-text-gaming/index.md":"chunks/index.8ca04472.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/top-6-dutch-asr-challenges/index.md":"chunks/index.6f1ceaf4.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/top-7-uses-speech-to-text-education/index.md":"chunks/index.632dd6c2.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/top-six-uses-cases-for-asr-social-media/index.md":"chunks/index.ca2fff70.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/topic-detection-with-python/index.md":"chunks/index.377fd69f.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/track-brand-mentions-across-podcast-episodes/index.md":"chunks/index.b4ca124c.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/train-a-deep-learning-speech-recognition-model-to-understand-your-voice/index.md":"chunks/index.41efaf5f.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/transcribe-google-drive-files-pipedream/index.md":"chunks/index.93bd89f4.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/transcribe-phone-calls-with-twilio-functions-and-deepgram/index.md":"chunks/index.a0d40915.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/transcribe-videos-nodejs/index.md":"chunks/index.4be7290e.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/transcribe-youtube-videos-from-terminal/index.md":"chunks/index.ac9156ca.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/transcribe-youtube-videos-nodejs/index.md":"chunks/index.71848df5.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/transcribing-browser-tab-audio-chrome-extensions/index.md":"chunks/index.bf4a61a7.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/transcription-netlify-functions/index.md":"chunks/index.714bf9ff.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/transfer-learning-spanish-portuguese/index.md":"chunks/index.6f477e0e.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/translation-itranslate/index.md":"chunks/index.91dc64a9.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/tune-in-deepgram-summit-11-18-21/index.md":"chunks/index.de9a003a.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/twilio-crm-log-js/index.md":"chunks/index.48ec6c41.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/understanding-webhooks/index.md":"chunks/index.e9acb239.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/upcoming-january-releases/index.md":"chunks/index.da1ff51e.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/use-openai-whisper-speech-recognition-with-the-deepgram-api/index.md":"chunks/index.6a04c158.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/voice-control-browser-stemm/index.md":"chunks/index.b11c6e2d.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/voice-controlled-music-with-python/index.md":"chunks/index.07ad7868.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/voice-in-healthcare-dr-yared-alemu-ceo-tqintelligence-project-voice-x/index.md":"chunks/index.c68f373c.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/voice-in-healthcare-henry-oconnell-ceo-canary-speech-project-voice-x/index.md":"chunks/index.3aeffe28.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/voice-technology-customer-experience/index.md":"chunks/index.c9d65b76.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/voicebots-will-enhance-your-life-not-destroy-it/index.md":"chunks/index.b0d8215f.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/we-raised-25-million/index.md":"chunks/index.b8a64bd5.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-are-the-top-mistakes-in-deep-learning-ai-show/index.md":"chunks/index.c16fdf55.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-does-as-the-crow-flies-mean/index.md":"chunks/index.7156df4b.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-does-it-mean-to-be-under-the-weather/index.md":"chunks/index.2a6b537b.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-does-thats-the-way-the-cookie-crumbles-mean/index.md":"chunks/index.d057e066.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-does-the-ai-dystopia-look-like-ai-show-2/index.md":"chunks/index.0c75d62a.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-is-asr/index.md":"chunks/index.09866bca.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-is-automatic-speech-recognition-past-present-and-future-ebook/index.md":"chunks/index.237f3456.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-is-automl-how-the-technology-paves-the-way-for-the-future-of-asr/index.md":"chunks/index.71cdf414.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-is-code-switching-and-how-did-it-make-english/index.md":"chunks/index.a5a3f70f.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-is-devrel-a-deepgram-approach/index.md":"chunks/index.791fc3f3.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-is-speaker-diarization/index.md":"chunks/index.56f1f925.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-is-the-most-important-channel-to-engage-your-customers-on/index.md":"chunks/index.8a162da0.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-is-word-error-rate/index.md":"chunks/index.ab8326af.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-makes-a-great-conversational-ai-experience/index.md":"chunks/index.ea5e6b68.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-makes-your-voice-uniquely-yours/index.md":"chunks/index.bb64d7e6.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/whats-the-best-infrastructure-for-machine-learning-ai-show/index.md":"chunks/index.0584ce64.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/why-does-your-speech-recognition-need-context/index.md":"chunks/index.0c92ddd5.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/why-enterprise-audio-requirements-are-more-nuanced-at-real-time-speeds/index.md":"chunks/index.65c8a32d.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/posts/why-iot-means-speech-recognition/index.md":"chunks/index.9cc5f48e.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/abdul-ajetunmobi.json":"chunks/abdul-ajetunmobi.68e10869.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/adam-sypniewski.json":"chunks/adam-sypniewski.14c0c9ed.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/aimie-ye.json":"chunks/aimie-ye.1fd72cba.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/alexa-de-la-torre.json":"chunks/alexa-de-la-torre.4e76fe25.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/bekah-hawrot-weigel.json":"chunks/bekah-hawrot-weigel.bd727c20.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/brian-barrow.json":"chunks/brian-barrow.dc49539f.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/call-tracking-metrics.json":"chunks/call-tracking-metrics.e3e73a8f.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/chris-doty.json":"chunks/chris-doty.e1f90cdd.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/claudia-ring.json":"chunks/claudia-ring.df407f56.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/conner-goodrum.json":"chunks/conner-goodrum.61e9354b.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/dan-shafer.json":"chunks/dan-shafer.771c71e9.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/duygu-altinok.json":"chunks/duygu-altinok.75d19b23.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/ehab-el-ali.json":"chunks/ehab-el-ali.768ce391.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/evan-henry.json":"chunks/evan-henry.1745cf14.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/greg-holmes.json":"chunks/greg-holmes.8bff212d.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/julia-strout.json":"chunks/julia-strout.88ea0c94.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/kate-weber.json":"chunks/kate-weber.1ab37f55.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/katie-byrne.json":"chunks/katie-byrne.6ac44433.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/keith-lam.json":"chunks/keith-lam.7b4bdfde.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/kevin-lewis.json":"chunks/kevin-lewis.3d5c6faa.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/luke-oliff.json":"chunks/luke-oliff.dd5920b2.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/michael-jolley.json":"chunks/michael-jolley.8a0343c9.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/morris-gevirtz.json":"chunks/morris-gevirtz.f5ad9d5a.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/natalie-rutgers.json":"chunks/natalie-rutgers.1605de85.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/nicole-ohanian.json":"chunks/nicole-ohanian.fab5aa9d.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/nikola-whallon.json":"chunks/nikola-whallon.21c3bac8.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/pankaj-trivedi.json":"chunks/pankaj-trivedi.ccc7ba5b.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/ralphette-english.json":"chunks/ralphette-english.fbe9b6b2.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/richard-stevenson.json":"chunks/richard-stevenson.ba324256.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/ross-oconnell.json":"chunks/ross-oconnell.4d628528.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/sam-zegas.json":"chunks/sam-zegas.ab344f64.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/sandra-rodgers.json":"chunks/sandra-rodgers.daef0bac.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/scott-stephenson.json":"chunks/scott-stephenson.b6eb7d80.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/shadi-baqleh.json":"chunks/shadi-baqleh.f85c9758.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/shae-burnette.json":"chunks/shae-burnette.28001891.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/shir-goldberg.json":"chunks/shir-goldberg.edc6eebe.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/tonya-sims.json":"chunks/tonya-sims.5536ea85.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/authors/yujian-tang.json":"chunks/yujian-tang.2b60b153.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/category/ai-and-engineering.json":"chunks/ai-and-engineering.7c931afe.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/category/announcement.json":"chunks/announcement.455318d5.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/category/best-practice.json":"chunks/best-practice.d6f8e90e.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/category/devlife.json":"chunks/devlife.1686061e.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/category/dg-insider.json":"chunks/dg-insider.67491215.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/category/identity-and-language.json":"chunks/identity-and-language.69202bd7.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/category/linguistics.json":"chunks/linguistics.82cb6b89.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/category/product-news.json":"chunks/product-news.1c007016.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/category/project-showcase.json":"chunks/project-showcase.38979593.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/category/speech-trends.json":"chunks/speech-trends.94e7f9c1.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/category/tutorial.json":"chunks/tutorial.5ecfce05.mjs","/Users/sandrarodgers/web-next/blog/src/content/blog/settings.json":"chunks/settings.e952b62e.mjs","/Users/sandrarodgers/web-next/blog/src/content/developers/api-specifications/openapi.json":"chunks/openapi.87ee39ab.mjs","/Users/sandrarodgers/web-next/blog/src/content/whitepapers/deepgram-whitepaper-how-deepgram-works.json":"chunks/deepgram-whitepaper-how-deepgram-works.15f2cbf7.mjs","/Users/sandrarodgers/web-next/blog/src/content/whitepapers/deepgram-whitepaper-make-application-voice-ready.json":"chunks/deepgram-whitepaper-make-application-voice-ready.f6bd3af3.mjs","/Users/sandrarodgers/web-next/blog/src/content/whitepapers/deepgram-whitepaper-newsletter.json":"chunks/deepgram-whitepaper-newsletter.a776801d.mjs","/Users/sandrarodgers/web-next/blog/src/content/whitepapers/deepgram-whitepaper-state-of-voice-2022.json":"chunks/deepgram-whitepaper-state-of-voice-2022.cd03fe6c.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/themes/css-variables.json":"chunks/css-variables.fec89dfe.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/themes/dark-plus.json":"chunks/dark-plus.c64d8286.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/themes/dracula-soft.json":"chunks/dracula-soft.97887887.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/themes/dracula.json":"chunks/dracula.16037935.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/themes/github-dark-dimmed.json":"chunks/github-dark-dimmed.887421d6.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/themes/github-dark.json":"chunks/github-dark.a141d561.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/themes/github-light.json":"chunks/github-light.4ab896e1.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/themes/hc_light.json":"chunks/hc_light.cffab4a5.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/themes/light-plus.json":"chunks/light-plus.82aac543.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/themes/material-darker.json":"chunks/material-darker.7d15313f.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/themes/material-default.json":"chunks/material-default.42b48278.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/themes/material-lighter.json":"chunks/material-lighter.3e844679.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/themes/material-ocean.json":"chunks/material-ocean.f900a915.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/themes/material-palenight.json":"chunks/material-palenight.2b604358.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/themes/min-dark.json":"chunks/min-dark.caa582f0.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/themes/min-light.json":"chunks/min-light.39619116.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/themes/monokai.json":"chunks/monokai.3f5e5246.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/themes/nord.json":"chunks/nord.c4e6234a.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/themes/one-dark-pro.json":"chunks/one-dark-pro.508ab27a.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/themes/poimandres.json":"chunks/poimandres.9d3a0da2.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/themes/rose-pine-dawn.json":"chunks/rose-pine-dawn.a1ac07b2.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/themes/rose-pine-moon.json":"chunks/rose-pine-moon.3502176d.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/themes/rose-pine.json":"chunks/rose-pine.4260fdab.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/themes/slack-dark.json":"chunks/slack-dark.ca95ebf9.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/themes/slack-ochin.json":"chunks/slack-ochin.92b84326.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/themes/solarized-dark.json":"chunks/solarized-dark.8c233c43.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/themes/solarized-light.json":"chunks/solarized-light.c6bb6780.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/themes/vitesse-dark.json":"chunks/vitesse-dark.3f9b485b.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/themes/vitesse-light.json":"chunks/vitesse-light.2a8df4bf.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/abap.tmLanguage.json":"chunks/abap.tmLanguage.90f43bd3.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/actionscript-3.tmLanguage.json":"chunks/actionscript-3.tmLanguage.22c45571.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/ada.tmLanguage.json":"chunks/ada.tmLanguage.bbada208.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/apache.tmLanguage.json":"chunks/apache.tmLanguage.14bf6d31.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/apex.tmLanguage.json":"chunks/apex.tmLanguage.53e45f77.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/apl.tmLanguage.json":"chunks/apl.tmLanguage.c2079fda.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/applescript.tmLanguage.json":"chunks/applescript.tmLanguage.3fd248fb.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/asm.tmLanguage.json":"chunks/asm.tmLanguage.15765988.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/astro.tmLanguage.json":"chunks/astro.tmLanguage.48a2894d.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/awk.tmLanguage.json":"chunks/awk.tmLanguage.2f62c203.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/ballerina.tmLanguage.json":"chunks/ballerina.tmLanguage.6024f645.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/bat.tmLanguage.json":"chunks/bat.tmLanguage.cded4316.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/berry.tmLanguage.json":"chunks/berry.tmLanguage.c980beee.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/bibtex.tmLanguage.json":"chunks/bibtex.tmLanguage.fc9af179.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/bicep.tmLanguage.json":"chunks/bicep.tmLanguage.63286f93.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/blade.tmLanguage.json":"chunks/blade.tmLanguage.5ab4f623.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/c.tmLanguage.json":"chunks/c.tmLanguage.cb7fbdd5.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/cadence.tmLanguage.json":"chunks/cadence.tmLanguage.582679a9.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/clarity.tmLanguage.json":"chunks/clarity.tmLanguage.c2023279.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/clojure.tmLanguage.json":"chunks/clojure.tmLanguage.552833f9.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/cmake.tmLanguage.json":"chunks/cmake.tmLanguage.837f2f7d.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/cobol.tmLanguage.json":"chunks/cobol.tmLanguage.1d855399.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/codeql.tmLanguage.json":"chunks/codeql.tmLanguage.ac48a804.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/coffee.tmLanguage.json":"chunks/coffee.tmLanguage.8d9f5e98.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/cpp-macro.tmLanguage.json":"chunks/cpp-macro.tmLanguage.56442b58.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/cpp.tmLanguage.json":"chunks/cpp.tmLanguage.0d62ebb3.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/crystal.tmLanguage.json":"chunks/crystal.tmLanguage.5dba6dad.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/csharp.tmLanguage.json":"chunks/csharp.tmLanguage.e843df0c.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/css.tmLanguage.json":"chunks/css.tmLanguage.aa42be68.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/cue.tmLanguage.json":"chunks/cue.tmLanguage.659a5c98.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/d.tmLanguage.json":"chunks/d.tmLanguage.2f917db0.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/dart.tmLanguage.json":"chunks/dart.tmLanguage.eeb4c7f9.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/diff.tmLanguage.json":"chunks/diff.tmLanguage.d31ff9b6.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/docker.tmLanguage.json":"chunks/docker.tmLanguage.d111d5ed.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/dream-maker.tmLanguage.json":"chunks/dream-maker.tmLanguage.0ffb65b4.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/elixir.tmLanguage.json":"chunks/elixir.tmLanguage.6d46e86b.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/elm.tmLanguage.json":"chunks/elm.tmLanguage.103aa363.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/erb.tmLanguage.json":"chunks/erb.tmLanguage.0fa47828.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/erlang.tmLanguage.json":"chunks/erlang.tmLanguage.cf3fb91b.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/fish.tmLanguage.json":"chunks/fish.tmLanguage.f1f0521b.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/fsharp.tmLanguage.json":"chunks/fsharp.tmLanguage.680ee921.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/gherkin.tmLanguage.json":"chunks/gherkin.tmLanguage.c3b24f9b.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/git-commit.tmLanguage.json":"chunks/git-commit.tmLanguage.14bba07b.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/git-rebase.tmLanguage.json":"chunks/git-rebase.tmLanguage.93cbf6c5.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/glsl.tmLanguage.json":"chunks/glsl.tmLanguage.f2313b4d.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/gnuplot.tmLanguage.json":"chunks/gnuplot.tmLanguage.b75af28f.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/go.tmLanguage.json":"chunks/go.tmLanguage.c2a5df11.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/graphql.tmLanguage.json":"chunks/graphql.tmLanguage.3f9696f4.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/groovy.tmLanguage.json":"chunks/groovy.tmLanguage.4caa187d.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/hack.tmLanguage.json":"chunks/hack.tmLanguage.4b437c29.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/haml.tmLanguage.json":"chunks/haml.tmLanguage.1856c417.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/handlebars.tmLanguage.json":"chunks/handlebars.tmLanguage.2efba05a.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/haskell.tmLanguage.json":"chunks/haskell.tmLanguage.4c61530d.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/hcl.tmLanguage.json":"chunks/hcl.tmLanguage.5d6b7314.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/hlsl.tmLanguage.json":"chunks/hlsl.tmLanguage.613dd968.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/html.tmLanguage.json":"chunks/html.tmLanguage.6fb5149b.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/ini.tmLanguage.json":"chunks/ini.tmLanguage.0cbf1d05.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/java.tmLanguage.json":"chunks/java.tmLanguage.c39bfaf6.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/javascript.tmLanguage.json":"chunks/javascript.tmLanguage.61e966ee.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/jinja-html.tmLanguage.json":"chunks/jinja-html.tmLanguage.39b5f211.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/jinja.tmLanguage.json":"chunks/jinja.tmLanguage.fa903c4e.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/json.tmLanguage.json":"chunks/json.tmLanguage.74144e27.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/jsonc.tmLanguage.json":"chunks/jsonc.tmLanguage.1a3b9109.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/jsonnet.tmLanguage.json":"chunks/jsonnet.tmLanguage.8d48996b.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/jssm.tmLanguage.json":"chunks/jssm.tmLanguage.6c1b9158.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/jsx.tmLanguage.json":"chunks/jsx.tmLanguage.583f1a16.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/julia.tmLanguage.json":"chunks/julia.tmLanguage.0a8b94e7.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/kotlin.tmLanguage.json":"chunks/kotlin.tmLanguage.e8a6b7a6.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/latex.tmLanguage.json":"chunks/latex.tmLanguage.ecadecb8.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/less.tmLanguage.json":"chunks/less.tmLanguage.bd7b8d56.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/liquid.tmLanguage.json":"chunks/liquid.tmLanguage.7754d6e3.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/lisp.tmLanguage.json":"chunks/lisp.tmLanguage.5f9b63b2.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/logo.tmLanguage.json":"chunks/logo.tmLanguage.b0c8f7e2.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/lua.tmLanguage.json":"chunks/lua.tmLanguage.8d5fb6ef.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/make.tmLanguage.json":"chunks/make.tmLanguage.c2039eb5.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/markdown.tmLanguage.json":"chunks/markdown.tmLanguage.a4092da8.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/marko.tmLanguage.json":"chunks/marko.tmLanguage.ba870c0b.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/matlab.tmLanguage.json":"chunks/matlab.tmLanguage.59d7a9f2.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/mdx.tmLanguage.json":"chunks/mdx.tmLanguage.55107cd9.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/mermaid.tmLanguage.json":"chunks/mermaid.tmLanguage.4de46447.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/nginx.tmLanguage.json":"chunks/nginx.tmLanguage.b75b10ef.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/nim.tmLanguage.json":"chunks/nim.tmLanguage.c387f2c9.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/nix.tmLanguage.json":"chunks/nix.tmLanguage.59e90ede.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/objective-c.tmLanguage.json":"chunks/objective-c.tmLanguage.20751fe3.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/objective-cpp.tmLanguage.json":"chunks/objective-cpp.tmLanguage.35952028.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/ocaml.tmLanguage.json":"chunks/ocaml.tmLanguage.0143759c.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/pascal.tmLanguage.json":"chunks/pascal.tmLanguage.24002509.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/perl.tmLanguage.json":"chunks/perl.tmLanguage.95aaa323.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/php-html.tmLanguage.json":"chunks/php-html.tmLanguage.9bf25695.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/php.tmLanguage.json":"chunks/php.tmLanguage.208cc284.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/plsql.tmLanguage.json":"chunks/plsql.tmLanguage.2f001168.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/postcss.tmLanguage.json":"chunks/postcss.tmLanguage.df844470.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/powershell.tmLanguage.json":"chunks/powershell.tmLanguage.557fecb1.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/prisma.tmLanguage.json":"chunks/prisma.tmLanguage.3d2ecd3f.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/prolog.tmLanguage.json":"chunks/prolog.tmLanguage.6aaa58fd.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/pug.tmLanguage.json":"chunks/pug.tmLanguage.923cd00e.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/puppet.tmLanguage.json":"chunks/puppet.tmLanguage.7c62b6f0.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/purescript.tmLanguage.json":"chunks/purescript.tmLanguage.a1fbe8e9.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/python.tmLanguage.json":"chunks/python.tmLanguage.255784a7.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/r.tmLanguage.json":"chunks/r.tmLanguage.27744799.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/raku.tmLanguage.json":"chunks/raku.tmLanguage.3eec78ae.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/razor.tmLanguage.json":"chunks/razor.tmLanguage.423995f0.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/rel.tmLanguage.json":"chunks/rel.tmLanguage.8d9faf37.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/riscv.tmLanguage.json":"chunks/riscv.tmLanguage.86c81d11.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/rst.tmLanguage.json":"chunks/rst.tmLanguage.3203d5d2.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/ruby.tmLanguage.json":"chunks/ruby.tmLanguage.5878ff9e.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/rust.tmLanguage.json":"chunks/rust.tmLanguage.ca198b9a.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/sas.tmLanguage.json":"chunks/sas.tmLanguage.96dffcab.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/sass.tmLanguage.json":"chunks/sass.tmLanguage.69993358.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/scala.tmLanguage.json":"chunks/scala.tmLanguage.f0618f94.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/scheme.tmLanguage.json":"chunks/scheme.tmLanguage.43867c45.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/scss.tmLanguage.json":"chunks/scss.tmLanguage.34ac990b.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/shaderlab.tmLanguage.json":"chunks/shaderlab.tmLanguage.32cc3af0.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/shellscript.tmLanguage.json":"chunks/shellscript.tmLanguage.709c69f9.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/smalltalk.tmLanguage.json":"chunks/smalltalk.tmLanguage.bed30313.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/solidity.tmLanguage.json":"chunks/solidity.tmLanguage.f49e6b87.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/sparql.tmLanguage.json":"chunks/sparql.tmLanguage.cca7e4fb.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/sql.tmLanguage.json":"chunks/sql.tmLanguage.53f84ea8.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/ssh-config.tmLanguage.json":"chunks/ssh-config.tmLanguage.88607f95.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/stata.tmLanguage.json":"chunks/stata.tmLanguage.7941b321.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/stylus.tmLanguage.json":"chunks/stylus.tmLanguage.aae41083.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/svelte.tmLanguage.json":"chunks/svelte.tmLanguage.51e3e183.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/swift.tmLanguage.json":"chunks/swift.tmLanguage.1758b78f.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/system-verilog.tmLanguage.json":"chunks/system-verilog.tmLanguage.98c0822c.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/tasl.tmLanguage.json":"chunks/tasl.tmLanguage.f048ca02.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/tcl.tmLanguage.json":"chunks/tcl.tmLanguage.331e619d.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/tex.tmLanguage.json":"chunks/tex.tmLanguage.378e91de.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/toml.tmLanguage.json":"chunks/toml.tmLanguage.ac48c2b1.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/tsx.tmLanguage.json":"chunks/tsx.tmLanguage.8c2c7b1b.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/turtle.tmLanguage.json":"chunks/turtle.tmLanguage.81eec047.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/twig.tmLanguage.json":"chunks/twig.tmLanguage.cdc9b736.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/typescript.tmLanguage.json":"chunks/typescript.tmLanguage.e7dbfd15.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/vb.tmLanguage.json":"chunks/vb.tmLanguage.b376ae92.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/verilog.tmLanguage.json":"chunks/verilog.tmLanguage.6c2eff21.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/vhdl.tmLanguage.json":"chunks/vhdl.tmLanguage.336d9759.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/viml.tmLanguage.json":"chunks/viml.tmLanguage.bf2daa01.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/vue-html.tmLanguage.json":"chunks/vue-html.tmLanguage.3a2e7543.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/vue.tmLanguage.json":"chunks/vue.tmLanguage.c77b2cf0.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/wasm.tmLanguage.json":"chunks/wasm.tmLanguage.b7f5d22e.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/wenyan.tmLanguage.json":"chunks/wenyan.tmLanguage.5d7089b7.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/xml.tmLanguage.json":"chunks/xml.tmLanguage.f76daefd.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/xsl.tmLanguage.json":"chunks/xsl.tmLanguage.ab9f8922.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/yaml.tmLanguage.json":"chunks/yaml.tmLanguage.2e704356.mjs","/Users/sandrarodgers/web-next/blog/node_modules/shiki/languages/zenscript.tmLanguage.json":"chunks/zenscript.tmLanguage.7f56cf0d.mjs","/Users/sandrarodgers/web-next/blog/src/shared/components/search/DocSearch":"DocSearch.38321d04.js","@astrojs/vue/client.js":"client.bbed378f.js","@astrojs/preact/client.js":"client.cfb9219f.js","/astro/hoisted.js?q=0":"hoisted.d8f274fa.js","/astro/hoisted.js?q=1":"hoisted.35d832ba.js","astro:scripts/page.js":"page.a5a3d0ae.js","/Users/sandrarodgers/web-next/blog/node_modules/@preact/signals/dist/signals.module.js":"chunks/signals.module.76e6dbda.js","astro:scripts/before-hydration.js":""},"assets":["/assets/Inter-ThinItalic.d82beee8.woff2","/assets/Inter-ExtraLight.b6cd094a.woff2","/assets/Inter-ExtraLightItalic.db229bf3.woff2","/assets/Inter-Thin.77d96c1c.woff2","/assets/Inter-LightItalic.737ac201.woff2","/assets/Inter-Light.36b86832.woff2","/assets/Inter-Regular.d612f121.woff2","/assets/Inter-Italic.900058df.woff2","/assets/Inter-Bold.c63158ba.woff2","/assets/Inter-MediumItalic.81600858.woff2","/assets/Inter-Medium.1b498b95.woff2","/assets/Inter-SemiBoldItalic.3b6df7d0.woff2","/assets/Inter-BoldItalic.3f211964.woff2","/assets/Inter-SemiBold.15226129.woff2","/assets/Inter-ExtraBold.307d9809.woff2","/assets/Inter-BlackItalic.bc80081d.woff2","/assets/Inter-Black.fc10113c.woff2","/assets/Inter-ExtraBoldItalic.cf6b1d6c.woff2","/assets/Inter-roman.var.17fe38ab.woff2","/assets/Inter-italic.var.d1401419.woff2","/assets/ABCFavorit-Bold.0be5b4a5.woff2","/assets/FiraCode-Light.9a0ab96c.ttf","/assets/Inter.var.85f08b5f.woff2","/assets/Inter-Thin.e6bced8e.woff","/assets/Inter-ExtraLightItalic.32e53d8a.woff","/assets/Inter-ThinItalic.70648e9b.woff","/assets/Inter-ExtraLight.015dad27.woff","/assets/Inter-LightItalic.7d291e85.woff","/assets/Inter-Light.4871aed0.woff","/assets/Inter-Italic.cd1eda97.woff","/assets/Inter-Regular.ef1f23c0.woff","/assets/Inter-Bold.3e242080.woff","/assets/Inter-MediumItalic.205c8989.woff","/assets/Inter-BoldItalic.ace8e094.woff","/assets/Inter-Medium.53deda46.woff","/assets/Inter-SemiBold.653fed7a.woff","/assets/Inter-SemiBoldItalic.95e68b6b.woff","/assets/Inter-ExtraBold.f053602c.woff","/assets/Inter-BlackItalic.87235581.woff","/assets/Inter-Black.8b21d5be.woff","/assets/Inter-ExtraBoldItalic.6deefddf.woff","/assets/_...page_.6c29be62.css","/assets/_...page_.8ae06e0a.css","/assets/_slug_.c2747b55.css","/assets/_slug_.13065b2e.css","/assets/_slug_.0da0288b.css","/assets/_slug_.3ebf720f.css","/assets/_slug_.99ec8802.css","/assets/index.9c23bf80.css","/DocSearch.38321d04.js","/_headers","/_redirects","/asr-comparison-promo-image.png","/asr-comparison-promo-image@2x.png","/build-something-great-with-voice.png","/client.bbed378f.js","/client.cfb9219f.js","/favicon.ico","/gear-question-how-dg-works-image@2x.png","/hoisted.35d832ba.js","/hoisted.d8f274fa.js","/humans.txt","/logo.svg","/netlify.toml","/page.a5a3d0ae.js","/quote-marks-make-app-voice-ready-image.svg","/red-green-orb-voice-report-image@2x.png","/robots.txt","/sound-wave-cloud-dark.svg","/sound-wave@2x.svg","/starburst-gradient.svg","/tag-gradient.svg","/chunks/Default.astro_astro_type_script_index_0_lang.0db659d0.js","/chunks/hooks.module.a05e4479.js","/chunks/preact.module.7d28e569.js","/chunks/signals.module.76e6dbda.js","/page.a5a3d0ae.js"]}), {
	pageMap: pageMap,
	renderers: renderers
});
const _args = {};
const _exports = adapter.createExports(_manifest, _args);
const handler = _exports['handler'];

const _start = 'start';
if(_start in adapter) {
	adapter[_start](_manifest, _args);
}

export { createComponent as a, renderHead as b, createAstro as c, renderComponent as d, handler, pageMap, renderTemplate as r, renderers };
