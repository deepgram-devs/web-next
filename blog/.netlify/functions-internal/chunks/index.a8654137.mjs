import { c as createAstro, a as createComponent, r as renderTemplate, b as renderHead, d as renderComponent } from '../entry.mjs';
import Slugger from 'github-slugger';
import '@astrojs/netlify/netlify-functions.js';
import 'preact';
import 'preact-render-to-string';
import 'vue';
import 'vue/server-renderer';
import 'html-escaper';
import 'node-html-parser';
import 'axios';
/* empty css                           *//* empty css                           *//* empty css                           */import '@storyblok/js';
/* empty css                           *//* empty css                          */import 'clone-deep';
import 'slugify';
import 'shiki';
/* empty css                           */import 'camelcase';
import '@astrojs/rss';
/* empty css                           */import 'mime';
import 'cookie';
import 'kleur/colors';
import 'string-width';
import 'path-browserify';
import 'path-to-regexp';

const metadata = { "headings": [{ "depth": 2, "slug": "early-years-hidden-markov-models-and-trigram-models", "text": "Early Years: Hidden Markov Models and Trigram Models" }, { "depth": 2, "slug": "new-generation-of-asr-neural-networks", "text": "New Generation of ASR: Neural Networks" }, { "depth": 2, "slug": "new-revolution-in-asr-deep-learning", "text": "New Revolution in ASR: Deep Learning" }], "source": `The most exciting time to be in the Automatic Speech Recognition (ASR) space is right now. Consumers are using Siri and Alexa daily to ask questions, order products, play music, play games, and do simple tasks. This is the norm, and it started less than 15 years ago with Google Voice Search. On the enterprise side, we see [voicebots & conversational AI](https://deepgram.com/solutions/voicebots/), and [speech analytics](https://deepgram.com/solutions/speech-analytics/) that can determine [sentiment and emotions](https://blog.deepgram.com/sentiment-analysis-emotion-regulation-difference/) as well as [languages](https://deepgram.com/product/languages/).

## **Early Years: Hidden Markov Models and Trigram Models**

The history of Automatic Speech Recognition started in 1952 with Bell Labs and a program called [Audrey](https://astaspeaks.wordpress.com/2014/10/13/audrey-the-first-speech-recognition-system/), which could transcribe simple numbers. The next breakthrough did not occur until the mid-1970 when researchers started using [Hidden Markov Models](https://jonathan-hui.medium.com/speech-recognition-gmm-hmm-8bb5eff8b196) (HMM).HMM uses probability functions to determine the correct words to transcribe. These ASR speech models take snippets of audio to determine the smallest unit of sound for a word or what is called a [phoneme](https://www.britannica.com/topic/phoneme).  The phoneme is then fed into another program that uses the HMM to guess the right word using a most common word probability function. These serial processing models are refined by adding noise reduction upfront and [beam search language models](https://towardsdatascience.com/an-intuitive-explanation-of-beam-search-9b1d744e7a0f) on the back end to create understandable text and sentences. Bean search is a time-dependent probability function and looks at the transcribed words before and after the target word to find the best fit for the target word. This whole serial process is called the ["trigram" model](https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9), and 80% of the ASR technology currently being used is a refined version of this 1970's model.

## **New Generation of ASR: Neural Networks**

The next big breakthrough came in the late 1980s with the addition of neural networks. This was also an inflection point for ASR. Most researchers and companies use these neural networks to improve their current trigram models with better upfront audio phoneme differentiation or better backend text and sentence creation. This trigram model works very well for consumer devices like Alexa and Siri that only have a small set of voice commands to respond to.  However, this model is not as effective with enterprise use cases, like meetings, phone calls, and automated voicebots. The refined trigram models require huge amounts of processing power to provide accurate transcription at speed. Businesses need to trade speed for accuracy or accuracy for costs. 

## **New Revolution in ASR: Deep Learning**

Other researchers believed that neural networks were the key to having a new type of ASR. With the advent of big data, faster computers, and graphical processing unit (GPU) processing, a new ASR method was developed, [end-to-end deep learning ASR](https://heartbeat.fritz.ai/the-3-deep-learning-frameworks-for-end-to-end-speech-recognition-that-power-your-devices-37b891ddc380). This new ASR method could "learn" and be "trained" to become more accurate as more data is fed into the neural networks. No more developers re-coding each part of the trigram serial model to add new languages, parse accents, reduce noise, and add new words. The other big advantage of using an end-to-end deep learning ASR is that you can have the [accuracy, speed, and scalability without sacrificing costs](https://offers.deepgram.com/how-to-evaluate-deep-learning-asr-platform-solution-brief).

![history of speech recognition](https://res.cloudinary.com/deepgram/image/upload/v1661976833/blog/the-history-of-automatic-speech-recognition/history-hmm-v-dg_2%402x-1024x580.png) 

*History of Speech Recognition and Hidden Markov Models*

This is how [Deepgram](https://deepgram.com/company/about/) was born; out of research that did not look at refining a 50-year-old ASR model but starting from deep learning neural networks. Check out the entire history of ASR in the image above. [Contact us](https://deepgram.com/contact-us/) to learn how you can decrease word error rate systematically without compromising speed, scale, or cost, or sign up for a [free API key](https://console.deepgram.com/signup) to get started today.

<WhitepaperPromo whitepaper="latest"></WhitepaperPromo>`, "html": '<p>The most exciting time to be in the Automatic Speech Recognition (ASR) space is right now. Consumers are using Siri and Alexa daily to ask questions, order products, play music, play games, and do simple tasks. This is the norm, and it started less than 15 years ago with Google Voice Search. On the enterprise side, we see <a href="https://deepgram.com/solutions/voicebots/">voicebots & conversational AI</a>, and <a href="https://deepgram.com/solutions/speech-analytics/">speech analytics</a> that can determine <a href="https://blog.deepgram.com/sentiment-analysis-emotion-regulation-difference/">sentiment and emotions</a> as well as <a href="https://deepgram.com/product/languages/">languages</a>.</p>\n<h2 id="early-years-hidden-markov-models-and-trigram-models"><strong>Early Years: Hidden Markov Models and Trigram Models</strong></h2>\n<p>The history of Automatic Speech Recognition started in 1952 with Bell Labs and a program called <a href="https://astaspeaks.wordpress.com/2014/10/13/audrey-the-first-speech-recognition-system/">Audrey</a>, which could transcribe simple numbers. The next breakthrough did not occur until the mid-1970 when researchers started using <a href="https://jonathan-hui.medium.com/speech-recognition-gmm-hmm-8bb5eff8b196">Hidden Markov Models</a> (HMM).HMM uses probability functions to determine the correct words to transcribe. These ASR speech models take snippets of audio to determine the smallest unit of sound for a word or what is called a <a href="https://www.britannica.com/topic/phoneme">phoneme</a>.  The phoneme is then fed into another program that uses the HMM to guess the right word using a most common word probability function. These serial processing models are refined by adding noise reduction upfront and <a href="https://towardsdatascience.com/an-intuitive-explanation-of-beam-search-9b1d744e7a0f">beam search language models</a> on the back end to create understandable text and sentences. Bean search is a time-dependent probability function and looks at the transcribed words before and after the target word to find the best fit for the target word. This whole serial process is called the <a href="https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9">\u201Ctrigram\u201D model</a>, and 80% of the ASR technology currently being used is a refined version of this 1970\u2019s model.</p>\n<h2 id="new-generation-of-asr-neural-networks"><strong>New Generation of ASR: Neural Networks</strong></h2>\n<p>The next big breakthrough came in the late 1980s with the addition of neural networks. This was also an inflection point for ASR. Most researchers and companies use these neural networks to improve their current trigram models with better upfront audio phoneme differentiation or better backend text and sentence creation. This trigram model works very well for consumer devices like Alexa and Siri that only have a small set of voice commands to respond to.  However, this model is not as effective with enterprise use cases, like meetings, phone calls, and automated voicebots. The refined trigram models require huge amounts of processing power to provide accurate transcription at speed. Businesses need to trade speed for accuracy or accuracy for costs.</p>\n<h2 id="new-revolution-in-asr-deep-learning"><strong>New Revolution in ASR: Deep Learning</strong></h2>\n<p>Other researchers believed that neural networks were the key to having a new type of ASR. With the advent of big data, faster computers, and graphical processing unit (GPU) processing, a new ASR method was developed, <a href="https://heartbeat.fritz.ai/the-3-deep-learning-frameworks-for-end-to-end-speech-recognition-that-power-your-devices-37b891ddc380">end-to-end deep learning ASR</a>. This new ASR method could \u201Clearn\u201D and be \u201Ctrained\u201D to become more accurate as more data is fed into the neural networks. No more developers re-coding each part of the trigram serial model to add new languages, parse accents, reduce noise, and add new words. The other big advantage of using an end-to-end deep learning ASR is that you can have the <a href="https://offers.deepgram.com/how-to-evaluate-deep-learning-asr-platform-solution-brief">accuracy, speed, and scalability without sacrificing costs</a>.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976833/blog/the-history-of-automatic-speech-recognition/history-hmm-v-dg_2%402x-1024x580.png" alt="history of speech recognition"></p>\n<p><em>History of Speech Recognition and Hidden Markov Models</em></p>\n<p>This is how <a href="https://deepgram.com/company/about/">Deepgram</a> was born; out of research that did not look at refining a 50-year-old ASR model but starting from deep learning neural networks. Check out the entire history of ASR in the image above. <a href="https://deepgram.com/contact-us/">Contact us</a> to learn how you can decrease word error rate systematically without compromising speed, scale, or cost, or sign up for a <a href="https://console.deepgram.com/signup">free API key</a> to get started today.</p>\n<WhitepaperPromo whitepaper="latest" />' };
const frontmatter = { "title": "The History of Automatic Speech Recognition", "description": "Learn about the history of automatic speech recognition (ASR) and how end-to-end deep learning is creating a new revolution in ASR.", "date": "2021-01-15T00:00:00.000Z", "cover": "https://res.cloudinary.com/deepgram/image/upload/v1661981358/blog/the-history-of-automatic-speech-recognition/history-of-asr-infogfx%402x.jpg", "authors": ["keith-lam"], "category": "speech-trends", "tags": ["education"], "seo": { "title": "The History of Automatic Speech Recognition", "description": "Learn about the history of automatic speech recognition (ASR) and how end-to-end deep learning is creating a new revolution in ASR." }, "og": { "image": "https://res.cloudinary.com/deepgram/image/upload/v1661981358/blog/the-history-of-automatic-speech-recognition/history-of-asr-infogfx%402x.jpg" }, "shorturls": { "share": "https://dpgr.am/799a8c3", "twitter": "https://dpgr.am/bfedebb", "linkedin": "https://dpgr.am/4fd7971", "reddit": "https://dpgr.am/1e7447d", "facebook": "https://dpgr.am/6844267" }, "astro": { "headings": [{ "depth": 2, "slug": "early-years-hidden-markov-models-and-trigram-models", "text": "Early Years: Hidden Markov Models and Trigram Models" }, { "depth": 2, "slug": "new-generation-of-asr-neural-networks", "text": "New Generation of ASR: Neural Networks" }, { "depth": 2, "slug": "new-revolution-in-asr-deep-learning", "text": "New Revolution in ASR: Deep Learning" }], "source": `The most exciting time to be in the Automatic Speech Recognition (ASR) space is right now. Consumers are using Siri and Alexa daily to ask questions, order products, play music, play games, and do simple tasks. This is the norm, and it started less than 15 years ago with Google Voice Search. On the enterprise side, we see [voicebots & conversational AI](https://deepgram.com/solutions/voicebots/), and [speech analytics](https://deepgram.com/solutions/speech-analytics/) that can determine [sentiment and emotions](https://blog.deepgram.com/sentiment-analysis-emotion-regulation-difference/) as well as [languages](https://deepgram.com/product/languages/).

## **Early Years: Hidden Markov Models and Trigram Models**

The history of Automatic Speech Recognition started in 1952 with Bell Labs and a program called [Audrey](https://astaspeaks.wordpress.com/2014/10/13/audrey-the-first-speech-recognition-system/), which could transcribe simple numbers. The next breakthrough did not occur until the mid-1970 when researchers started using [Hidden Markov Models](https://jonathan-hui.medium.com/speech-recognition-gmm-hmm-8bb5eff8b196) (HMM).HMM uses probability functions to determine the correct words to transcribe. These ASR speech models take snippets of audio to determine the smallest unit of sound for a word or what is called a [phoneme](https://www.britannica.com/topic/phoneme).  The phoneme is then fed into another program that uses the HMM to guess the right word using a most common word probability function. These serial processing models are refined by adding noise reduction upfront and [beam search language models](https://towardsdatascience.com/an-intuitive-explanation-of-beam-search-9b1d744e7a0f) on the back end to create understandable text and sentences. Bean search is a time-dependent probability function and looks at the transcribed words before and after the target word to find the best fit for the target word. This whole serial process is called the ["trigram" model](https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9), and 80% of the ASR technology currently being used is a refined version of this 1970's model.

## **New Generation of ASR: Neural Networks**

The next big breakthrough came in the late 1980s with the addition of neural networks. This was also an inflection point for ASR. Most researchers and companies use these neural networks to improve their current trigram models with better upfront audio phoneme differentiation or better backend text and sentence creation. This trigram model works very well for consumer devices like Alexa and Siri that only have a small set of voice commands to respond to.  However, this model is not as effective with enterprise use cases, like meetings, phone calls, and automated voicebots. The refined trigram models require huge amounts of processing power to provide accurate transcription at speed. Businesses need to trade speed for accuracy or accuracy for costs. 

## **New Revolution in ASR: Deep Learning**

Other researchers believed that neural networks were the key to having a new type of ASR. With the advent of big data, faster computers, and graphical processing unit (GPU) processing, a new ASR method was developed, [end-to-end deep learning ASR](https://heartbeat.fritz.ai/the-3-deep-learning-frameworks-for-end-to-end-speech-recognition-that-power-your-devices-37b891ddc380). This new ASR method could "learn" and be "trained" to become more accurate as more data is fed into the neural networks. No more developers re-coding each part of the trigram serial model to add new languages, parse accents, reduce noise, and add new words. The other big advantage of using an end-to-end deep learning ASR is that you can have the [accuracy, speed, and scalability without sacrificing costs](https://offers.deepgram.com/how-to-evaluate-deep-learning-asr-platform-solution-brief).

![history of speech recognition](https://res.cloudinary.com/deepgram/image/upload/v1661976833/blog/the-history-of-automatic-speech-recognition/history-hmm-v-dg_2%402x-1024x580.png) 

*History of Speech Recognition and Hidden Markov Models*

This is how [Deepgram](https://deepgram.com/company/about/) was born; out of research that did not look at refining a 50-year-old ASR model but starting from deep learning neural networks. Check out the entire history of ASR in the image above. [Contact us](https://deepgram.com/contact-us/) to learn how you can decrease word error rate systematically without compromising speed, scale, or cost, or sign up for a [free API key](https://console.deepgram.com/signup) to get started today.

<WhitepaperPromo whitepaper="latest"></WhitepaperPromo>`, "html": '<p>The most exciting time to be in the Automatic Speech Recognition (ASR) space is right now. Consumers are using Siri and Alexa daily to ask questions, order products, play music, play games, and do simple tasks. This is the norm, and it started less than 15 years ago with Google Voice Search. On the enterprise side, we see <a href="https://deepgram.com/solutions/voicebots/">voicebots & conversational AI</a>, and <a href="https://deepgram.com/solutions/speech-analytics/">speech analytics</a> that can determine <a href="https://blog.deepgram.com/sentiment-analysis-emotion-regulation-difference/">sentiment and emotions</a> as well as <a href="https://deepgram.com/product/languages/">languages</a>.</p>\n<h2 id="early-years-hidden-markov-models-and-trigram-models"><strong>Early Years: Hidden Markov Models and Trigram Models</strong></h2>\n<p>The history of Automatic Speech Recognition started in 1952 with Bell Labs and a program called <a href="https://astaspeaks.wordpress.com/2014/10/13/audrey-the-first-speech-recognition-system/">Audrey</a>, which could transcribe simple numbers. The next breakthrough did not occur until the mid-1970 when researchers started using <a href="https://jonathan-hui.medium.com/speech-recognition-gmm-hmm-8bb5eff8b196">Hidden Markov Models</a> (HMM).HMM uses probability functions to determine the correct words to transcribe. These ASR speech models take snippets of audio to determine the smallest unit of sound for a word or what is called a <a href="https://www.britannica.com/topic/phoneme">phoneme</a>.  The phoneme is then fed into another program that uses the HMM to guess the right word using a most common word probability function. These serial processing models are refined by adding noise reduction upfront and <a href="https://towardsdatascience.com/an-intuitive-explanation-of-beam-search-9b1d744e7a0f">beam search language models</a> on the back end to create understandable text and sentences. Bean search is a time-dependent probability function and looks at the transcribed words before and after the target word to find the best fit for the target word. This whole serial process is called the <a href="https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9">\u201Ctrigram\u201D model</a>, and 80% of the ASR technology currently being used is a refined version of this 1970\u2019s model.</p>\n<h2 id="new-generation-of-asr-neural-networks"><strong>New Generation of ASR: Neural Networks</strong></h2>\n<p>The next big breakthrough came in the late 1980s with the addition of neural networks. This was also an inflection point for ASR. Most researchers and companies use these neural networks to improve their current trigram models with better upfront audio phoneme differentiation or better backend text and sentence creation. This trigram model works very well for consumer devices like Alexa and Siri that only have a small set of voice commands to respond to.  However, this model is not as effective with enterprise use cases, like meetings, phone calls, and automated voicebots. The refined trigram models require huge amounts of processing power to provide accurate transcription at speed. Businesses need to trade speed for accuracy or accuracy for costs.</p>\n<h2 id="new-revolution-in-asr-deep-learning"><strong>New Revolution in ASR: Deep Learning</strong></h2>\n<p>Other researchers believed that neural networks were the key to having a new type of ASR. With the advent of big data, faster computers, and graphical processing unit (GPU) processing, a new ASR method was developed, <a href="https://heartbeat.fritz.ai/the-3-deep-learning-frameworks-for-end-to-end-speech-recognition-that-power-your-devices-37b891ddc380">end-to-end deep learning ASR</a>. This new ASR method could \u201Clearn\u201D and be \u201Ctrained\u201D to become more accurate as more data is fed into the neural networks. No more developers re-coding each part of the trigram serial model to add new languages, parse accents, reduce noise, and add new words. The other big advantage of using an end-to-end deep learning ASR is that you can have the <a href="https://offers.deepgram.com/how-to-evaluate-deep-learning-asr-platform-solution-brief">accuracy, speed, and scalability without sacrificing costs</a>.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976833/blog/the-history-of-automatic-speech-recognition/history-hmm-v-dg_2%402x-1024x580.png" alt="history of speech recognition"></p>\n<p><em>History of Speech Recognition and Hidden Markov Models</em></p>\n<p>This is how <a href="https://deepgram.com/company/about/">Deepgram</a> was born; out of research that did not look at refining a 50-year-old ASR model but starting from deep learning neural networks. Check out the entire history of ASR in the image above. <a href="https://deepgram.com/contact-us/">Contact us</a> to learn how you can decrease word error rate systematically without compromising speed, scale, or cost, or sign up for a <a href="https://console.deepgram.com/signup">free API key</a> to get started today.</p>\n<WhitepaperPromo whitepaper="latest" />' }, "file": "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/the-history-of-automatic-speech-recognition/index.md" };
function rawContent() {
  return `The most exciting time to be in the Automatic Speech Recognition (ASR) space is right now. Consumers are using Siri and Alexa daily to ask questions, order products, play music, play games, and do simple tasks. This is the norm, and it started less than 15 years ago with Google Voice Search. On the enterprise side, we see [voicebots & conversational AI](https://deepgram.com/solutions/voicebots/), and [speech analytics](https://deepgram.com/solutions/speech-analytics/) that can determine [sentiment and emotions](https://blog.deepgram.com/sentiment-analysis-emotion-regulation-difference/) as well as [languages](https://deepgram.com/product/languages/).

## **Early Years: Hidden Markov Models and Trigram Models**

The history of Automatic Speech Recognition started in 1952 with Bell Labs and a program called [Audrey](https://astaspeaks.wordpress.com/2014/10/13/audrey-the-first-speech-recognition-system/), which could transcribe simple numbers. The next breakthrough did not occur until the mid-1970 when researchers started using [Hidden Markov Models](https://jonathan-hui.medium.com/speech-recognition-gmm-hmm-8bb5eff8b196) (HMM).HMM uses probability functions to determine the correct words to transcribe. These ASR speech models take snippets of audio to determine the smallest unit of sound for a word or what is called a [phoneme](https://www.britannica.com/topic/phoneme).  The phoneme is then fed into another program that uses the HMM to guess the right word using a most common word probability function. These serial processing models are refined by adding noise reduction upfront and [beam search language models](https://towardsdatascience.com/an-intuitive-explanation-of-beam-search-9b1d744e7a0f) on the back end to create understandable text and sentences. Bean search is a time-dependent probability function and looks at the transcribed words before and after the target word to find the best fit for the target word. This whole serial process is called the ["trigram" model](https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9), and 80% of the ASR technology currently being used is a refined version of this 1970's model.

## **New Generation of ASR: Neural Networks**

The next big breakthrough came in the late 1980s with the addition of neural networks. This was also an inflection point for ASR. Most researchers and companies use these neural networks to improve their current trigram models with better upfront audio phoneme differentiation or better backend text and sentence creation. This trigram model works very well for consumer devices like Alexa and Siri that only have a small set of voice commands to respond to.  However, this model is not as effective with enterprise use cases, like meetings, phone calls, and automated voicebots. The refined trigram models require huge amounts of processing power to provide accurate transcription at speed. Businesses need to trade speed for accuracy or accuracy for costs. 

## **New Revolution in ASR: Deep Learning**

Other researchers believed that neural networks were the key to having a new type of ASR. With the advent of big data, faster computers, and graphical processing unit (GPU) processing, a new ASR method was developed, [end-to-end deep learning ASR](https://heartbeat.fritz.ai/the-3-deep-learning-frameworks-for-end-to-end-speech-recognition-that-power-your-devices-37b891ddc380). This new ASR method could "learn" and be "trained" to become more accurate as more data is fed into the neural networks. No more developers re-coding each part of the trigram serial model to add new languages, parse accents, reduce noise, and add new words. The other big advantage of using an end-to-end deep learning ASR is that you can have the [accuracy, speed, and scalability without sacrificing costs](https://offers.deepgram.com/how-to-evaluate-deep-learning-asr-platform-solution-brief).

![history of speech recognition](https://res.cloudinary.com/deepgram/image/upload/v1661976833/blog/the-history-of-automatic-speech-recognition/history-hmm-v-dg_2%402x-1024x580.png) 

*History of Speech Recognition and Hidden Markov Models*

This is how [Deepgram](https://deepgram.com/company/about/) was born; out of research that did not look at refining a 50-year-old ASR model but starting from deep learning neural networks. Check out the entire history of ASR in the image above. [Contact us](https://deepgram.com/contact-us/) to learn how you can decrease word error rate systematically without compromising speed, scale, or cost, or sign up for a [free API key](https://console.deepgram.com/signup) to get started today.

<WhitepaperPromo whitepaper="latest"></WhitepaperPromo>`;
}
function compiledContent() {
  return '<p>The most exciting time to be in the Automatic Speech Recognition (ASR) space is right now. Consumers are using Siri and Alexa daily to ask questions, order products, play music, play games, and do simple tasks. This is the norm, and it started less than 15 years ago with Google Voice Search. On the enterprise side, we see <a href="https://deepgram.com/solutions/voicebots/">voicebots & conversational AI</a>, and <a href="https://deepgram.com/solutions/speech-analytics/">speech analytics</a> that can determine <a href="https://blog.deepgram.com/sentiment-analysis-emotion-regulation-difference/">sentiment and emotions</a> as well as <a href="https://deepgram.com/product/languages/">languages</a>.</p>\n<h2 id="early-years-hidden-markov-models-and-trigram-models"><strong>Early Years: Hidden Markov Models and Trigram Models</strong></h2>\n<p>The history of Automatic Speech Recognition started in 1952 with Bell Labs and a program called <a href="https://astaspeaks.wordpress.com/2014/10/13/audrey-the-first-speech-recognition-system/">Audrey</a>, which could transcribe simple numbers. The next breakthrough did not occur until the mid-1970 when researchers started using <a href="https://jonathan-hui.medium.com/speech-recognition-gmm-hmm-8bb5eff8b196">Hidden Markov Models</a> (HMM).HMM uses probability functions to determine the correct words to transcribe. These ASR speech models take snippets of audio to determine the smallest unit of sound for a word or what is called a <a href="https://www.britannica.com/topic/phoneme">phoneme</a>.  The phoneme is then fed into another program that uses the HMM to guess the right word using a most common word probability function. These serial processing models are refined by adding noise reduction upfront and <a href="https://towardsdatascience.com/an-intuitive-explanation-of-beam-search-9b1d744e7a0f">beam search language models</a> on the back end to create understandable text and sentences. Bean search is a time-dependent probability function and looks at the transcribed words before and after the target word to find the best fit for the target word. This whole serial process is called the <a href="https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9">\u201Ctrigram\u201D model</a>, and 80% of the ASR technology currently being used is a refined version of this 1970\u2019s model.</p>\n<h2 id="new-generation-of-asr-neural-networks"><strong>New Generation of ASR: Neural Networks</strong></h2>\n<p>The next big breakthrough came in the late 1980s with the addition of neural networks. This was also an inflection point for ASR. Most researchers and companies use these neural networks to improve their current trigram models with better upfront audio phoneme differentiation or better backend text and sentence creation. This trigram model works very well for consumer devices like Alexa and Siri that only have a small set of voice commands to respond to.  However, this model is not as effective with enterprise use cases, like meetings, phone calls, and automated voicebots. The refined trigram models require huge amounts of processing power to provide accurate transcription at speed. Businesses need to trade speed for accuracy or accuracy for costs.</p>\n<h2 id="new-revolution-in-asr-deep-learning"><strong>New Revolution in ASR: Deep Learning</strong></h2>\n<p>Other researchers believed that neural networks were the key to having a new type of ASR. With the advent of big data, faster computers, and graphical processing unit (GPU) processing, a new ASR method was developed, <a href="https://heartbeat.fritz.ai/the-3-deep-learning-frameworks-for-end-to-end-speech-recognition-that-power-your-devices-37b891ddc380">end-to-end deep learning ASR</a>. This new ASR method could \u201Clearn\u201D and be \u201Ctrained\u201D to become more accurate as more data is fed into the neural networks. No more developers re-coding each part of the trigram serial model to add new languages, parse accents, reduce noise, and add new words. The other big advantage of using an end-to-end deep learning ASR is that you can have the <a href="https://offers.deepgram.com/how-to-evaluate-deep-learning-asr-platform-solution-brief">accuracy, speed, and scalability without sacrificing costs</a>.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976833/blog/the-history-of-automatic-speech-recognition/history-hmm-v-dg_2%402x-1024x580.png" alt="history of speech recognition"></p>\n<p><em>History of Speech Recognition and Hidden Markov Models</em></p>\n<p>This is how <a href="https://deepgram.com/company/about/">Deepgram</a> was born; out of research that did not look at refining a 50-year-old ASR model but starting from deep learning neural networks. Check out the entire history of ASR in the image above. <a href="https://deepgram.com/contact-us/">Contact us</a> to learn how you can decrease word error rate systematically without compromising speed, scale, or cost, or sign up for a <a href="https://console.deepgram.com/signup">free API key</a> to get started today.</p>\n<WhitepaperPromo whitepaper="latest" />';
}
const $$Astro = createAstro("/Users/sandrarodgers/web-next/blog/src/content/blog/posts/the-history-of-automatic-speech-recognition/index.md", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Index = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro, $$props, $$slots);
  Astro2.self = $$Index;
  new Slugger();
  return renderTemplate`<head>${renderHead($$result)}</head><p>The most exciting time to be in the Automatic Speech Recognition (ASR) space is right now. Consumers are using Siri and Alexa daily to ask questions, order products, play music, play games, and do simple tasks. This is the norm, and it started less than 15 years ago with Google Voice Search. On the enterprise side, we see <a href="https://deepgram.com/solutions/voicebots/">voicebots & conversational AI</a>, and <a href="https://deepgram.com/solutions/speech-analytics/">speech analytics</a> that can determine <a href="https://blog.deepgram.com/sentiment-analysis-emotion-regulation-difference/">sentiment and emotions</a> as well as <a href="https://deepgram.com/product/languages/">languages</a>.</p>
<h2 id="early-years-hidden-markov-models-and-trigram-models"><strong>Early Years: Hidden Markov Models and Trigram Models</strong></h2>
<p>The history of Automatic Speech Recognition started in 1952 with Bell Labs and a program called <a href="https://astaspeaks.wordpress.com/2014/10/13/audrey-the-first-speech-recognition-system/">Audrey</a>, which could transcribe simple numbers. The next breakthrough did not occur until the mid-1970 when researchers started using <a href="https://jonathan-hui.medium.com/speech-recognition-gmm-hmm-8bb5eff8b196">Hidden Markov Models</a> (HMM).HMM uses probability functions to determine the correct words to transcribe. These ASR speech models take snippets of audio to determine the smallest unit of sound for a word or what is called a <a href="https://www.britannica.com/topic/phoneme">phoneme</a>.  The phoneme is then fed into another program that uses the HMM to guess the right word using a most common word probability function. These serial processing models are refined by adding noise reduction upfront and <a href="https://towardsdatascience.com/an-intuitive-explanation-of-beam-search-9b1d744e7a0f">beam search language models</a> on the back end to create understandable text and sentences. Bean search is a time-dependent probability function and looks at the transcribed words before and after the target word to find the best fit for the target word. This whole serial process is called the <a href="https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9">“trigram” model</a>, and 80% of the ASR technology currently being used is a refined version of this 1970’s model.</p>
<h2 id="new-generation-of-asr-neural-networks"><strong>New Generation of ASR: Neural Networks</strong></h2>
<p>The next big breakthrough came in the late 1980s with the addition of neural networks. This was also an inflection point for ASR. Most researchers and companies use these neural networks to improve their current trigram models with better upfront audio phoneme differentiation or better backend text and sentence creation. This trigram model works very well for consumer devices like Alexa and Siri that only have a small set of voice commands to respond to.  However, this model is not as effective with enterprise use cases, like meetings, phone calls, and automated voicebots. The refined trigram models require huge amounts of processing power to provide accurate transcription at speed. Businesses need to trade speed for accuracy or accuracy for costs.</p>
<h2 id="new-revolution-in-asr-deep-learning"><strong>New Revolution in ASR: Deep Learning</strong></h2>
<p>Other researchers believed that neural networks were the key to having a new type of ASR. With the advent of big data, faster computers, and graphical processing unit (GPU) processing, a new ASR method was developed, <a href="https://heartbeat.fritz.ai/the-3-deep-learning-frameworks-for-end-to-end-speech-recognition-that-power-your-devices-37b891ddc380">end-to-end deep learning ASR</a>. This new ASR method could “learn” and be “trained” to become more accurate as more data is fed into the neural networks. No more developers re-coding each part of the trigram serial model to add new languages, parse accents, reduce noise, and add new words. The other big advantage of using an end-to-end deep learning ASR is that you can have the <a href="https://offers.deepgram.com/how-to-evaluate-deep-learning-asr-platform-solution-brief">accuracy, speed, and scalability without sacrificing costs</a>.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976833/blog/the-history-of-automatic-speech-recognition/history-hmm-v-dg_2%402x-1024x580.png" alt="history of speech recognition"></p>
<p><em>History of Speech Recognition and Hidden Markov Models</em></p>
<p>This is how <a href="https://deepgram.com/company/about/">Deepgram</a> was born; out of research that did not look at refining a 50-year-old ASR model but starting from deep learning neural networks. Check out the entire history of ASR in the image above. <a href="https://deepgram.com/contact-us/">Contact us</a> to learn how you can decrease word error rate systematically without compromising speed, scale, or cost, or sign up for a <a href="https://console.deepgram.com/signup">free API key</a> to get started today.</p>
${renderComponent($$result, "WhitepaperPromo", WhitepaperPromo, { "whitepaper": "latest" })}`;
}, "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/the-history-of-automatic-speech-recognition/index.md");

export { compiledContent, $$Index as default, frontmatter, metadata, rawContent };
