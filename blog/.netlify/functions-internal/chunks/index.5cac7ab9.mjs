import { c as createAstro, a as createComponent, r as renderTemplate, b as renderHead } from '../entry.mjs';
import Slugger from 'github-slugger';
import '@astrojs/netlify/netlify-functions.js';
import 'preact';
import 'preact-render-to-string';
import 'vue';
import 'vue/server-renderer';
import 'html-escaper';
import 'node-html-parser';
import 'axios';
/* empty css                           *//* empty css                           *//* empty css                           *//* empty css                           *//* empty css                          */import 'clone-deep';
import 'slugify';
import 'shiki';
/* empty css                           */import '@astrojs/rss';
/* empty css                           */import 'mime';
import 'cookie';
import 'kleur/colors';
import 'string-width';
import 'path-browserify';
import 'path-to-regexp';

const metadata = { "headings": [{ "depth": 2, "slug": "decoding", "text": "Decoding" }, { "depth": 3, "slug": "non-speech", "text": "Non-speech" }], "source": '\nOpenAI recently released a new open source ASR model named Whisper and a repo full of tools that make it easy to try it out. In this blog, we will explore some of the options in Whisper\u2019s inference and see how they impact results.\n\nWhen experimenting with Whisper, you have a few options. You can run their command line tool, which will set a bunch of parameters for you, but you can also play around with those parameters and change what kind of results you get.\n\nTo install Whisper on your machine, you can follow the Setup guide in their [readme](https://github.com/openai/whisper) which walks you through the steps.\n\nThen, you can import Whisper in your own python code and use their `load_model` function to download the pre-trained weights and initialize one of the models.\n\n```bash\nimport whisper\r\nmodel = whisper.load_model("medium.en", device="cuda")\n```\n\nWhisper is available as multilingual models, but we will focus on the english only versions here. The options are:\n\n*   `tiny.en`\n*   `base.en`\n*   `small.en`\n*   `medium.en`\n*   `large` (the large model is only available in the multi-language form)\n\nWhisper is an encoder-decoder transformer model that takes in audio features and generates text. The models\u2019 parameter sizes range from 39 M for tiny and up to 1550 M for large.\n\nWhisper makes it very easy to transcribe an audio file from its path on the file system and will take care of loading the audio using `ffmpeg` and featurizing it before running inference on the file.\n\nIn order to run the transcribe function, you need to make some decisions about how you want to decode the model\'s predictions into text.\n\nYou can call the transcribe function without explicitly setting the decode options and it will set some defaults for you.\n\n```python\ntranscription = model.transcribe("hello_world.mp3", task=\u201Dtranscribe\u201D, language="en")\r\nprint(transcription["text"])\r\n\' Hello world.\'\n```\n\n## Decoding\n\nIn the Whisper [paper](https://cdn.openai.com/papers/whisper.pdf), they describe their complex decoding strategy including a few heuristics they landed on to try and make transcription more reliable. The way these strategies are currently implemented in their code results in slightly improved test results, but can slow down inference by up to 6x. This is because they repeatedly run inference with different decoding strategies until they meet the heuristics.\n\nTwo of the heuristics they use are:\n\n**Compression ratio:** the compression ratio heuristic is defined by the calculation:\n\n`compression_ratio = len(text)/len(zlib.compress(text.encode("utf-8")))`\n\nzlib\u2019s compression capitalizes on repeated sequences, so a text string with many repeated sequences will be able to compress down more than a string with more unique sequences. The goal of this threshold is to prevent the model from outputting predictions where it has gotten stuck and generated the same phrase over and over. You can see an example of this in the Non-speech section below.\n\nIn the whisper code, they set their ratio threshold to be `2.4`. A sequence that has a higher ratio is re-inferenced and decoded with a different strategy.\n\n**Average log probability:** after taking the log softmax of the network\'s output logits, the average log probability of the tokens chosen by the decoding is used. This can be thought of as a confidence measure of the model\'s predictions. The whisper authors use `-1.0` as their threshold.\n\nBy default, the whisper cli tool runs inference and decoding up to 6 times with the following decoding strategies. For each strategy, if the results pass the compression and log probability heuristics, the predicted tokens are used. If the heuristics are not met, the segment is re-inference and re-decoded with the next strategy.\n\nThe decoding strategies are:\n\n1.  Beam search with 5 beams using log probability for the score function\n2.  Greedy decoding with best of 5 sampling. The following temperatures are used for successive attempts: (0.0, 0.2, 0.4, 0.6, 0.8, 1.0)\n\nThe authors quantify the effects of these strategies in [Table 7](https://cdn.openai.com/papers/whisper.pdf) of their paper. They have a varied impact across datasets, but overall the effects for any single intervention are not large.\n\nHere we can look at a few different decoding strategies for the same file and see how they impact results. For this experiment we use the following [clip](https://www.youtube.com/watch?v=FTrxDBDBOHU) from Star Wars. It\'s a useful example because the different decoding strategies make the most difference in areas where the model is less certain. In this clip, there is dramatic music at the beginning, followed by the words "hello there" in a normal voice and then "General Kenobi" in a robotic voice with some strange noises thrown in.\n\nFirst we try running the full decoding strategy, and see that it correctly transcribes the file:\n\n```python\nbeam_size=5\r\nbest_of=5\r\ntemperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0)\r\n\r\ndecode_options = dict(language="en", best_of=best_of, beam_size=beam_size, temperature=temperature)\r\ntranscribe_options = dict(task="transcribe", **decode_options)\r\n\r\ntranscription = model.transcribe("Obi-Wan.mp3", **transcribe_options)\r\nprint(transcription["text"])\r\n\r\n\' Hello there. General Kenobi!\'\n```\n\nNext, we try removing the beam size strategy and dropping the best\\_of value for greedy decoding down to 1. Here we can see that now the model is outputting "ominious music" before the actual speech. This suggests the model was trained on some form of closed captioning.\n\n```python\nbeam_size=None\r\nbest_of=1\r\ntemperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0)\r\n\r\ndecode_options = dict(language="en", best_of=best_of, beam_size=beam_size, temperature=temperature)\r\ntranscribe_options = dict(task="transcribe", **decode_options)\r\n\r\ntranscription = model.transcribe(\'Obi-Wan.mp3\', **transcribe_options)\r\nprint(transcription[\'text\'])\r\n\r\n\'ominous music hello there General Kenobi!\'\n```\n\nFinally, we try increasing the best\\_of value for greedy decoding, but reducing the temperature values tried.\n\n```python\nbeam_size=None\r\nbest_of=3\r\ntemperature=(0.0, 0.2, 0.4)\r\n\r\ndecode_options = dict(language="en", best_of=best_of, beam_size=beam_size, temperature=temperature)\r\ntranscribe_options = dict(task="transcribe", **decode_options)\r\n\r\ntranscription = model.transcribe("Obi-Wan.mp3", **transcribe_options)\r\nprint(transcription["text"])\r\n\r\n\' Thanks for watching!\'\n```\n\nThere we can see that with only very low temperature values, and a best\\_of set to 3, we get a remnant from the weak supervision from internet transcripts. There is randomness in this process, so repeating those experiments will different results each time.\n\nThese examples help explain why the authors recommend attempting multiple decodings and checking the heuristics as a way of improving performance. Trying to hone in on what decoding strategy works best for your data can simplify the inference process for running Whisper and make the current code run faster.\n\n### Non-speech\n\nOne area the model struggles with is periods of non-speech. After the decoding algorithms requested are run, the authors check one more heursitic:\n\n**No speech threshold:** this is another heuristic the authors check after all the decoding is finished. It is based on the probability assigned to the no speech token during inference. The authors use a threshold of `0.6` to detect non-speech. However, if the average log probability passes its threshold then this heuristic is ignored.\n\nTo explore the models\' performances on areas of non-speech, we use this [cat video](https://www.youtube.com/watch?v=gBx4IwYe3L8) which contains only background music and cat noises.\n\nSending the first 30 seconds of the cat video through the model with the recommended decoding produces some text.\n\n```python\nbeam_size=5\r\nbest_of=5\r\ntemperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0)\r\n\r\ndecode_options = dict(language="en", best_of=best_of, beam_size=beam_size, temperature=temperature)\r\ntranscribe_options = dict(task="transcribe", **decode_options)\r\n\r\ntranscription = model.transcribe("kittens_30secs.mp3", **transcribe_options)\r\nprint(transcription["text"])\r\n\r\n\' parrot one parrot you\'\n```\n\nThis means that the no speech threshold described above was not met. And if we check the model\'s predicted probability for the non-speech token we see that it was only 0.535. Too low to hit the threshold of 0.6.\n\nIf we run this same test with the `base.en` model instead of the `medium.en`:\n\n```python\nmodel = whisper.load_model("base.en", device="cuda")\r\nbeam_size=5\r\nbest_of=5\r\ntemperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0)\r\n\r\ndecode_options = dict(language="en", best_of=best_of, beam_size=beam_size, temperature=temperature)\r\ntranscribe_options = dict(task="transcribe", **decode_options)\r\n\r\ntranscription = model.transcribe("kittens_30secs.mp3", **transcribe_options)\r\nprint(transcription["text"])\r\n\r\n\'\'\n```\n\nFor the base model, the probability for the non speech token was `0.641`, which hit the threshold, leading the model to correctly output no speech for the audio.\n\nIf we run the same experiment for more of the models, we can see how they handle the non-speech differently.\n\n| Model     | no\\_speech probability | Predicted text |\r\n|-----------|-----------------------|----------------|\r\n| base.en   | 0.64                  |\'\'\r\n| small.en | 0.467                 |\' you\'          |\r\n| medium.en | 0.53                  |\'few weeks ago\'|\r\n| large     | 0.5                   |"Last couple days I almost lost it. I am excited for the day cuz I am Bye!"|\n\nIt doesn\'t correlate perfectly with size, but we can see that the larger models may need a higher no silence threshold for audio known to contain periods of background noise and no speech. This could also be solved by using a voice activity detection algorithm in parallel with Whisper, and only running Whisper on segments believed to contain speech.\n\nThe other thing noticeable in those results is how the larger models are also more "wordy" under uncertainty, with the large model outputting two random sentences. This points to the high capacity for the large models to memorize some of the lower quality supervision. The authors try to counter the effects of the weak supervision by providing a lot of it, over 600k hours, but its clear that some of the noise in the truth quality ends up stored in the weights of the larger models.\n\nFinally, this file is also a good example to see the scenario that the compression ratio is designed to prevent. If we look at results on this non-speech file with only running beam search decoding:\n\n```python\nbeam_size=5\r\nbest_of=None\r\ntemperature=0.0\r\n\r\ndecode_options = dict(language="en", best_of=best_of, beam_size=beam_size, temperature=temperature)\r\ntranscribe_options = dict(task="transcribe", **decode_options)\r\n\r\ntranscription = model.transcribe("kittens_30secs.mp3", **transcribe_options)\r\nprint(transcription["text"])\r\n\r\n" I\'m not going to lie, I\'m not going to lie, I\'m not going to lie."\n```\n\nThe compression ratio for that prediction string is actually only `1.91`, which is less than their threshold of `2.4`. It\'s worth noting that if you don\'t use their full decoding scheme and start encountering a lot of repetitious predictions in audio that the model is uncertain about, you might want to drop the compression ratio threshold a little lower.\n\nHopefully this blog helped you explore the many options in the Whisper repo and find a configuration that works best for your tasks!\n\n', "html": '<p>OpenAI recently released a new open source ASR model named Whisper and a repo full of tools that make it easy to try it out. In this blog, we will explore some of the options in Whisper\u2019s inference and see how they impact results.</p>\n<p>When experimenting with Whisper, you have a few options. You can run their command line tool, which will set a bunch of parameters for you, but you can also play around with those parameters and change what kind of results you get.</p>\n<p>To install Whisper on your machine, you can follow the Setup guide in their <a href="https://github.com/openai/whisper">readme</a> which walks you through the steps.</p>\n<p>Then, you can import Whisper in your own python code and use their <code is:raw>load_model</code> function to download the pre-trained weights and initialize one of the models.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">import whisper</span></span>\n<span class="line"><span style="color: #C9D1D9">model = whisper.load_model(</span><span style="color: #A5D6FF">&quot;medium.en&quot;</span><span style="color: #C9D1D9">, device=</span><span style="color: #A5D6FF">&quot;cuda&quot;</span><span style="color: #C9D1D9">)</span></span></code></pre>\n<p>Whisper is available as multilingual models, but we will focus on the english only versions here. The options are:</p>\n<ul>\n<li><code is:raw>tiny.en</code></li>\n<li><code is:raw>base.en</code></li>\n<li><code is:raw>small.en</code></li>\n<li><code is:raw>medium.en</code></li>\n<li><code is:raw>large</code> (the large model is only available in the multi-language form)</li>\n</ul>\n<p>Whisper is an encoder-decoder transformer model that takes in audio features and generates text. The models\u2019 parameter sizes range from 39 M for tiny and up to 1550 M for large.</p>\n<p>Whisper makes it very easy to transcribe an audio file from its path on the file system and will take care of loading the audio using <code is:raw>ffmpeg</code> and featurizing it before running inference on the file.</p>\n<p>In order to run the transcribe function, you need to make some decisions about how you want to decode the model\u2019s predictions into text.</p>\n<p>You can call the transcribe function without explicitly setting the decode options and it will set some defaults for you.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">transcription </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> model.transcribe(</span><span style="color: #A5D6FF">&quot;hello_world.mp3&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">task</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">\u201Dtranscribe\u201D, </span><span style="color: #FFA657">language</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;en&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcription[</span><span style="color: #A5D6FF">&quot;text&quot;</span><span style="color: #C9D1D9">])</span></span>\n<span class="line"><span style="color: #A5D6FF">&#39; Hello world.&#39;</span></span></code></pre>\n<h2 id="decoding">Decoding</h2>\n<p>In the Whisper <a href="https://cdn.openai.com/papers/whisper.pdf">paper</a>, they describe their complex decoding strategy including a few heuristics they landed on to try and make transcription more reliable. The way these strategies are currently implemented in their code results in slightly improved test results, but can slow down inference by up to 6x. This is because they repeatedly run inference with different decoding strategies until they meet the heuristics.</p>\n<p>Two of the heuristics they use are:</p>\n<p><strong>Compression ratio:</strong> the compression ratio heuristic is defined by the calculation:</p>\n<p><code is:raw>compression_ratio = len(text)/len(zlib.compress(text.encode("utf-8")))</code></p>\n<p>zlib\u2019s compression capitalizes on repeated sequences, so a text string with many repeated sequences will be able to compress down more than a string with more unique sequences. The goal of this threshold is to prevent the model from outputting predictions where it has gotten stuck and generated the same phrase over and over. You can see an example of this in the Non-speech section below.</p>\n<p>In the whisper code, they set their ratio threshold to be <code is:raw>2.4</code>. A sequence that has a higher ratio is re-inferenced and decoded with a different strategy.</p>\n<p><strong>Average log probability:</strong> after taking the log softmax of the network\u2019s output logits, the average log probability of the tokens chosen by the decoding is used. This can be thought of as a confidence measure of the model\u2019s predictions. The whisper authors use <code is:raw>-1.0</code> as their threshold.</p>\n<p>By default, the whisper cli tool runs inference and decoding up to 6 times with the following decoding strategies. For each strategy, if the results pass the compression and log probability heuristics, the predicted tokens are used. If the heuristics are not met, the segment is re-inference and re-decoded with the next strategy.</p>\n<p>The decoding strategies are:</p>\n<ol>\n<li>Beam search with 5 beams using log probability for the score function</li>\n<li>Greedy decoding with best of 5 sampling. The following temperatures are used for successive attempts: (0.0, 0.2, 0.4, 0.6, 0.8, 1.0)</li>\n</ol>\n<p>The authors quantify the effects of these strategies in <a href="https://cdn.openai.com/papers/whisper.pdf">Table 7</a> of their paper. They have a varied impact across datasets, but overall the effects for any single intervention are not large.</p>\n<p>Here we can look at a few different decoding strategies for the same file and see how they impact results. For this experiment we use the following <a href="https://www.youtube.com/watch?v=FTrxDBDBOHU">clip</a> from Star Wars. It\u2019s a useful example because the different decoding strategies make the most difference in areas where the model is less certain. In this clip, there is dramatic music at the beginning, followed by the words \u201Chello there\u201D in a normal voice and then \u201CGeneral Kenobi\u201D in a robotic voice with some strange noises thrown in.</p>\n<p>First we try running the full decoding strategy, and see that it correctly transcribes the file:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">5</span></span>\n<span class="line"><span style="color: #C9D1D9">best_of</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">5</span></span>\n<span class="line"><span style="color: #C9D1D9">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">0.0</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.2</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.4</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.6</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.8</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">1.0</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">decode_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">language</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;en&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">best_of</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">best_of, </span><span style="color: #FFA657">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">beam_size, </span><span style="color: #FFA657">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">temperature)</span></span>\n<span class="line"><span style="color: #C9D1D9">transcribe_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">task</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;transcribe&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">decode_options)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">transcription </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> model.transcribe(</span><span style="color: #A5D6FF">&quot;Obi-Wan.mp3&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">transcribe_options)</span></span>\n<span class="line"><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcription[</span><span style="color: #A5D6FF">&quot;text&quot;</span><span style="color: #C9D1D9">])</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #A5D6FF">&#39; Hello there. General Kenobi!&#39;</span></span></code></pre>\n<p>Next, we try removing the beam size strategy and dropping the best_of value for greedy decoding down to 1. Here we can see that now the model is outputting \u201Cominious music\u201D before the actual speech. This suggests the model was trained on some form of closed captioning.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">None</span></span>\n<span class="line"><span style="color: #C9D1D9">best_of</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">1</span></span>\n<span class="line"><span style="color: #C9D1D9">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">0.0</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.2</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.4</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.6</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.8</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">1.0</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">decode_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">language</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;en&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">best_of</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">best_of, </span><span style="color: #FFA657">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">beam_size, </span><span style="color: #FFA657">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">temperature)</span></span>\n<span class="line"><span style="color: #C9D1D9">transcribe_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">task</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;transcribe&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">decode_options)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">transcription </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> model.transcribe(</span><span style="color: #A5D6FF">&#39;Obi-Wan.mp3&#39;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">transcribe_options)</span></span>\n<span class="line"><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcription[</span><span style="color: #A5D6FF">&#39;text&#39;</span><span style="color: #C9D1D9">])</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #A5D6FF">&#39;ominous music hello there General Kenobi!&#39;</span></span></code></pre>\n<p>Finally, we try increasing the best_of value for greedy decoding, but reducing the temperature values tried.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">None</span></span>\n<span class="line"><span style="color: #C9D1D9">best_of</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">3</span></span>\n<span class="line"><span style="color: #C9D1D9">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">0.0</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.2</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.4</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">decode_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">language</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;en&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">best_of</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">best_of, </span><span style="color: #FFA657">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">beam_size, </span><span style="color: #FFA657">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">temperature)</span></span>\n<span class="line"><span style="color: #C9D1D9">transcribe_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">task</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;transcribe&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">decode_options)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">transcription </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> model.transcribe(</span><span style="color: #A5D6FF">&quot;Obi-Wan.mp3&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">transcribe_options)</span></span>\n<span class="line"><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcription[</span><span style="color: #A5D6FF">&quot;text&quot;</span><span style="color: #C9D1D9">])</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #A5D6FF">&#39; Thanks for watching!&#39;</span></span></code></pre>\n<p>There we can see that with only very low temperature values, and a best_of set to 3, we get a remnant from the weak supervision from internet transcripts. There is randomness in this process, so repeating those experiments will different results each time.</p>\n<p>These examples help explain why the authors recommend attempting multiple decodings and checking the heuristics as a way of improving performance. Trying to hone in on what decoding strategy works best for your data can simplify the inference process for running Whisper and make the current code run faster.</p>\n<h3 id="non-speech">Non-speech</h3>\n<p>One area the model struggles with is periods of non-speech. After the decoding algorithms requested are run, the authors check one more heursitic:</p>\n<p><strong>No speech threshold:</strong> this is another heuristic the authors check after all the decoding is finished. It is based on the probability assigned to the no speech token during inference. The authors use a threshold of <code is:raw>0.6</code> to detect non-speech. However, if the average log probability passes its threshold then this heuristic is ignored.</p>\n<p>To explore the models\u2019 performances on areas of non-speech, we use this <a href="https://www.youtube.com/watch?v=gBx4IwYe3L8">cat video</a> which contains only background music and cat noises.</p>\n<p>Sending the first 30 seconds of the cat video through the model with the recommended decoding produces some text.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">5</span></span>\n<span class="line"><span style="color: #C9D1D9">best_of</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">5</span></span>\n<span class="line"><span style="color: #C9D1D9">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">0.0</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.2</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.4</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.6</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.8</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">1.0</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">decode_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">language</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;en&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">best_of</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">best_of, </span><span style="color: #FFA657">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">beam_size, </span><span style="color: #FFA657">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">temperature)</span></span>\n<span class="line"><span style="color: #C9D1D9">transcribe_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">task</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;transcribe&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">decode_options)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">transcription </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> model.transcribe(</span><span style="color: #A5D6FF">&quot;kittens_30secs.mp3&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">transcribe_options)</span></span>\n<span class="line"><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcription[</span><span style="color: #A5D6FF">&quot;text&quot;</span><span style="color: #C9D1D9">])</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #A5D6FF">&#39; parrot one parrot you&#39;</span></span></code></pre>\n<p>This means that the no speech threshold described above was not met. And if we check the model\u2019s predicted probability for the non-speech token we see that it was only 0.535. Too low to hit the threshold of 0.6.</p>\n<p>If we run this same test with the <code is:raw>base.en</code> model instead of the <code is:raw>medium.en</code>:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">model </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> whisper.load_model(</span><span style="color: #A5D6FF">&quot;base.en&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">device</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;cuda&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">5</span></span>\n<span class="line"><span style="color: #C9D1D9">best_of</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">5</span></span>\n<span class="line"><span style="color: #C9D1D9">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">0.0</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.2</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.4</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.6</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.8</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">1.0</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">decode_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">language</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;en&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">best_of</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">best_of, </span><span style="color: #FFA657">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">beam_size, </span><span style="color: #FFA657">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">temperature)</span></span>\n<span class="line"><span style="color: #C9D1D9">transcribe_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">task</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;transcribe&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">decode_options)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">transcription </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> model.transcribe(</span><span style="color: #A5D6FF">&quot;kittens_30secs.mp3&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">transcribe_options)</span></span>\n<span class="line"><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcription[</span><span style="color: #A5D6FF">&quot;text&quot;</span><span style="color: #C9D1D9">])</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #A5D6FF">&#39;&#39;</span></span></code></pre>\n<p>For the base model, the probability for the non speech token was <code is:raw>0.641</code>, which hit the threshold, leading the model to correctly output no speech for the audio.</p>\n<p>If we run the same experiment for more of the models, we can see how they handle the non-speech differently.</p>\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>no_speech probability</th>\n<th>Predicted text</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>base.en</td>\n<td>0.64</td>\n<td>\u201D</td>\n</tr>\n<tr>\n<td>small.en</td>\n<td>0.467</td>\n<td>\u2019 you\u2019</td>\n</tr>\n<tr>\n<td>medium.en</td>\n<td>0.53</td>\n<td>\u2018few weeks ago\u2019</td>\n</tr>\n<tr>\n<td>large</td>\n<td>0.5</td>\n<td>\u201CLast couple days I almost lost it. I am excited for the day cuz I am Bye!\u201D</td>\n</tr>\n</tbody>\n</table>\n<p>It doesn\u2019t correlate perfectly with size, but we can see that the larger models may need a higher no silence threshold for audio known to contain periods of background noise and no speech. This could also be solved by using a voice activity detection algorithm in parallel with Whisper, and only running Whisper on segments believed to contain speech.</p>\n<p>The other thing noticeable in those results is how the larger models are also more \u201Cwordy\u201D under uncertainty, with the large model outputting two random sentences. This points to the high capacity for the large models to memorize some of the lower quality supervision. The authors try to counter the effects of the weak supervision by providing a lot of it, over 600k hours, but its clear that some of the noise in the truth quality ends up stored in the weights of the larger models.</p>\n<p>Finally, this file is also a good example to see the scenario that the compression ratio is designed to prevent. If we look at results on this non-speech file with only running beam search decoding:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">5</span></span>\n<span class="line"><span style="color: #C9D1D9">best_of</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">None</span></span>\n<span class="line"><span style="color: #C9D1D9">temperature</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">0.0</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">decode_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">language</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;en&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">best_of</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">best_of, </span><span style="color: #FFA657">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">beam_size, </span><span style="color: #FFA657">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">temperature)</span></span>\n<span class="line"><span style="color: #C9D1D9">transcribe_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">task</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;transcribe&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">decode_options)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">transcription </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> model.transcribe(</span><span style="color: #A5D6FF">&quot;kittens_30secs.mp3&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">transcribe_options)</span></span>\n<span class="line"><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcription[</span><span style="color: #A5D6FF">&quot;text&quot;</span><span style="color: #C9D1D9">])</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #A5D6FF">&quot; I&#39;m not going to lie, I&#39;m not going to lie, I&#39;m not going to lie.&quot;</span></span></code></pre>\n<p>The compression ratio for that prediction string is actually only <code is:raw>1.91</code>, which is less than their threshold of <code is:raw>2.4</code>. It\u2019s worth noting that if you don\u2019t use their full decoding scheme and start encountering a lot of repetitious predictions in audio that the model is uncertain about, you might want to drop the compression ratio threshold a little lower.</p>\n<p>Hopefully this blog helped you explore the many options in the Whisper repo and find a configuration that works best for your tasks!</p>' };
const frontmatter = { "title": "Exploring OpenAI Whisper Speech Recognition", "description": "In Deepgram's latest blog, we will explore some of the options in OpenAI Whisper\u2019s inference and see how they impact results. Read more here!", "date": "2022-10-13T20:59:18.175Z", "cover": "https://res.cloudinary.com/deepgram/image/upload/v1665415885/blog/exploring-whisper/2210-Exploring_Whisper-featured-1200x630_2x_nut4uw.png", "authors": ["julia-strout"], "category": "ai-and-engineering", "tags": ["whisper", "machine-learning"], "shorturls": { "share": "https://dpgr.am/81a7829", "twitter": "https://dpgr.am/cc15f31", "linkedin": "https://dpgr.am/2468dad", "reddit": "https://dpgr.am/8e86cec", "facebook": "https://dpgr.am/f98594a" }, "astro": { "headings": [{ "depth": 2, "slug": "decoding", "text": "Decoding" }, { "depth": 3, "slug": "non-speech", "text": "Non-speech" }], "source": '\nOpenAI recently released a new open source ASR model named Whisper and a repo full of tools that make it easy to try it out. In this blog, we will explore some of the options in Whisper\u2019s inference and see how they impact results.\n\nWhen experimenting with Whisper, you have a few options. You can run their command line tool, which will set a bunch of parameters for you, but you can also play around with those parameters and change what kind of results you get.\n\nTo install Whisper on your machine, you can follow the Setup guide in their [readme](https://github.com/openai/whisper) which walks you through the steps.\n\nThen, you can import Whisper in your own python code and use their `load_model` function to download the pre-trained weights and initialize one of the models.\n\n```bash\nimport whisper\r\nmodel = whisper.load_model("medium.en", device="cuda")\n```\n\nWhisper is available as multilingual models, but we will focus on the english only versions here. The options are:\n\n*   `tiny.en`\n*   `base.en`\n*   `small.en`\n*   `medium.en`\n*   `large` (the large model is only available in the multi-language form)\n\nWhisper is an encoder-decoder transformer model that takes in audio features and generates text. The models\u2019 parameter sizes range from 39 M for tiny and up to 1550 M for large.\n\nWhisper makes it very easy to transcribe an audio file from its path on the file system and will take care of loading the audio using `ffmpeg` and featurizing it before running inference on the file.\n\nIn order to run the transcribe function, you need to make some decisions about how you want to decode the model\'s predictions into text.\n\nYou can call the transcribe function without explicitly setting the decode options and it will set some defaults for you.\n\n```python\ntranscription = model.transcribe("hello_world.mp3", task=\u201Dtranscribe\u201D, language="en")\r\nprint(transcription["text"])\r\n\' Hello world.\'\n```\n\n## Decoding\n\nIn the Whisper [paper](https://cdn.openai.com/papers/whisper.pdf), they describe their complex decoding strategy including a few heuristics they landed on to try and make transcription more reliable. The way these strategies are currently implemented in their code results in slightly improved test results, but can slow down inference by up to 6x. This is because they repeatedly run inference with different decoding strategies until they meet the heuristics.\n\nTwo of the heuristics they use are:\n\n**Compression ratio:** the compression ratio heuristic is defined by the calculation:\n\n`compression_ratio = len(text)/len(zlib.compress(text.encode("utf-8")))`\n\nzlib\u2019s compression capitalizes on repeated sequences, so a text string with many repeated sequences will be able to compress down more than a string with more unique sequences. The goal of this threshold is to prevent the model from outputting predictions where it has gotten stuck and generated the same phrase over and over. You can see an example of this in the Non-speech section below.\n\nIn the whisper code, they set their ratio threshold to be `2.4`. A sequence that has a higher ratio is re-inferenced and decoded with a different strategy.\n\n**Average log probability:** after taking the log softmax of the network\'s output logits, the average log probability of the tokens chosen by the decoding is used. This can be thought of as a confidence measure of the model\'s predictions. The whisper authors use `-1.0` as their threshold.\n\nBy default, the whisper cli tool runs inference and decoding up to 6 times with the following decoding strategies. For each strategy, if the results pass the compression and log probability heuristics, the predicted tokens are used. If the heuristics are not met, the segment is re-inference and re-decoded with the next strategy.\n\nThe decoding strategies are:\n\n1.  Beam search with 5 beams using log probability for the score function\n2.  Greedy decoding with best of 5 sampling. The following temperatures are used for successive attempts: (0.0, 0.2, 0.4, 0.6, 0.8, 1.0)\n\nThe authors quantify the effects of these strategies in [Table 7](https://cdn.openai.com/papers/whisper.pdf) of their paper. They have a varied impact across datasets, but overall the effects for any single intervention are not large.\n\nHere we can look at a few different decoding strategies for the same file and see how they impact results. For this experiment we use the following [clip](https://www.youtube.com/watch?v=FTrxDBDBOHU) from Star Wars. It\'s a useful example because the different decoding strategies make the most difference in areas where the model is less certain. In this clip, there is dramatic music at the beginning, followed by the words "hello there" in a normal voice and then "General Kenobi" in a robotic voice with some strange noises thrown in.\n\nFirst we try running the full decoding strategy, and see that it correctly transcribes the file:\n\n```python\nbeam_size=5\r\nbest_of=5\r\ntemperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0)\r\n\r\ndecode_options = dict(language="en", best_of=best_of, beam_size=beam_size, temperature=temperature)\r\ntranscribe_options = dict(task="transcribe", **decode_options)\r\n\r\ntranscription = model.transcribe("Obi-Wan.mp3", **transcribe_options)\r\nprint(transcription["text"])\r\n\r\n\' Hello there. General Kenobi!\'\n```\n\nNext, we try removing the beam size strategy and dropping the best\\_of value for greedy decoding down to 1. Here we can see that now the model is outputting "ominious music" before the actual speech. This suggests the model was trained on some form of closed captioning.\n\n```python\nbeam_size=None\r\nbest_of=1\r\ntemperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0)\r\n\r\ndecode_options = dict(language="en", best_of=best_of, beam_size=beam_size, temperature=temperature)\r\ntranscribe_options = dict(task="transcribe", **decode_options)\r\n\r\ntranscription = model.transcribe(\'Obi-Wan.mp3\', **transcribe_options)\r\nprint(transcription[\'text\'])\r\n\r\n\'ominous music hello there General Kenobi!\'\n```\n\nFinally, we try increasing the best\\_of value for greedy decoding, but reducing the temperature values tried.\n\n```python\nbeam_size=None\r\nbest_of=3\r\ntemperature=(0.0, 0.2, 0.4)\r\n\r\ndecode_options = dict(language="en", best_of=best_of, beam_size=beam_size, temperature=temperature)\r\ntranscribe_options = dict(task="transcribe", **decode_options)\r\n\r\ntranscription = model.transcribe("Obi-Wan.mp3", **transcribe_options)\r\nprint(transcription["text"])\r\n\r\n\' Thanks for watching!\'\n```\n\nThere we can see that with only very low temperature values, and a best\\_of set to 3, we get a remnant from the weak supervision from internet transcripts. There is randomness in this process, so repeating those experiments will different results each time.\n\nThese examples help explain why the authors recommend attempting multiple decodings and checking the heuristics as a way of improving performance. Trying to hone in on what decoding strategy works best for your data can simplify the inference process for running Whisper and make the current code run faster.\n\n### Non-speech\n\nOne area the model struggles with is periods of non-speech. After the decoding algorithms requested are run, the authors check one more heursitic:\n\n**No speech threshold:** this is another heuristic the authors check after all the decoding is finished. It is based on the probability assigned to the no speech token during inference. The authors use a threshold of `0.6` to detect non-speech. However, if the average log probability passes its threshold then this heuristic is ignored.\n\nTo explore the models\' performances on areas of non-speech, we use this [cat video](https://www.youtube.com/watch?v=gBx4IwYe3L8) which contains only background music and cat noises.\n\nSending the first 30 seconds of the cat video through the model with the recommended decoding produces some text.\n\n```python\nbeam_size=5\r\nbest_of=5\r\ntemperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0)\r\n\r\ndecode_options = dict(language="en", best_of=best_of, beam_size=beam_size, temperature=temperature)\r\ntranscribe_options = dict(task="transcribe", **decode_options)\r\n\r\ntranscription = model.transcribe("kittens_30secs.mp3", **transcribe_options)\r\nprint(transcription["text"])\r\n\r\n\' parrot one parrot you\'\n```\n\nThis means that the no speech threshold described above was not met. And if we check the model\'s predicted probability for the non-speech token we see that it was only 0.535. Too low to hit the threshold of 0.6.\n\nIf we run this same test with the `base.en` model instead of the `medium.en`:\n\n```python\nmodel = whisper.load_model("base.en", device="cuda")\r\nbeam_size=5\r\nbest_of=5\r\ntemperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0)\r\n\r\ndecode_options = dict(language="en", best_of=best_of, beam_size=beam_size, temperature=temperature)\r\ntranscribe_options = dict(task="transcribe", **decode_options)\r\n\r\ntranscription = model.transcribe("kittens_30secs.mp3", **transcribe_options)\r\nprint(transcription["text"])\r\n\r\n\'\'\n```\n\nFor the base model, the probability for the non speech token was `0.641`, which hit the threshold, leading the model to correctly output no speech for the audio.\n\nIf we run the same experiment for more of the models, we can see how they handle the non-speech differently.\n\n| Model     | no\\_speech probability | Predicted text |\r\n|-----------|-----------------------|----------------|\r\n| base.en   | 0.64                  |\'\'\r\n| small.en | 0.467                 |\' you\'          |\r\n| medium.en | 0.53                  |\'few weeks ago\'|\r\n| large     | 0.5                   |"Last couple days I almost lost it. I am excited for the day cuz I am Bye!"|\n\nIt doesn\'t correlate perfectly with size, but we can see that the larger models may need a higher no silence threshold for audio known to contain periods of background noise and no speech. This could also be solved by using a voice activity detection algorithm in parallel with Whisper, and only running Whisper on segments believed to contain speech.\n\nThe other thing noticeable in those results is how the larger models are also more "wordy" under uncertainty, with the large model outputting two random sentences. This points to the high capacity for the large models to memorize some of the lower quality supervision. The authors try to counter the effects of the weak supervision by providing a lot of it, over 600k hours, but its clear that some of the noise in the truth quality ends up stored in the weights of the larger models.\n\nFinally, this file is also a good example to see the scenario that the compression ratio is designed to prevent. If we look at results on this non-speech file with only running beam search decoding:\n\n```python\nbeam_size=5\r\nbest_of=None\r\ntemperature=0.0\r\n\r\ndecode_options = dict(language="en", best_of=best_of, beam_size=beam_size, temperature=temperature)\r\ntranscribe_options = dict(task="transcribe", **decode_options)\r\n\r\ntranscription = model.transcribe("kittens_30secs.mp3", **transcribe_options)\r\nprint(transcription["text"])\r\n\r\n" I\'m not going to lie, I\'m not going to lie, I\'m not going to lie."\n```\n\nThe compression ratio for that prediction string is actually only `1.91`, which is less than their threshold of `2.4`. It\'s worth noting that if you don\'t use their full decoding scheme and start encountering a lot of repetitious predictions in audio that the model is uncertain about, you might want to drop the compression ratio threshold a little lower.\n\nHopefully this blog helped you explore the many options in the Whisper repo and find a configuration that works best for your tasks!\n\n', "html": '<p>OpenAI recently released a new open source ASR model named Whisper and a repo full of tools that make it easy to try it out. In this blog, we will explore some of the options in Whisper\u2019s inference and see how they impact results.</p>\n<p>When experimenting with Whisper, you have a few options. You can run their command line tool, which will set a bunch of parameters for you, but you can also play around with those parameters and change what kind of results you get.</p>\n<p>To install Whisper on your machine, you can follow the Setup guide in their <a href="https://github.com/openai/whisper">readme</a> which walks you through the steps.</p>\n<p>Then, you can import Whisper in your own python code and use their <code is:raw>load_model</code> function to download the pre-trained weights and initialize one of the models.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">import whisper</span></span>\n<span class="line"><span style="color: #C9D1D9">model = whisper.load_model(</span><span style="color: #A5D6FF">&quot;medium.en&quot;</span><span style="color: #C9D1D9">, device=</span><span style="color: #A5D6FF">&quot;cuda&quot;</span><span style="color: #C9D1D9">)</span></span></code></pre>\n<p>Whisper is available as multilingual models, but we will focus on the english only versions here. The options are:</p>\n<ul>\n<li><code is:raw>tiny.en</code></li>\n<li><code is:raw>base.en</code></li>\n<li><code is:raw>small.en</code></li>\n<li><code is:raw>medium.en</code></li>\n<li><code is:raw>large</code> (the large model is only available in the multi-language form)</li>\n</ul>\n<p>Whisper is an encoder-decoder transformer model that takes in audio features and generates text. The models\u2019 parameter sizes range from 39 M for tiny and up to 1550 M for large.</p>\n<p>Whisper makes it very easy to transcribe an audio file from its path on the file system and will take care of loading the audio using <code is:raw>ffmpeg</code> and featurizing it before running inference on the file.</p>\n<p>In order to run the transcribe function, you need to make some decisions about how you want to decode the model\u2019s predictions into text.</p>\n<p>You can call the transcribe function without explicitly setting the decode options and it will set some defaults for you.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">transcription </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> model.transcribe(</span><span style="color: #A5D6FF">&quot;hello_world.mp3&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">task</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">\u201Dtranscribe\u201D, </span><span style="color: #FFA657">language</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;en&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcription[</span><span style="color: #A5D6FF">&quot;text&quot;</span><span style="color: #C9D1D9">])</span></span>\n<span class="line"><span style="color: #A5D6FF">&#39; Hello world.&#39;</span></span></code></pre>\n<h2 id="decoding">Decoding</h2>\n<p>In the Whisper <a href="https://cdn.openai.com/papers/whisper.pdf">paper</a>, they describe their complex decoding strategy including a few heuristics they landed on to try and make transcription more reliable. The way these strategies are currently implemented in their code results in slightly improved test results, but can slow down inference by up to 6x. This is because they repeatedly run inference with different decoding strategies until they meet the heuristics.</p>\n<p>Two of the heuristics they use are:</p>\n<p><strong>Compression ratio:</strong> the compression ratio heuristic is defined by the calculation:</p>\n<p><code is:raw>compression_ratio = len(text)/len(zlib.compress(text.encode("utf-8")))</code></p>\n<p>zlib\u2019s compression capitalizes on repeated sequences, so a text string with many repeated sequences will be able to compress down more than a string with more unique sequences. The goal of this threshold is to prevent the model from outputting predictions where it has gotten stuck and generated the same phrase over and over. You can see an example of this in the Non-speech section below.</p>\n<p>In the whisper code, they set their ratio threshold to be <code is:raw>2.4</code>. A sequence that has a higher ratio is re-inferenced and decoded with a different strategy.</p>\n<p><strong>Average log probability:</strong> after taking the log softmax of the network\u2019s output logits, the average log probability of the tokens chosen by the decoding is used. This can be thought of as a confidence measure of the model\u2019s predictions. The whisper authors use <code is:raw>-1.0</code> as their threshold.</p>\n<p>By default, the whisper cli tool runs inference and decoding up to 6 times with the following decoding strategies. For each strategy, if the results pass the compression and log probability heuristics, the predicted tokens are used. If the heuristics are not met, the segment is re-inference and re-decoded with the next strategy.</p>\n<p>The decoding strategies are:</p>\n<ol>\n<li>Beam search with 5 beams using log probability for the score function</li>\n<li>Greedy decoding with best of 5 sampling. The following temperatures are used for successive attempts: (0.0, 0.2, 0.4, 0.6, 0.8, 1.0)</li>\n</ol>\n<p>The authors quantify the effects of these strategies in <a href="https://cdn.openai.com/papers/whisper.pdf">Table 7</a> of their paper. They have a varied impact across datasets, but overall the effects for any single intervention are not large.</p>\n<p>Here we can look at a few different decoding strategies for the same file and see how they impact results. For this experiment we use the following <a href="https://www.youtube.com/watch?v=FTrxDBDBOHU">clip</a> from Star Wars. It\u2019s a useful example because the different decoding strategies make the most difference in areas where the model is less certain. In this clip, there is dramatic music at the beginning, followed by the words \u201Chello there\u201D in a normal voice and then \u201CGeneral Kenobi\u201D in a robotic voice with some strange noises thrown in.</p>\n<p>First we try running the full decoding strategy, and see that it correctly transcribes the file:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">5</span></span>\n<span class="line"><span style="color: #C9D1D9">best_of</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">5</span></span>\n<span class="line"><span style="color: #C9D1D9">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">0.0</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.2</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.4</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.6</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.8</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">1.0</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">decode_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">language</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;en&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">best_of</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">best_of, </span><span style="color: #FFA657">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">beam_size, </span><span style="color: #FFA657">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">temperature)</span></span>\n<span class="line"><span style="color: #C9D1D9">transcribe_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">task</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;transcribe&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">decode_options)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">transcription </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> model.transcribe(</span><span style="color: #A5D6FF">&quot;Obi-Wan.mp3&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">transcribe_options)</span></span>\n<span class="line"><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcription[</span><span style="color: #A5D6FF">&quot;text&quot;</span><span style="color: #C9D1D9">])</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #A5D6FF">&#39; Hello there. General Kenobi!&#39;</span></span></code></pre>\n<p>Next, we try removing the beam size strategy and dropping the best_of value for greedy decoding down to 1. Here we can see that now the model is outputting \u201Cominious music\u201D before the actual speech. This suggests the model was trained on some form of closed captioning.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">None</span></span>\n<span class="line"><span style="color: #C9D1D9">best_of</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">1</span></span>\n<span class="line"><span style="color: #C9D1D9">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">0.0</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.2</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.4</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.6</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.8</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">1.0</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">decode_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">language</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;en&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">best_of</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">best_of, </span><span style="color: #FFA657">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">beam_size, </span><span style="color: #FFA657">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">temperature)</span></span>\n<span class="line"><span style="color: #C9D1D9">transcribe_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">task</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;transcribe&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">decode_options)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">transcription </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> model.transcribe(</span><span style="color: #A5D6FF">&#39;Obi-Wan.mp3&#39;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">transcribe_options)</span></span>\n<span class="line"><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcription[</span><span style="color: #A5D6FF">&#39;text&#39;</span><span style="color: #C9D1D9">])</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #A5D6FF">&#39;ominous music hello there General Kenobi!&#39;</span></span></code></pre>\n<p>Finally, we try increasing the best_of value for greedy decoding, but reducing the temperature values tried.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">None</span></span>\n<span class="line"><span style="color: #C9D1D9">best_of</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">3</span></span>\n<span class="line"><span style="color: #C9D1D9">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">0.0</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.2</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.4</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">decode_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">language</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;en&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">best_of</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">best_of, </span><span style="color: #FFA657">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">beam_size, </span><span style="color: #FFA657">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">temperature)</span></span>\n<span class="line"><span style="color: #C9D1D9">transcribe_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">task</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;transcribe&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">decode_options)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">transcription </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> model.transcribe(</span><span style="color: #A5D6FF">&quot;Obi-Wan.mp3&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">transcribe_options)</span></span>\n<span class="line"><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcription[</span><span style="color: #A5D6FF">&quot;text&quot;</span><span style="color: #C9D1D9">])</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #A5D6FF">&#39; Thanks for watching!&#39;</span></span></code></pre>\n<p>There we can see that with only very low temperature values, and a best_of set to 3, we get a remnant from the weak supervision from internet transcripts. There is randomness in this process, so repeating those experiments will different results each time.</p>\n<p>These examples help explain why the authors recommend attempting multiple decodings and checking the heuristics as a way of improving performance. Trying to hone in on what decoding strategy works best for your data can simplify the inference process for running Whisper and make the current code run faster.</p>\n<h3 id="non-speech">Non-speech</h3>\n<p>One area the model struggles with is periods of non-speech. After the decoding algorithms requested are run, the authors check one more heursitic:</p>\n<p><strong>No speech threshold:</strong> this is another heuristic the authors check after all the decoding is finished. It is based on the probability assigned to the no speech token during inference. The authors use a threshold of <code is:raw>0.6</code> to detect non-speech. However, if the average log probability passes its threshold then this heuristic is ignored.</p>\n<p>To explore the models\u2019 performances on areas of non-speech, we use this <a href="https://www.youtube.com/watch?v=gBx4IwYe3L8">cat video</a> which contains only background music and cat noises.</p>\n<p>Sending the first 30 seconds of the cat video through the model with the recommended decoding produces some text.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">5</span></span>\n<span class="line"><span style="color: #C9D1D9">best_of</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">5</span></span>\n<span class="line"><span style="color: #C9D1D9">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">0.0</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.2</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.4</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.6</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.8</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">1.0</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">decode_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">language</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;en&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">best_of</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">best_of, </span><span style="color: #FFA657">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">beam_size, </span><span style="color: #FFA657">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">temperature)</span></span>\n<span class="line"><span style="color: #C9D1D9">transcribe_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">task</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;transcribe&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">decode_options)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">transcription </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> model.transcribe(</span><span style="color: #A5D6FF">&quot;kittens_30secs.mp3&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">transcribe_options)</span></span>\n<span class="line"><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcription[</span><span style="color: #A5D6FF">&quot;text&quot;</span><span style="color: #C9D1D9">])</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #A5D6FF">&#39; parrot one parrot you&#39;</span></span></code></pre>\n<p>This means that the no speech threshold described above was not met. And if we check the model\u2019s predicted probability for the non-speech token we see that it was only 0.535. Too low to hit the threshold of 0.6.</p>\n<p>If we run this same test with the <code is:raw>base.en</code> model instead of the <code is:raw>medium.en</code>:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">model </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> whisper.load_model(</span><span style="color: #A5D6FF">&quot;base.en&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">device</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;cuda&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">5</span></span>\n<span class="line"><span style="color: #C9D1D9">best_of</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">5</span></span>\n<span class="line"><span style="color: #C9D1D9">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">0.0</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.2</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.4</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.6</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.8</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">1.0</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">decode_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">language</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;en&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">best_of</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">best_of, </span><span style="color: #FFA657">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">beam_size, </span><span style="color: #FFA657">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">temperature)</span></span>\n<span class="line"><span style="color: #C9D1D9">transcribe_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">task</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;transcribe&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">decode_options)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">transcription </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> model.transcribe(</span><span style="color: #A5D6FF">&quot;kittens_30secs.mp3&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">transcribe_options)</span></span>\n<span class="line"><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcription[</span><span style="color: #A5D6FF">&quot;text&quot;</span><span style="color: #C9D1D9">])</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #A5D6FF">&#39;&#39;</span></span></code></pre>\n<p>For the base model, the probability for the non speech token was <code is:raw>0.641</code>, which hit the threshold, leading the model to correctly output no speech for the audio.</p>\n<p>If we run the same experiment for more of the models, we can see how they handle the non-speech differently.</p>\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>no_speech probability</th>\n<th>Predicted text</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>base.en</td>\n<td>0.64</td>\n<td>\u201D</td>\n</tr>\n<tr>\n<td>small.en</td>\n<td>0.467</td>\n<td>\u2019 you\u2019</td>\n</tr>\n<tr>\n<td>medium.en</td>\n<td>0.53</td>\n<td>\u2018few weeks ago\u2019</td>\n</tr>\n<tr>\n<td>large</td>\n<td>0.5</td>\n<td>\u201CLast couple days I almost lost it. I am excited for the day cuz I am Bye!\u201D</td>\n</tr>\n</tbody>\n</table>\n<p>It doesn\u2019t correlate perfectly with size, but we can see that the larger models may need a higher no silence threshold for audio known to contain periods of background noise and no speech. This could also be solved by using a voice activity detection algorithm in parallel with Whisper, and only running Whisper on segments believed to contain speech.</p>\n<p>The other thing noticeable in those results is how the larger models are also more \u201Cwordy\u201D under uncertainty, with the large model outputting two random sentences. This points to the high capacity for the large models to memorize some of the lower quality supervision. The authors try to counter the effects of the weak supervision by providing a lot of it, over 600k hours, but its clear that some of the noise in the truth quality ends up stored in the weights of the larger models.</p>\n<p>Finally, this file is also a good example to see the scenario that the compression ratio is designed to prevent. If we look at results on this non-speech file with only running beam search decoding:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">5</span></span>\n<span class="line"><span style="color: #C9D1D9">best_of</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">None</span></span>\n<span class="line"><span style="color: #C9D1D9">temperature</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">0.0</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">decode_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">language</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;en&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">best_of</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">best_of, </span><span style="color: #FFA657">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">beam_size, </span><span style="color: #FFA657">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">temperature)</span></span>\n<span class="line"><span style="color: #C9D1D9">transcribe_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">task</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;transcribe&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">decode_options)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">transcription </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> model.transcribe(</span><span style="color: #A5D6FF">&quot;kittens_30secs.mp3&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">transcribe_options)</span></span>\n<span class="line"><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcription[</span><span style="color: #A5D6FF">&quot;text&quot;</span><span style="color: #C9D1D9">])</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #A5D6FF">&quot; I&#39;m not going to lie, I&#39;m not going to lie, I&#39;m not going to lie.&quot;</span></span></code></pre>\n<p>The compression ratio for that prediction string is actually only <code is:raw>1.91</code>, which is less than their threshold of <code is:raw>2.4</code>. It\u2019s worth noting that if you don\u2019t use their full decoding scheme and start encountering a lot of repetitious predictions in audio that the model is uncertain about, you might want to drop the compression ratio threshold a little lower.</p>\n<p>Hopefully this blog helped you explore the many options in the Whisper repo and find a configuration that works best for your tasks!</p>' }, "file": "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/exploring-whisper/index.md" };
function rawContent() {
  return '\nOpenAI recently released a new open source ASR model named Whisper and a repo full of tools that make it easy to try it out. In this blog, we will explore some of the options in Whisper\u2019s inference and see how they impact results.\n\nWhen experimenting with Whisper, you have a few options. You can run their command line tool, which will set a bunch of parameters for you, but you can also play around with those parameters and change what kind of results you get.\n\nTo install Whisper on your machine, you can follow the Setup guide in their [readme](https://github.com/openai/whisper) which walks you through the steps.\n\nThen, you can import Whisper in your own python code and use their `load_model` function to download the pre-trained weights and initialize one of the models.\n\n```bash\nimport whisper\r\nmodel = whisper.load_model("medium.en", device="cuda")\n```\n\nWhisper is available as multilingual models, but we will focus on the english only versions here. The options are:\n\n*   `tiny.en`\n*   `base.en`\n*   `small.en`\n*   `medium.en`\n*   `large` (the large model is only available in the multi-language form)\n\nWhisper is an encoder-decoder transformer model that takes in audio features and generates text. The models\u2019 parameter sizes range from 39 M for tiny and up to 1550 M for large.\n\nWhisper makes it very easy to transcribe an audio file from its path on the file system and will take care of loading the audio using `ffmpeg` and featurizing it before running inference on the file.\n\nIn order to run the transcribe function, you need to make some decisions about how you want to decode the model\'s predictions into text.\n\nYou can call the transcribe function without explicitly setting the decode options and it will set some defaults for you.\n\n```python\ntranscription = model.transcribe("hello_world.mp3", task=\u201Dtranscribe\u201D, language="en")\r\nprint(transcription["text"])\r\n\' Hello world.\'\n```\n\n## Decoding\n\nIn the Whisper [paper](https://cdn.openai.com/papers/whisper.pdf), they describe their complex decoding strategy including a few heuristics they landed on to try and make transcription more reliable. The way these strategies are currently implemented in their code results in slightly improved test results, but can slow down inference by up to 6x. This is because they repeatedly run inference with different decoding strategies until they meet the heuristics.\n\nTwo of the heuristics they use are:\n\n**Compression ratio:** the compression ratio heuristic is defined by the calculation:\n\n`compression_ratio = len(text)/len(zlib.compress(text.encode("utf-8")))`\n\nzlib\u2019s compression capitalizes on repeated sequences, so a text string with many repeated sequences will be able to compress down more than a string with more unique sequences. The goal of this threshold is to prevent the model from outputting predictions where it has gotten stuck and generated the same phrase over and over. You can see an example of this in the Non-speech section below.\n\nIn the whisper code, they set their ratio threshold to be `2.4`. A sequence that has a higher ratio is re-inferenced and decoded with a different strategy.\n\n**Average log probability:** after taking the log softmax of the network\'s output logits, the average log probability of the tokens chosen by the decoding is used. This can be thought of as a confidence measure of the model\'s predictions. The whisper authors use `-1.0` as their threshold.\n\nBy default, the whisper cli tool runs inference and decoding up to 6 times with the following decoding strategies. For each strategy, if the results pass the compression and log probability heuristics, the predicted tokens are used. If the heuristics are not met, the segment is re-inference and re-decoded with the next strategy.\n\nThe decoding strategies are:\n\n1.  Beam search with 5 beams using log probability for the score function\n2.  Greedy decoding with best of 5 sampling. The following temperatures are used for successive attempts: (0.0, 0.2, 0.4, 0.6, 0.8, 1.0)\n\nThe authors quantify the effects of these strategies in [Table 7](https://cdn.openai.com/papers/whisper.pdf) of their paper. They have a varied impact across datasets, but overall the effects for any single intervention are not large.\n\nHere we can look at a few different decoding strategies for the same file and see how they impact results. For this experiment we use the following [clip](https://www.youtube.com/watch?v=FTrxDBDBOHU) from Star Wars. It\'s a useful example because the different decoding strategies make the most difference in areas where the model is less certain. In this clip, there is dramatic music at the beginning, followed by the words "hello there" in a normal voice and then "General Kenobi" in a robotic voice with some strange noises thrown in.\n\nFirst we try running the full decoding strategy, and see that it correctly transcribes the file:\n\n```python\nbeam_size=5\r\nbest_of=5\r\ntemperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0)\r\n\r\ndecode_options = dict(language="en", best_of=best_of, beam_size=beam_size, temperature=temperature)\r\ntranscribe_options = dict(task="transcribe", **decode_options)\r\n\r\ntranscription = model.transcribe("Obi-Wan.mp3", **transcribe_options)\r\nprint(transcription["text"])\r\n\r\n\' Hello there. General Kenobi!\'\n```\n\nNext, we try removing the beam size strategy and dropping the best\\_of value for greedy decoding down to 1. Here we can see that now the model is outputting "ominious music" before the actual speech. This suggests the model was trained on some form of closed captioning.\n\n```python\nbeam_size=None\r\nbest_of=1\r\ntemperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0)\r\n\r\ndecode_options = dict(language="en", best_of=best_of, beam_size=beam_size, temperature=temperature)\r\ntranscribe_options = dict(task="transcribe", **decode_options)\r\n\r\ntranscription = model.transcribe(\'Obi-Wan.mp3\', **transcribe_options)\r\nprint(transcription[\'text\'])\r\n\r\n\'ominous music hello there General Kenobi!\'\n```\n\nFinally, we try increasing the best\\_of value for greedy decoding, but reducing the temperature values tried.\n\n```python\nbeam_size=None\r\nbest_of=3\r\ntemperature=(0.0, 0.2, 0.4)\r\n\r\ndecode_options = dict(language="en", best_of=best_of, beam_size=beam_size, temperature=temperature)\r\ntranscribe_options = dict(task="transcribe", **decode_options)\r\n\r\ntranscription = model.transcribe("Obi-Wan.mp3", **transcribe_options)\r\nprint(transcription["text"])\r\n\r\n\' Thanks for watching!\'\n```\n\nThere we can see that with only very low temperature values, and a best\\_of set to 3, we get a remnant from the weak supervision from internet transcripts. There is randomness in this process, so repeating those experiments will different results each time.\n\nThese examples help explain why the authors recommend attempting multiple decodings and checking the heuristics as a way of improving performance. Trying to hone in on what decoding strategy works best for your data can simplify the inference process for running Whisper and make the current code run faster.\n\n### Non-speech\n\nOne area the model struggles with is periods of non-speech. After the decoding algorithms requested are run, the authors check one more heursitic:\n\n**No speech threshold:** this is another heuristic the authors check after all the decoding is finished. It is based on the probability assigned to the no speech token during inference. The authors use a threshold of `0.6` to detect non-speech. However, if the average log probability passes its threshold then this heuristic is ignored.\n\nTo explore the models\' performances on areas of non-speech, we use this [cat video](https://www.youtube.com/watch?v=gBx4IwYe3L8) which contains only background music and cat noises.\n\nSending the first 30 seconds of the cat video through the model with the recommended decoding produces some text.\n\n```python\nbeam_size=5\r\nbest_of=5\r\ntemperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0)\r\n\r\ndecode_options = dict(language="en", best_of=best_of, beam_size=beam_size, temperature=temperature)\r\ntranscribe_options = dict(task="transcribe", **decode_options)\r\n\r\ntranscription = model.transcribe("kittens_30secs.mp3", **transcribe_options)\r\nprint(transcription["text"])\r\n\r\n\' parrot one parrot you\'\n```\n\nThis means that the no speech threshold described above was not met. And if we check the model\'s predicted probability for the non-speech token we see that it was only 0.535. Too low to hit the threshold of 0.6.\n\nIf we run this same test with the `base.en` model instead of the `medium.en`:\n\n```python\nmodel = whisper.load_model("base.en", device="cuda")\r\nbeam_size=5\r\nbest_of=5\r\ntemperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0)\r\n\r\ndecode_options = dict(language="en", best_of=best_of, beam_size=beam_size, temperature=temperature)\r\ntranscribe_options = dict(task="transcribe", **decode_options)\r\n\r\ntranscription = model.transcribe("kittens_30secs.mp3", **transcribe_options)\r\nprint(transcription["text"])\r\n\r\n\'\'\n```\n\nFor the base model, the probability for the non speech token was `0.641`, which hit the threshold, leading the model to correctly output no speech for the audio.\n\nIf we run the same experiment for more of the models, we can see how they handle the non-speech differently.\n\n| Model     | no\\_speech probability | Predicted text |\r\n|-----------|-----------------------|----------------|\r\n| base.en   | 0.64                  |\'\'\r\n| small.en | 0.467                 |\' you\'          |\r\n| medium.en | 0.53                  |\'few weeks ago\'|\r\n| large     | 0.5                   |"Last couple days I almost lost it. I am excited for the day cuz I am Bye!"|\n\nIt doesn\'t correlate perfectly with size, but we can see that the larger models may need a higher no silence threshold for audio known to contain periods of background noise and no speech. This could also be solved by using a voice activity detection algorithm in parallel with Whisper, and only running Whisper on segments believed to contain speech.\n\nThe other thing noticeable in those results is how the larger models are also more "wordy" under uncertainty, with the large model outputting two random sentences. This points to the high capacity for the large models to memorize some of the lower quality supervision. The authors try to counter the effects of the weak supervision by providing a lot of it, over 600k hours, but its clear that some of the noise in the truth quality ends up stored in the weights of the larger models.\n\nFinally, this file is also a good example to see the scenario that the compression ratio is designed to prevent. If we look at results on this non-speech file with only running beam search decoding:\n\n```python\nbeam_size=5\r\nbest_of=None\r\ntemperature=0.0\r\n\r\ndecode_options = dict(language="en", best_of=best_of, beam_size=beam_size, temperature=temperature)\r\ntranscribe_options = dict(task="transcribe", **decode_options)\r\n\r\ntranscription = model.transcribe("kittens_30secs.mp3", **transcribe_options)\r\nprint(transcription["text"])\r\n\r\n" I\'m not going to lie, I\'m not going to lie, I\'m not going to lie."\n```\n\nThe compression ratio for that prediction string is actually only `1.91`, which is less than their threshold of `2.4`. It\'s worth noting that if you don\'t use their full decoding scheme and start encountering a lot of repetitious predictions in audio that the model is uncertain about, you might want to drop the compression ratio threshold a little lower.\n\nHopefully this blog helped you explore the many options in the Whisper repo and find a configuration that works best for your tasks!\n\n';
}
function compiledContent() {
  return '<p>OpenAI recently released a new open source ASR model named Whisper and a repo full of tools that make it easy to try it out. In this blog, we will explore some of the options in Whisper\u2019s inference and see how they impact results.</p>\n<p>When experimenting with Whisper, you have a few options. You can run their command line tool, which will set a bunch of parameters for you, but you can also play around with those parameters and change what kind of results you get.</p>\n<p>To install Whisper on your machine, you can follow the Setup guide in their <a href="https://github.com/openai/whisper">readme</a> which walks you through the steps.</p>\n<p>Then, you can import Whisper in your own python code and use their <code is:raw>load_model</code> function to download the pre-trained weights and initialize one of the models.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">import whisper</span></span>\n<span class="line"><span style="color: #C9D1D9">model = whisper.load_model(</span><span style="color: #A5D6FF">&quot;medium.en&quot;</span><span style="color: #C9D1D9">, device=</span><span style="color: #A5D6FF">&quot;cuda&quot;</span><span style="color: #C9D1D9">)</span></span></code></pre>\n<p>Whisper is available as multilingual models, but we will focus on the english only versions here. The options are:</p>\n<ul>\n<li><code is:raw>tiny.en</code></li>\n<li><code is:raw>base.en</code></li>\n<li><code is:raw>small.en</code></li>\n<li><code is:raw>medium.en</code></li>\n<li><code is:raw>large</code> (the large model is only available in the multi-language form)</li>\n</ul>\n<p>Whisper is an encoder-decoder transformer model that takes in audio features and generates text. The models\u2019 parameter sizes range from 39 M for tiny and up to 1550 M for large.</p>\n<p>Whisper makes it very easy to transcribe an audio file from its path on the file system and will take care of loading the audio using <code is:raw>ffmpeg</code> and featurizing it before running inference on the file.</p>\n<p>In order to run the transcribe function, you need to make some decisions about how you want to decode the model\u2019s predictions into text.</p>\n<p>You can call the transcribe function without explicitly setting the decode options and it will set some defaults for you.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">transcription </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> model.transcribe(</span><span style="color: #A5D6FF">&quot;hello_world.mp3&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">task</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">\u201Dtranscribe\u201D, </span><span style="color: #FFA657">language</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;en&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcription[</span><span style="color: #A5D6FF">&quot;text&quot;</span><span style="color: #C9D1D9">])</span></span>\n<span class="line"><span style="color: #A5D6FF">&#39; Hello world.&#39;</span></span></code></pre>\n<h2 id="decoding">Decoding</h2>\n<p>In the Whisper <a href="https://cdn.openai.com/papers/whisper.pdf">paper</a>, they describe their complex decoding strategy including a few heuristics they landed on to try and make transcription more reliable. The way these strategies are currently implemented in their code results in slightly improved test results, but can slow down inference by up to 6x. This is because they repeatedly run inference with different decoding strategies until they meet the heuristics.</p>\n<p>Two of the heuristics they use are:</p>\n<p><strong>Compression ratio:</strong> the compression ratio heuristic is defined by the calculation:</p>\n<p><code is:raw>compression_ratio = len(text)/len(zlib.compress(text.encode("utf-8")))</code></p>\n<p>zlib\u2019s compression capitalizes on repeated sequences, so a text string with many repeated sequences will be able to compress down more than a string with more unique sequences. The goal of this threshold is to prevent the model from outputting predictions where it has gotten stuck and generated the same phrase over and over. You can see an example of this in the Non-speech section below.</p>\n<p>In the whisper code, they set their ratio threshold to be <code is:raw>2.4</code>. A sequence that has a higher ratio is re-inferenced and decoded with a different strategy.</p>\n<p><strong>Average log probability:</strong> after taking the log softmax of the network\u2019s output logits, the average log probability of the tokens chosen by the decoding is used. This can be thought of as a confidence measure of the model\u2019s predictions. The whisper authors use <code is:raw>-1.0</code> as their threshold.</p>\n<p>By default, the whisper cli tool runs inference and decoding up to 6 times with the following decoding strategies. For each strategy, if the results pass the compression and log probability heuristics, the predicted tokens are used. If the heuristics are not met, the segment is re-inference and re-decoded with the next strategy.</p>\n<p>The decoding strategies are:</p>\n<ol>\n<li>Beam search with 5 beams using log probability for the score function</li>\n<li>Greedy decoding with best of 5 sampling. The following temperatures are used for successive attempts: (0.0, 0.2, 0.4, 0.6, 0.8, 1.0)</li>\n</ol>\n<p>The authors quantify the effects of these strategies in <a href="https://cdn.openai.com/papers/whisper.pdf">Table 7</a> of their paper. They have a varied impact across datasets, but overall the effects for any single intervention are not large.</p>\n<p>Here we can look at a few different decoding strategies for the same file and see how they impact results. For this experiment we use the following <a href="https://www.youtube.com/watch?v=FTrxDBDBOHU">clip</a> from Star Wars. It\u2019s a useful example because the different decoding strategies make the most difference in areas where the model is less certain. In this clip, there is dramatic music at the beginning, followed by the words \u201Chello there\u201D in a normal voice and then \u201CGeneral Kenobi\u201D in a robotic voice with some strange noises thrown in.</p>\n<p>First we try running the full decoding strategy, and see that it correctly transcribes the file:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">5</span></span>\n<span class="line"><span style="color: #C9D1D9">best_of</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">5</span></span>\n<span class="line"><span style="color: #C9D1D9">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">0.0</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.2</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.4</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.6</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.8</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">1.0</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">decode_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">language</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;en&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">best_of</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">best_of, </span><span style="color: #FFA657">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">beam_size, </span><span style="color: #FFA657">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">temperature)</span></span>\n<span class="line"><span style="color: #C9D1D9">transcribe_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">task</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;transcribe&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">decode_options)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">transcription </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> model.transcribe(</span><span style="color: #A5D6FF">&quot;Obi-Wan.mp3&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">transcribe_options)</span></span>\n<span class="line"><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcription[</span><span style="color: #A5D6FF">&quot;text&quot;</span><span style="color: #C9D1D9">])</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #A5D6FF">&#39; Hello there. General Kenobi!&#39;</span></span></code></pre>\n<p>Next, we try removing the beam size strategy and dropping the best_of value for greedy decoding down to 1. Here we can see that now the model is outputting \u201Cominious music\u201D before the actual speech. This suggests the model was trained on some form of closed captioning.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">None</span></span>\n<span class="line"><span style="color: #C9D1D9">best_of</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">1</span></span>\n<span class="line"><span style="color: #C9D1D9">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">0.0</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.2</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.4</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.6</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.8</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">1.0</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">decode_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">language</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;en&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">best_of</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">best_of, </span><span style="color: #FFA657">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">beam_size, </span><span style="color: #FFA657">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">temperature)</span></span>\n<span class="line"><span style="color: #C9D1D9">transcribe_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">task</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;transcribe&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">decode_options)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">transcription </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> model.transcribe(</span><span style="color: #A5D6FF">&#39;Obi-Wan.mp3&#39;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">transcribe_options)</span></span>\n<span class="line"><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcription[</span><span style="color: #A5D6FF">&#39;text&#39;</span><span style="color: #C9D1D9">])</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #A5D6FF">&#39;ominous music hello there General Kenobi!&#39;</span></span></code></pre>\n<p>Finally, we try increasing the best_of value for greedy decoding, but reducing the temperature values tried.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">None</span></span>\n<span class="line"><span style="color: #C9D1D9">best_of</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">3</span></span>\n<span class="line"><span style="color: #C9D1D9">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">0.0</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.2</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.4</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">decode_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">language</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;en&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">best_of</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">best_of, </span><span style="color: #FFA657">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">beam_size, </span><span style="color: #FFA657">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">temperature)</span></span>\n<span class="line"><span style="color: #C9D1D9">transcribe_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">task</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;transcribe&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">decode_options)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">transcription </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> model.transcribe(</span><span style="color: #A5D6FF">&quot;Obi-Wan.mp3&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">transcribe_options)</span></span>\n<span class="line"><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcription[</span><span style="color: #A5D6FF">&quot;text&quot;</span><span style="color: #C9D1D9">])</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #A5D6FF">&#39; Thanks for watching!&#39;</span></span></code></pre>\n<p>There we can see that with only very low temperature values, and a best_of set to 3, we get a remnant from the weak supervision from internet transcripts. There is randomness in this process, so repeating those experiments will different results each time.</p>\n<p>These examples help explain why the authors recommend attempting multiple decodings and checking the heuristics as a way of improving performance. Trying to hone in on what decoding strategy works best for your data can simplify the inference process for running Whisper and make the current code run faster.</p>\n<h3 id="non-speech">Non-speech</h3>\n<p>One area the model struggles with is periods of non-speech. After the decoding algorithms requested are run, the authors check one more heursitic:</p>\n<p><strong>No speech threshold:</strong> this is another heuristic the authors check after all the decoding is finished. It is based on the probability assigned to the no speech token during inference. The authors use a threshold of <code is:raw>0.6</code> to detect non-speech. However, if the average log probability passes its threshold then this heuristic is ignored.</p>\n<p>To explore the models\u2019 performances on areas of non-speech, we use this <a href="https://www.youtube.com/watch?v=gBx4IwYe3L8">cat video</a> which contains only background music and cat noises.</p>\n<p>Sending the first 30 seconds of the cat video through the model with the recommended decoding produces some text.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">5</span></span>\n<span class="line"><span style="color: #C9D1D9">best_of</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">5</span></span>\n<span class="line"><span style="color: #C9D1D9">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">0.0</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.2</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.4</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.6</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.8</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">1.0</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">decode_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">language</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;en&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">best_of</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">best_of, </span><span style="color: #FFA657">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">beam_size, </span><span style="color: #FFA657">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">temperature)</span></span>\n<span class="line"><span style="color: #C9D1D9">transcribe_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">task</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;transcribe&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">decode_options)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">transcription </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> model.transcribe(</span><span style="color: #A5D6FF">&quot;kittens_30secs.mp3&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">transcribe_options)</span></span>\n<span class="line"><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcription[</span><span style="color: #A5D6FF">&quot;text&quot;</span><span style="color: #C9D1D9">])</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #A5D6FF">&#39; parrot one parrot you&#39;</span></span></code></pre>\n<p>This means that the no speech threshold described above was not met. And if we check the model\u2019s predicted probability for the non-speech token we see that it was only 0.535. Too low to hit the threshold of 0.6.</p>\n<p>If we run this same test with the <code is:raw>base.en</code> model instead of the <code is:raw>medium.en</code>:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">model </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> whisper.load_model(</span><span style="color: #A5D6FF">&quot;base.en&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">device</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;cuda&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">5</span></span>\n<span class="line"><span style="color: #C9D1D9">best_of</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">5</span></span>\n<span class="line"><span style="color: #C9D1D9">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">0.0</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.2</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.4</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.6</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.8</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">1.0</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">decode_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">language</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;en&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">best_of</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">best_of, </span><span style="color: #FFA657">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">beam_size, </span><span style="color: #FFA657">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">temperature)</span></span>\n<span class="line"><span style="color: #C9D1D9">transcribe_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">task</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;transcribe&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">decode_options)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">transcription </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> model.transcribe(</span><span style="color: #A5D6FF">&quot;kittens_30secs.mp3&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">transcribe_options)</span></span>\n<span class="line"><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcription[</span><span style="color: #A5D6FF">&quot;text&quot;</span><span style="color: #C9D1D9">])</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #A5D6FF">&#39;&#39;</span></span></code></pre>\n<p>For the base model, the probability for the non speech token was <code is:raw>0.641</code>, which hit the threshold, leading the model to correctly output no speech for the audio.</p>\n<p>If we run the same experiment for more of the models, we can see how they handle the non-speech differently.</p>\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>no_speech probability</th>\n<th>Predicted text</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>base.en</td>\n<td>0.64</td>\n<td>\u201D</td>\n</tr>\n<tr>\n<td>small.en</td>\n<td>0.467</td>\n<td>\u2019 you\u2019</td>\n</tr>\n<tr>\n<td>medium.en</td>\n<td>0.53</td>\n<td>\u2018few weeks ago\u2019</td>\n</tr>\n<tr>\n<td>large</td>\n<td>0.5</td>\n<td>\u201CLast couple days I almost lost it. I am excited for the day cuz I am Bye!\u201D</td>\n</tr>\n</tbody>\n</table>\n<p>It doesn\u2019t correlate perfectly with size, but we can see that the larger models may need a higher no silence threshold for audio known to contain periods of background noise and no speech. This could also be solved by using a voice activity detection algorithm in parallel with Whisper, and only running Whisper on segments believed to contain speech.</p>\n<p>The other thing noticeable in those results is how the larger models are also more \u201Cwordy\u201D under uncertainty, with the large model outputting two random sentences. This points to the high capacity for the large models to memorize some of the lower quality supervision. The authors try to counter the effects of the weak supervision by providing a lot of it, over 600k hours, but its clear that some of the noise in the truth quality ends up stored in the weights of the larger models.</p>\n<p>Finally, this file is also a good example to see the scenario that the compression ratio is designed to prevent. If we look at results on this non-speech file with only running beam search decoding:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">5</span></span>\n<span class="line"><span style="color: #C9D1D9">best_of</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">None</span></span>\n<span class="line"><span style="color: #C9D1D9">temperature</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">0.0</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">decode_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">language</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;en&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">best_of</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">best_of, </span><span style="color: #FFA657">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">beam_size, </span><span style="color: #FFA657">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">temperature)</span></span>\n<span class="line"><span style="color: #C9D1D9">transcribe_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">task</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;transcribe&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">decode_options)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">transcription </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> model.transcribe(</span><span style="color: #A5D6FF">&quot;kittens_30secs.mp3&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">transcribe_options)</span></span>\n<span class="line"><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcription[</span><span style="color: #A5D6FF">&quot;text&quot;</span><span style="color: #C9D1D9">])</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #A5D6FF">&quot; I&#39;m not going to lie, I&#39;m not going to lie, I&#39;m not going to lie.&quot;</span></span></code></pre>\n<p>The compression ratio for that prediction string is actually only <code is:raw>1.91</code>, which is less than their threshold of <code is:raw>2.4</code>. It\u2019s worth noting that if you don\u2019t use their full decoding scheme and start encountering a lot of repetitious predictions in audio that the model is uncertain about, you might want to drop the compression ratio threshold a little lower.</p>\n<p>Hopefully this blog helped you explore the many options in the Whisper repo and find a configuration that works best for your tasks!</p>';
}
const $$Astro = createAstro("/Users/sandrarodgers/web-next/blog/src/content/blog/posts/exploring-whisper/index.md", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Index = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro, $$props, $$slots);
  Astro2.self = $$Index;
  new Slugger();
  return renderTemplate`<head>${renderHead($$result)}</head><p>OpenAI recently released a new open source ASR model named Whisper and a repo full of tools that make it easy to try it out. In this blog, we will explore some of the options in Whisper’s inference and see how they impact results.</p>
<p>When experimenting with Whisper, you have a few options. You can run their command line tool, which will set a bunch of parameters for you, but you can also play around with those parameters and change what kind of results you get.</p>
<p>To install Whisper on your machine, you can follow the Setup guide in their <a href="https://github.com/openai/whisper">readme</a> which walks you through the steps.</p>
<p>Then, you can import Whisper in your own python code and use their <code>load_model</code> function to download the pre-trained weights and initialize one of the models.</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">import whisper</span></span>
<span class="line"><span style="color: #C9D1D9">model = whisper.load_model(</span><span style="color: #A5D6FF">&quot;medium.en&quot;</span><span style="color: #C9D1D9">, device=</span><span style="color: #A5D6FF">&quot;cuda&quot;</span><span style="color: #C9D1D9">)</span></span></code></pre>
<p>Whisper is available as multilingual models, but we will focus on the english only versions here. The options are:</p>
<ul>
<li><code>tiny.en</code></li>
<li><code>base.en</code></li>
<li><code>small.en</code></li>
<li><code>medium.en</code></li>
<li><code>large</code> (the large model is only available in the multi-language form)</li>
</ul>
<p>Whisper is an encoder-decoder transformer model that takes in audio features and generates text. The models’ parameter sizes range from 39 M for tiny and up to 1550 M for large.</p>
<p>Whisper makes it very easy to transcribe an audio file from its path on the file system and will take care of loading the audio using <code>ffmpeg</code> and featurizing it before running inference on the file.</p>
<p>In order to run the transcribe function, you need to make some decisions about how you want to decode the model’s predictions into text.</p>
<p>You can call the transcribe function without explicitly setting the decode options and it will set some defaults for you.</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">transcription </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> model.transcribe(</span><span style="color: #A5D6FF">&quot;hello_world.mp3&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">task</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">”transcribe”, </span><span style="color: #FFA657">language</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;en&quot;</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcription[</span><span style="color: #A5D6FF">&quot;text&quot;</span><span style="color: #C9D1D9">])</span></span>
<span class="line"><span style="color: #A5D6FF">&#39; Hello world.&#39;</span></span></code></pre>
<h2 id="decoding">Decoding</h2>
<p>In the Whisper <a href="https://cdn.openai.com/papers/whisper.pdf">paper</a>, they describe their complex decoding strategy including a few heuristics they landed on to try and make transcription more reliable. The way these strategies are currently implemented in their code results in slightly improved test results, but can slow down inference by up to 6x. This is because they repeatedly run inference with different decoding strategies until they meet the heuristics.</p>
<p>Two of the heuristics they use are:</p>
<p><strong>Compression ratio:</strong> the compression ratio heuristic is defined by the calculation:</p>
<p><code>compression_ratio = len(text)/len(zlib.compress(text.encode("utf-8")))</code></p>
<p>zlib’s compression capitalizes on repeated sequences, so a text string with many repeated sequences will be able to compress down more than a string with more unique sequences. The goal of this threshold is to prevent the model from outputting predictions where it has gotten stuck and generated the same phrase over and over. You can see an example of this in the Non-speech section below.</p>
<p>In the whisper code, they set their ratio threshold to be <code>2.4</code>. A sequence that has a higher ratio is re-inferenced and decoded with a different strategy.</p>
<p><strong>Average log probability:</strong> after taking the log softmax of the network’s output logits, the average log probability of the tokens chosen by the decoding is used. This can be thought of as a confidence measure of the model’s predictions. The whisper authors use <code>-1.0</code> as their threshold.</p>
<p>By default, the whisper cli tool runs inference and decoding up to 6 times with the following decoding strategies. For each strategy, if the results pass the compression and log probability heuristics, the predicted tokens are used. If the heuristics are not met, the segment is re-inference and re-decoded with the next strategy.</p>
<p>The decoding strategies are:</p>
<ol>
<li>Beam search with 5 beams using log probability for the score function</li>
<li>Greedy decoding with best of 5 sampling. The following temperatures are used for successive attempts: (0.0, 0.2, 0.4, 0.6, 0.8, 1.0)</li>
</ol>
<p>The authors quantify the effects of these strategies in <a href="https://cdn.openai.com/papers/whisper.pdf">Table 7</a> of their paper. They have a varied impact across datasets, but overall the effects for any single intervention are not large.</p>
<p>Here we can look at a few different decoding strategies for the same file and see how they impact results. For this experiment we use the following <a href="https://www.youtube.com/watch?v=FTrxDBDBOHU">clip</a> from Star Wars. It’s a useful example because the different decoding strategies make the most difference in areas where the model is less certain. In this clip, there is dramatic music at the beginning, followed by the words “hello there” in a normal voice and then “General Kenobi” in a robotic voice with some strange noises thrown in.</p>
<p>First we try running the full decoding strategy, and see that it correctly transcribes the file:</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">5</span></span>
<span class="line"><span style="color: #C9D1D9">best_of</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">5</span></span>
<span class="line"><span style="color: #C9D1D9">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">0.0</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.2</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.4</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.6</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.8</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">1.0</span><span style="color: #C9D1D9">)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #C9D1D9">decode_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">language</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;en&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">best_of</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">best_of, </span><span style="color: #FFA657">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">beam_size, </span><span style="color: #FFA657">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">temperature)</span></span>
<span class="line"><span style="color: #C9D1D9">transcribe_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">task</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;transcribe&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">decode_options)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #C9D1D9">transcription </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> model.transcribe(</span><span style="color: #A5D6FF">&quot;Obi-Wan.mp3&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">transcribe_options)</span></span>
<span class="line"><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcription[</span><span style="color: #A5D6FF">&quot;text&quot;</span><span style="color: #C9D1D9">])</span></span>
<span class="line"></span>
<span class="line"><span style="color: #A5D6FF">&#39; Hello there. General Kenobi!&#39;</span></span></code></pre>
<p>Next, we try removing the beam size strategy and dropping the best_of value for greedy decoding down to 1. Here we can see that now the model is outputting “ominious music” before the actual speech. This suggests the model was trained on some form of closed captioning.</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">None</span></span>
<span class="line"><span style="color: #C9D1D9">best_of</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">1</span></span>
<span class="line"><span style="color: #C9D1D9">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">0.0</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.2</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.4</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.6</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.8</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">1.0</span><span style="color: #C9D1D9">)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #C9D1D9">decode_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">language</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;en&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">best_of</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">best_of, </span><span style="color: #FFA657">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">beam_size, </span><span style="color: #FFA657">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">temperature)</span></span>
<span class="line"><span style="color: #C9D1D9">transcribe_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">task</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;transcribe&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">decode_options)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #C9D1D9">transcription </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> model.transcribe(</span><span style="color: #A5D6FF">&#39;Obi-Wan.mp3&#39;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">transcribe_options)</span></span>
<span class="line"><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcription[</span><span style="color: #A5D6FF">&#39;text&#39;</span><span style="color: #C9D1D9">])</span></span>
<span class="line"></span>
<span class="line"><span style="color: #A5D6FF">&#39;ominous music hello there General Kenobi!&#39;</span></span></code></pre>
<p>Finally, we try increasing the best_of value for greedy decoding, but reducing the temperature values tried.</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">None</span></span>
<span class="line"><span style="color: #C9D1D9">best_of</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">3</span></span>
<span class="line"><span style="color: #C9D1D9">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">0.0</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.2</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.4</span><span style="color: #C9D1D9">)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #C9D1D9">decode_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">language</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;en&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">best_of</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">best_of, </span><span style="color: #FFA657">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">beam_size, </span><span style="color: #FFA657">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">temperature)</span></span>
<span class="line"><span style="color: #C9D1D9">transcribe_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">task</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;transcribe&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">decode_options)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #C9D1D9">transcription </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> model.transcribe(</span><span style="color: #A5D6FF">&quot;Obi-Wan.mp3&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">transcribe_options)</span></span>
<span class="line"><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcription[</span><span style="color: #A5D6FF">&quot;text&quot;</span><span style="color: #C9D1D9">])</span></span>
<span class="line"></span>
<span class="line"><span style="color: #A5D6FF">&#39; Thanks for watching!&#39;</span></span></code></pre>
<p>There we can see that with only very low temperature values, and a best_of set to 3, we get a remnant from the weak supervision from internet transcripts. There is randomness in this process, so repeating those experiments will different results each time.</p>
<p>These examples help explain why the authors recommend attempting multiple decodings and checking the heuristics as a way of improving performance. Trying to hone in on what decoding strategy works best for your data can simplify the inference process for running Whisper and make the current code run faster.</p>
<h3 id="non-speech">Non-speech</h3>
<p>One area the model struggles with is periods of non-speech. After the decoding algorithms requested are run, the authors check one more heursitic:</p>
<p><strong>No speech threshold:</strong> this is another heuristic the authors check after all the decoding is finished. It is based on the probability assigned to the no speech token during inference. The authors use a threshold of <code>0.6</code> to detect non-speech. However, if the average log probability passes its threshold then this heuristic is ignored.</p>
<p>To explore the models’ performances on areas of non-speech, we use this <a href="https://www.youtube.com/watch?v=gBx4IwYe3L8">cat video</a> which contains only background music and cat noises.</p>
<p>Sending the first 30 seconds of the cat video through the model with the recommended decoding produces some text.</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">5</span></span>
<span class="line"><span style="color: #C9D1D9">best_of</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">5</span></span>
<span class="line"><span style="color: #C9D1D9">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">0.0</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.2</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.4</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.6</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.8</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">1.0</span><span style="color: #C9D1D9">)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #C9D1D9">decode_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">language</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;en&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">best_of</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">best_of, </span><span style="color: #FFA657">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">beam_size, </span><span style="color: #FFA657">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">temperature)</span></span>
<span class="line"><span style="color: #C9D1D9">transcribe_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">task</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;transcribe&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">decode_options)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #C9D1D9">transcription </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> model.transcribe(</span><span style="color: #A5D6FF">&quot;kittens_30secs.mp3&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">transcribe_options)</span></span>
<span class="line"><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcription[</span><span style="color: #A5D6FF">&quot;text&quot;</span><span style="color: #C9D1D9">])</span></span>
<span class="line"></span>
<span class="line"><span style="color: #A5D6FF">&#39; parrot one parrot you&#39;</span></span></code></pre>
<p>This means that the no speech threshold described above was not met. And if we check the model’s predicted probability for the non-speech token we see that it was only 0.535. Too low to hit the threshold of 0.6.</p>
<p>If we run this same test with the <code>base.en</code> model instead of the <code>medium.en</code>:</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">model </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> whisper.load_model(</span><span style="color: #A5D6FF">&quot;base.en&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">device</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;cuda&quot;</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">5</span></span>
<span class="line"><span style="color: #C9D1D9">best_of</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">5</span></span>
<span class="line"><span style="color: #C9D1D9">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">0.0</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.2</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.4</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.6</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">0.8</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">1.0</span><span style="color: #C9D1D9">)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #C9D1D9">decode_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">language</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;en&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">best_of</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">best_of, </span><span style="color: #FFA657">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">beam_size, </span><span style="color: #FFA657">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">temperature)</span></span>
<span class="line"><span style="color: #C9D1D9">transcribe_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">task</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;transcribe&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">decode_options)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #C9D1D9">transcription </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> model.transcribe(</span><span style="color: #A5D6FF">&quot;kittens_30secs.mp3&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">transcribe_options)</span></span>
<span class="line"><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcription[</span><span style="color: #A5D6FF">&quot;text&quot;</span><span style="color: #C9D1D9">])</span></span>
<span class="line"></span>
<span class="line"><span style="color: #A5D6FF">&#39;&#39;</span></span></code></pre>
<p>For the base model, the probability for the non speech token was <code>0.641</code>, which hit the threshold, leading the model to correctly output no speech for the audio.</p>
<p>If we run the same experiment for more of the models, we can see how they handle the non-speech differently.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>no_speech probability</th>
<th>Predicted text</th>
</tr>
</thead>
<tbody>
<tr>
<td>base.en</td>
<td>0.64</td>
<td>”</td>
</tr>
<tr>
<td>small.en</td>
<td>0.467</td>
<td>’ you’</td>
</tr>
<tr>
<td>medium.en</td>
<td>0.53</td>
<td>‘few weeks ago’</td>
</tr>
<tr>
<td>large</td>
<td>0.5</td>
<td>“Last couple days I almost lost it. I am excited for the day cuz I am Bye!”</td>
</tr>
</tbody>
</table>
<p>It doesn’t correlate perfectly with size, but we can see that the larger models may need a higher no silence threshold for audio known to contain periods of background noise and no speech. This could also be solved by using a voice activity detection algorithm in parallel with Whisper, and only running Whisper on segments believed to contain speech.</p>
<p>The other thing noticeable in those results is how the larger models are also more “wordy” under uncertainty, with the large model outputting two random sentences. This points to the high capacity for the large models to memorize some of the lower quality supervision. The authors try to counter the effects of the weak supervision by providing a lot of it, over 600k hours, but its clear that some of the noise in the truth quality ends up stored in the weights of the larger models.</p>
<p>Finally, this file is also a good example to see the scenario that the compression ratio is designed to prevent. If we look at results on this non-speech file with only running beam search decoding:</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">5</span></span>
<span class="line"><span style="color: #C9D1D9">best_of</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">None</span></span>
<span class="line"><span style="color: #C9D1D9">temperature</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">0.0</span></span>
<span class="line"></span>
<span class="line"><span style="color: #C9D1D9">decode_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">language</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;en&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">best_of</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">best_of, </span><span style="color: #FFA657">beam_size</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">beam_size, </span><span style="color: #FFA657">temperature</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">temperature)</span></span>
<span class="line"><span style="color: #C9D1D9">transcribe_options </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">dict</span><span style="color: #C9D1D9">(</span><span style="color: #FFA657">task</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;transcribe&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">decode_options)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #C9D1D9">transcription </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> model.transcribe(</span><span style="color: #A5D6FF">&quot;kittens_30secs.mp3&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FF7B72">**</span><span style="color: #C9D1D9">transcribe_options)</span></span>
<span class="line"><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcription[</span><span style="color: #A5D6FF">&quot;text&quot;</span><span style="color: #C9D1D9">])</span></span>
<span class="line"></span>
<span class="line"><span style="color: #A5D6FF">&quot; I&#39;m not going to lie, I&#39;m not going to lie, I&#39;m not going to lie.&quot;</span></span></code></pre>
<p>The compression ratio for that prediction string is actually only <code>1.91</code>, which is less than their threshold of <code>2.4</code>. It’s worth noting that if you don’t use their full decoding scheme and start encountering a lot of repetitious predictions in audio that the model is uncertain about, you might want to drop the compression ratio threshold a little lower.</p>
<p>Hopefully this blog helped you explore the many options in the Whisper repo and find a configuration that works best for your tasks!</p>`;
}, "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/exploring-whisper/index.md");

export { compiledContent, $$Index as default, frontmatter, metadata, rawContent };
