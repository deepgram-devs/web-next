import { c as createAstro, a as createComponent, r as renderTemplate, b as renderHead, d as renderComponent } from '../entry.mjs';
import Slugger from 'github-slugger';
import '@astrojs/netlify/netlify-functions.js';
import 'preact';
import 'preact-render-to-string';
import 'vue';
import 'vue/server-renderer';
import 'html-escaper';
import 'node-html-parser';
import 'axios';
/* empty css                           *//* empty css                           *//* empty css                           */import '@storyblok/js';
/* empty css                           *//* empty css                          */import 'clone-deep';
import 'slugify';
import 'shiki';
/* empty css                           */import 'camelcase';
import '@astrojs/rss';
/* empty css                           */import 'mime';
import 'cookie';
import 'kleur/colors';
import 'string-width';
import 'path-browserify';
import 'path-to-regexp';

const metadata = { "headings": [{ "depth": 2, "slug": "before-we-start", "text": "Before We Start" }, { "depth": 2, "slug": "get-microphone-permissions", "text": "Get Microphone Permissions" }, { "depth": 2, "slug": "setting-up-avaudioengine", "text": "Setting Up AVAudioEngine" }, { "depth": 2, "slug": "get-microphone-data", "text": "Get Microphone Data" }, { "depth": 2, "slug": "connect-to-deepgram", "text": "Connect to Deepgram" }, { "depth": 2, "slug": "send-data-to-deepgram", "text": "Send Data to Deepgram" }, { "depth": 2, "slug": "handle-the-deepgram-response", "text": "Handle the Deepgram Response" }], "source": '\n<guest-author></guest-author>\n\nDeepgram\'s Speech Recognition API can provide accurate transcripts with pre-recorded and live audio. In this post, you will build an iOS app that takes the user\'s microphone and prints transcripts to the screen in real-time.\n\nThe final project code is available at https://github.com/deepgram-devs/deepgram-live-transcripts-ios.\n\n## Before We Start\n\nFor this project, you will need:\n\n*   A Deepgram API Key - [get one here](https://console.deepgram.com/signup?jump=keys)\n*   [Xcode](https://developer.apple.com/xcode/) installed on your machine.\n\nOnce you open Xcode, create a new Xcode project. Select the App template, give it a product name, select "Storyboard" for the interface, and "Swift" for the language. Untick "User Core Data" and "Include Tests", then hit next.\n\nChoose a place to save your project and hit finish to open your new project.\n\nYou will be using the WebSocket library [Starscream](https://github.com/daltoniam/Starscream) for this project. You can add it as a Swift Package by going to *File > Add Packages*, then entering the URL (https://github.com/daltoniam/Starscream) into the search box. Click add package and wait for the package to download.\n\n## Get Microphone Permissions\n\nYou will be using the microphone for this project, so you will need to add a microphone usage description to your project. When an iOS app asks for microphone permission, a reason why the permission is being requested is required and will be shown to the user. Open the `info.plist` file to add a description.\n\nHover over the last entry and click the plus icon to add a new entry. The key should be "Privacy - Microphone Usage Description", and you should add a value that describes why you need the microphone. You can use "To transcribe audio with Deepgram".\n\n![Plist file with the microphone usage description](https://res.cloudinary.com/deepgram/image/upload/v1638827585/blog/2022/01/ios-live-transcription/plist.png)\n\n## Setting Up `AVAudioEngine`\n\nTo access a data stream from the microphone, you will use [`AVAudioEngine`](https://developer.apple.com/documentation/avfaudio/avaudioengine).\n\nBefore you get started, it is worth having a mental model of how `AVAudioEngine` works. Apple\'s description is: "An audio engine object contains a group of AVAudioNode instances that you attach to form an audio processing chain".\n\nFor this example, you will create a chain of nodes that will take the microphone\'s input, convert it to a format that Deepgram can process, then send the data to Deepgram. The nodes and conversion will be discussed further in the blog.\n\n![AVAudioEngine nodes for this project](https://res.cloudinary.com/deepgram/image/upload/v1638827585/blog/2022/01/ios-live-transcription/engine-diagram.png)\n\nOpen the `ViewController.swift` file and import AVFoundation and Starscream at the top\n\n    import AVFoundation\n    Import Starscream\n\nThis will give you access to the `AVAudioEngine` class. Then inside the `ViewController` class, create an instance of `AVAudioEngine` by adding a property:\n\n```swift\nprivate let audioEngine = AVAudioEngine()\n```\n\nNext, create a function to analyse the audio and declare some constants:\n\n```swift\nprivate func startAnalyzingAudio() {\n  let inputNode = audioEngine.inputNode\n  let inputFormat = inputNode.inputFormat(forBus: 0)\n  let outputFormat = AVAudioFormat(commonFormat: .pcmFormatInt16, sampleRate: inputFormat.sampleRate, channels: inputFormat.channelCount, interleaved: true)\n}\n```\n\nFrom the top, you have the `inputNode`, which you can think of as the microphone followed by the `inputFormat`.\n\nNext is the `outputFormat` - The iOS microphone is a 32-bit depth format by default, so you will be converting to a 16-bit depth format before sending the data to Deepgram. While we will use a PCM 16-bit depth format for the audio encoding, Deepgram supports many audio encodings that can be specified to the API. Consider using a more compressed format if your use case requires low network usage. Just below those, add the new nodes that you will need and attach them to the audio engine:\n\n```swift\nlet converterNode = AVAudioMixerNode()\nlet sinkNode = AVAudioMixerNode()\n\naudioEngine.attach(converterNode)\naudioEngine.attach(sinkNode)\n```\n\nThe sinkNode is needed because to get the data in the correct format you need to access the data after it has passed through the converterNode, or its output. If you refer to the diagram above, notice how the stream of data to Deepgram is coming from the connection between nodes, the output of the `converterNode`, and not the nodes themselves.\n\n## Get Microphone Data\n\nUse the `installTap` function on the `AVAudioMixerNode` class to get the microphone data. This function gives you a closure that returns the output audio data of a node as a buffer. Call `installTap` on the `converterNode`, continuing the function from above:\n\n```swift\nconverterNode.installTap(onBus: 0, bufferSize: 1024, format: converterNode.outputFormat(forBus: 0)) { (buffer: AVAudioPCMBuffer!, time: AVAudioTime!) -> Void in\n\n}\n```\n\nYou will come back to this closure later. Now finish off the `startAnalyzingAudio` by connecting all the nodes and starting the audio engine:\n\n```swift\naudioEngine.connect(inputNode, to: converterNode, format: inputFormat)\naudioEngine.connect(converterNode, to: sinkNode, format: outputFormat)\naudioEngine.prepare()\n\ndo {\ntry AVAudioSession.sharedInstance().setCategory(.record)\n  try audioEngine.start()\n} catch {\n  print(error)\n}\n```\n\nYou can see now how the processing pipeline mirrors the diagram from earlier. Before the audio engine is started, the audio session of the application needs to be set to a record category. This lets the app know that the audio engine is solely being used for recording purposes. Note how the `converterNode` is connected with `outputFormat`.\n\n## Connect to Deepgram\n\nYou will be using the Deepgram WebSocket Streaming API, create a `WebSocket` instance at the top of the `ViewController` class:\n\n```swift\nprivate let apiKey = "Token YOUR_DEEPGRAM_API_KEY"\nprivate lazy var socket: WebSocket = {\n  let url = URL(string: "wss://api.deepgram.com/v1/listen?encoding=linear16&sample_rate=48000&channels=1")!\n  var urlRequest = URLRequest(url: url)\n  urlRequest.setValue(apiKey, forHTTPHeaderField: "Authorization")\n  return WebSocket(request: urlRequest)\n}()\n```\n\n<Alert type="warning">A reminder that this key is in your app\'s source code and, therefore, it is not secure. Please factor this into your actual projects.</Alert>\n\nNote how the URL has an encoding matching the `outputFormat` from earlier. In the `viewDidLoad` function, set the socket\'s delegate to this class and open the connection:\n\n```swift\noverride func viewDidLoad() {\n  super.viewDidLoad()\n  socket.delegate = self\n  socket.connect()\n}\n```\n\nYou will implement the delegate later in the post.\n\n## Send Data to Deepgram\n\nNow that the socket connection is open, you can now send the microphone data to Deepgram. To send data over a WebSocket in iOS, it needs to be converted to a `Data` object. Add the following function to the `ViewController` class that does the conversion:\n\n```swift\nprivate func toNSData(buffer: AVAudioPCMBuffer) -> Data? {\n  let audioBuffer = buffer.audioBufferList.pointee.mBuffers\n  return Data(bytes: audioBuffer.mData!, count: Int(audioBuffer.mDataByteSize))\n}\n```\n\nThen return to the `startAnalyzingAudio` function, and within the `installTap` closure, you can send the data to Deepgram. Your tap code should look like this now:\n\n```swift\nconverterNode.installTap(onBus: bus, bufferSize: 1024, format: converterNode.outputFormat(forBus: bus)) { (buffer: AVAudioPCMBuffer!, time: AVAudioTime!) -> Void in\n  if let data = self.toNSData(buffer: buffer) {\n     self.socket.write(data: data)\n  }\n}\n```\n\nCall `startAnalyzingAudio` in the `viewDidLoad` function below the WebSocket configuration.\n\n## Handle the Deepgram Response\n\nYou will get updates from the WebSocket via its [delegate](https://www.hackingwithswift.com/example-code/language/what-is-a-delegate-in-ios). In the `ViewController.swift` file *outside* the class, create an extension for the `WebSocketDelegate` and a `DeepgramResponse` struct:\n\n```swift\nextension ViewController: WebSocketDelegate {\n  func didReceive(event: WebSocketEvent, client: WebSocket) {\n     switch event {\n     case .text(let text):\n\n     case .error(let error):\n        print(error ?? "")\n     default:\n        break\n     }\n  }\n}\n\nstruct DeepgramResponse: Codable {\n  let isFinal: Bool\n  let channel: Channel\n\n  struct Channel: Codable {\n     let alternatives: [Alternatives]\n  }\n\n  struct Alternatives: Codable {\n     let transcript: String\n  }\n}\n```\n\nThe `didReceive` function on the `WebSocketDelegate` will be called whenever you get an update on the WebSocket. Before you finish implementing `didReceive`, you need to prepare to decode the data and update the UI to display the transcripts. At the top of the `ViewController` class, add the following properties:\n\n```swift\nprivate let jsonDecoder: JSONDecoder = {\n  let decoder = JSONDecoder()\n  decoder.keyDecodingStrategy = .convertFromSnakeCase\n  return decoder\n}()\n\nprivate let transcriptView: UITextView = {\n  let textView = UITextView()\n  textView.isScrollEnabled = true\n  textView.backgroundColor = .lightGray\n  textView.translatesAutoresizingMaskIntoConstraints = false\n  return textView\n}()\n```\n\nCreate a new function called `setupView` and configure your UI:\n\n```swift\nprivate func setupView() {\n  view.addSubview(transcriptView)\n  NSLayoutConstraint.activate([\n     transcriptView.topAnchor.constraint(equalTo: view.safeAreaLayoutGuide.topAnchor),\n     transcriptView.leadingAnchor.constraint(equalTo: view.leadingAnchor, constant: 16),\n     transcriptView.trailingAnchor.constraint(equalTo: view.trailingAnchor, constant: -16),\n     transcriptView.bottomAnchor.constraint(equalTo: view.safeAreaLayoutGuide.bottomAnchor)\n  ])\n}\n```\n\nThen call `setupView` in the `viewDidLoad` function.\n\nReturning to the `didReceive` function of the `WebSocketDelegate`, within the text case; you need to convert the text, which is a `String` type, into a `Data` type so you can decode it into an instance of `DeepgramResponse`:\n\n```swift\nextension ViewController: WebSocketDelegate {\n  func didReceive(event: WebSocketEvent, client: WebSocket) {\n     switch event {\n     case .text(let text):\n        let jsonData = Data(text.utf8)\n        let response = try! jsonDecoder.decode(DeepgramResponse.self, from: jsonData)\n        let transcript = response.channel.alternatives.first!.transcript\n\n        if response.isFinal && !transcript.isEmpty {\n          if transcriptView.text.isEmpty {\n             transcriptView.text = transcript\n          } else {\n             transcriptView.text = transcriptView.text + " " + transcript\n          }\n        }\n     case .error(let error):\n        print(error ?? "")\n     default:\n        break\n     }\n  }\n}\n```\n\nOnce you have a `DeepgramResponse` instance, you will check if it is final, meaning it is ready to add to the `transcriptView` while handling the empty scenarios. If you now build and run the project (CMD + R), it will open in the simulator, where you will be prompted to give microphone access. Then you can start transcribing!\n\n![iPhone simulator showing the text hello blog](https://res.cloudinary.com/deepgram/image/upload/v1638827585/blog/2022/01/ios-live-transcription/simulator.png)\n\nThe final project code is available at https://github.com/deepgram-devs/deepgram-live-transcripts-ios, and if you have any questions, please feel free to reach out to the Deepgram team on Twitter - [@DeepgramDevs](https://twitter.com/DeepgramDevs).\n\n        ', "html": '<guest-author />\n<p>Deepgram\u2019s Speech Recognition API can provide accurate transcripts with pre-recorded and live audio. In this post, you will build an iOS app that takes the user\u2019s microphone and prints transcripts to the screen in real-time.</p>\n<p>The final project code is available at <a href="https://github.com/deepgram-devs/deepgram-live-transcripts-ios">https://github.com/deepgram-devs/deepgram-live-transcripts-ios</a>.</p>\n<h2 id="before-we-start">Before We Start</h2>\n<p>For this project, you will need:</p>\n<ul>\n<li>A Deepgram API Key - <a href="https://console.deepgram.com/signup?jump=keys">get one here</a></li>\n<li><a href="https://developer.apple.com/xcode/">Xcode</a> installed on your machine.</li>\n</ul>\n<p>Once you open Xcode, create a new Xcode project. Select the App template, give it a product name, select \u201CStoryboard\u201D for the interface, and \u201CSwift\u201D for the language. Untick \u201CUser Core Data\u201D and \u201CInclude Tests\u201D, then hit next.</p>\n<p>Choose a place to save your project and hit finish to open your new project.</p>\n<p>You will be using the WebSocket library <a href="https://github.com/daltoniam/Starscream">Starscream</a> for this project. You can add it as a Swift Package by going to <em>File > Add Packages</em>, then entering the URL (<a href="https://github.com/daltoniam/Starscream">https://github.com/daltoniam/Starscream</a>) into the search box. Click add package and wait for the package to download.</p>\n<h2 id="get-microphone-permissions">Get Microphone Permissions</h2>\n<p>You will be using the microphone for this project, so you will need to add a microphone usage description to your project. When an iOS app asks for microphone permission, a reason why the permission is being requested is required and will be shown to the user. Open the <code is:raw>info.plist</code> file to add a description.</p>\n<p>Hover over the last entry and click the plus icon to add a new entry. The key should be \u201CPrivacy - Microphone Usage Description\u201D, and you should add a value that describes why you need the microphone. You can use \u201CTo transcribe audio with Deepgram\u201D.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1638827585/blog/2022/01/ios-live-transcription/plist.png" alt="Plist file with the microphone usage description"></p>\n<h2 id="setting-up-avaudioengine">Setting Up <code is:raw>AVAudioEngine</code></h2>\n<p>To access a data stream from the microphone, you will use <a href="https://developer.apple.com/documentation/avfaudio/avaudioengine"><code is:raw>AVAudioEngine</code></a>.</p>\n<p>Before you get started, it is worth having a mental model of how <code is:raw>AVAudioEngine</code> works. Apple\u2019s description is: \u201CAn audio engine object contains a group of AVAudioNode instances that you attach to form an audio processing chain\u201D.</p>\n<p>For this example, you will create a chain of nodes that will take the microphone\u2019s input, convert it to a format that Deepgram can process, then send the data to Deepgram. The nodes and conversion will be discussed further in the blog.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1638827585/blog/2022/01/ios-live-transcription/engine-diagram.png" alt="AVAudioEngine nodes for this project"></p>\n<p>Open the <code is:raw>ViewController.swift</code> file and import AVFoundation and Starscream at the top</p>\n<p>import AVFoundation\nImport Starscream</p>\n<p>This will give you access to the <code is:raw>AVAudioEngine</code> class. Then inside the <code is:raw>ViewController</code> class, create an instance of <code is:raw>AVAudioEngine</code> by adding a property:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">private</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> audioEngine </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">AVAudioEngine</span><span style="color: #C9D1D9">()</span></span></code></pre>\n<p>Next, create a function to analyse the audio and declare some constants:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">private</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">func</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">startAnalyzingAudio</span><span style="color: #C9D1D9">() {</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> inputNode </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> audioEngine.inputNode</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> inputFormat </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> inputNode.</span><span style="color: #79C0FF">inputFormat</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">forBus</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> outputFormat </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">AVAudioFormat</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">commonFormat</span><span style="color: #C9D1D9">: .pcmFormatInt16, </span><span style="color: #79C0FF">sampleRate</span><span style="color: #C9D1D9">: inputFormat.sampleRate, </span><span style="color: #79C0FF">channels</span><span style="color: #C9D1D9">: inputFormat.channelCount, </span><span style="color: #79C0FF">interleaved</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">true</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>\n<p>From the top, you have the <code is:raw>inputNode</code>, which you can think of as the microphone followed by the <code is:raw>inputFormat</code>.</p>\n<p>Next is the <code is:raw>outputFormat</code> - The iOS microphone is a 32-bit depth format by default, so you will be converting to a 16-bit depth format before sending the data to Deepgram. While we will use a PCM 16-bit depth format for the audio encoding, Deepgram supports many audio encodings that can be specified to the API. Consider using a more compressed format if your use case requires low network usage. Just below those, add the new nodes that you will need and attach them to the audio engine:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> converterNode </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">AVAudioMixerNode</span><span style="color: #C9D1D9">()</span></span>\n<span class="line"><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> sinkNode </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">AVAudioMixerNode</span><span style="color: #C9D1D9">()</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">audioEngine.</span><span style="color: #79C0FF">attach</span><span style="color: #C9D1D9">(converterNode)</span></span>\n<span class="line"><span style="color: #C9D1D9">audioEngine.</span><span style="color: #79C0FF">attach</span><span style="color: #C9D1D9">(sinkNode)</span></span></code></pre>\n<p>The sinkNode is needed because to get the data in the correct format you need to access the data after it has passed through the converterNode, or its output. If you refer to the diagram above, notice how the stream of data to Deepgram is coming from the connection between nodes, the output of the <code is:raw>converterNode</code>, and not the nodes themselves.</p>\n<h2 id="get-microphone-data">Get Microphone Data</h2>\n<p>Use the <code is:raw>installTap</code> function on the <code is:raw>AVAudioMixerNode</code> class to get the microphone data. This function gives you a closure that returns the output audio data of a node as a buffer. Call <code is:raw>installTap</code> on the <code is:raw>converterNode</code>, continuing the function from above:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">converterNode.</span><span style="color: #79C0FF">installTap</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">onBus</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">bufferSize</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">1024</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">format</span><span style="color: #C9D1D9">: converterNode.</span><span style="color: #79C0FF">outputFormat</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">forBus</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">)) { (</span><span style="color: #79C0FF">buffer</span><span style="color: #C9D1D9">: AVAudioPCMBuffer</span><span style="color: #FF7B72">!</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">time</span><span style="color: #C9D1D9">: AVAudioTime</span><span style="color: #FF7B72">!</span><span style="color: #C9D1D9">) </span><span style="color: #FF7B72">-&gt;</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">Void</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">in</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>\n<p>You will come back to this closure later. Now finish off the <code is:raw>startAnalyzingAudio</code> by connecting all the nodes and starting the audio engine:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">audioEngine.</span><span style="color: #79C0FF">connect</span><span style="color: #C9D1D9">(inputNode, </span><span style="color: #79C0FF">to</span><span style="color: #C9D1D9">: converterNode, </span><span style="color: #79C0FF">format</span><span style="color: #C9D1D9">: inputFormat)</span></span>\n<span class="line"><span style="color: #C9D1D9">audioEngine.</span><span style="color: #79C0FF">connect</span><span style="color: #C9D1D9">(converterNode, </span><span style="color: #79C0FF">to</span><span style="color: #C9D1D9">: sinkNode, </span><span style="color: #79C0FF">format</span><span style="color: #C9D1D9">: outputFormat)</span></span>\n<span class="line"><span style="color: #C9D1D9">audioEngine.</span><span style="color: #79C0FF">prepare</span><span style="color: #C9D1D9">()</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #FF7B72">do</span><span style="color: #C9D1D9"> {</span></span>\n<span class="line"><span style="color: #FF7B72">try</span><span style="color: #C9D1D9"> AVAudioSession.</span><span style="color: #79C0FF">sharedInstance</span><span style="color: #C9D1D9">().</span><span style="color: #79C0FF">setCategory</span><span style="color: #C9D1D9">(.record)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">try</span><span style="color: #C9D1D9"> audioEngine.</span><span style="color: #79C0FF">start</span><span style="color: #C9D1D9">()</span></span>\n<span class="line"><span style="color: #C9D1D9">} </span><span style="color: #FF7B72">catch</span><span style="color: #C9D1D9"> {</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(error)</span></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>\n<p>You can see now how the processing pipeline mirrors the diagram from earlier. Before the audio engine is started, the audio session of the application needs to be set to a record category. This lets the app know that the audio engine is solely being used for recording purposes. Note how the <code is:raw>converterNode</code> is connected with <code is:raw>outputFormat</code>.</p>\n<h2 id="connect-to-deepgram">Connect to Deepgram</h2>\n<p>You will be using the Deepgram WebSocket Streaming API, create a <code is:raw>WebSocket</code> instance at the top of the <code is:raw>ViewController</code> class:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">private</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> apiKey </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;Token YOUR_DEEPGRAM_API_KEY&quot;</span></span>\n<span class="line"><span style="color: #FF7B72">private</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">lazy</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">var</span><span style="color: #C9D1D9"> socket: WebSocket </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> {</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> url </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">URL</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">string</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;wss://api.deepgram.com/v1/listen?encoding=linear16&amp;sample_rate=48000&amp;channels=1&quot;</span><span style="color: #C9D1D9">)</span><span style="color: #FF7B72">!</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">var</span><span style="color: #C9D1D9"> urlRequest </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">URLRequest</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">url</span><span style="color: #C9D1D9">: url)</span></span>\n<span class="line"><span style="color: #C9D1D9">  urlRequest.</span><span style="color: #79C0FF">setValue</span><span style="color: #C9D1D9">(apiKey, </span><span style="color: #79C0FF">forHTTPHeaderField</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;Authorization&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">WebSocket</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">request</span><span style="color: #C9D1D9">: urlRequest)</span></span>\n<span class="line"><span style="color: #C9D1D9">}()</span></span></code></pre>\n<Alert type="warning">A reminder that this key is in your app\u2019s source code and, therefore, it is not secure. Please factor this into your actual projects.</Alert>\n<p>Note how the URL has an encoding matching the <code is:raw>outputFormat</code> from earlier. In the <code is:raw>viewDidLoad</code> function, set the socket\u2019s delegate to this class and open the connection:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">override</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">func</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">viewDidLoad</span><span style="color: #C9D1D9">() {</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #79C0FF">super</span><span style="color: #C9D1D9">.</span><span style="color: #79C0FF">viewDidLoad</span><span style="color: #C9D1D9">()</span></span>\n<span class="line"><span style="color: #C9D1D9">  socket.delegate </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span></span>\n<span class="line"><span style="color: #C9D1D9">  socket.</span><span style="color: #79C0FF">connect</span><span style="color: #C9D1D9">()</span></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>\n<p>You will implement the delegate later in the post.</p>\n<h2 id="send-data-to-deepgram">Send Data to Deepgram</h2>\n<p>Now that the socket connection is open, you can now send the microphone data to Deepgram. To send data over a WebSocket in iOS, it needs to be converted to a <code is:raw>Data</code> object. Add the following function to the <code is:raw>ViewController</code> class that does the conversion:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">private</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">func</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">toNSData</span><span style="color: #C9D1D9">(</span><span style="color: #D2A8FF">buffer</span><span style="color: #C9D1D9">: AVAudioPCMBuffer) </span><span style="color: #FF7B72">-&gt;</span><span style="color: #C9D1D9"> Data</span><span style="color: #FF7B72">?</span><span style="color: #C9D1D9"> {</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> audioBuffer </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> buffer.audioBufferList.</span><span style="color: #79C0FF">pointee</span><span style="color: #C9D1D9">.mBuffers</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">Data</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">bytes</span><span style="color: #C9D1D9">: audioBuffer.mData</span><span style="color: #FF7B72">!</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">count</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">Int</span><span style="color: #C9D1D9">(audioBuffer.mDataByteSize))</span></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>\n<p>Then return to the <code is:raw>startAnalyzingAudio</code> function, and within the <code is:raw>installTap</code> closure, you can send the data to Deepgram. Your tap code should look like this now:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">converterNode.</span><span style="color: #79C0FF">installTap</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">onBus</span><span style="color: #C9D1D9">: bus, </span><span style="color: #79C0FF">bufferSize</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">1024</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">format</span><span style="color: #C9D1D9">: converterNode.</span><span style="color: #79C0FF">outputFormat</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">forBus</span><span style="color: #C9D1D9">: bus)) { (</span><span style="color: #79C0FF">buffer</span><span style="color: #C9D1D9">: AVAudioPCMBuffer</span><span style="color: #FF7B72">!</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">time</span><span style="color: #C9D1D9">: AVAudioTime</span><span style="color: #FF7B72">!</span><span style="color: #C9D1D9">) </span><span style="color: #FF7B72">-&gt;</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">Void</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">in</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> data </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.</span><span style="color: #79C0FF">toNSData</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">buffer</span><span style="color: #C9D1D9">: buffer) {</span></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.socket.</span><span style="color: #79C0FF">write</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">data</span><span style="color: #C9D1D9">: data)</span></span>\n<span class="line"><span style="color: #C9D1D9">  }</span></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>\n<p>Call <code is:raw>startAnalyzingAudio</code> in the <code is:raw>viewDidLoad</code> function below the WebSocket configuration.</p>\n<h2 id="handle-the-deepgram-response">Handle the Deepgram Response</h2>\n<p>You will get updates from the WebSocket via its <a href="https://www.hackingwithswift.com/example-code/language/what-is-a-delegate-in-ios">delegate</a>. In the <code is:raw>ViewController.swift</code> file <em>outside</em> the class, create an extension for the <code is:raw>WebSocketDelegate</code> and a <code is:raw>DeepgramResponse</code> struct:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">extension</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">ViewController</span><span style="color: #C9D1D9">: WebSocketDelegate {</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">func</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">didReceive</span><span style="color: #C9D1D9">(</span><span style="color: #D2A8FF">event</span><span style="color: #C9D1D9">: WebSocketEvent, </span><span style="color: #D2A8FF">client</span><span style="color: #C9D1D9">: WebSocket) {</span></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">switch</span><span style="color: #C9D1D9"> event {</span></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">case</span><span style="color: #C9D1D9"> .</span><span style="color: #79C0FF">text</span><span style="color: #C9D1D9">(</span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> text)</span><span style="color: #FF7B72">:</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">case</span><span style="color: #C9D1D9"> .</span><span style="color: #79C0FF">error</span><span style="color: #C9D1D9">(</span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> error)</span><span style="color: #FF7B72">:</span></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(error </span><span style="color: #FF7B72">??</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">default:</span></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #FF7B72">break</span></span>\n<span class="line"><span style="color: #C9D1D9">     }</span></span>\n<span class="line"><span style="color: #C9D1D9">  }</span></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #FF7B72">struct</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">DeepgramResponse</span><span style="color: #C9D1D9">: Codable {</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> isFinal: </span><span style="color: #79C0FF">Bool</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> channel: Channel</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">struct</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">Channel</span><span style="color: #C9D1D9">: Codable {</span></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> alternatives: [Alternatives]</span></span>\n<span class="line"><span style="color: #C9D1D9">  }</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">struct</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">Alternatives</span><span style="color: #C9D1D9">: Codable {</span></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> transcript: </span><span style="color: #79C0FF">String</span></span>\n<span class="line"><span style="color: #C9D1D9">  }</span></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>\n<p>The <code is:raw>didReceive</code> function on the <code is:raw>WebSocketDelegate</code> will be called whenever you get an update on the WebSocket. Before you finish implementing <code is:raw>didReceive</code>, you need to prepare to decode the data and update the UI to display the transcripts. At the top of the <code is:raw>ViewController</code> class, add the following properties:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">private</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> jsonDecoder: JSONDecoder </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> {</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> decoder </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">JSONDecoder</span><span style="color: #C9D1D9">()</span></span>\n<span class="line"><span style="color: #C9D1D9">  decoder.keyDecodingStrategy </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> .convertFromSnakeCase</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> decoder</span></span>\n<span class="line"><span style="color: #C9D1D9">}()</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #FF7B72">private</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> transcriptView: UITextView </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> {</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> textView </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">UITextView</span><span style="color: #C9D1D9">()</span></span>\n<span class="line"><span style="color: #C9D1D9">  textView.isScrollEnabled </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">true</span></span>\n<span class="line"><span style="color: #C9D1D9">  textView.backgroundColor </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> .lightGray</span></span>\n<span class="line"><span style="color: #C9D1D9">  textView.translatesAutoresizingMaskIntoConstraints </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">false</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> textView</span></span>\n<span class="line"><span style="color: #C9D1D9">}()</span></span></code></pre>\n<p>Create a new function called <code is:raw>setupView</code> and configure your UI:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">private</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">func</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">setupView</span><span style="color: #C9D1D9">() {</span></span>\n<span class="line"><span style="color: #C9D1D9">  view.</span><span style="color: #79C0FF">addSubview</span><span style="color: #C9D1D9">(transcriptView)</span></span>\n<span class="line"><span style="color: #C9D1D9">  NSLayoutConstraint.</span><span style="color: #79C0FF">activate</span><span style="color: #C9D1D9">([</span></span>\n<span class="line"><span style="color: #C9D1D9">     transcriptView.topAnchor.</span><span style="color: #79C0FF">constraint</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">equalTo</span><span style="color: #C9D1D9">: view.safeAreaLayoutGuide.topAnchor),</span></span>\n<span class="line"><span style="color: #C9D1D9">     transcriptView.leadingAnchor.</span><span style="color: #79C0FF">constraint</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">equalTo</span><span style="color: #C9D1D9">: view.leadingAnchor, </span><span style="color: #79C0FF">constant</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">16</span><span style="color: #C9D1D9">),</span></span>\n<span class="line"><span style="color: #C9D1D9">     transcriptView.trailingAnchor.</span><span style="color: #79C0FF">constraint</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">equalTo</span><span style="color: #C9D1D9">: view.trailingAnchor, </span><span style="color: #79C0FF">constant</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">-16</span><span style="color: #C9D1D9">),</span></span>\n<span class="line"><span style="color: #C9D1D9">     transcriptView.bottomAnchor.</span><span style="color: #79C0FF">constraint</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">equalTo</span><span style="color: #C9D1D9">: view.safeAreaLayoutGuide.bottomAnchor)</span></span>\n<span class="line"><span style="color: #C9D1D9">  ])</span></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>\n<p>Then call <code is:raw>setupView</code> in the <code is:raw>viewDidLoad</code> function.</p>\n<p>Returning to the <code is:raw>didReceive</code> function of the <code is:raw>WebSocketDelegate</code>, within the text case; you need to convert the text, which is a <code is:raw>String</code> type, into a <code is:raw>Data</code> type so you can decode it into an instance of <code is:raw>DeepgramResponse</code>:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">extension</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">ViewController</span><span style="color: #C9D1D9">: WebSocketDelegate {</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">func</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">didReceive</span><span style="color: #C9D1D9">(</span><span style="color: #D2A8FF">event</span><span style="color: #C9D1D9">: WebSocketEvent, </span><span style="color: #D2A8FF">client</span><span style="color: #C9D1D9">: WebSocket) {</span></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">switch</span><span style="color: #C9D1D9"> event {</span></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">case</span><span style="color: #C9D1D9"> .</span><span style="color: #79C0FF">text</span><span style="color: #C9D1D9">(</span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> text)</span><span style="color: #FF7B72">:</span></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> jsonData </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">Data</span><span style="color: #C9D1D9">(text.</span><span style="color: #79C0FF">utf8</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> response </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">try!</span><span style="color: #C9D1D9"> jsonDecoder.</span><span style="color: #79C0FF">decode</span><span style="color: #C9D1D9">(DeepgramResponse.</span><span style="color: #FF7B72">self</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">from</span><span style="color: #C9D1D9">: jsonData)</span></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> transcript </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> response.channel.alternatives.</span><span style="color: #79C0FF">first</span><span style="color: #FF7B72">!</span><span style="color: #C9D1D9">.transcript</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> response.isFinal </span><span style="color: #FF7B72">&amp;&amp;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">!</span><span style="color: #C9D1D9">transcript.</span><span style="color: #79C0FF">isEmpty</span><span style="color: #C9D1D9"> {</span></span>\n<span class="line"><span style="color: #C9D1D9">          </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> transcriptView.</span><span style="color: #79C0FF">text</span><span style="color: #C9D1D9">.</span><span style="color: #79C0FF">isEmpty</span><span style="color: #C9D1D9"> {</span></span>\n<span class="line"><span style="color: #C9D1D9">             transcriptView.</span><span style="color: #79C0FF">text</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> transcript</span></span>\n<span class="line"><span style="color: #C9D1D9">          } </span><span style="color: #FF7B72">else</span><span style="color: #C9D1D9"> {</span></span>\n<span class="line"><span style="color: #C9D1D9">             transcriptView.</span><span style="color: #79C0FF">text</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> transcriptView.</span><span style="color: #79C0FF">text</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot; &quot;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> transcript</span></span>\n<span class="line"><span style="color: #C9D1D9">          }</span></span>\n<span class="line"><span style="color: #C9D1D9">        }</span></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">case</span><span style="color: #C9D1D9"> .</span><span style="color: #79C0FF">error</span><span style="color: #C9D1D9">(</span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> error)</span><span style="color: #FF7B72">:</span></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(error </span><span style="color: #FF7B72">??</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">default:</span></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #FF7B72">break</span></span>\n<span class="line"><span style="color: #C9D1D9">     }</span></span>\n<span class="line"><span style="color: #C9D1D9">  }</span></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>\n<p>Once you have a <code is:raw>DeepgramResponse</code> instance, you will check if it is final, meaning it is ready to add to the <code is:raw>transcriptView</code> while handling the empty scenarios. If you now build and run the project (CMD + R), it will open in the simulator, where you will be prompted to give microphone access. Then you can start transcribing!</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1638827585/blog/2022/01/ios-live-transcription/simulator.png" alt="iPhone simulator showing the text hello blog"></p>\n<p>The final project code is available at <a href="https://github.com/deepgram-devs/deepgram-live-transcripts-ios">https://github.com/deepgram-devs/deepgram-live-transcripts-ios</a>, and if you have any questions, please feel free to reach out to the Deepgram team on Twitter - <a href="https://twitter.com/DeepgramDevs">@DeepgramDevs</a>.</p>' };
const frontmatter = { "title": "Live Transcriptions with iOS and Deepgram", "description": "Build an iOS app which takes the user\u2019s microphone and prints transcripts to the screen in real-time.", "date": "2022-01-03T00:00:00.000Z", "cover": "https://res.cloudinary.com/deepgram/image/upload/v1638827580/blog/2022/01/ios-live-transcription/Live-Transcriptions-w-iOS-Deepgram-A%402x.jpg", "authors": ["abdul-ajetunmobi"], "category": "tutorial", "tags": ["ios", "swift"], "seo": { "title": "Live Transcriptions with iOS and Deepgram", "description": "Build an iOS app which takes the user\u2019s microphone and prints transcripts to the screen in real-time." }, "shorturls": { "share": "https://dpgr.am/fe8fa3f", "twitter": "https://dpgr.am/520da60", "linkedin": "https://dpgr.am/d25200f", "reddit": "https://dpgr.am/c4783e8", "facebook": "https://dpgr.am/f66d6dc" }, "og": { "image": "https://res.cloudinary.com/deepgram/image/upload/v1661453843/blog/ios-live-transcription/ograph.png" }, "astro": { "headings": [{ "depth": 2, "slug": "before-we-start", "text": "Before We Start" }, { "depth": 2, "slug": "get-microphone-permissions", "text": "Get Microphone Permissions" }, { "depth": 2, "slug": "setting-up-avaudioengine", "text": "Setting Up AVAudioEngine" }, { "depth": 2, "slug": "get-microphone-data", "text": "Get Microphone Data" }, { "depth": 2, "slug": "connect-to-deepgram", "text": "Connect to Deepgram" }, { "depth": 2, "slug": "send-data-to-deepgram", "text": "Send Data to Deepgram" }, { "depth": 2, "slug": "handle-the-deepgram-response", "text": "Handle the Deepgram Response" }], "source": '\n<guest-author></guest-author>\n\nDeepgram\'s Speech Recognition API can provide accurate transcripts with pre-recorded and live audio. In this post, you will build an iOS app that takes the user\'s microphone and prints transcripts to the screen in real-time.\n\nThe final project code is available at https://github.com/deepgram-devs/deepgram-live-transcripts-ios.\n\n## Before We Start\n\nFor this project, you will need:\n\n*   A Deepgram API Key - [get one here](https://console.deepgram.com/signup?jump=keys)\n*   [Xcode](https://developer.apple.com/xcode/) installed on your machine.\n\nOnce you open Xcode, create a new Xcode project. Select the App template, give it a product name, select "Storyboard" for the interface, and "Swift" for the language. Untick "User Core Data" and "Include Tests", then hit next.\n\nChoose a place to save your project and hit finish to open your new project.\n\nYou will be using the WebSocket library [Starscream](https://github.com/daltoniam/Starscream) for this project. You can add it as a Swift Package by going to *File > Add Packages*, then entering the URL (https://github.com/daltoniam/Starscream) into the search box. Click add package and wait for the package to download.\n\n## Get Microphone Permissions\n\nYou will be using the microphone for this project, so you will need to add a microphone usage description to your project. When an iOS app asks for microphone permission, a reason why the permission is being requested is required and will be shown to the user. Open the `info.plist` file to add a description.\n\nHover over the last entry and click the plus icon to add a new entry. The key should be "Privacy - Microphone Usage Description", and you should add a value that describes why you need the microphone. You can use "To transcribe audio with Deepgram".\n\n![Plist file with the microphone usage description](https://res.cloudinary.com/deepgram/image/upload/v1638827585/blog/2022/01/ios-live-transcription/plist.png)\n\n## Setting Up `AVAudioEngine`\n\nTo access a data stream from the microphone, you will use [`AVAudioEngine`](https://developer.apple.com/documentation/avfaudio/avaudioengine).\n\nBefore you get started, it is worth having a mental model of how `AVAudioEngine` works. Apple\'s description is: "An audio engine object contains a group of AVAudioNode instances that you attach to form an audio processing chain".\n\nFor this example, you will create a chain of nodes that will take the microphone\'s input, convert it to a format that Deepgram can process, then send the data to Deepgram. The nodes and conversion will be discussed further in the blog.\n\n![AVAudioEngine nodes for this project](https://res.cloudinary.com/deepgram/image/upload/v1638827585/blog/2022/01/ios-live-transcription/engine-diagram.png)\n\nOpen the `ViewController.swift` file and import AVFoundation and Starscream at the top\n\n    import AVFoundation\n    Import Starscream\n\nThis will give you access to the `AVAudioEngine` class. Then inside the `ViewController` class, create an instance of `AVAudioEngine` by adding a property:\n\n```swift\nprivate let audioEngine = AVAudioEngine()\n```\n\nNext, create a function to analyse the audio and declare some constants:\n\n```swift\nprivate func startAnalyzingAudio() {\n  let inputNode = audioEngine.inputNode\n  let inputFormat = inputNode.inputFormat(forBus: 0)\n  let outputFormat = AVAudioFormat(commonFormat: .pcmFormatInt16, sampleRate: inputFormat.sampleRate, channels: inputFormat.channelCount, interleaved: true)\n}\n```\n\nFrom the top, you have the `inputNode`, which you can think of as the microphone followed by the `inputFormat`.\n\nNext is the `outputFormat` - The iOS microphone is a 32-bit depth format by default, so you will be converting to a 16-bit depth format before sending the data to Deepgram. While we will use a PCM 16-bit depth format for the audio encoding, Deepgram supports many audio encodings that can be specified to the API. Consider using a more compressed format if your use case requires low network usage. Just below those, add the new nodes that you will need and attach them to the audio engine:\n\n```swift\nlet converterNode = AVAudioMixerNode()\nlet sinkNode = AVAudioMixerNode()\n\naudioEngine.attach(converterNode)\naudioEngine.attach(sinkNode)\n```\n\nThe sinkNode is needed because to get the data in the correct format you need to access the data after it has passed through the converterNode, or its output. If you refer to the diagram above, notice how the stream of data to Deepgram is coming from the connection between nodes, the output of the `converterNode`, and not the nodes themselves.\n\n## Get Microphone Data\n\nUse the `installTap` function on the `AVAudioMixerNode` class to get the microphone data. This function gives you a closure that returns the output audio data of a node as a buffer. Call `installTap` on the `converterNode`, continuing the function from above:\n\n```swift\nconverterNode.installTap(onBus: 0, bufferSize: 1024, format: converterNode.outputFormat(forBus: 0)) { (buffer: AVAudioPCMBuffer!, time: AVAudioTime!) -> Void in\n\n}\n```\n\nYou will come back to this closure later. Now finish off the `startAnalyzingAudio` by connecting all the nodes and starting the audio engine:\n\n```swift\naudioEngine.connect(inputNode, to: converterNode, format: inputFormat)\naudioEngine.connect(converterNode, to: sinkNode, format: outputFormat)\naudioEngine.prepare()\n\ndo {\ntry AVAudioSession.sharedInstance().setCategory(.record)\n  try audioEngine.start()\n} catch {\n  print(error)\n}\n```\n\nYou can see now how the processing pipeline mirrors the diagram from earlier. Before the audio engine is started, the audio session of the application needs to be set to a record category. This lets the app know that the audio engine is solely being used for recording purposes. Note how the `converterNode` is connected with `outputFormat`.\n\n## Connect to Deepgram\n\nYou will be using the Deepgram WebSocket Streaming API, create a `WebSocket` instance at the top of the `ViewController` class:\n\n```swift\nprivate let apiKey = "Token YOUR_DEEPGRAM_API_KEY"\nprivate lazy var socket: WebSocket = {\n  let url = URL(string: "wss://api.deepgram.com/v1/listen?encoding=linear16&sample_rate=48000&channels=1")!\n  var urlRequest = URLRequest(url: url)\n  urlRequest.setValue(apiKey, forHTTPHeaderField: "Authorization")\n  return WebSocket(request: urlRequest)\n}()\n```\n\n<Alert type="warning">A reminder that this key is in your app\'s source code and, therefore, it is not secure. Please factor this into your actual projects.</Alert>\n\nNote how the URL has an encoding matching the `outputFormat` from earlier. In the `viewDidLoad` function, set the socket\'s delegate to this class and open the connection:\n\n```swift\noverride func viewDidLoad() {\n  super.viewDidLoad()\n  socket.delegate = self\n  socket.connect()\n}\n```\n\nYou will implement the delegate later in the post.\n\n## Send Data to Deepgram\n\nNow that the socket connection is open, you can now send the microphone data to Deepgram. To send data over a WebSocket in iOS, it needs to be converted to a `Data` object. Add the following function to the `ViewController` class that does the conversion:\n\n```swift\nprivate func toNSData(buffer: AVAudioPCMBuffer) -> Data? {\n  let audioBuffer = buffer.audioBufferList.pointee.mBuffers\n  return Data(bytes: audioBuffer.mData!, count: Int(audioBuffer.mDataByteSize))\n}\n```\n\nThen return to the `startAnalyzingAudio` function, and within the `installTap` closure, you can send the data to Deepgram. Your tap code should look like this now:\n\n```swift\nconverterNode.installTap(onBus: bus, bufferSize: 1024, format: converterNode.outputFormat(forBus: bus)) { (buffer: AVAudioPCMBuffer!, time: AVAudioTime!) -> Void in\n  if let data = self.toNSData(buffer: buffer) {\n     self.socket.write(data: data)\n  }\n}\n```\n\nCall `startAnalyzingAudio` in the `viewDidLoad` function below the WebSocket configuration.\n\n## Handle the Deepgram Response\n\nYou will get updates from the WebSocket via its [delegate](https://www.hackingwithswift.com/example-code/language/what-is-a-delegate-in-ios). In the `ViewController.swift` file *outside* the class, create an extension for the `WebSocketDelegate` and a `DeepgramResponse` struct:\n\n```swift\nextension ViewController: WebSocketDelegate {\n  func didReceive(event: WebSocketEvent, client: WebSocket) {\n     switch event {\n     case .text(let text):\n\n     case .error(let error):\n        print(error ?? "")\n     default:\n        break\n     }\n  }\n}\n\nstruct DeepgramResponse: Codable {\n  let isFinal: Bool\n  let channel: Channel\n\n  struct Channel: Codable {\n     let alternatives: [Alternatives]\n  }\n\n  struct Alternatives: Codable {\n     let transcript: String\n  }\n}\n```\n\nThe `didReceive` function on the `WebSocketDelegate` will be called whenever you get an update on the WebSocket. Before you finish implementing `didReceive`, you need to prepare to decode the data and update the UI to display the transcripts. At the top of the `ViewController` class, add the following properties:\n\n```swift\nprivate let jsonDecoder: JSONDecoder = {\n  let decoder = JSONDecoder()\n  decoder.keyDecodingStrategy = .convertFromSnakeCase\n  return decoder\n}()\n\nprivate let transcriptView: UITextView = {\n  let textView = UITextView()\n  textView.isScrollEnabled = true\n  textView.backgroundColor = .lightGray\n  textView.translatesAutoresizingMaskIntoConstraints = false\n  return textView\n}()\n```\n\nCreate a new function called `setupView` and configure your UI:\n\n```swift\nprivate func setupView() {\n  view.addSubview(transcriptView)\n  NSLayoutConstraint.activate([\n     transcriptView.topAnchor.constraint(equalTo: view.safeAreaLayoutGuide.topAnchor),\n     transcriptView.leadingAnchor.constraint(equalTo: view.leadingAnchor, constant: 16),\n     transcriptView.trailingAnchor.constraint(equalTo: view.trailingAnchor, constant: -16),\n     transcriptView.bottomAnchor.constraint(equalTo: view.safeAreaLayoutGuide.bottomAnchor)\n  ])\n}\n```\n\nThen call `setupView` in the `viewDidLoad` function.\n\nReturning to the `didReceive` function of the `WebSocketDelegate`, within the text case; you need to convert the text, which is a `String` type, into a `Data` type so you can decode it into an instance of `DeepgramResponse`:\n\n```swift\nextension ViewController: WebSocketDelegate {\n  func didReceive(event: WebSocketEvent, client: WebSocket) {\n     switch event {\n     case .text(let text):\n        let jsonData = Data(text.utf8)\n        let response = try! jsonDecoder.decode(DeepgramResponse.self, from: jsonData)\n        let transcript = response.channel.alternatives.first!.transcript\n\n        if response.isFinal && !transcript.isEmpty {\n          if transcriptView.text.isEmpty {\n             transcriptView.text = transcript\n          } else {\n             transcriptView.text = transcriptView.text + " " + transcript\n          }\n        }\n     case .error(let error):\n        print(error ?? "")\n     default:\n        break\n     }\n  }\n}\n```\n\nOnce you have a `DeepgramResponse` instance, you will check if it is final, meaning it is ready to add to the `transcriptView` while handling the empty scenarios. If you now build and run the project (CMD + R), it will open in the simulator, where you will be prompted to give microphone access. Then you can start transcribing!\n\n![iPhone simulator showing the text hello blog](https://res.cloudinary.com/deepgram/image/upload/v1638827585/blog/2022/01/ios-live-transcription/simulator.png)\n\nThe final project code is available at https://github.com/deepgram-devs/deepgram-live-transcripts-ios, and if you have any questions, please feel free to reach out to the Deepgram team on Twitter - [@DeepgramDevs](https://twitter.com/DeepgramDevs).\n\n        ', "html": '<guest-author />\n<p>Deepgram\u2019s Speech Recognition API can provide accurate transcripts with pre-recorded and live audio. In this post, you will build an iOS app that takes the user\u2019s microphone and prints transcripts to the screen in real-time.</p>\n<p>The final project code is available at <a href="https://github.com/deepgram-devs/deepgram-live-transcripts-ios">https://github.com/deepgram-devs/deepgram-live-transcripts-ios</a>.</p>\n<h2 id="before-we-start">Before We Start</h2>\n<p>For this project, you will need:</p>\n<ul>\n<li>A Deepgram API Key - <a href="https://console.deepgram.com/signup?jump=keys">get one here</a></li>\n<li><a href="https://developer.apple.com/xcode/">Xcode</a> installed on your machine.</li>\n</ul>\n<p>Once you open Xcode, create a new Xcode project. Select the App template, give it a product name, select \u201CStoryboard\u201D for the interface, and \u201CSwift\u201D for the language. Untick \u201CUser Core Data\u201D and \u201CInclude Tests\u201D, then hit next.</p>\n<p>Choose a place to save your project and hit finish to open your new project.</p>\n<p>You will be using the WebSocket library <a href="https://github.com/daltoniam/Starscream">Starscream</a> for this project. You can add it as a Swift Package by going to <em>File > Add Packages</em>, then entering the URL (<a href="https://github.com/daltoniam/Starscream">https://github.com/daltoniam/Starscream</a>) into the search box. Click add package and wait for the package to download.</p>\n<h2 id="get-microphone-permissions">Get Microphone Permissions</h2>\n<p>You will be using the microphone for this project, so you will need to add a microphone usage description to your project. When an iOS app asks for microphone permission, a reason why the permission is being requested is required and will be shown to the user. Open the <code is:raw>info.plist</code> file to add a description.</p>\n<p>Hover over the last entry and click the plus icon to add a new entry. The key should be \u201CPrivacy - Microphone Usage Description\u201D, and you should add a value that describes why you need the microphone. You can use \u201CTo transcribe audio with Deepgram\u201D.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1638827585/blog/2022/01/ios-live-transcription/plist.png" alt="Plist file with the microphone usage description"></p>\n<h2 id="setting-up-avaudioengine">Setting Up <code is:raw>AVAudioEngine</code></h2>\n<p>To access a data stream from the microphone, you will use <a href="https://developer.apple.com/documentation/avfaudio/avaudioengine"><code is:raw>AVAudioEngine</code></a>.</p>\n<p>Before you get started, it is worth having a mental model of how <code is:raw>AVAudioEngine</code> works. Apple\u2019s description is: \u201CAn audio engine object contains a group of AVAudioNode instances that you attach to form an audio processing chain\u201D.</p>\n<p>For this example, you will create a chain of nodes that will take the microphone\u2019s input, convert it to a format that Deepgram can process, then send the data to Deepgram. The nodes and conversion will be discussed further in the blog.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1638827585/blog/2022/01/ios-live-transcription/engine-diagram.png" alt="AVAudioEngine nodes for this project"></p>\n<p>Open the <code is:raw>ViewController.swift</code> file and import AVFoundation and Starscream at the top</p>\n<p>import AVFoundation\nImport Starscream</p>\n<p>This will give you access to the <code is:raw>AVAudioEngine</code> class. Then inside the <code is:raw>ViewController</code> class, create an instance of <code is:raw>AVAudioEngine</code> by adding a property:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">private</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> audioEngine </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">AVAudioEngine</span><span style="color: #C9D1D9">()</span></span></code></pre>\n<p>Next, create a function to analyse the audio and declare some constants:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">private</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">func</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">startAnalyzingAudio</span><span style="color: #C9D1D9">() {</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> inputNode </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> audioEngine.inputNode</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> inputFormat </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> inputNode.</span><span style="color: #79C0FF">inputFormat</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">forBus</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> outputFormat </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">AVAudioFormat</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">commonFormat</span><span style="color: #C9D1D9">: .pcmFormatInt16, </span><span style="color: #79C0FF">sampleRate</span><span style="color: #C9D1D9">: inputFormat.sampleRate, </span><span style="color: #79C0FF">channels</span><span style="color: #C9D1D9">: inputFormat.channelCount, </span><span style="color: #79C0FF">interleaved</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">true</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>\n<p>From the top, you have the <code is:raw>inputNode</code>, which you can think of as the microphone followed by the <code is:raw>inputFormat</code>.</p>\n<p>Next is the <code is:raw>outputFormat</code> - The iOS microphone is a 32-bit depth format by default, so you will be converting to a 16-bit depth format before sending the data to Deepgram. While we will use a PCM 16-bit depth format for the audio encoding, Deepgram supports many audio encodings that can be specified to the API. Consider using a more compressed format if your use case requires low network usage. Just below those, add the new nodes that you will need and attach them to the audio engine:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> converterNode </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">AVAudioMixerNode</span><span style="color: #C9D1D9">()</span></span>\n<span class="line"><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> sinkNode </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">AVAudioMixerNode</span><span style="color: #C9D1D9">()</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">audioEngine.</span><span style="color: #79C0FF">attach</span><span style="color: #C9D1D9">(converterNode)</span></span>\n<span class="line"><span style="color: #C9D1D9">audioEngine.</span><span style="color: #79C0FF">attach</span><span style="color: #C9D1D9">(sinkNode)</span></span></code></pre>\n<p>The sinkNode is needed because to get the data in the correct format you need to access the data after it has passed through the converterNode, or its output. If you refer to the diagram above, notice how the stream of data to Deepgram is coming from the connection between nodes, the output of the <code is:raw>converterNode</code>, and not the nodes themselves.</p>\n<h2 id="get-microphone-data">Get Microphone Data</h2>\n<p>Use the <code is:raw>installTap</code> function on the <code is:raw>AVAudioMixerNode</code> class to get the microphone data. This function gives you a closure that returns the output audio data of a node as a buffer. Call <code is:raw>installTap</code> on the <code is:raw>converterNode</code>, continuing the function from above:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">converterNode.</span><span style="color: #79C0FF">installTap</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">onBus</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">bufferSize</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">1024</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">format</span><span style="color: #C9D1D9">: converterNode.</span><span style="color: #79C0FF">outputFormat</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">forBus</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">)) { (</span><span style="color: #79C0FF">buffer</span><span style="color: #C9D1D9">: AVAudioPCMBuffer</span><span style="color: #FF7B72">!</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">time</span><span style="color: #C9D1D9">: AVAudioTime</span><span style="color: #FF7B72">!</span><span style="color: #C9D1D9">) </span><span style="color: #FF7B72">-&gt;</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">Void</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">in</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>\n<p>You will come back to this closure later. Now finish off the <code is:raw>startAnalyzingAudio</code> by connecting all the nodes and starting the audio engine:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">audioEngine.</span><span style="color: #79C0FF">connect</span><span style="color: #C9D1D9">(inputNode, </span><span style="color: #79C0FF">to</span><span style="color: #C9D1D9">: converterNode, </span><span style="color: #79C0FF">format</span><span style="color: #C9D1D9">: inputFormat)</span></span>\n<span class="line"><span style="color: #C9D1D9">audioEngine.</span><span style="color: #79C0FF">connect</span><span style="color: #C9D1D9">(converterNode, </span><span style="color: #79C0FF">to</span><span style="color: #C9D1D9">: sinkNode, </span><span style="color: #79C0FF">format</span><span style="color: #C9D1D9">: outputFormat)</span></span>\n<span class="line"><span style="color: #C9D1D9">audioEngine.</span><span style="color: #79C0FF">prepare</span><span style="color: #C9D1D9">()</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #FF7B72">do</span><span style="color: #C9D1D9"> {</span></span>\n<span class="line"><span style="color: #FF7B72">try</span><span style="color: #C9D1D9"> AVAudioSession.</span><span style="color: #79C0FF">sharedInstance</span><span style="color: #C9D1D9">().</span><span style="color: #79C0FF">setCategory</span><span style="color: #C9D1D9">(.record)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">try</span><span style="color: #C9D1D9"> audioEngine.</span><span style="color: #79C0FF">start</span><span style="color: #C9D1D9">()</span></span>\n<span class="line"><span style="color: #C9D1D9">} </span><span style="color: #FF7B72">catch</span><span style="color: #C9D1D9"> {</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(error)</span></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>\n<p>You can see now how the processing pipeline mirrors the diagram from earlier. Before the audio engine is started, the audio session of the application needs to be set to a record category. This lets the app know that the audio engine is solely being used for recording purposes. Note how the <code is:raw>converterNode</code> is connected with <code is:raw>outputFormat</code>.</p>\n<h2 id="connect-to-deepgram">Connect to Deepgram</h2>\n<p>You will be using the Deepgram WebSocket Streaming API, create a <code is:raw>WebSocket</code> instance at the top of the <code is:raw>ViewController</code> class:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">private</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> apiKey </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;Token YOUR_DEEPGRAM_API_KEY&quot;</span></span>\n<span class="line"><span style="color: #FF7B72">private</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">lazy</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">var</span><span style="color: #C9D1D9"> socket: WebSocket </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> {</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> url </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">URL</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">string</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;wss://api.deepgram.com/v1/listen?encoding=linear16&amp;sample_rate=48000&amp;channels=1&quot;</span><span style="color: #C9D1D9">)</span><span style="color: #FF7B72">!</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">var</span><span style="color: #C9D1D9"> urlRequest </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">URLRequest</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">url</span><span style="color: #C9D1D9">: url)</span></span>\n<span class="line"><span style="color: #C9D1D9">  urlRequest.</span><span style="color: #79C0FF">setValue</span><span style="color: #C9D1D9">(apiKey, </span><span style="color: #79C0FF">forHTTPHeaderField</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;Authorization&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">WebSocket</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">request</span><span style="color: #C9D1D9">: urlRequest)</span></span>\n<span class="line"><span style="color: #C9D1D9">}()</span></span></code></pre>\n<Alert type="warning">A reminder that this key is in your app\u2019s source code and, therefore, it is not secure. Please factor this into your actual projects.</Alert>\n<p>Note how the URL has an encoding matching the <code is:raw>outputFormat</code> from earlier. In the <code is:raw>viewDidLoad</code> function, set the socket\u2019s delegate to this class and open the connection:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">override</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">func</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">viewDidLoad</span><span style="color: #C9D1D9">() {</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #79C0FF">super</span><span style="color: #C9D1D9">.</span><span style="color: #79C0FF">viewDidLoad</span><span style="color: #C9D1D9">()</span></span>\n<span class="line"><span style="color: #C9D1D9">  socket.delegate </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span></span>\n<span class="line"><span style="color: #C9D1D9">  socket.</span><span style="color: #79C0FF">connect</span><span style="color: #C9D1D9">()</span></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>\n<p>You will implement the delegate later in the post.</p>\n<h2 id="send-data-to-deepgram">Send Data to Deepgram</h2>\n<p>Now that the socket connection is open, you can now send the microphone data to Deepgram. To send data over a WebSocket in iOS, it needs to be converted to a <code is:raw>Data</code> object. Add the following function to the <code is:raw>ViewController</code> class that does the conversion:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">private</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">func</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">toNSData</span><span style="color: #C9D1D9">(</span><span style="color: #D2A8FF">buffer</span><span style="color: #C9D1D9">: AVAudioPCMBuffer) </span><span style="color: #FF7B72">-&gt;</span><span style="color: #C9D1D9"> Data</span><span style="color: #FF7B72">?</span><span style="color: #C9D1D9"> {</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> audioBuffer </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> buffer.audioBufferList.</span><span style="color: #79C0FF">pointee</span><span style="color: #C9D1D9">.mBuffers</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">Data</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">bytes</span><span style="color: #C9D1D9">: audioBuffer.mData</span><span style="color: #FF7B72">!</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">count</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">Int</span><span style="color: #C9D1D9">(audioBuffer.mDataByteSize))</span></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>\n<p>Then return to the <code is:raw>startAnalyzingAudio</code> function, and within the <code is:raw>installTap</code> closure, you can send the data to Deepgram. Your tap code should look like this now:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">converterNode.</span><span style="color: #79C0FF">installTap</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">onBus</span><span style="color: #C9D1D9">: bus, </span><span style="color: #79C0FF">bufferSize</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">1024</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">format</span><span style="color: #C9D1D9">: converterNode.</span><span style="color: #79C0FF">outputFormat</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">forBus</span><span style="color: #C9D1D9">: bus)) { (</span><span style="color: #79C0FF">buffer</span><span style="color: #C9D1D9">: AVAudioPCMBuffer</span><span style="color: #FF7B72">!</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">time</span><span style="color: #C9D1D9">: AVAudioTime</span><span style="color: #FF7B72">!</span><span style="color: #C9D1D9">) </span><span style="color: #FF7B72">-&gt;</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">Void</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">in</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> data </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.</span><span style="color: #79C0FF">toNSData</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">buffer</span><span style="color: #C9D1D9">: buffer) {</span></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.socket.</span><span style="color: #79C0FF">write</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">data</span><span style="color: #C9D1D9">: data)</span></span>\n<span class="line"><span style="color: #C9D1D9">  }</span></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>\n<p>Call <code is:raw>startAnalyzingAudio</code> in the <code is:raw>viewDidLoad</code> function below the WebSocket configuration.</p>\n<h2 id="handle-the-deepgram-response">Handle the Deepgram Response</h2>\n<p>You will get updates from the WebSocket via its <a href="https://www.hackingwithswift.com/example-code/language/what-is-a-delegate-in-ios">delegate</a>. In the <code is:raw>ViewController.swift</code> file <em>outside</em> the class, create an extension for the <code is:raw>WebSocketDelegate</code> and a <code is:raw>DeepgramResponse</code> struct:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">extension</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">ViewController</span><span style="color: #C9D1D9">: WebSocketDelegate {</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">func</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">didReceive</span><span style="color: #C9D1D9">(</span><span style="color: #D2A8FF">event</span><span style="color: #C9D1D9">: WebSocketEvent, </span><span style="color: #D2A8FF">client</span><span style="color: #C9D1D9">: WebSocket) {</span></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">switch</span><span style="color: #C9D1D9"> event {</span></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">case</span><span style="color: #C9D1D9"> .</span><span style="color: #79C0FF">text</span><span style="color: #C9D1D9">(</span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> text)</span><span style="color: #FF7B72">:</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">case</span><span style="color: #C9D1D9"> .</span><span style="color: #79C0FF">error</span><span style="color: #C9D1D9">(</span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> error)</span><span style="color: #FF7B72">:</span></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(error </span><span style="color: #FF7B72">??</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">default:</span></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #FF7B72">break</span></span>\n<span class="line"><span style="color: #C9D1D9">     }</span></span>\n<span class="line"><span style="color: #C9D1D9">  }</span></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #FF7B72">struct</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">DeepgramResponse</span><span style="color: #C9D1D9">: Codable {</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> isFinal: </span><span style="color: #79C0FF">Bool</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> channel: Channel</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">struct</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">Channel</span><span style="color: #C9D1D9">: Codable {</span></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> alternatives: [Alternatives]</span></span>\n<span class="line"><span style="color: #C9D1D9">  }</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">struct</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">Alternatives</span><span style="color: #C9D1D9">: Codable {</span></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> transcript: </span><span style="color: #79C0FF">String</span></span>\n<span class="line"><span style="color: #C9D1D9">  }</span></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>\n<p>The <code is:raw>didReceive</code> function on the <code is:raw>WebSocketDelegate</code> will be called whenever you get an update on the WebSocket. Before you finish implementing <code is:raw>didReceive</code>, you need to prepare to decode the data and update the UI to display the transcripts. At the top of the <code is:raw>ViewController</code> class, add the following properties:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">private</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> jsonDecoder: JSONDecoder </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> {</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> decoder </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">JSONDecoder</span><span style="color: #C9D1D9">()</span></span>\n<span class="line"><span style="color: #C9D1D9">  decoder.keyDecodingStrategy </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> .convertFromSnakeCase</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> decoder</span></span>\n<span class="line"><span style="color: #C9D1D9">}()</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #FF7B72">private</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> transcriptView: UITextView </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> {</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> textView </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">UITextView</span><span style="color: #C9D1D9">()</span></span>\n<span class="line"><span style="color: #C9D1D9">  textView.isScrollEnabled </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">true</span></span>\n<span class="line"><span style="color: #C9D1D9">  textView.backgroundColor </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> .lightGray</span></span>\n<span class="line"><span style="color: #C9D1D9">  textView.translatesAutoresizingMaskIntoConstraints </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">false</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> textView</span></span>\n<span class="line"><span style="color: #C9D1D9">}()</span></span></code></pre>\n<p>Create a new function called <code is:raw>setupView</code> and configure your UI:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">private</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">func</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">setupView</span><span style="color: #C9D1D9">() {</span></span>\n<span class="line"><span style="color: #C9D1D9">  view.</span><span style="color: #79C0FF">addSubview</span><span style="color: #C9D1D9">(transcriptView)</span></span>\n<span class="line"><span style="color: #C9D1D9">  NSLayoutConstraint.</span><span style="color: #79C0FF">activate</span><span style="color: #C9D1D9">([</span></span>\n<span class="line"><span style="color: #C9D1D9">     transcriptView.topAnchor.</span><span style="color: #79C0FF">constraint</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">equalTo</span><span style="color: #C9D1D9">: view.safeAreaLayoutGuide.topAnchor),</span></span>\n<span class="line"><span style="color: #C9D1D9">     transcriptView.leadingAnchor.</span><span style="color: #79C0FF">constraint</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">equalTo</span><span style="color: #C9D1D9">: view.leadingAnchor, </span><span style="color: #79C0FF">constant</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">16</span><span style="color: #C9D1D9">),</span></span>\n<span class="line"><span style="color: #C9D1D9">     transcriptView.trailingAnchor.</span><span style="color: #79C0FF">constraint</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">equalTo</span><span style="color: #C9D1D9">: view.trailingAnchor, </span><span style="color: #79C0FF">constant</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">-16</span><span style="color: #C9D1D9">),</span></span>\n<span class="line"><span style="color: #C9D1D9">     transcriptView.bottomAnchor.</span><span style="color: #79C0FF">constraint</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">equalTo</span><span style="color: #C9D1D9">: view.safeAreaLayoutGuide.bottomAnchor)</span></span>\n<span class="line"><span style="color: #C9D1D9">  ])</span></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>\n<p>Then call <code is:raw>setupView</code> in the <code is:raw>viewDidLoad</code> function.</p>\n<p>Returning to the <code is:raw>didReceive</code> function of the <code is:raw>WebSocketDelegate</code>, within the text case; you need to convert the text, which is a <code is:raw>String</code> type, into a <code is:raw>Data</code> type so you can decode it into an instance of <code is:raw>DeepgramResponse</code>:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">extension</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">ViewController</span><span style="color: #C9D1D9">: WebSocketDelegate {</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">func</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">didReceive</span><span style="color: #C9D1D9">(</span><span style="color: #D2A8FF">event</span><span style="color: #C9D1D9">: WebSocketEvent, </span><span style="color: #D2A8FF">client</span><span style="color: #C9D1D9">: WebSocket) {</span></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">switch</span><span style="color: #C9D1D9"> event {</span></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">case</span><span style="color: #C9D1D9"> .</span><span style="color: #79C0FF">text</span><span style="color: #C9D1D9">(</span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> text)</span><span style="color: #FF7B72">:</span></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> jsonData </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">Data</span><span style="color: #C9D1D9">(text.</span><span style="color: #79C0FF">utf8</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> response </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">try!</span><span style="color: #C9D1D9"> jsonDecoder.</span><span style="color: #79C0FF">decode</span><span style="color: #C9D1D9">(DeepgramResponse.</span><span style="color: #FF7B72">self</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">from</span><span style="color: #C9D1D9">: jsonData)</span></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> transcript </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> response.channel.alternatives.</span><span style="color: #79C0FF">first</span><span style="color: #FF7B72">!</span><span style="color: #C9D1D9">.transcript</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> response.isFinal </span><span style="color: #FF7B72">&amp;&amp;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">!</span><span style="color: #C9D1D9">transcript.</span><span style="color: #79C0FF">isEmpty</span><span style="color: #C9D1D9"> {</span></span>\n<span class="line"><span style="color: #C9D1D9">          </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> transcriptView.</span><span style="color: #79C0FF">text</span><span style="color: #C9D1D9">.</span><span style="color: #79C0FF">isEmpty</span><span style="color: #C9D1D9"> {</span></span>\n<span class="line"><span style="color: #C9D1D9">             transcriptView.</span><span style="color: #79C0FF">text</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> transcript</span></span>\n<span class="line"><span style="color: #C9D1D9">          } </span><span style="color: #FF7B72">else</span><span style="color: #C9D1D9"> {</span></span>\n<span class="line"><span style="color: #C9D1D9">             transcriptView.</span><span style="color: #79C0FF">text</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> transcriptView.</span><span style="color: #79C0FF">text</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot; &quot;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> transcript</span></span>\n<span class="line"><span style="color: #C9D1D9">          }</span></span>\n<span class="line"><span style="color: #C9D1D9">        }</span></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">case</span><span style="color: #C9D1D9"> .</span><span style="color: #79C0FF">error</span><span style="color: #C9D1D9">(</span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> error)</span><span style="color: #FF7B72">:</span></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(error </span><span style="color: #FF7B72">??</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">default:</span></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #FF7B72">break</span></span>\n<span class="line"><span style="color: #C9D1D9">     }</span></span>\n<span class="line"><span style="color: #C9D1D9">  }</span></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>\n<p>Once you have a <code is:raw>DeepgramResponse</code> instance, you will check if it is final, meaning it is ready to add to the <code is:raw>transcriptView</code> while handling the empty scenarios. If you now build and run the project (CMD + R), it will open in the simulator, where you will be prompted to give microphone access. Then you can start transcribing!</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1638827585/blog/2022/01/ios-live-transcription/simulator.png" alt="iPhone simulator showing the text hello blog"></p>\n<p>The final project code is available at <a href="https://github.com/deepgram-devs/deepgram-live-transcripts-ios">https://github.com/deepgram-devs/deepgram-live-transcripts-ios</a>, and if you have any questions, please feel free to reach out to the Deepgram team on Twitter - <a href="https://twitter.com/DeepgramDevs">@DeepgramDevs</a>.</p>' }, "file": "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/ios-live-transcription/index.md" };
function rawContent() {
  return '\n<guest-author></guest-author>\n\nDeepgram\'s Speech Recognition API can provide accurate transcripts with pre-recorded and live audio. In this post, you will build an iOS app that takes the user\'s microphone and prints transcripts to the screen in real-time.\n\nThe final project code is available at https://github.com/deepgram-devs/deepgram-live-transcripts-ios.\n\n## Before We Start\n\nFor this project, you will need:\n\n*   A Deepgram API Key - [get one here](https://console.deepgram.com/signup?jump=keys)\n*   [Xcode](https://developer.apple.com/xcode/) installed on your machine.\n\nOnce you open Xcode, create a new Xcode project. Select the App template, give it a product name, select "Storyboard" for the interface, and "Swift" for the language. Untick "User Core Data" and "Include Tests", then hit next.\n\nChoose a place to save your project and hit finish to open your new project.\n\nYou will be using the WebSocket library [Starscream](https://github.com/daltoniam/Starscream) for this project. You can add it as a Swift Package by going to *File > Add Packages*, then entering the URL (https://github.com/daltoniam/Starscream) into the search box. Click add package and wait for the package to download.\n\n## Get Microphone Permissions\n\nYou will be using the microphone for this project, so you will need to add a microphone usage description to your project. When an iOS app asks for microphone permission, a reason why the permission is being requested is required and will be shown to the user. Open the `info.plist` file to add a description.\n\nHover over the last entry and click the plus icon to add a new entry. The key should be "Privacy - Microphone Usage Description", and you should add a value that describes why you need the microphone. You can use "To transcribe audio with Deepgram".\n\n![Plist file with the microphone usage description](https://res.cloudinary.com/deepgram/image/upload/v1638827585/blog/2022/01/ios-live-transcription/plist.png)\n\n## Setting Up `AVAudioEngine`\n\nTo access a data stream from the microphone, you will use [`AVAudioEngine`](https://developer.apple.com/documentation/avfaudio/avaudioengine).\n\nBefore you get started, it is worth having a mental model of how `AVAudioEngine` works. Apple\'s description is: "An audio engine object contains a group of AVAudioNode instances that you attach to form an audio processing chain".\n\nFor this example, you will create a chain of nodes that will take the microphone\'s input, convert it to a format that Deepgram can process, then send the data to Deepgram. The nodes and conversion will be discussed further in the blog.\n\n![AVAudioEngine nodes for this project](https://res.cloudinary.com/deepgram/image/upload/v1638827585/blog/2022/01/ios-live-transcription/engine-diagram.png)\n\nOpen the `ViewController.swift` file and import AVFoundation and Starscream at the top\n\n    import AVFoundation\n    Import Starscream\n\nThis will give you access to the `AVAudioEngine` class. Then inside the `ViewController` class, create an instance of `AVAudioEngine` by adding a property:\n\n```swift\nprivate let audioEngine = AVAudioEngine()\n```\n\nNext, create a function to analyse the audio and declare some constants:\n\n```swift\nprivate func startAnalyzingAudio() {\n  let inputNode = audioEngine.inputNode\n  let inputFormat = inputNode.inputFormat(forBus: 0)\n  let outputFormat = AVAudioFormat(commonFormat: .pcmFormatInt16, sampleRate: inputFormat.sampleRate, channels: inputFormat.channelCount, interleaved: true)\n}\n```\n\nFrom the top, you have the `inputNode`, which you can think of as the microphone followed by the `inputFormat`.\n\nNext is the `outputFormat` - The iOS microphone is a 32-bit depth format by default, so you will be converting to a 16-bit depth format before sending the data to Deepgram. While we will use a PCM 16-bit depth format for the audio encoding, Deepgram supports many audio encodings that can be specified to the API. Consider using a more compressed format if your use case requires low network usage. Just below those, add the new nodes that you will need and attach them to the audio engine:\n\n```swift\nlet converterNode = AVAudioMixerNode()\nlet sinkNode = AVAudioMixerNode()\n\naudioEngine.attach(converterNode)\naudioEngine.attach(sinkNode)\n```\n\nThe sinkNode is needed because to get the data in the correct format you need to access the data after it has passed through the converterNode, or its output. If you refer to the diagram above, notice how the stream of data to Deepgram is coming from the connection between nodes, the output of the `converterNode`, and not the nodes themselves.\n\n## Get Microphone Data\n\nUse the `installTap` function on the `AVAudioMixerNode` class to get the microphone data. This function gives you a closure that returns the output audio data of a node as a buffer. Call `installTap` on the `converterNode`, continuing the function from above:\n\n```swift\nconverterNode.installTap(onBus: 0, bufferSize: 1024, format: converterNode.outputFormat(forBus: 0)) { (buffer: AVAudioPCMBuffer!, time: AVAudioTime!) -> Void in\n\n}\n```\n\nYou will come back to this closure later. Now finish off the `startAnalyzingAudio` by connecting all the nodes and starting the audio engine:\n\n```swift\naudioEngine.connect(inputNode, to: converterNode, format: inputFormat)\naudioEngine.connect(converterNode, to: sinkNode, format: outputFormat)\naudioEngine.prepare()\n\ndo {\ntry AVAudioSession.sharedInstance().setCategory(.record)\n  try audioEngine.start()\n} catch {\n  print(error)\n}\n```\n\nYou can see now how the processing pipeline mirrors the diagram from earlier. Before the audio engine is started, the audio session of the application needs to be set to a record category. This lets the app know that the audio engine is solely being used for recording purposes. Note how the `converterNode` is connected with `outputFormat`.\n\n## Connect to Deepgram\n\nYou will be using the Deepgram WebSocket Streaming API, create a `WebSocket` instance at the top of the `ViewController` class:\n\n```swift\nprivate let apiKey = "Token YOUR_DEEPGRAM_API_KEY"\nprivate lazy var socket: WebSocket = {\n  let url = URL(string: "wss://api.deepgram.com/v1/listen?encoding=linear16&sample_rate=48000&channels=1")!\n  var urlRequest = URLRequest(url: url)\n  urlRequest.setValue(apiKey, forHTTPHeaderField: "Authorization")\n  return WebSocket(request: urlRequest)\n}()\n```\n\n<Alert type="warning">A reminder that this key is in your app\'s source code and, therefore, it is not secure. Please factor this into your actual projects.</Alert>\n\nNote how the URL has an encoding matching the `outputFormat` from earlier. In the `viewDidLoad` function, set the socket\'s delegate to this class and open the connection:\n\n```swift\noverride func viewDidLoad() {\n  super.viewDidLoad()\n  socket.delegate = self\n  socket.connect()\n}\n```\n\nYou will implement the delegate later in the post.\n\n## Send Data to Deepgram\n\nNow that the socket connection is open, you can now send the microphone data to Deepgram. To send data over a WebSocket in iOS, it needs to be converted to a `Data` object. Add the following function to the `ViewController` class that does the conversion:\n\n```swift\nprivate func toNSData(buffer: AVAudioPCMBuffer) -> Data? {\n  let audioBuffer = buffer.audioBufferList.pointee.mBuffers\n  return Data(bytes: audioBuffer.mData!, count: Int(audioBuffer.mDataByteSize))\n}\n```\n\nThen return to the `startAnalyzingAudio` function, and within the `installTap` closure, you can send the data to Deepgram. Your tap code should look like this now:\n\n```swift\nconverterNode.installTap(onBus: bus, bufferSize: 1024, format: converterNode.outputFormat(forBus: bus)) { (buffer: AVAudioPCMBuffer!, time: AVAudioTime!) -> Void in\n  if let data = self.toNSData(buffer: buffer) {\n     self.socket.write(data: data)\n  }\n}\n```\n\nCall `startAnalyzingAudio` in the `viewDidLoad` function below the WebSocket configuration.\n\n## Handle the Deepgram Response\n\nYou will get updates from the WebSocket via its [delegate](https://www.hackingwithswift.com/example-code/language/what-is-a-delegate-in-ios). In the `ViewController.swift` file *outside* the class, create an extension for the `WebSocketDelegate` and a `DeepgramResponse` struct:\n\n```swift\nextension ViewController: WebSocketDelegate {\n  func didReceive(event: WebSocketEvent, client: WebSocket) {\n     switch event {\n     case .text(let text):\n\n     case .error(let error):\n        print(error ?? "")\n     default:\n        break\n     }\n  }\n}\n\nstruct DeepgramResponse: Codable {\n  let isFinal: Bool\n  let channel: Channel\n\n  struct Channel: Codable {\n     let alternatives: [Alternatives]\n  }\n\n  struct Alternatives: Codable {\n     let transcript: String\n  }\n}\n```\n\nThe `didReceive` function on the `WebSocketDelegate` will be called whenever you get an update on the WebSocket. Before you finish implementing `didReceive`, you need to prepare to decode the data and update the UI to display the transcripts. At the top of the `ViewController` class, add the following properties:\n\n```swift\nprivate let jsonDecoder: JSONDecoder = {\n  let decoder = JSONDecoder()\n  decoder.keyDecodingStrategy = .convertFromSnakeCase\n  return decoder\n}()\n\nprivate let transcriptView: UITextView = {\n  let textView = UITextView()\n  textView.isScrollEnabled = true\n  textView.backgroundColor = .lightGray\n  textView.translatesAutoresizingMaskIntoConstraints = false\n  return textView\n}()\n```\n\nCreate a new function called `setupView` and configure your UI:\n\n```swift\nprivate func setupView() {\n  view.addSubview(transcriptView)\n  NSLayoutConstraint.activate([\n     transcriptView.topAnchor.constraint(equalTo: view.safeAreaLayoutGuide.topAnchor),\n     transcriptView.leadingAnchor.constraint(equalTo: view.leadingAnchor, constant: 16),\n     transcriptView.trailingAnchor.constraint(equalTo: view.trailingAnchor, constant: -16),\n     transcriptView.bottomAnchor.constraint(equalTo: view.safeAreaLayoutGuide.bottomAnchor)\n  ])\n}\n```\n\nThen call `setupView` in the `viewDidLoad` function.\n\nReturning to the `didReceive` function of the `WebSocketDelegate`, within the text case; you need to convert the text, which is a `String` type, into a `Data` type so you can decode it into an instance of `DeepgramResponse`:\n\n```swift\nextension ViewController: WebSocketDelegate {\n  func didReceive(event: WebSocketEvent, client: WebSocket) {\n     switch event {\n     case .text(let text):\n        let jsonData = Data(text.utf8)\n        let response = try! jsonDecoder.decode(DeepgramResponse.self, from: jsonData)\n        let transcript = response.channel.alternatives.first!.transcript\n\n        if response.isFinal && !transcript.isEmpty {\n          if transcriptView.text.isEmpty {\n             transcriptView.text = transcript\n          } else {\n             transcriptView.text = transcriptView.text + " " + transcript\n          }\n        }\n     case .error(let error):\n        print(error ?? "")\n     default:\n        break\n     }\n  }\n}\n```\n\nOnce you have a `DeepgramResponse` instance, you will check if it is final, meaning it is ready to add to the `transcriptView` while handling the empty scenarios. If you now build and run the project (CMD + R), it will open in the simulator, where you will be prompted to give microphone access. Then you can start transcribing!\n\n![iPhone simulator showing the text hello blog](https://res.cloudinary.com/deepgram/image/upload/v1638827585/blog/2022/01/ios-live-transcription/simulator.png)\n\nThe final project code is available at https://github.com/deepgram-devs/deepgram-live-transcripts-ios, and if you have any questions, please feel free to reach out to the Deepgram team on Twitter - [@DeepgramDevs](https://twitter.com/DeepgramDevs).\n\n        ';
}
function compiledContent() {
  return '<guest-author />\n<p>Deepgram\u2019s Speech Recognition API can provide accurate transcripts with pre-recorded and live audio. In this post, you will build an iOS app that takes the user\u2019s microphone and prints transcripts to the screen in real-time.</p>\n<p>The final project code is available at <a href="https://github.com/deepgram-devs/deepgram-live-transcripts-ios">https://github.com/deepgram-devs/deepgram-live-transcripts-ios</a>.</p>\n<h2 id="before-we-start">Before We Start</h2>\n<p>For this project, you will need:</p>\n<ul>\n<li>A Deepgram API Key - <a href="https://console.deepgram.com/signup?jump=keys">get one here</a></li>\n<li><a href="https://developer.apple.com/xcode/">Xcode</a> installed on your machine.</li>\n</ul>\n<p>Once you open Xcode, create a new Xcode project. Select the App template, give it a product name, select \u201CStoryboard\u201D for the interface, and \u201CSwift\u201D for the language. Untick \u201CUser Core Data\u201D and \u201CInclude Tests\u201D, then hit next.</p>\n<p>Choose a place to save your project and hit finish to open your new project.</p>\n<p>You will be using the WebSocket library <a href="https://github.com/daltoniam/Starscream">Starscream</a> for this project. You can add it as a Swift Package by going to <em>File > Add Packages</em>, then entering the URL (<a href="https://github.com/daltoniam/Starscream">https://github.com/daltoniam/Starscream</a>) into the search box. Click add package and wait for the package to download.</p>\n<h2 id="get-microphone-permissions">Get Microphone Permissions</h2>\n<p>You will be using the microphone for this project, so you will need to add a microphone usage description to your project. When an iOS app asks for microphone permission, a reason why the permission is being requested is required and will be shown to the user. Open the <code is:raw>info.plist</code> file to add a description.</p>\n<p>Hover over the last entry and click the plus icon to add a new entry. The key should be \u201CPrivacy - Microphone Usage Description\u201D, and you should add a value that describes why you need the microphone. You can use \u201CTo transcribe audio with Deepgram\u201D.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1638827585/blog/2022/01/ios-live-transcription/plist.png" alt="Plist file with the microphone usage description"></p>\n<h2 id="setting-up-avaudioengine">Setting Up <code is:raw>AVAudioEngine</code></h2>\n<p>To access a data stream from the microphone, you will use <a href="https://developer.apple.com/documentation/avfaudio/avaudioengine"><code is:raw>AVAudioEngine</code></a>.</p>\n<p>Before you get started, it is worth having a mental model of how <code is:raw>AVAudioEngine</code> works. Apple\u2019s description is: \u201CAn audio engine object contains a group of AVAudioNode instances that you attach to form an audio processing chain\u201D.</p>\n<p>For this example, you will create a chain of nodes that will take the microphone\u2019s input, convert it to a format that Deepgram can process, then send the data to Deepgram. The nodes and conversion will be discussed further in the blog.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1638827585/blog/2022/01/ios-live-transcription/engine-diagram.png" alt="AVAudioEngine nodes for this project"></p>\n<p>Open the <code is:raw>ViewController.swift</code> file and import AVFoundation and Starscream at the top</p>\n<p>import AVFoundation\nImport Starscream</p>\n<p>This will give you access to the <code is:raw>AVAudioEngine</code> class. Then inside the <code is:raw>ViewController</code> class, create an instance of <code is:raw>AVAudioEngine</code> by adding a property:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">private</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> audioEngine </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">AVAudioEngine</span><span style="color: #C9D1D9">()</span></span></code></pre>\n<p>Next, create a function to analyse the audio and declare some constants:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">private</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">func</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">startAnalyzingAudio</span><span style="color: #C9D1D9">() {</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> inputNode </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> audioEngine.inputNode</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> inputFormat </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> inputNode.</span><span style="color: #79C0FF">inputFormat</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">forBus</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> outputFormat </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">AVAudioFormat</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">commonFormat</span><span style="color: #C9D1D9">: .pcmFormatInt16, </span><span style="color: #79C0FF">sampleRate</span><span style="color: #C9D1D9">: inputFormat.sampleRate, </span><span style="color: #79C0FF">channels</span><span style="color: #C9D1D9">: inputFormat.channelCount, </span><span style="color: #79C0FF">interleaved</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">true</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>\n<p>From the top, you have the <code is:raw>inputNode</code>, which you can think of as the microphone followed by the <code is:raw>inputFormat</code>.</p>\n<p>Next is the <code is:raw>outputFormat</code> - The iOS microphone is a 32-bit depth format by default, so you will be converting to a 16-bit depth format before sending the data to Deepgram. While we will use a PCM 16-bit depth format for the audio encoding, Deepgram supports many audio encodings that can be specified to the API. Consider using a more compressed format if your use case requires low network usage. Just below those, add the new nodes that you will need and attach them to the audio engine:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> converterNode </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">AVAudioMixerNode</span><span style="color: #C9D1D9">()</span></span>\n<span class="line"><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> sinkNode </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">AVAudioMixerNode</span><span style="color: #C9D1D9">()</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">audioEngine.</span><span style="color: #79C0FF">attach</span><span style="color: #C9D1D9">(converterNode)</span></span>\n<span class="line"><span style="color: #C9D1D9">audioEngine.</span><span style="color: #79C0FF">attach</span><span style="color: #C9D1D9">(sinkNode)</span></span></code></pre>\n<p>The sinkNode is needed because to get the data in the correct format you need to access the data after it has passed through the converterNode, or its output. If you refer to the diagram above, notice how the stream of data to Deepgram is coming from the connection between nodes, the output of the <code is:raw>converterNode</code>, and not the nodes themselves.</p>\n<h2 id="get-microphone-data">Get Microphone Data</h2>\n<p>Use the <code is:raw>installTap</code> function on the <code is:raw>AVAudioMixerNode</code> class to get the microphone data. This function gives you a closure that returns the output audio data of a node as a buffer. Call <code is:raw>installTap</code> on the <code is:raw>converterNode</code>, continuing the function from above:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">converterNode.</span><span style="color: #79C0FF">installTap</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">onBus</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">bufferSize</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">1024</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">format</span><span style="color: #C9D1D9">: converterNode.</span><span style="color: #79C0FF">outputFormat</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">forBus</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">)) { (</span><span style="color: #79C0FF">buffer</span><span style="color: #C9D1D9">: AVAudioPCMBuffer</span><span style="color: #FF7B72">!</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">time</span><span style="color: #C9D1D9">: AVAudioTime</span><span style="color: #FF7B72">!</span><span style="color: #C9D1D9">) </span><span style="color: #FF7B72">-&gt;</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">Void</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">in</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>\n<p>You will come back to this closure later. Now finish off the <code is:raw>startAnalyzingAudio</code> by connecting all the nodes and starting the audio engine:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">audioEngine.</span><span style="color: #79C0FF">connect</span><span style="color: #C9D1D9">(inputNode, </span><span style="color: #79C0FF">to</span><span style="color: #C9D1D9">: converterNode, </span><span style="color: #79C0FF">format</span><span style="color: #C9D1D9">: inputFormat)</span></span>\n<span class="line"><span style="color: #C9D1D9">audioEngine.</span><span style="color: #79C0FF">connect</span><span style="color: #C9D1D9">(converterNode, </span><span style="color: #79C0FF">to</span><span style="color: #C9D1D9">: sinkNode, </span><span style="color: #79C0FF">format</span><span style="color: #C9D1D9">: outputFormat)</span></span>\n<span class="line"><span style="color: #C9D1D9">audioEngine.</span><span style="color: #79C0FF">prepare</span><span style="color: #C9D1D9">()</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #FF7B72">do</span><span style="color: #C9D1D9"> {</span></span>\n<span class="line"><span style="color: #FF7B72">try</span><span style="color: #C9D1D9"> AVAudioSession.</span><span style="color: #79C0FF">sharedInstance</span><span style="color: #C9D1D9">().</span><span style="color: #79C0FF">setCategory</span><span style="color: #C9D1D9">(.record)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">try</span><span style="color: #C9D1D9"> audioEngine.</span><span style="color: #79C0FF">start</span><span style="color: #C9D1D9">()</span></span>\n<span class="line"><span style="color: #C9D1D9">} </span><span style="color: #FF7B72">catch</span><span style="color: #C9D1D9"> {</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(error)</span></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>\n<p>You can see now how the processing pipeline mirrors the diagram from earlier. Before the audio engine is started, the audio session of the application needs to be set to a record category. This lets the app know that the audio engine is solely being used for recording purposes. Note how the <code is:raw>converterNode</code> is connected with <code is:raw>outputFormat</code>.</p>\n<h2 id="connect-to-deepgram">Connect to Deepgram</h2>\n<p>You will be using the Deepgram WebSocket Streaming API, create a <code is:raw>WebSocket</code> instance at the top of the <code is:raw>ViewController</code> class:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">private</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> apiKey </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;Token YOUR_DEEPGRAM_API_KEY&quot;</span></span>\n<span class="line"><span style="color: #FF7B72">private</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">lazy</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">var</span><span style="color: #C9D1D9"> socket: WebSocket </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> {</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> url </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">URL</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">string</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;wss://api.deepgram.com/v1/listen?encoding=linear16&amp;sample_rate=48000&amp;channels=1&quot;</span><span style="color: #C9D1D9">)</span><span style="color: #FF7B72">!</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">var</span><span style="color: #C9D1D9"> urlRequest </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">URLRequest</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">url</span><span style="color: #C9D1D9">: url)</span></span>\n<span class="line"><span style="color: #C9D1D9">  urlRequest.</span><span style="color: #79C0FF">setValue</span><span style="color: #C9D1D9">(apiKey, </span><span style="color: #79C0FF">forHTTPHeaderField</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;Authorization&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">WebSocket</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">request</span><span style="color: #C9D1D9">: urlRequest)</span></span>\n<span class="line"><span style="color: #C9D1D9">}()</span></span></code></pre>\n<Alert type="warning">A reminder that this key is in your app\u2019s source code and, therefore, it is not secure. Please factor this into your actual projects.</Alert>\n<p>Note how the URL has an encoding matching the <code is:raw>outputFormat</code> from earlier. In the <code is:raw>viewDidLoad</code> function, set the socket\u2019s delegate to this class and open the connection:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">override</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">func</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">viewDidLoad</span><span style="color: #C9D1D9">() {</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #79C0FF">super</span><span style="color: #C9D1D9">.</span><span style="color: #79C0FF">viewDidLoad</span><span style="color: #C9D1D9">()</span></span>\n<span class="line"><span style="color: #C9D1D9">  socket.delegate </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span></span>\n<span class="line"><span style="color: #C9D1D9">  socket.</span><span style="color: #79C0FF">connect</span><span style="color: #C9D1D9">()</span></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>\n<p>You will implement the delegate later in the post.</p>\n<h2 id="send-data-to-deepgram">Send Data to Deepgram</h2>\n<p>Now that the socket connection is open, you can now send the microphone data to Deepgram. To send data over a WebSocket in iOS, it needs to be converted to a <code is:raw>Data</code> object. Add the following function to the <code is:raw>ViewController</code> class that does the conversion:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">private</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">func</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">toNSData</span><span style="color: #C9D1D9">(</span><span style="color: #D2A8FF">buffer</span><span style="color: #C9D1D9">: AVAudioPCMBuffer) </span><span style="color: #FF7B72">-&gt;</span><span style="color: #C9D1D9"> Data</span><span style="color: #FF7B72">?</span><span style="color: #C9D1D9"> {</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> audioBuffer </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> buffer.audioBufferList.</span><span style="color: #79C0FF">pointee</span><span style="color: #C9D1D9">.mBuffers</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">Data</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">bytes</span><span style="color: #C9D1D9">: audioBuffer.mData</span><span style="color: #FF7B72">!</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">count</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">Int</span><span style="color: #C9D1D9">(audioBuffer.mDataByteSize))</span></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>\n<p>Then return to the <code is:raw>startAnalyzingAudio</code> function, and within the <code is:raw>installTap</code> closure, you can send the data to Deepgram. Your tap code should look like this now:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">converterNode.</span><span style="color: #79C0FF">installTap</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">onBus</span><span style="color: #C9D1D9">: bus, </span><span style="color: #79C0FF">bufferSize</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">1024</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">format</span><span style="color: #C9D1D9">: converterNode.</span><span style="color: #79C0FF">outputFormat</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">forBus</span><span style="color: #C9D1D9">: bus)) { (</span><span style="color: #79C0FF">buffer</span><span style="color: #C9D1D9">: AVAudioPCMBuffer</span><span style="color: #FF7B72">!</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">time</span><span style="color: #C9D1D9">: AVAudioTime</span><span style="color: #FF7B72">!</span><span style="color: #C9D1D9">) </span><span style="color: #FF7B72">-&gt;</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">Void</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">in</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> data </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.</span><span style="color: #79C0FF">toNSData</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">buffer</span><span style="color: #C9D1D9">: buffer) {</span></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.socket.</span><span style="color: #79C0FF">write</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">data</span><span style="color: #C9D1D9">: data)</span></span>\n<span class="line"><span style="color: #C9D1D9">  }</span></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>\n<p>Call <code is:raw>startAnalyzingAudio</code> in the <code is:raw>viewDidLoad</code> function below the WebSocket configuration.</p>\n<h2 id="handle-the-deepgram-response">Handle the Deepgram Response</h2>\n<p>You will get updates from the WebSocket via its <a href="https://www.hackingwithswift.com/example-code/language/what-is-a-delegate-in-ios">delegate</a>. In the <code is:raw>ViewController.swift</code> file <em>outside</em> the class, create an extension for the <code is:raw>WebSocketDelegate</code> and a <code is:raw>DeepgramResponse</code> struct:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">extension</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">ViewController</span><span style="color: #C9D1D9">: WebSocketDelegate {</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">func</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">didReceive</span><span style="color: #C9D1D9">(</span><span style="color: #D2A8FF">event</span><span style="color: #C9D1D9">: WebSocketEvent, </span><span style="color: #D2A8FF">client</span><span style="color: #C9D1D9">: WebSocket) {</span></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">switch</span><span style="color: #C9D1D9"> event {</span></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">case</span><span style="color: #C9D1D9"> .</span><span style="color: #79C0FF">text</span><span style="color: #C9D1D9">(</span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> text)</span><span style="color: #FF7B72">:</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">case</span><span style="color: #C9D1D9"> .</span><span style="color: #79C0FF">error</span><span style="color: #C9D1D9">(</span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> error)</span><span style="color: #FF7B72">:</span></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(error </span><span style="color: #FF7B72">??</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">default:</span></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #FF7B72">break</span></span>\n<span class="line"><span style="color: #C9D1D9">     }</span></span>\n<span class="line"><span style="color: #C9D1D9">  }</span></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #FF7B72">struct</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">DeepgramResponse</span><span style="color: #C9D1D9">: Codable {</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> isFinal: </span><span style="color: #79C0FF">Bool</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> channel: Channel</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">struct</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">Channel</span><span style="color: #C9D1D9">: Codable {</span></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> alternatives: [Alternatives]</span></span>\n<span class="line"><span style="color: #C9D1D9">  }</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">struct</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">Alternatives</span><span style="color: #C9D1D9">: Codable {</span></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> transcript: </span><span style="color: #79C0FF">String</span></span>\n<span class="line"><span style="color: #C9D1D9">  }</span></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>\n<p>The <code is:raw>didReceive</code> function on the <code is:raw>WebSocketDelegate</code> will be called whenever you get an update on the WebSocket. Before you finish implementing <code is:raw>didReceive</code>, you need to prepare to decode the data and update the UI to display the transcripts. At the top of the <code is:raw>ViewController</code> class, add the following properties:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">private</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> jsonDecoder: JSONDecoder </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> {</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> decoder </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">JSONDecoder</span><span style="color: #C9D1D9">()</span></span>\n<span class="line"><span style="color: #C9D1D9">  decoder.keyDecodingStrategy </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> .convertFromSnakeCase</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> decoder</span></span>\n<span class="line"><span style="color: #C9D1D9">}()</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #FF7B72">private</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> transcriptView: UITextView </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> {</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> textView </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">UITextView</span><span style="color: #C9D1D9">()</span></span>\n<span class="line"><span style="color: #C9D1D9">  textView.isScrollEnabled </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">true</span></span>\n<span class="line"><span style="color: #C9D1D9">  textView.backgroundColor </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> .lightGray</span></span>\n<span class="line"><span style="color: #C9D1D9">  textView.translatesAutoresizingMaskIntoConstraints </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">false</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> textView</span></span>\n<span class="line"><span style="color: #C9D1D9">}()</span></span></code></pre>\n<p>Create a new function called <code is:raw>setupView</code> and configure your UI:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">private</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">func</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">setupView</span><span style="color: #C9D1D9">() {</span></span>\n<span class="line"><span style="color: #C9D1D9">  view.</span><span style="color: #79C0FF">addSubview</span><span style="color: #C9D1D9">(transcriptView)</span></span>\n<span class="line"><span style="color: #C9D1D9">  NSLayoutConstraint.</span><span style="color: #79C0FF">activate</span><span style="color: #C9D1D9">([</span></span>\n<span class="line"><span style="color: #C9D1D9">     transcriptView.topAnchor.</span><span style="color: #79C0FF">constraint</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">equalTo</span><span style="color: #C9D1D9">: view.safeAreaLayoutGuide.topAnchor),</span></span>\n<span class="line"><span style="color: #C9D1D9">     transcriptView.leadingAnchor.</span><span style="color: #79C0FF">constraint</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">equalTo</span><span style="color: #C9D1D9">: view.leadingAnchor, </span><span style="color: #79C0FF">constant</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">16</span><span style="color: #C9D1D9">),</span></span>\n<span class="line"><span style="color: #C9D1D9">     transcriptView.trailingAnchor.</span><span style="color: #79C0FF">constraint</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">equalTo</span><span style="color: #C9D1D9">: view.trailingAnchor, </span><span style="color: #79C0FF">constant</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">-16</span><span style="color: #C9D1D9">),</span></span>\n<span class="line"><span style="color: #C9D1D9">     transcriptView.bottomAnchor.</span><span style="color: #79C0FF">constraint</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">equalTo</span><span style="color: #C9D1D9">: view.safeAreaLayoutGuide.bottomAnchor)</span></span>\n<span class="line"><span style="color: #C9D1D9">  ])</span></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>\n<p>Then call <code is:raw>setupView</code> in the <code is:raw>viewDidLoad</code> function.</p>\n<p>Returning to the <code is:raw>didReceive</code> function of the <code is:raw>WebSocketDelegate</code>, within the text case; you need to convert the text, which is a <code is:raw>String</code> type, into a <code is:raw>Data</code> type so you can decode it into an instance of <code is:raw>DeepgramResponse</code>:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">extension</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">ViewController</span><span style="color: #C9D1D9">: WebSocketDelegate {</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">func</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">didReceive</span><span style="color: #C9D1D9">(</span><span style="color: #D2A8FF">event</span><span style="color: #C9D1D9">: WebSocketEvent, </span><span style="color: #D2A8FF">client</span><span style="color: #C9D1D9">: WebSocket) {</span></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">switch</span><span style="color: #C9D1D9"> event {</span></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">case</span><span style="color: #C9D1D9"> .</span><span style="color: #79C0FF">text</span><span style="color: #C9D1D9">(</span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> text)</span><span style="color: #FF7B72">:</span></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> jsonData </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">Data</span><span style="color: #C9D1D9">(text.</span><span style="color: #79C0FF">utf8</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> response </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">try!</span><span style="color: #C9D1D9"> jsonDecoder.</span><span style="color: #79C0FF">decode</span><span style="color: #C9D1D9">(DeepgramResponse.</span><span style="color: #FF7B72">self</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">from</span><span style="color: #C9D1D9">: jsonData)</span></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> transcript </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> response.channel.alternatives.</span><span style="color: #79C0FF">first</span><span style="color: #FF7B72">!</span><span style="color: #C9D1D9">.transcript</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> response.isFinal </span><span style="color: #FF7B72">&amp;&amp;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">!</span><span style="color: #C9D1D9">transcript.</span><span style="color: #79C0FF">isEmpty</span><span style="color: #C9D1D9"> {</span></span>\n<span class="line"><span style="color: #C9D1D9">          </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> transcriptView.</span><span style="color: #79C0FF">text</span><span style="color: #C9D1D9">.</span><span style="color: #79C0FF">isEmpty</span><span style="color: #C9D1D9"> {</span></span>\n<span class="line"><span style="color: #C9D1D9">             transcriptView.</span><span style="color: #79C0FF">text</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> transcript</span></span>\n<span class="line"><span style="color: #C9D1D9">          } </span><span style="color: #FF7B72">else</span><span style="color: #C9D1D9"> {</span></span>\n<span class="line"><span style="color: #C9D1D9">             transcriptView.</span><span style="color: #79C0FF">text</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> transcriptView.</span><span style="color: #79C0FF">text</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot; &quot;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> transcript</span></span>\n<span class="line"><span style="color: #C9D1D9">          }</span></span>\n<span class="line"><span style="color: #C9D1D9">        }</span></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">case</span><span style="color: #C9D1D9"> .</span><span style="color: #79C0FF">error</span><span style="color: #C9D1D9">(</span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> error)</span><span style="color: #FF7B72">:</span></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(error </span><span style="color: #FF7B72">??</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">default:</span></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #FF7B72">break</span></span>\n<span class="line"><span style="color: #C9D1D9">     }</span></span>\n<span class="line"><span style="color: #C9D1D9">  }</span></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>\n<p>Once you have a <code is:raw>DeepgramResponse</code> instance, you will check if it is final, meaning it is ready to add to the <code is:raw>transcriptView</code> while handling the empty scenarios. If you now build and run the project (CMD + R), it will open in the simulator, where you will be prompted to give microphone access. Then you can start transcribing!</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1638827585/blog/2022/01/ios-live-transcription/simulator.png" alt="iPhone simulator showing the text hello blog"></p>\n<p>The final project code is available at <a href="https://github.com/deepgram-devs/deepgram-live-transcripts-ios">https://github.com/deepgram-devs/deepgram-live-transcripts-ios</a>, and if you have any questions, please feel free to reach out to the Deepgram team on Twitter - <a href="https://twitter.com/DeepgramDevs">@DeepgramDevs</a>.</p>';
}
const $$Astro = createAstro("/Users/sandrarodgers/web-next/blog/src/content/blog/posts/ios-live-transcription/index.md", "https://blog.deepgram.com/", "file:///Users/sandrarodgers/web-next/blog/");
const $$Index = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro, $$props, $$slots);
  Astro2.self = $$Index;
  new Slugger();
  return renderTemplate`<head>${renderHead($$result)}</head>${renderComponent($$result, "guest-author", "guest-author", {})}
<p>Deepgram’s Speech Recognition API can provide accurate transcripts with pre-recorded and live audio. In this post, you will build an iOS app that takes the user’s microphone and prints transcripts to the screen in real-time.</p>
<p>The final project code is available at <a href="https://github.com/deepgram-devs/deepgram-live-transcripts-ios">https://github.com/deepgram-devs/deepgram-live-transcripts-ios</a>.</p>
<h2 id="before-we-start">Before We Start</h2>
<p>For this project, you will need:</p>
<ul>
<li>A Deepgram API Key - <a href="https://console.deepgram.com/signup?jump=keys">get one here</a></li>
<li><a href="https://developer.apple.com/xcode/">Xcode</a> installed on your machine.</li>
</ul>
<p>Once you open Xcode, create a new Xcode project. Select the App template, give it a product name, select “Storyboard” for the interface, and “Swift” for the language. Untick “User Core Data” and “Include Tests”, then hit next.</p>
<p>Choose a place to save your project and hit finish to open your new project.</p>
<p>You will be using the WebSocket library <a href="https://github.com/daltoniam/Starscream">Starscream</a> for this project. You can add it as a Swift Package by going to <em>File > Add Packages</em>, then entering the URL (<a href="https://github.com/daltoniam/Starscream">https://github.com/daltoniam/Starscream</a>) into the search box. Click add package and wait for the package to download.</p>
<h2 id="get-microphone-permissions">Get Microphone Permissions</h2>
<p>You will be using the microphone for this project, so you will need to add a microphone usage description to your project. When an iOS app asks for microphone permission, a reason why the permission is being requested is required and will be shown to the user. Open the <code>info.plist</code> file to add a description.</p>
<p>Hover over the last entry and click the plus icon to add a new entry. The key should be “Privacy - Microphone Usage Description”, and you should add a value that describes why you need the microphone. You can use “To transcribe audio with Deepgram”.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1638827585/blog/2022/01/ios-live-transcription/plist.png" alt="Plist file with the microphone usage description"></p>
<h2 id="setting-up-avaudioengine">Setting Up <code>AVAudioEngine</code></h2>
<p>To access a data stream from the microphone, you will use <a href="https://developer.apple.com/documentation/avfaudio/avaudioengine"><code>AVAudioEngine</code></a>.</p>
<p>Before you get started, it is worth having a mental model of how <code>AVAudioEngine</code> works. Apple’s description is: “An audio engine object contains a group of AVAudioNode instances that you attach to form an audio processing chain”.</p>
<p>For this example, you will create a chain of nodes that will take the microphone’s input, convert it to a format that Deepgram can process, then send the data to Deepgram. The nodes and conversion will be discussed further in the blog.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1638827585/blog/2022/01/ios-live-transcription/engine-diagram.png" alt="AVAudioEngine nodes for this project"></p>
<p>Open the <code>ViewController.swift</code> file and import AVFoundation and Starscream at the top</p>
<p>import AVFoundation
Import Starscream</p>
<p>This will give you access to the <code>AVAudioEngine</code> class. Then inside the <code>ViewController</code> class, create an instance of <code>AVAudioEngine</code> by adding a property:</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">private</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> audioEngine </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">AVAudioEngine</span><span style="color: #C9D1D9">()</span></span></code></pre>
<p>Next, create a function to analyse the audio and declare some constants:</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">private</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">func</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">startAnalyzingAudio</span><span style="color: #C9D1D9">() {</span></span>
<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> inputNode </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> audioEngine.inputNode</span></span>
<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> inputFormat </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> inputNode.</span><span style="color: #79C0FF">inputFormat</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">forBus</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> outputFormat </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">AVAudioFormat</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">commonFormat</span><span style="color: #C9D1D9">: .pcmFormatInt16, </span><span style="color: #79C0FF">sampleRate</span><span style="color: #C9D1D9">: inputFormat.sampleRate, </span><span style="color: #79C0FF">channels</span><span style="color: #C9D1D9">: inputFormat.channelCount, </span><span style="color: #79C0FF">interleaved</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">true</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>
<p>From the top, you have the <code>inputNode</code>, which you can think of as the microphone followed by the <code>inputFormat</code>.</p>
<p>Next is the <code>outputFormat</code> - The iOS microphone is a 32-bit depth format by default, so you will be converting to a 16-bit depth format before sending the data to Deepgram. While we will use a PCM 16-bit depth format for the audio encoding, Deepgram supports many audio encodings that can be specified to the API. Consider using a more compressed format if your use case requires low network usage. Just below those, add the new nodes that you will need and attach them to the audio engine:</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> converterNode </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">AVAudioMixerNode</span><span style="color: #C9D1D9">()</span></span>
<span class="line"><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> sinkNode </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">AVAudioMixerNode</span><span style="color: #C9D1D9">()</span></span>
<span class="line"></span>
<span class="line"><span style="color: #C9D1D9">audioEngine.</span><span style="color: #79C0FF">attach</span><span style="color: #C9D1D9">(converterNode)</span></span>
<span class="line"><span style="color: #C9D1D9">audioEngine.</span><span style="color: #79C0FF">attach</span><span style="color: #C9D1D9">(sinkNode)</span></span></code></pre>
<p>The sinkNode is needed because to get the data in the correct format you need to access the data after it has passed through the converterNode, or its output. If you refer to the diagram above, notice how the stream of data to Deepgram is coming from the connection between nodes, the output of the <code>converterNode</code>, and not the nodes themselves.</p>
<h2 id="get-microphone-data">Get Microphone Data</h2>
<p>Use the <code>installTap</code> function on the <code>AVAudioMixerNode</code> class to get the microphone data. This function gives you a closure that returns the output audio data of a node as a buffer. Call <code>installTap</code> on the <code>converterNode</code>, continuing the function from above:</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">converterNode.</span><span style="color: #79C0FF">installTap</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">onBus</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">bufferSize</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">1024</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">format</span><span style="color: #C9D1D9">: converterNode.</span><span style="color: #79C0FF">outputFormat</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">forBus</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">)) { (</span><span style="color: #79C0FF">buffer</span><span style="color: #C9D1D9">: AVAudioPCMBuffer</span><span style="color: #FF7B72">!</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">time</span><span style="color: #C9D1D9">: AVAudioTime</span><span style="color: #FF7B72">!</span><span style="color: #C9D1D9">) </span><span style="color: #FF7B72">-&gt;</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">Void</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">in</span></span>
<span class="line"></span>
<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>
<p>You will come back to this closure later. Now finish off the <code>startAnalyzingAudio</code> by connecting all the nodes and starting the audio engine:</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">audioEngine.</span><span style="color: #79C0FF">connect</span><span style="color: #C9D1D9">(inputNode, </span><span style="color: #79C0FF">to</span><span style="color: #C9D1D9">: converterNode, </span><span style="color: #79C0FF">format</span><span style="color: #C9D1D9">: inputFormat)</span></span>
<span class="line"><span style="color: #C9D1D9">audioEngine.</span><span style="color: #79C0FF">connect</span><span style="color: #C9D1D9">(converterNode, </span><span style="color: #79C0FF">to</span><span style="color: #C9D1D9">: sinkNode, </span><span style="color: #79C0FF">format</span><span style="color: #C9D1D9">: outputFormat)</span></span>
<span class="line"><span style="color: #C9D1D9">audioEngine.</span><span style="color: #79C0FF">prepare</span><span style="color: #C9D1D9">()</span></span>
<span class="line"></span>
<span class="line"><span style="color: #FF7B72">do</span><span style="color: #C9D1D9"> {</span></span>
<span class="line"><span style="color: #FF7B72">try</span><span style="color: #C9D1D9"> AVAudioSession.</span><span style="color: #79C0FF">sharedInstance</span><span style="color: #C9D1D9">().</span><span style="color: #79C0FF">setCategory</span><span style="color: #C9D1D9">(.record)</span></span>
<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">try</span><span style="color: #C9D1D9"> audioEngine.</span><span style="color: #79C0FF">start</span><span style="color: #C9D1D9">()</span></span>
<span class="line"><span style="color: #C9D1D9">} </span><span style="color: #FF7B72">catch</span><span style="color: #C9D1D9"> {</span></span>
<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(error)</span></span>
<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>
<p>You can see now how the processing pipeline mirrors the diagram from earlier. Before the audio engine is started, the audio session of the application needs to be set to a record category. This lets the app know that the audio engine is solely being used for recording purposes. Note how the <code>converterNode</code> is connected with <code>outputFormat</code>.</p>
<h2 id="connect-to-deepgram">Connect to Deepgram</h2>
<p>You will be using the Deepgram WebSocket Streaming API, create a <code>WebSocket</code> instance at the top of the <code>ViewController</code> class:</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">private</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> apiKey </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;Token YOUR_DEEPGRAM_API_KEY&quot;</span></span>
<span class="line"><span style="color: #FF7B72">private</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">lazy</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">var</span><span style="color: #C9D1D9"> socket: WebSocket </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> {</span></span>
<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> url </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">URL</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">string</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;wss://api.deepgram.com/v1/listen?encoding=linear16&amp;sample_rate=48000&amp;channels=1&quot;</span><span style="color: #C9D1D9">)</span><span style="color: #FF7B72">!</span></span>
<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">var</span><span style="color: #C9D1D9"> urlRequest </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">URLRequest</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">url</span><span style="color: #C9D1D9">: url)</span></span>
<span class="line"><span style="color: #C9D1D9">  urlRequest.</span><span style="color: #79C0FF">setValue</span><span style="color: #C9D1D9">(apiKey, </span><span style="color: #79C0FF">forHTTPHeaderField</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;Authorization&quot;</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">WebSocket</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">request</span><span style="color: #C9D1D9">: urlRequest)</span></span>
<span class="line"><span style="color: #C9D1D9">}()</span></span></code></pre>
${renderComponent($$result, "Alert", Alert, { "type": "warning" }, { "default": () => renderTemplate`A reminder that this key is in your app’s source code and, therefore, it is not secure. Please factor this into your actual projects.` })}
<p>Note how the URL has an encoding matching the <code>outputFormat</code> from earlier. In the <code>viewDidLoad</code> function, set the socket’s delegate to this class and open the connection:</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">override</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">func</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">viewDidLoad</span><span style="color: #C9D1D9">() {</span></span>
<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #79C0FF">super</span><span style="color: #C9D1D9">.</span><span style="color: #79C0FF">viewDidLoad</span><span style="color: #C9D1D9">()</span></span>
<span class="line"><span style="color: #C9D1D9">  socket.delegate </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span></span>
<span class="line"><span style="color: #C9D1D9">  socket.</span><span style="color: #79C0FF">connect</span><span style="color: #C9D1D9">()</span></span>
<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>
<p>You will implement the delegate later in the post.</p>
<h2 id="send-data-to-deepgram">Send Data to Deepgram</h2>
<p>Now that the socket connection is open, you can now send the microphone data to Deepgram. To send data over a WebSocket in iOS, it needs to be converted to a <code>Data</code> object. Add the following function to the <code>ViewController</code> class that does the conversion:</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">private</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">func</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">toNSData</span><span style="color: #C9D1D9">(</span><span style="color: #D2A8FF">buffer</span><span style="color: #C9D1D9">: AVAudioPCMBuffer) </span><span style="color: #FF7B72">-&gt;</span><span style="color: #C9D1D9"> Data</span><span style="color: #FF7B72">?</span><span style="color: #C9D1D9"> {</span></span>
<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> audioBuffer </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> buffer.audioBufferList.</span><span style="color: #79C0FF">pointee</span><span style="color: #C9D1D9">.mBuffers</span></span>
<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">Data</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">bytes</span><span style="color: #C9D1D9">: audioBuffer.mData</span><span style="color: #FF7B72">!</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">count</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">Int</span><span style="color: #C9D1D9">(audioBuffer.mDataByteSize))</span></span>
<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>
<p>Then return to the <code>startAnalyzingAudio</code> function, and within the <code>installTap</code> closure, you can send the data to Deepgram. Your tap code should look like this now:</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">converterNode.</span><span style="color: #79C0FF">installTap</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">onBus</span><span style="color: #C9D1D9">: bus, </span><span style="color: #79C0FF">bufferSize</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">1024</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">format</span><span style="color: #C9D1D9">: converterNode.</span><span style="color: #79C0FF">outputFormat</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">forBus</span><span style="color: #C9D1D9">: bus)) { (</span><span style="color: #79C0FF">buffer</span><span style="color: #C9D1D9">: AVAudioPCMBuffer</span><span style="color: #FF7B72">!</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">time</span><span style="color: #C9D1D9">: AVAudioTime</span><span style="color: #FF7B72">!</span><span style="color: #C9D1D9">) </span><span style="color: #FF7B72">-&gt;</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">Void</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">in</span></span>
<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> data </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.</span><span style="color: #79C0FF">toNSData</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">buffer</span><span style="color: #C9D1D9">: buffer) {</span></span>
<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.socket.</span><span style="color: #79C0FF">write</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">data</span><span style="color: #C9D1D9">: data)</span></span>
<span class="line"><span style="color: #C9D1D9">  }</span></span>
<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>
<p>Call <code>startAnalyzingAudio</code> in the <code>viewDidLoad</code> function below the WebSocket configuration.</p>
<h2 id="handle-the-deepgram-response">Handle the Deepgram Response</h2>
<p>You will get updates from the WebSocket via its <a href="https://www.hackingwithswift.com/example-code/language/what-is-a-delegate-in-ios">delegate</a>. In the <code>ViewController.swift</code> file <em>outside</em> the class, create an extension for the <code>WebSocketDelegate</code> and a <code>DeepgramResponse</code> struct:</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">extension</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">ViewController</span><span style="color: #C9D1D9">: WebSocketDelegate {</span></span>
<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">func</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">didReceive</span><span style="color: #C9D1D9">(</span><span style="color: #D2A8FF">event</span><span style="color: #C9D1D9">: WebSocketEvent, </span><span style="color: #D2A8FF">client</span><span style="color: #C9D1D9">: WebSocket) {</span></span>
<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">switch</span><span style="color: #C9D1D9"> event {</span></span>
<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">case</span><span style="color: #C9D1D9"> .</span><span style="color: #79C0FF">text</span><span style="color: #C9D1D9">(</span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> text)</span><span style="color: #FF7B72">:</span></span>
<span class="line"></span>
<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">case</span><span style="color: #C9D1D9"> .</span><span style="color: #79C0FF">error</span><span style="color: #C9D1D9">(</span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> error)</span><span style="color: #FF7B72">:</span></span>
<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(error </span><span style="color: #FF7B72">??</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;&quot;</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">default:</span></span>
<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #FF7B72">break</span></span>
<span class="line"><span style="color: #C9D1D9">     }</span></span>
<span class="line"><span style="color: #C9D1D9">  }</span></span>
<span class="line"><span style="color: #C9D1D9">}</span></span>
<span class="line"></span>
<span class="line"><span style="color: #FF7B72">struct</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">DeepgramResponse</span><span style="color: #C9D1D9">: Codable {</span></span>
<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> isFinal: </span><span style="color: #79C0FF">Bool</span></span>
<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> channel: Channel</span></span>
<span class="line"></span>
<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">struct</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">Channel</span><span style="color: #C9D1D9">: Codable {</span></span>
<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> alternatives: [Alternatives]</span></span>
<span class="line"><span style="color: #C9D1D9">  }</span></span>
<span class="line"></span>
<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">struct</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">Alternatives</span><span style="color: #C9D1D9">: Codable {</span></span>
<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> transcript: </span><span style="color: #79C0FF">String</span></span>
<span class="line"><span style="color: #C9D1D9">  }</span></span>
<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>
<p>The <code>didReceive</code> function on the <code>WebSocketDelegate</code> will be called whenever you get an update on the WebSocket. Before you finish implementing <code>didReceive</code>, you need to prepare to decode the data and update the UI to display the transcripts. At the top of the <code>ViewController</code> class, add the following properties:</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">private</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> jsonDecoder: JSONDecoder </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> {</span></span>
<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> decoder </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">JSONDecoder</span><span style="color: #C9D1D9">()</span></span>
<span class="line"><span style="color: #C9D1D9">  decoder.keyDecodingStrategy </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> .convertFromSnakeCase</span></span>
<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> decoder</span></span>
<span class="line"><span style="color: #C9D1D9">}()</span></span>
<span class="line"></span>
<span class="line"><span style="color: #FF7B72">private</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> transcriptView: UITextView </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> {</span></span>
<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> textView </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">UITextView</span><span style="color: #C9D1D9">()</span></span>
<span class="line"><span style="color: #C9D1D9">  textView.isScrollEnabled </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">true</span></span>
<span class="line"><span style="color: #C9D1D9">  textView.backgroundColor </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> .lightGray</span></span>
<span class="line"><span style="color: #C9D1D9">  textView.translatesAutoresizingMaskIntoConstraints </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">false</span></span>
<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> textView</span></span>
<span class="line"><span style="color: #C9D1D9">}()</span></span></code></pre>
<p>Create a new function called <code>setupView</code> and configure your UI:</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">private</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">func</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">setupView</span><span style="color: #C9D1D9">() {</span></span>
<span class="line"><span style="color: #C9D1D9">  view.</span><span style="color: #79C0FF">addSubview</span><span style="color: #C9D1D9">(transcriptView)</span></span>
<span class="line"><span style="color: #C9D1D9">  NSLayoutConstraint.</span><span style="color: #79C0FF">activate</span><span style="color: #C9D1D9">([</span></span>
<span class="line"><span style="color: #C9D1D9">     transcriptView.topAnchor.</span><span style="color: #79C0FF">constraint</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">equalTo</span><span style="color: #C9D1D9">: view.safeAreaLayoutGuide.topAnchor),</span></span>
<span class="line"><span style="color: #C9D1D9">     transcriptView.leadingAnchor.</span><span style="color: #79C0FF">constraint</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">equalTo</span><span style="color: #C9D1D9">: view.leadingAnchor, </span><span style="color: #79C0FF">constant</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">16</span><span style="color: #C9D1D9">),</span></span>
<span class="line"><span style="color: #C9D1D9">     transcriptView.trailingAnchor.</span><span style="color: #79C0FF">constraint</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">equalTo</span><span style="color: #C9D1D9">: view.trailingAnchor, </span><span style="color: #79C0FF">constant</span><span style="color: #C9D1D9">: </span><span style="color: #79C0FF">-16</span><span style="color: #C9D1D9">),</span></span>
<span class="line"><span style="color: #C9D1D9">     transcriptView.bottomAnchor.</span><span style="color: #79C0FF">constraint</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">equalTo</span><span style="color: #C9D1D9">: view.safeAreaLayoutGuide.bottomAnchor)</span></span>
<span class="line"><span style="color: #C9D1D9">  ])</span></span>
<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>
<p>Then call <code>setupView</code> in the <code>viewDidLoad</code> function.</p>
<p>Returning to the <code>didReceive</code> function of the <code>WebSocketDelegate</code>, within the text case; you need to convert the text, which is a <code>String</code> type, into a <code>Data</code> type so you can decode it into an instance of <code>DeepgramResponse</code>:</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">extension</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">ViewController</span><span style="color: #C9D1D9">: WebSocketDelegate {</span></span>
<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">func</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">didReceive</span><span style="color: #C9D1D9">(</span><span style="color: #D2A8FF">event</span><span style="color: #C9D1D9">: WebSocketEvent, </span><span style="color: #D2A8FF">client</span><span style="color: #C9D1D9">: WebSocket) {</span></span>
<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">switch</span><span style="color: #C9D1D9"> event {</span></span>
<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">case</span><span style="color: #C9D1D9"> .</span><span style="color: #79C0FF">text</span><span style="color: #C9D1D9">(</span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> text)</span><span style="color: #FF7B72">:</span></span>
<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> jsonData </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">Data</span><span style="color: #C9D1D9">(text.</span><span style="color: #79C0FF">utf8</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> response </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">try!</span><span style="color: #C9D1D9"> jsonDecoder.</span><span style="color: #79C0FF">decode</span><span style="color: #C9D1D9">(DeepgramResponse.</span><span style="color: #FF7B72">self</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">from</span><span style="color: #C9D1D9">: jsonData)</span></span>
<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> transcript </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> response.channel.alternatives.</span><span style="color: #79C0FF">first</span><span style="color: #FF7B72">!</span><span style="color: #C9D1D9">.transcript</span></span>
<span class="line"></span>
<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> response.isFinal </span><span style="color: #FF7B72">&amp;&amp;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">!</span><span style="color: #C9D1D9">transcript.</span><span style="color: #79C0FF">isEmpty</span><span style="color: #C9D1D9"> {</span></span>
<span class="line"><span style="color: #C9D1D9">          </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> transcriptView.</span><span style="color: #79C0FF">text</span><span style="color: #C9D1D9">.</span><span style="color: #79C0FF">isEmpty</span><span style="color: #C9D1D9"> {</span></span>
<span class="line"><span style="color: #C9D1D9">             transcriptView.</span><span style="color: #79C0FF">text</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> transcript</span></span>
<span class="line"><span style="color: #C9D1D9">          } </span><span style="color: #FF7B72">else</span><span style="color: #C9D1D9"> {</span></span>
<span class="line"><span style="color: #C9D1D9">             transcriptView.</span><span style="color: #79C0FF">text</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> transcriptView.</span><span style="color: #79C0FF">text</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot; &quot;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> transcript</span></span>
<span class="line"><span style="color: #C9D1D9">          }</span></span>
<span class="line"><span style="color: #C9D1D9">        }</span></span>
<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">case</span><span style="color: #C9D1D9"> .</span><span style="color: #79C0FF">error</span><span style="color: #C9D1D9">(</span><span style="color: #FF7B72">let</span><span style="color: #C9D1D9"> error)</span><span style="color: #FF7B72">:</span></span>
<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(error </span><span style="color: #FF7B72">??</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;&quot;</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">     </span><span style="color: #FF7B72">default:</span></span>
<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #FF7B72">break</span></span>
<span class="line"><span style="color: #C9D1D9">     }</span></span>
<span class="line"><span style="color: #C9D1D9">  }</span></span>
<span class="line"><span style="color: #C9D1D9">}</span></span></code></pre>
<p>Once you have a <code>DeepgramResponse</code> instance, you will check if it is final, meaning it is ready to add to the <code>transcriptView</code> while handling the empty scenarios. If you now build and run the project (CMD + R), it will open in the simulator, where you will be prompted to give microphone access. Then you can start transcribing!</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1638827585/blog/2022/01/ios-live-transcription/simulator.png" alt="iPhone simulator showing the text hello blog"></p>
<p>The final project code is available at <a href="https://github.com/deepgram-devs/deepgram-live-transcripts-ios">https://github.com/deepgram-devs/deepgram-live-transcripts-ios</a>, and if you have any questions, please feel free to reach out to the Deepgram team on Twitter - <a href="https://twitter.com/DeepgramDevs">@DeepgramDevs</a>.</p>`;
}, "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/ios-live-transcription/index.md");

export { compiledContent, $$Index as default, frontmatter, metadata, rawContent };
