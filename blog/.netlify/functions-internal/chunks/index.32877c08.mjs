import { c as createAstro, a as createComponent, r as renderTemplate, b as renderHead } from '../entry.mjs';
import Slugger from 'github-slugger';
import '@astrojs/netlify/netlify-functions.js';
import 'preact';
import 'preact-render-to-string';
import 'vue';
import 'vue/server-renderer';
import 'html-escaper';
import 'node-html-parser';
import 'axios';
/* empty css                           *//* empty css                           *//* empty css                           */import '@storyblok/js';
/* empty css                           *//* empty css                          */import 'clone-deep';
import 'slugify';
import 'shiki';
/* empty css                           */import 'camelcase';
import '@astrojs/rss';
/* empty css                           */import 'mime';
import 'cookie';
import 'kleur/colors';
import 'string-width';
import 'path-browserify';
import 'path-to-regexp';

const metadata = { "headings": [{ "depth": 2, "slug": "dont-try-to-do-something-that-is-impossible", "text": "Don\u2019t try to do something that is impossible." }, { "depth": 2, "slug": "trying-deep-learning-first", "text": "Trying deep learning first." }, { "depth": 2, "slug": "dont-throw-away-your-standard-stats-and-machine-learning-techniques", "text": "Don\u2019t throw away your standard stats and machine learning techniques." }, { "depth": 2, "slug": "validation-versus-training", "text": "Validation versus training" }, { "depth": 2, "slug": "overfit-your-hyper-parameters", "text": "Overfit your hyper parameters." }, { "depth": 2, "slug": "choosing-the-wrong-learning-rate", "text": "Choosing the wrong learning rate" }, { "depth": 2, "slug": "not-normalizing-your-input-data", "text": "Not normalizing your input data" }, { "depth": 2, "slug": "more-examples-of-common-pitfalls", "text": "More examples of common pitfalls" }, { "depth": 2, "slug": "the-process-side-of-the-house-is-to-me-the-biggest-problem-here", "text": "The process side of the house is to me the biggest problem here." }], "source": `**Scott:** Welcome to the AI Show. Today we're asking the question: What are the top mistakes in deep learning?

**Susan:** We've got huge ones! We make them all the time.

**Scott:** The mistake is the rule in deep learning. You make nine mistakes and one maybe good move.

**Susan:** How are you gonna find new things if you don't make mistakes?

**Scott:** It's frontier tech, right?

**Susan:** There's just so many fun pitfalls you can fall into that everybody's fallen into.

**Scott:** What do you think the big areas are?

**Susan:** I think that there's really two major areas. There's just analyzing the problem; the basics of analyzing whatever problem you're going down and the pitfalls around there. Then there's the model and data. If you don't analyze your problem correctly then it really doesn't matter what you do with the model and data. You've fallen off the tracks. Once you finally got a good understanding you can get down there and fall into brand new pitfalls. Have you fallen into any analysis problems there, Scott?

**Scott:** I'd say there's too many to talk about, but yes. Analyze the problem, make sure your data is good, these are all good traps. Make sure your model is actually a model that we'd actually be able to perform the task you care about and then in the end you actually have to train it too. Training has its own history, as well. Probably I can name some things that I've done that are not good. Some of it just when you're doing deep learning you're doing programming. Programming is hard. Copy pasting, you're reusing old code, you're iterating as you go along and you're using the problems and errors that you're getting to help you along until you finally get water through the pipes.

**Susan:** That's really important: water through the pipes.
![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976785/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/business-equipment-factory-357440.jpg)

**Scott:** But usually once you first get water through the pipes and you get your first result everything is totally wrong anyway, so you're backtracking. Okay, where did that mistake come in?

**Susan:** Comes out more mud than water.

**Scott:** How do you get the pieces to fit together in the model at all? How do you get even a few pieces of data into the right form to be jammed into the model? Then the model finally has to output something and its output doesn't make any sense at all. But biggest thing is trying to learn an impossible task.

**Susan:** I've done that.\r
## Don't try to do something that is impossible.\r

**Susan:** I've a fairly good rule of thumb for that one. If you as a human looking at your training data can't learn the task, then I can pretty well guarantee you that no matter how good of a model builder you are it's probably not gonna be able to learn that task.

**Scott:** Even if you think you can learn it, but if you ask somebody else and they're like, no that's a totally different thing. This is something like summarizing a scientific article and its jammed full of information. People would summarize it in different ways. Maybe that's not an impossible task, but that's a very, very, very hard task.

**Susan:** The big point here is: pick battles that are winnable. Make sure you're doing the steps necessary to figure out that this is a winnable battle. Don't let yourself believe that magic black box that is deep learning will be able to learn anything so long as you throw in enough data at it.\r

**Scott:** There's so many ways that it can go wrong. You don't want the number one reason of just this isn't even possible to take you down. Pick something that maybe a human could do. This is a good rule of thumb. If a human could do it with maybe a second worth of thinking and come out with a reliable answer every time, maybe that's something a machine could do.

**Susan:** The second half of that is truly important. A reliable answer. What does that mean? An objective answer. Something that if you had ten humans and you asked them the same question, you would get probably nine of them at least agreeing. Because if you have five of them saying one thing and five of them saying another thing, even if they're super positive that's the right answer, a machine's gonna have a very hard time figuring out what side of those five to be on.

**Scott:** Yeah ask ten people what five times five is. Okay, maybe you can teach a machine how to do that. We know that for sure. But ask them what the meaning of life is or who won the argument.

**Susan:** Was that dress blue or was that dress ... Well maybe that one a machine could figure out perfectly.

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976785/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/The_Dress_-viral_phenomenon-.png)\r

## Trying deep learning first.\r

**Susan:** Deep learning thinks it's everything.

**Scott:** Pump the brakes here a little bit.

**Susan:** What is the simple approach? Also, you'd be surprised, just the most basic of algorithms can solve some really, really hard problems, or what we think of as hard problems. Just add it all up and find the mean and suddenly you got 90% of your answer in a lot of cases. Don't skip to the most complex possible huge-chain answer that you can right off the bat.

**Scott:** You have to load up a ton of context and tools and the know-how in your brain and time and the iteration cycle on deep learning is so long that it's hard. You could be doing something with just regular stats or traditional tree based machine learning technique or something that would go through the data and you'd have a result in five minutes. That often is better than waiting two days for deep learning. You want to get on that track, like the five minutes.

**Susan:** Even then, just doing a simple least squares fit. I've done that so many times and that is an amazingly great, quick way. Say, hey, this is a tackle-able problem. Also, sometimes the answer is right where you need it to be.\r
## Don't throw away your standard stats and machine learning techniques.\r

**Scott:** Go to those first. If you think this isn't performing as well as it should and:\r
*   I've turned lots of knobs,
*   I have a ton of data,\r
*   I have the computing power,\r
*   I have the time to go through and try to figure this out\r
Maybe try deep learning then. And it's not impossible. What about data?

**Susan:** I'm gonna tell an embarrassing story. I'm going to hope most people have had the same embarrassing story. I started playing around with deep learning probably just for fun. Not deep learning, just machine learning in general, about 10 years ago, 15 years ago. I was playing around with little things and then I hit the stock market like everybody does.

**Scott:** Yeah, first thing. Oh, machine learning, here we come!

**Susan:** I'm gonna predict the stock market! I started building my little model and man, it was accurate.

**Scott:** Super accurate?

**Susan:** It was crazy accurate. I was like, I'm gonna be a millionaire! What did I not do properly?

**Validation versus training.**\r
## Validation versus training\r

**Scott:** What you're trying to do is set up the problem, so I'm gonna give you this data and you have to learn from it, but then I'm gonna hold this other data out, which is true data, again, but the model's never seen it, and now I'm gonna show it to you and see how correct you are. Well, if you don't do that properly, then you really fool yourself.

**Susan:** You can delude yourself so amazingly well. And the great thing about this particular trap is that there's so many variations to this trap. Let's go to the audiobook world and you're trying to do something like speech recognition. You have 500 audiobooks and 18 people reading those audiobooks. You could split your data one of two ways: off of audiobook or off of people.

**Scott:** Well there's 500 audiobooks, so ...

**Susan:** I can guarantee you if you do it off of audiobook you won't be able to use it in the real world. There's 500 audiobooks, but now you suddenly look a lot better than you actually are because you got the same person in your train versus validates.

**Scott:** It learned how that person spoke in the other books and even though it hadn't heard that exact audio, it's heard that voice before and it's learned a lot form that voice and so it can do a good job on it.

**Susan:** It's not just, hey: "I'm gonna randomly select." I have unfortunately learned this lesson on several times in my life. You want to know how the model performs in the wild. Real, wild data. This is a hard thing to just keep track of, especially in deep learning because it takes so long to do your experiments. It's easy to say what if I use this? What if I do this? What if I look over here? If you train just once on one of your validation data sets, you have a big problem. You can't undo that. If you've been training your model for the last three months and you then spend a day training on your validation data set, you might as well throw that validation data set away, put it in your training data set, or start over on the model.

**Susan:** Hopefully you had a checkpoint before you did that.

**Scott:** Or go back. Go back into a checkpoint.

**Susan:** You contaminate that model and you are done. And it can happen so many ways you don't even realize it. You get a new data set you didn't realize was actually derived data set and it has different IDs associated with the thing it was derived from and suddenly, wow, I'm doing really well on this, except for it's not real.

**Scott:** Training a model is not a reversible process. There's no undo button.

**Susan:** It's very incredibly easy to mess up your various sets of data, so treat them wisely. But, you know what's another great way to mess up training versus validate?

**Scott:** What?\r
## Overfit your hyper parameters.\r

**Scott:** Oh yeah, for sure. You're always picking how wide should it be, how deep should it be, how many input this do I need, how many that? What should my learning rate be? And all these different hyper parameters and then you try to use it on some wild data and things aren't working. Why aren't they working?

**Susan:** What happened? I ran 10,000 tests, each one tweaking it to be the best possible result on my validation. Why isn't that matching reality?

**Scott:** You're training yourself a little bit here. It isn't that the model got adjusted so much, it's that you are thinking: "Oh, what if I tune these knobs a little bit?" Now it does really well on your validation data set, but if you give it some real wild data, it doesn't do so well.

**Susan:** It's hard to grasp at, but the first time you really truly run into it and smacks you in the face, you definitely feel it hard. It's like that big gap between validation and reality is huge. There's still even more ways you can mess up your validation set. We could talk about this forever.

**Scott:** There's a good way to combat this, which is you have your large training data set, then you have a validation data set, then you have another validation data set, then another validation data set, then you at least have these backups. Hey, you can test over here, test over here, test over here, get your hyper parameters where you think you need them, and then test them on these other data sets that are maybe they're small too like your validation data set, but at least it's giving you a little cross check.

**Susan:** Do what you can. Also, on that note, grooming these sets as time goes on is important. Just like you're talking about setting aside a secret or whatever you wanna call these additional data sets.

**Scott:** Test validation secret.

**Susan:** There's a lot of different ways to split these up, but if you're getting data over time, make sure that you keep on adding to these sets over time, because data changes over time. The English language changes over time. All it takes is one popular new song and people shift their language a little bit. We keep going back to the language world because that's what we think about a lot, but this is also images. Just think about how cameras have changed, just the quality settings and all that. Your average distribution of colors are probably changing a little bit.

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976786/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/english-evolution_.jpg)

_Here we see how one verse of the Old Testament changed over time. By comparing the Old English (Anglo Saxon) and Middle English (Wycliffe's) versions you can see quite a bit of sound change as well as grammatical change. The King James version is radically different, showing the changes in culture that led to serious change in interpretation. The older two versions reflect a closer translation from the Latin bible._

**Scott:** If you take a picture on an iPhone now versus eight years ago it's gonna look vastly different.

**Susan:** Or the self driving world. If you did nothing but from the 80s, a whole bunch of video from the 80s, cars would've changed a lot, clothes would've changed a lot. Do you think that would all affect how well you can recognize everything out there? All this is talking about how you manage your data and the disconnect between the reality of the data you're training on and the current real world production environment you're gonna be on. That's the big mistake that a lot of people run into. You've been in this little locked in world and then suddenly reality is somehow different. That's because you're training and your validate probably aren't as good as representing the real world as you thought.

**Scott:** Some other good ones are you have to choose your hyper parameters, which is how big is your network, how deep is it, how wide is it? What learning rate are you going to use, et cetera? On the topic of learning rate, if you chose too high of a learning rate, your gradients get huge.\r
\r
## Choosing the wrong learning rate
**Susan:** Blow your weights.

**Scott:** It tries to learn too much too fast and it learns nothing.

**Susan:** There's so much that goes into hand tweaking all of that stuff. One of the first things that you need to do is do a quick exploration. Whatever mechanism that you do for that grid searcher or whatever, try to find those big hyperparameter sweet spots, but then at the same time, you've gotta fight that problem of overfitting your hyper parameters. It's a real challenge and it becomes a real pitfall that you do things like wrong learning rates and then you restart and this and that and the other, then suddenly at the end of it you have a set of hyper parameters that seem to fit and you've gotten into that overfit world. Minimize how often you change it.

**Scott:** Try to get into a good region and then don't change it too much and then rely on your data to do the talking of shaping where the model should go. You could have too low of a learning rate too, where it just takes way too long. We've talked about this before, where you're patience limited in deep learning because things take a while, there's a lot of parameters, it uses a lot of computational power, and if you're training models for days and it's just only getting better very slowly, maybe your learning rate needs to be jacked up again.

**Scott:** Again, that's part of that search.

**Susan:** That gets into one that I've re-learned recently, which is people pay a lot of attention to data, rightfully so. A lot of attention to model structure, rightfully so, but less attention to the optimizer that they use.

**Scott:** I completely agree.

**Susan:** There's just an epic difference out there based off that. It's hard to even begin with how important that selection is you forget about.

**Scott:** Heuristically, it's like, if I know what my error is and I know the errors I've been making in the past, how far should I project where I should go? You can think about this as a human too. If you're walking around in the day and you keep hearing the same wrong thing, over and over, and you're like, maybe I really need to go look into that and adjust it or something. It's weighting all of that. How much of the past should I keep for now and use in the future? How much should I throw away? There's a momentum side to that too. 

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976787/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/conifer-dawn-daylight-167698.jpg)

**Susan:** One way to think of it is this; going back to the classic forest problem. Dense forest and how do I get to low ground? The optimizer's are like how fast you should be walking at any one given time?

**Scott:** If it was all curvy right there, maybe you should do something else. But there are different programs there, basically, or different rules you should follow and that's what picking your optimizer is.

**Susan:** And how it adjusts that rate of walking as you go along. Those are big, huge, massive things there. What about the bottom end of a model? What kind of output errors have you run into here, Scott?

**Scott:** A lot.

**Susan:** I can give you my favorite one. If you got one right off the top, go for it.

**Scott:** Not outputting good results is the biggest. Outputting all zeros when you're loss function is all screwed up or whatever.

**Susan:** My personal favorite is outputting something that's useful in a training set, but not useful in the real world. Forgetting the difference between training and production. When you look at code and when you look at training models and stuff like that, you're going to take this thing, you're gonna wrap it in a loss function, you're going to do all these different things. I remember building my first models, when I actually tried to apply them to a real world problem, I realized that was completely not useful. The training set that I had been training against and the outputs I had been using there were giving me these great numbers of loss.

**Scott:** 99% accuracy.

**Susan:** And accuracy and I was like, oh, and I was tweaking the various setting and all that stuff. Then I disconnected the model from all that apparatus and tried to do something with it.

**Scott:** Yeah, feed it something real.

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976788/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/box-1.jpg)

_[Black box problem](https://en.wikipedia.org/wiki/Black_box)_

**Susan:** Suddenly I had no idea what it was actually doing. It just had this black box of data in, black box of data out, these metrics around it. You've always, always gotta keep in your mind that the whole point is not the training set, the whole point is not the accuracy number, the whole point is not all this apparatus over there, it's when you finally get to production. What is that output supposed to look like and how is it useful?

**Susan:** If you can't take whatever output it's got coming out there and make it useful, then you've gotta address that right away. It's like the water through the pipes that we were talking about there, 'cause water through the pipes on the training side and also water through the pipes on the production side.

**Scott:** Does it get sensible outputs in production-land as well?

**Susan:** Are you really able to take that 10,000 classes and do something with it or should have you stripped it down to those five because that's all you cared about in production?

**Scott:** I think some real nuts and bolts ones could help too. Like normalizing your input data.

**Susan:** Oh, yeah.\r
\r
## Not normalizing your input data
**Scott:** What is normalizing your input data mean? It's if you have an image, for example, and each pixel value goes from zero to 10,000, what scale should you keep it at? Should it be zero to 10,000 or should it be some other number? Typically machine learning algorithms are tuned to work well when you're talking about negative one to one or zero to one, or something like that. You should normalize your data so that zero to 10,000 now is like zero to one, or something like that.

**Susan:** It's staggering what normalization can do for you. You just think about humans. Think about looking around your environment. The first physical step of light going into your body goes through a normalization layer and that is what's your eye doing right now when it's stares into a light versus stares into a dark room?

**Scott:** It's getting smaller and bigger.

**Susan:** It shows you how important that step is that you physically do that. You do that with hearing too. It's a loud environment, what do we do? We put earmuffs on. We're trying to get all the sound normalized.

**Scott:** Or it's loud but it doesn't seem that loud. Hearing is logarithmic. There's tons of energy being pumped into your ear but it's only a little bit louder. One more on hearing. You've got two ears and the ability to swivel your head, when you listen to something, when you really wanna listen to something, you actually swivel your head and that helps your hearing. That physical motion of adjusting the two microphones in your body to be better aligned with [the source makes a big different to understanding](https://www.youtube.com/watch?v=Oai7HUqncAA&t=185). Don't forget that lesson when you're in the machine learning world. Normalize your data, don't forget those initial purely simple algorithmic filters you could apply to data beforehand.

**Scott:** We haven't discussed all that much, but essentially if you're Amazon Alexa or Google Home, or something like that, they usually have seven or eight microphones on them. But humans do pretty good with just two.

**Susan:** Sure, however humans have the ability to turn their heads.

**Scott:** Yeah we can move around. You have these infinite amount of microphones.

**Susan:** Yeah. We've got a lot of great things there.\r
\r
## More examples of common pitfalls\r



**Scott:** You have a deep learning model and in there are tons of parameters. Those parameters are initialized initially to what? What do I mean by initialize? I mean you fill up a bunch of matrices with numbers, what do you fill them with? Zeros? Is that a good answer?

**Susan:** Bad answer.

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976789/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/Wolfsgrube.jpg)

_[Pitfalls](https://en.wikipedia.org/wiki/Trapping_pit) were one method of hunting animals and defeating enemies in war. Now it's just a metaphor._

**Scott:** Very bad answer.

**Susan:** Very bad answer. As soon as you get to two layers everything goes out the window.

**Scott:** Do you fill them with all the same number? One?

**Susan:** Nope, it can't be the same number. Even just pure randomness is not good. We're not gonna get into the math and why, but look up some algorithms for how to fill up your parameters.

**Scott:** You want it round and peaked with some variance.

**Susan:** Based off of size, based off of number parameters.

**Scott:** But random still, but off a distribution.

**Susan:** Randomness is such an amazingly strong tool in the world. It's just crazy and that one example shows that you don't randomize things that literally will not learn.

**Scott:** You can stitch together networks or loss functions or pieces of code that other people have written and you use part of it, but you use it incorrectly. In other words, it should have a soft max that it's run through before it gets put in there, or it shouldn't. Do you strip those pieces off or not? This is an impedance mismatch.

**Susan:** To me this happens mostly going back to the training environment versus the production environment. You see this happen a lot. This is a basic one, luckily you fix it pretty quick just by changing that little piece of code. In training you may or may not need something at the end of your model that you'll strip off for production. Keeping that in mind will help save some debugging steps. Ask yourself: Is this really what I'm gonna use for production or was this here just for training? Forgetting that you've got dropout set wrong, or something like that.

**Scott:** You want it turned off for the evaluation inference probably.

**Susan:** You need to make sure your model's set for the right environment.

**Scott:** Drop out shuts off part of the deep learning brain basically. It's like oh, don't you think you might want that in there?

**Susan:** To get it in there if you've got a bunch of parameters. You can then randomly select a few of them to just ignore basically. The important thing about this is it tries to get the same level of output signal, so when you go to the production environment, you want those neurons to be there. You want those parameters to be useful, but if they were all turned on, suddenly your signal goes up, say, by 25% if you've got 25% drop out on there. You treat the evaluation side different than the train side. Some pretty important things to remember otherwise it's just not as good as it could be.

<iframe src="https://www.youtube.com/embed/KcrAkPNB8jc" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe>

_If you train your model based on speaker (narrator) then you have a narrator detector. You probably don't want that._

**Scott:** One of my favorite things to do though is overfitting. Which is normally a bad thing, but when you're in the water through the pipes stage, where hey, I've finally got my data going, I finally got my model built, I finally got some kind of output and I can verify that it's coming now. Now, take a really small training data set, maybe just one example or maybe 100, or maybe eight or whatever and train the model and make sure that you can very quickly get 100% accurate.

**Susan:** You wanna see that curve of learning.

**Scott:** Your error just drops like a rock because the model now has so many parameters it's able to just focus on getting those correct. All that it's doing at that point is memorizing, but if it can't memorize those eight things, it's probably not gonna do anything else.

**Susan:** That helps you prove that your loss function is working. It helps you prove you have some inputs and outputs there, at least in the right realm. It doesn't prove that you will have a good model structure.

**Scott:** No, you could have an awful model structure, but at least it has some hope.

**Susan:** That's your water through the pipes basic all the plumbing is involved here is working here. There's a ton of common, big, huge pitfalls. What's is the biggest one overall?

**Scott:** For me, I think the biggest one is like deep learning and hey, you can do anything. And it's like, okay cool, not anything, but there are a lot of things that you can do that you couldn't do before. Or it can do it better or something like that.\r
\r
They think, "I'm gonna go to TensorFlow and then I'm gonna solve my problem." If that's your thought process, you gotta back up a little bit and think we need to take baby steps here because you're not going to just go download a model or an example or train it with a couple days of toying around and get a real production thing done. It's just not gonna happen.\r
\r
Even from the engineering standpoint, you're not gonna do that. From the model building standpoint, you're not gonna get there basically unless it's something super simple you could've done with normal stats or machine learning.\r
\r
## The process side of the house is to me the biggest problem here.\r
\r

**Susan:** Did you analyze the problem correctly from the start? This is any engineering problem on the planet.

**Scott:** This takes budgeting the appropriate amount of time and the appropriate amount of time is at least days, and probably weeks, to get a good look at that problem. It might be months.

**Susan:** That might actually be the biggest pitfall in deep learning, is the assumption that it'll only be a day or two.

**Scott:** Yeah. Yeah. A day or two minimum. That happens frequently.

**Susan:** Oh, that's no problem. I'll have something in a week.

**Scott:** Yeah, "Oh weird. It's not doing what I thought it would do. But what if I just do this little trick?" Two days later: "Huh, okay," and then eight days later, "Huh."

**Susan:** It's a big time thing, that's for sure.\r`, "html": '<p><strong>Scott:</strong> Welcome to the AI Show. Today we\u2019re asking the question: What are the top mistakes in deep learning?</p>\n<p><strong>Susan:</strong> We\u2019ve got huge ones! We make them all the time.</p>\n<p><strong>Scott:</strong> The mistake is the rule in deep learning. You make nine mistakes and one maybe good move.</p>\n<p><strong>Susan:</strong> How are you gonna find new things if you don\u2019t make mistakes?</p>\n<p><strong>Scott:</strong> It\u2019s frontier tech, right?</p>\n<p><strong>Susan:</strong> There\u2019s just so many fun pitfalls you can fall into that everybody\u2019s fallen into.</p>\n<p><strong>Scott:</strong> What do you think the big areas are?</p>\n<p><strong>Susan:</strong> I think that there\u2019s really two major areas. There\u2019s just analyzing the problem; the basics of analyzing whatever problem you\u2019re going down and the pitfalls around there. Then there\u2019s the model and data. If you don\u2019t analyze your problem correctly then it really doesn\u2019t matter what you do with the model and data. You\u2019ve fallen off the tracks. Once you finally got a good understanding you can get down there and fall into brand new pitfalls. Have you fallen into any analysis problems there, Scott?</p>\n<p><strong>Scott:</strong> I\u2019d say there\u2019s too many to talk about, but yes. Analyze the problem, make sure your data is good, these are all good traps. Make sure your model is actually a model that we\u2019d actually be able to perform the task you care about and then in the end you actually have to train it too. Training has its own history, as well. Probably I can name some things that I\u2019ve done that are not good. Some of it just when you\u2019re doing deep learning you\u2019re doing programming. Programming is hard. Copy pasting, you\u2019re reusing old code, you\u2019re iterating as you go along and you\u2019re using the problems and errors that you\u2019re getting to help you along until you finally get water through the pipes.</p>\n<p><strong>Susan:</strong> That\u2019s really important: water through the pipes.\n<img src="https://res.cloudinary.com/deepgram/image/upload/v1661976785/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/business-equipment-factory-357440.jpg" alt="Alt"></p>\n<p><strong>Scott:</strong> But usually once you first get water through the pipes and you get your first result everything is totally wrong anyway, so you\u2019re backtracking. Okay, where did that mistake come in?</p>\n<p><strong>Susan:</strong> Comes out more mud than water.</p>\n<p><strong>Scott:</strong> How do you get the pieces to fit together in the model at all? How do you get even a few pieces of data into the right form to be jammed into the model? Then the model finally has to output something and its output doesn\u2019t make any sense at all. But biggest thing is trying to learn an impossible task.</p>\n<p><strong>Susan:</strong> I\u2019ve done that.</p>\n<h2 id="dont-try-to-do-something-that-is-impossible">Don\u2019t try to do something that is impossible.</h2>\n<p><strong>Susan:</strong> I\u2019ve a fairly good rule of thumb for that one. If you as a human looking at your training data can\u2019t learn the task, then I can pretty well guarantee you that no matter how good of a model builder you are it\u2019s probably not gonna be able to learn that task.</p>\n<p><strong>Scott:</strong> Even if you think you can learn it, but if you ask somebody else and they\u2019re like, no that\u2019s a totally different thing. This is something like summarizing a scientific article and its jammed full of information. People would summarize it in different ways. Maybe that\u2019s not an impossible task, but that\u2019s a very, very, very hard task.</p>\n<p><strong>Susan:</strong> The big point here is: pick battles that are winnable. Make sure you\u2019re doing the steps necessary to figure out that this is a winnable battle. Don\u2019t let yourself believe that magic black box that is deep learning will be able to learn anything so long as you throw in enough data at it.</p>\n<p><strong>Scott:</strong> There\u2019s so many ways that it can go wrong. You don\u2019t want the number one reason of just this isn\u2019t even possible to take you down. Pick something that maybe a human could do. This is a good rule of thumb. If a human could do it with maybe a second worth of thinking and come out with a reliable answer every time, maybe that\u2019s something a machine could do.</p>\n<p><strong>Susan:</strong> The second half of that is truly important. A reliable answer. What does that mean? An objective answer. Something that if you had ten humans and you asked them the same question, you would get probably nine of them at least agreeing. Because if you have five of them saying one thing and five of them saying another thing, even if they\u2019re super positive that\u2019s the right answer, a machine\u2019s gonna have a very hard time figuring out what side of those five to be on.</p>\n<p><strong>Scott:</strong> Yeah ask ten people what five times five is. Okay, maybe you can teach a machine how to do that. We know that for sure. But ask them what the meaning of life is or who won the argument.</p>\n<p><strong>Susan:</strong> Was that dress blue or was that dress \u2026 Well maybe that one a machine could figure out perfectly.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976785/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/The_Dress_-viral_phenomenon-.png" alt="Alt"></p>\n<h2 id="trying-deep-learning-first">Trying deep learning first.</h2>\n<p><strong>Susan:</strong> Deep learning thinks it\u2019s everything.</p>\n<p><strong>Scott:</strong> Pump the brakes here a little bit.</p>\n<p><strong>Susan:</strong> What is the simple approach? Also, you\u2019d be surprised, just the most basic of algorithms can solve some really, really hard problems, or what we think of as hard problems. Just add it all up and find the mean and suddenly you got 90% of your answer in a lot of cases. Don\u2019t skip to the most complex possible huge-chain answer that you can right off the bat.</p>\n<p><strong>Scott:</strong> You have to load up a ton of context and tools and the know-how in your brain and time and the iteration cycle on deep learning is so long that it\u2019s hard. You could be doing something with just regular stats or traditional tree based machine learning technique or something that would go through the data and you\u2019d have a result in five minutes. That often is better than waiting two days for deep learning. You want to get on that track, like the five minutes.</p>\n<p><strong>Susan:</strong> Even then, just doing a simple least squares fit. I\u2019ve done that so many times and that is an amazingly great, quick way. Say, hey, this is a tackle-able problem. Also, sometimes the answer is right where you need it to be.</p>\n<h2 id="dont-throw-away-your-standard-stats-and-machine-learning-techniques">Don\u2019t throw away your standard stats and machine learning techniques.</h2>\n<p><strong>Scott:</strong> Go to those first. If you think this isn\u2019t performing as well as it should and:</p>\n<ul>\n<li>I\u2019ve turned lots of knobs,</li>\n<li>I have a ton of data,</li>\n<li>I have the computing power,</li>\n<li>I have the time to go through and try to figure this out\r\nMaybe try deep learning then. And it\u2019s not impossible. What about data?</li>\n</ul>\n<p><strong>Susan:</strong> I\u2019m gonna tell an embarrassing story. I\u2019m going to hope most people have had the same embarrassing story. I started playing around with deep learning probably just for fun. Not deep learning, just machine learning in general, about 10 years ago, 15 years ago. I was playing around with little things and then I hit the stock market like everybody does.</p>\n<p><strong>Scott:</strong> Yeah, first thing. Oh, machine learning, here we come!</p>\n<p><strong>Susan:</strong> I\u2019m gonna predict the stock market! I started building my little model and man, it was accurate.</p>\n<p><strong>Scott:</strong> Super accurate?</p>\n<p><strong>Susan:</strong> It was crazy accurate. I was like, I\u2019m gonna be a millionaire! What did I not do properly?</p>\n<p><strong>Validation versus training.</strong></p>\n<h2 id="validation-versus-training">Validation versus training</h2>\n<p><strong>Scott:</strong> What you\u2019re trying to do is set up the problem, so I\u2019m gonna give you this data and you have to learn from it, but then I\u2019m gonna hold this other data out, which is true data, again, but the model\u2019s never seen it, and now I\u2019m gonna show it to you and see how correct you are. Well, if you don\u2019t do that properly, then you really fool yourself.</p>\n<p><strong>Susan:</strong> You can delude yourself so amazingly well. And the great thing about this particular trap is that there\u2019s so many variations to this trap. Let\u2019s go to the audiobook world and you\u2019re trying to do something like speech recognition. You have 500 audiobooks and 18 people reading those audiobooks. You could split your data one of two ways: off of audiobook or off of people.</p>\n<p><strong>Scott:</strong> Well there\u2019s 500 audiobooks, so \u2026</p>\n<p><strong>Susan:</strong> I can guarantee you if you do it off of audiobook you won\u2019t be able to use it in the real world. There\u2019s 500 audiobooks, but now you suddenly look a lot better than you actually are because you got the same person in your train versus validates.</p>\n<p><strong>Scott:</strong> It learned how that person spoke in the other books and even though it hadn\u2019t heard that exact audio, it\u2019s heard that voice before and it\u2019s learned a lot form that voice and so it can do a good job on it.</p>\n<p><strong>Susan:</strong> It\u2019s not just, hey: \u201CI\u2019m gonna randomly select.\u201D I have unfortunately learned this lesson on several times in my life. You want to know how the model performs in the wild. Real, wild data. This is a hard thing to just keep track of, especially in deep learning because it takes so long to do your experiments. It\u2019s easy to say what if I use this? What if I do this? What if I look over here? If you train just once on one of your validation data sets, you have a big problem. You can\u2019t undo that. If you\u2019ve been training your model for the last three months and you then spend a day training on your validation data set, you might as well throw that validation data set away, put it in your training data set, or start over on the model.</p>\n<p><strong>Susan:</strong> Hopefully you had a checkpoint before you did that.</p>\n<p><strong>Scott:</strong> Or go back. Go back into a checkpoint.</p>\n<p><strong>Susan:</strong> You contaminate that model and you are done. And it can happen so many ways you don\u2019t even realize it. You get a new data set you didn\u2019t realize was actually derived data set and it has different IDs associated with the thing it was derived from and suddenly, wow, I\u2019m doing really well on this, except for it\u2019s not real.</p>\n<p><strong>Scott:</strong> Training a model is not a reversible process. There\u2019s no undo button.</p>\n<p><strong>Susan:</strong> It\u2019s very incredibly easy to mess up your various sets of data, so treat them wisely. But, you know what\u2019s another great way to mess up training versus validate?</p>\n<p><strong>Scott:</strong> What?</p>\n<h2 id="overfit-your-hyper-parameters">Overfit your hyper parameters.</h2>\n<p><strong>Scott:</strong> Oh yeah, for sure. You\u2019re always picking how wide should it be, how deep should it be, how many input this do I need, how many that? What should my learning rate be? And all these different hyper parameters and then you try to use it on some wild data and things aren\u2019t working. Why aren\u2019t they working?</p>\n<p><strong>Susan:</strong> What happened? I ran 10,000 tests, each one tweaking it to be the best possible result on my validation. Why isn\u2019t that matching reality?</p>\n<p><strong>Scott:</strong> You\u2019re training yourself a little bit here. It isn\u2019t that the model got adjusted so much, it\u2019s that you are thinking: \u201COh, what if I tune these knobs a little bit?\u201D Now it does really well on your validation data set, but if you give it some real wild data, it doesn\u2019t do so well.</p>\n<p><strong>Susan:</strong> It\u2019s hard to grasp at, but the first time you really truly run into it and smacks you in the face, you definitely feel it hard. It\u2019s like that big gap between validation and reality is huge. There\u2019s still even more ways you can mess up your validation set. We could talk about this forever.</p>\n<p><strong>Scott:</strong> There\u2019s a good way to combat this, which is you have your large training data set, then you have a validation data set, then you have another validation data set, then another validation data set, then you at least have these backups. Hey, you can test over here, test over here, test over here, get your hyper parameters where you think you need them, and then test them on these other data sets that are maybe they\u2019re small too like your validation data set, but at least it\u2019s giving you a little cross check.</p>\n<p><strong>Susan:</strong> Do what you can. Also, on that note, grooming these sets as time goes on is important. Just like you\u2019re talking about setting aside a secret or whatever you wanna call these additional data sets.</p>\n<p><strong>Scott:</strong> Test validation secret.</p>\n<p><strong>Susan:</strong> There\u2019s a lot of different ways to split these up, but if you\u2019re getting data over time, make sure that you keep on adding to these sets over time, because data changes over time. The English language changes over time. All it takes is one popular new song and people shift their language a little bit. We keep going back to the language world because that\u2019s what we think about a lot, but this is also images. Just think about how cameras have changed, just the quality settings and all that. Your average distribution of colors are probably changing a little bit.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976786/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/english-evolution_.jpg" alt="Alt"></p>\n<p><em>Here we see how one verse of the Old Testament changed over time. By comparing the Old English (Anglo Saxon) and Middle English (Wycliffe\u2019s) versions you can see quite a bit of sound change as well as grammatical change. The King James version is radically different, showing the changes in culture that led to serious change in interpretation. The older two versions reflect a closer translation from the Latin bible.</em></p>\n<p><strong>Scott:</strong> If you take a picture on an iPhone now versus eight years ago it\u2019s gonna look vastly different.</p>\n<p><strong>Susan:</strong> Or the self driving world. If you did nothing but from the 80s, a whole bunch of video from the 80s, cars would\u2019ve changed a lot, clothes would\u2019ve changed a lot. Do you think that would all affect how well you can recognize everything out there? All this is talking about how you manage your data and the disconnect between the reality of the data you\u2019re training on and the current real world production environment you\u2019re gonna be on. That\u2019s the big mistake that a lot of people run into. You\u2019ve been in this little locked in world and then suddenly reality is somehow different. That\u2019s because you\u2019re training and your validate probably aren\u2019t as good as representing the real world as you thought.</p>\n<p><strong>Scott:</strong> Some other good ones are you have to choose your hyper parameters, which is how big is your network, how deep is it, how wide is it? What learning rate are you going to use, et cetera? On the topic of learning rate, if you chose too high of a learning rate, your gradients get huge.</p>\n<h2 id="choosing-the-wrong-learning-rate">Choosing the wrong learning rate</h2>\n<p><strong>Susan:</strong> Blow your weights.</p>\n<p><strong>Scott:</strong> It tries to learn too much too fast and it learns nothing.</p>\n<p><strong>Susan:</strong> There\u2019s so much that goes into hand tweaking all of that stuff. One of the first things that you need to do is do a quick exploration. Whatever mechanism that you do for that grid searcher or whatever, try to find those big hyperparameter sweet spots, but then at the same time, you\u2019ve gotta fight that problem of overfitting your hyper parameters. It\u2019s a real challenge and it becomes a real pitfall that you do things like wrong learning rates and then you restart and this and that and the other, then suddenly at the end of it you have a set of hyper parameters that seem to fit and you\u2019ve gotten into that overfit world. Minimize how often you change it.</p>\n<p><strong>Scott:</strong> Try to get into a good region and then don\u2019t change it too much and then rely on your data to do the talking of shaping where the model should go. You could have too low of a learning rate too, where it just takes way too long. We\u2019ve talked about this before, where you\u2019re patience limited in deep learning because things take a while, there\u2019s a lot of parameters, it uses a lot of computational power, and if you\u2019re training models for days and it\u2019s just only getting better very slowly, maybe your learning rate needs to be jacked up again.</p>\n<p><strong>Scott:</strong> Again, that\u2019s part of that search.</p>\n<p><strong>Susan:</strong> That gets into one that I\u2019ve re-learned recently, which is people pay a lot of attention to data, rightfully so. A lot of attention to model structure, rightfully so, but less attention to the optimizer that they use.</p>\n<p><strong>Scott:</strong> I completely agree.</p>\n<p><strong>Susan:</strong> There\u2019s just an epic difference out there based off that. It\u2019s hard to even begin with how important that selection is you forget about.</p>\n<p><strong>Scott:</strong> Heuristically, it\u2019s like, if I know what my error is and I know the errors I\u2019ve been making in the past, how far should I project where I should go? You can think about this as a human too. If you\u2019re walking around in the day and you keep hearing the same wrong thing, over and over, and you\u2019re like, maybe I really need to go look into that and adjust it or something. It\u2019s weighting all of that. How much of the past should I keep for now and use in the future? How much should I throw away? There\u2019s a momentum side to that too.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976787/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/conifer-dawn-daylight-167698.jpg" alt="Alt"></p>\n<p><strong>Susan:</strong> One way to think of it is this; going back to the classic forest problem. Dense forest and how do I get to low ground? The optimizer\u2019s are like how fast you should be walking at any one given time?</p>\n<p><strong>Scott:</strong> If it was all curvy right there, maybe you should do something else. But there are different programs there, basically, or different rules you should follow and that\u2019s what picking your optimizer is.</p>\n<p><strong>Susan:</strong> And how it adjusts that rate of walking as you go along. Those are big, huge, massive things there. What about the bottom end of a model? What kind of output errors have you run into here, Scott?</p>\n<p><strong>Scott:</strong> A lot.</p>\n<p><strong>Susan:</strong> I can give you my favorite one. If you got one right off the top, go for it.</p>\n<p><strong>Scott:</strong> Not outputting good results is the biggest. Outputting all zeros when you\u2019re loss function is all screwed up or whatever.</p>\n<p><strong>Susan:</strong> My personal favorite is outputting something that\u2019s useful in a training set, but not useful in the real world. Forgetting the difference between training and production. When you look at code and when you look at training models and stuff like that, you\u2019re going to take this thing, you\u2019re gonna wrap it in a loss function, you\u2019re going to do all these different things. I remember building my first models, when I actually tried to apply them to a real world problem, I realized that was completely not useful. The training set that I had been training against and the outputs I had been using there were giving me these great numbers of loss.</p>\n<p><strong>Scott:</strong> 99% accuracy.</p>\n<p><strong>Susan:</strong> And accuracy and I was like, oh, and I was tweaking the various setting and all that stuff. Then I disconnected the model from all that apparatus and tried to do something with it.</p>\n<p><strong>Scott:</strong> Yeah, feed it something real.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976788/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/box-1.jpg" alt="Alt"></p>\n<p><em><a href="https://en.wikipedia.org/wiki/Black_box">Black box problem</a></em></p>\n<p><strong>Susan:</strong> Suddenly I had no idea what it was actually doing. It just had this black box of data in, black box of data out, these metrics around it. You\u2019ve always, always gotta keep in your mind that the whole point is not the training set, the whole point is not the accuracy number, the whole point is not all this apparatus over there, it\u2019s when you finally get to production. What is that output supposed to look like and how is it useful?</p>\n<p><strong>Susan:</strong> If you can\u2019t take whatever output it\u2019s got coming out there and make it useful, then you\u2019ve gotta address that right away. It\u2019s like the water through the pipes that we were talking about there, \u2018cause water through the pipes on the training side and also water through the pipes on the production side.</p>\n<p><strong>Scott:</strong> Does it get sensible outputs in production-land as well?</p>\n<p><strong>Susan:</strong> Are you really able to take that 10,000 classes and do something with it or should have you stripped it down to those five because that\u2019s all you cared about in production?</p>\n<p><strong>Scott:</strong> I think some real nuts and bolts ones could help too. Like normalizing your input data.</p>\n<p><strong>Susan:</strong> Oh, yeah.</p>\n<h2 id="not-normalizing-your-input-data">Not normalizing your input data</h2>\n<p><strong>Scott:</strong> What is normalizing your input data mean? It\u2019s if you have an image, for example, and each pixel value goes from zero to 10,000, what scale should you keep it at? Should it be zero to 10,000 or should it be some other number? Typically machine learning algorithms are tuned to work well when you\u2019re talking about negative one to one or zero to one, or something like that. You should normalize your data so that zero to 10,000 now is like zero to one, or something like that.</p>\n<p><strong>Susan:</strong> It\u2019s staggering what normalization can do for you. You just think about humans. Think about looking around your environment. The first physical step of light going into your body goes through a normalization layer and that is what\u2019s your eye doing right now when it\u2019s stares into a light versus stares into a dark room?</p>\n<p><strong>Scott:</strong> It\u2019s getting smaller and bigger.</p>\n<p><strong>Susan:</strong> It shows you how important that step is that you physically do that. You do that with hearing too. It\u2019s a loud environment, what do we do? We put earmuffs on. We\u2019re trying to get all the sound normalized.</p>\n<p><strong>Scott:</strong> Or it\u2019s loud but it doesn\u2019t seem that loud. Hearing is logarithmic. There\u2019s tons of energy being pumped into your ear but it\u2019s only a little bit louder. One more on hearing. You\u2019ve got two ears and the ability to swivel your head, when you listen to something, when you really wanna listen to something, you actually swivel your head and that helps your hearing. That physical motion of adjusting the two microphones in your body to be better aligned with <a href="https://www.youtube.com/watch?v=Oai7HUqncAA&#x26;t=185">the source makes a big different to understanding</a>. Don\u2019t forget that lesson when you\u2019re in the machine learning world. Normalize your data, don\u2019t forget those initial purely simple algorithmic filters you could apply to data beforehand.</p>\n<p><strong>Scott:</strong> We haven\u2019t discussed all that much, but essentially if you\u2019re Amazon Alexa or Google Home, or something like that, they usually have seven or eight microphones on them. But humans do pretty good with just two.</p>\n<p><strong>Susan:</strong> Sure, however humans have the ability to turn their heads.</p>\n<p><strong>Scott:</strong> Yeah we can move around. You have these infinite amount of microphones.</p>\n<p><strong>Susan:</strong> Yeah. We\u2019ve got a lot of great things there.</p>\n<h2 id="more-examples-of-common-pitfalls">More examples of common pitfalls</h2>\n<p><strong>Scott:</strong> You have a deep learning model and in there are tons of parameters. Those parameters are initialized initially to what? What do I mean by initialize? I mean you fill up a bunch of matrices with numbers, what do you fill them with? Zeros? Is that a good answer?</p>\n<p><strong>Susan:</strong> Bad answer.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976789/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/Wolfsgrube.jpg" alt="Alt"></p>\n<p><em><a href="https://en.wikipedia.org/wiki/Trapping_pit">Pitfalls</a> were one method of hunting animals and defeating enemies in war. Now it\u2019s just a metaphor.</em></p>\n<p><strong>Scott:</strong> Very bad answer.</p>\n<p><strong>Susan:</strong> Very bad answer. As soon as you get to two layers everything goes out the window.</p>\n<p><strong>Scott:</strong> Do you fill them with all the same number? One?</p>\n<p><strong>Susan:</strong> Nope, it can\u2019t be the same number. Even just pure randomness is not good. We\u2019re not gonna get into the math and why, but look up some algorithms for how to fill up your parameters.</p>\n<p><strong>Scott:</strong> You want it round and peaked with some variance.</p>\n<p><strong>Susan:</strong> Based off of size, based off of number parameters.</p>\n<p><strong>Scott:</strong> But random still, but off a distribution.</p>\n<p><strong>Susan:</strong> Randomness is such an amazingly strong tool in the world. It\u2019s just crazy and that one example shows that you don\u2019t randomize things that literally will not learn.</p>\n<p><strong>Scott:</strong> You can stitch together networks or loss functions or pieces of code that other people have written and you use part of it, but you use it incorrectly. In other words, it should have a soft max that it\u2019s run through before it gets put in there, or it shouldn\u2019t. Do you strip those pieces off or not? This is an impedance mismatch.</p>\n<p><strong>Susan:</strong> To me this happens mostly going back to the training environment versus the production environment. You see this happen a lot. This is a basic one, luckily you fix it pretty quick just by changing that little piece of code. In training you may or may not need something at the end of your model that you\u2019ll strip off for production. Keeping that in mind will help save some debugging steps. Ask yourself: Is this really what I\u2019m gonna use for production or was this here just for training? Forgetting that you\u2019ve got dropout set wrong, or something like that.</p>\n<p><strong>Scott:</strong> You want it turned off for the evaluation inference probably.</p>\n<p><strong>Susan:</strong> You need to make sure your model\u2019s set for the right environment.</p>\n<p><strong>Scott:</strong> Drop out shuts off part of the deep learning brain basically. It\u2019s like oh, don\u2019t you think you might want that in there?</p>\n<p><strong>Susan:</strong> To get it in there if you\u2019ve got a bunch of parameters. You can then randomly select a few of them to just ignore basically. The important thing about this is it tries to get the same level of output signal, so when you go to the production environment, you want those neurons to be there. You want those parameters to be useful, but if they were all turned on, suddenly your signal goes up, say, by 25% if you\u2019ve got 25% drop out on there. You treat the evaluation side different than the train side. Some pretty important things to remember otherwise it\u2019s just not as good as it could be.</p>\n<iframe src="https://www.youtube.com/embed/KcrAkPNB8jc" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen" />\n<p><em>If you train your model based on speaker (narrator) then you have a narrator detector. You probably don\u2019t want that.</em></p>\n<p><strong>Scott:</strong> One of my favorite things to do though is overfitting. Which is normally a bad thing, but when you\u2019re in the water through the pipes stage, where hey, I\u2019ve finally got my data going, I finally got my model built, I finally got some kind of output and I can verify that it\u2019s coming now. Now, take a really small training data set, maybe just one example or maybe 100, or maybe eight or whatever and train the model and make sure that you can very quickly get 100% accurate.</p>\n<p><strong>Susan:</strong> You wanna see that curve of learning.</p>\n<p><strong>Scott:</strong> Your error just drops like a rock because the model now has so many parameters it\u2019s able to just focus on getting those correct. All that it\u2019s doing at that point is memorizing, but if it can\u2019t memorize those eight things, it\u2019s probably not gonna do anything else.</p>\n<p><strong>Susan:</strong> That helps you prove that your loss function is working. It helps you prove you have some inputs and outputs there, at least in the right realm. It doesn\u2019t prove that you will have a good model structure.</p>\n<p><strong>Scott:</strong> No, you could have an awful model structure, but at least it has some hope.</p>\n<p><strong>Susan:</strong> That\u2019s your water through the pipes basic all the plumbing is involved here is working here. There\u2019s a ton of common, big, huge pitfalls. What\u2019s is the biggest one overall?</p>\n<p><strong>Scott:</strong> For me, I think the biggest one is like deep learning and hey, you can do anything. And it\u2019s like, okay cool, not anything, but there are a lot of things that you can do that you couldn\u2019t do before. Or it can do it better or something like that.</p>\n<p>They think, \u201CI\u2019m gonna go to TensorFlow and then I\u2019m gonna solve my problem.\u201D If that\u2019s your thought process, you gotta back up a little bit and think we need to take baby steps here because you\u2019re not going to just go download a model or an example or train it with a couple days of toying around and get a real production thing done. It\u2019s just not gonna happen.</p>\n<p>Even from the engineering standpoint, you\u2019re not gonna do that. From the model building standpoint, you\u2019re not gonna get there basically unless it\u2019s something super simple you could\u2019ve done with normal stats or machine learning.</p>\n<h2 id="the-process-side-of-the-house-is-to-me-the-biggest-problem-here">The process side of the house is to me the biggest problem here.</h2>\n<p><strong>Susan:</strong> Did you analyze the problem correctly from the start? This is any engineering problem on the planet.</p>\n<p><strong>Scott:</strong> This takes budgeting the appropriate amount of time and the appropriate amount of time is at least days, and probably weeks, to get a good look at that problem. It might be months.</p>\n<p><strong>Susan:</strong> That might actually be the biggest pitfall in deep learning, is the assumption that it\u2019ll only be a day or two.</p>\n<p><strong>Scott:</strong> Yeah. Yeah. A day or two minimum. That happens frequently.</p>\n<p><strong>Susan:</strong> Oh, that\u2019s no problem. I\u2019ll have something in a week.</p>\n<p><strong>Scott:</strong> Yeah, \u201COh weird. It\u2019s not doing what I thought it would do. But what if I just do this little trick?\u201D Two days later: \u201CHuh, okay,\u201D and then eight days later, \u201CHuh.\u201D</p>\n<p><strong>Susan:</strong> It\u2019s a big time thing, that\u2019s for sure.</p>' };
const frontmatter = { "title": "What Are the Top Mistakes in Deep Learning? \u2014 AI Show", "description": "In this episode of the AI Show, we explore some of the most common deep learning mistakes.", "date": "2018-12-07T00:00:00.000Z", "cover": "https://res.cloudinary.com/deepgram/image/upload/v1662069470/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/placeholder-post-image%402x.jpg", "authors": ["morris-gevirtz"], "category": "ai-and-engineering", "tags": ["deep-learning"], "seo": { "title": "What Are the Top Mistakes in Deep Learning? \u2014 AI Show", "description": "" }, "og": { "image": "https://res.cloudinary.com/deepgram/image/upload/v1662069470/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/placeholder-post-image%402x.jpg" }, "shorturls": { "share": "https://dpgr.am/655d533", "twitter": "https://dpgr.am/1fd794b", "linkedin": "https://dpgr.am/8abb8be", "reddit": "https://dpgr.am/6f2d31f", "facebook": "https://dpgr.am/9c3f4af" }, "astro": { "headings": [{ "depth": 2, "slug": "dont-try-to-do-something-that-is-impossible", "text": "Don\u2019t try to do something that is impossible." }, { "depth": 2, "slug": "trying-deep-learning-first", "text": "Trying deep learning first." }, { "depth": 2, "slug": "dont-throw-away-your-standard-stats-and-machine-learning-techniques", "text": "Don\u2019t throw away your standard stats and machine learning techniques." }, { "depth": 2, "slug": "validation-versus-training", "text": "Validation versus training" }, { "depth": 2, "slug": "overfit-your-hyper-parameters", "text": "Overfit your hyper parameters." }, { "depth": 2, "slug": "choosing-the-wrong-learning-rate", "text": "Choosing the wrong learning rate" }, { "depth": 2, "slug": "not-normalizing-your-input-data", "text": "Not normalizing your input data" }, { "depth": 2, "slug": "more-examples-of-common-pitfalls", "text": "More examples of common pitfalls" }, { "depth": 2, "slug": "the-process-side-of-the-house-is-to-me-the-biggest-problem-here", "text": "The process side of the house is to me the biggest problem here." }], "source": `**Scott:** Welcome to the AI Show. Today we're asking the question: What are the top mistakes in deep learning?

**Susan:** We've got huge ones! We make them all the time.

**Scott:** The mistake is the rule in deep learning. You make nine mistakes and one maybe good move.

**Susan:** How are you gonna find new things if you don't make mistakes?

**Scott:** It's frontier tech, right?

**Susan:** There's just so many fun pitfalls you can fall into that everybody's fallen into.

**Scott:** What do you think the big areas are?

**Susan:** I think that there's really two major areas. There's just analyzing the problem; the basics of analyzing whatever problem you're going down and the pitfalls around there. Then there's the model and data. If you don't analyze your problem correctly then it really doesn't matter what you do with the model and data. You've fallen off the tracks. Once you finally got a good understanding you can get down there and fall into brand new pitfalls. Have you fallen into any analysis problems there, Scott?

**Scott:** I'd say there's too many to talk about, but yes. Analyze the problem, make sure your data is good, these are all good traps. Make sure your model is actually a model that we'd actually be able to perform the task you care about and then in the end you actually have to train it too. Training has its own history, as well. Probably I can name some things that I've done that are not good. Some of it just when you're doing deep learning you're doing programming. Programming is hard. Copy pasting, you're reusing old code, you're iterating as you go along and you're using the problems and errors that you're getting to help you along until you finally get water through the pipes.

**Susan:** That's really important: water through the pipes.
![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976785/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/business-equipment-factory-357440.jpg)

**Scott:** But usually once you first get water through the pipes and you get your first result everything is totally wrong anyway, so you're backtracking. Okay, where did that mistake come in?

**Susan:** Comes out more mud than water.

**Scott:** How do you get the pieces to fit together in the model at all? How do you get even a few pieces of data into the right form to be jammed into the model? Then the model finally has to output something and its output doesn't make any sense at all. But biggest thing is trying to learn an impossible task.

**Susan:** I've done that.\r
## Don't try to do something that is impossible.\r

**Susan:** I've a fairly good rule of thumb for that one. If you as a human looking at your training data can't learn the task, then I can pretty well guarantee you that no matter how good of a model builder you are it's probably not gonna be able to learn that task.

**Scott:** Even if you think you can learn it, but if you ask somebody else and they're like, no that's a totally different thing. This is something like summarizing a scientific article and its jammed full of information. People would summarize it in different ways. Maybe that's not an impossible task, but that's a very, very, very hard task.

**Susan:** The big point here is: pick battles that are winnable. Make sure you're doing the steps necessary to figure out that this is a winnable battle. Don't let yourself believe that magic black box that is deep learning will be able to learn anything so long as you throw in enough data at it.\r

**Scott:** There's so many ways that it can go wrong. You don't want the number one reason of just this isn't even possible to take you down. Pick something that maybe a human could do. This is a good rule of thumb. If a human could do it with maybe a second worth of thinking and come out with a reliable answer every time, maybe that's something a machine could do.

**Susan:** The second half of that is truly important. A reliable answer. What does that mean? An objective answer. Something that if you had ten humans and you asked them the same question, you would get probably nine of them at least agreeing. Because if you have five of them saying one thing and five of them saying another thing, even if they're super positive that's the right answer, a machine's gonna have a very hard time figuring out what side of those five to be on.

**Scott:** Yeah ask ten people what five times five is. Okay, maybe you can teach a machine how to do that. We know that for sure. But ask them what the meaning of life is or who won the argument.

**Susan:** Was that dress blue or was that dress ... Well maybe that one a machine could figure out perfectly.

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976785/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/The_Dress_-viral_phenomenon-.png)\r

## Trying deep learning first.\r

**Susan:** Deep learning thinks it's everything.

**Scott:** Pump the brakes here a little bit.

**Susan:** What is the simple approach? Also, you'd be surprised, just the most basic of algorithms can solve some really, really hard problems, or what we think of as hard problems. Just add it all up and find the mean and suddenly you got 90% of your answer in a lot of cases. Don't skip to the most complex possible huge-chain answer that you can right off the bat.

**Scott:** You have to load up a ton of context and tools and the know-how in your brain and time and the iteration cycle on deep learning is so long that it's hard. You could be doing something with just regular stats or traditional tree based machine learning technique or something that would go through the data and you'd have a result in five minutes. That often is better than waiting two days for deep learning. You want to get on that track, like the five minutes.

**Susan:** Even then, just doing a simple least squares fit. I've done that so many times and that is an amazingly great, quick way. Say, hey, this is a tackle-able problem. Also, sometimes the answer is right where you need it to be.\r
## Don't throw away your standard stats and machine learning techniques.\r

**Scott:** Go to those first. If you think this isn't performing as well as it should and:\r
*   I've turned lots of knobs,
*   I have a ton of data,\r
*   I have the computing power,\r
*   I have the time to go through and try to figure this out\r
Maybe try deep learning then. And it's not impossible. What about data?

**Susan:** I'm gonna tell an embarrassing story. I'm going to hope most people have had the same embarrassing story. I started playing around with deep learning probably just for fun. Not deep learning, just machine learning in general, about 10 years ago, 15 years ago. I was playing around with little things and then I hit the stock market like everybody does.

**Scott:** Yeah, first thing. Oh, machine learning, here we come!

**Susan:** I'm gonna predict the stock market! I started building my little model and man, it was accurate.

**Scott:** Super accurate?

**Susan:** It was crazy accurate. I was like, I'm gonna be a millionaire! What did I not do properly?

**Validation versus training.**\r
## Validation versus training\r

**Scott:** What you're trying to do is set up the problem, so I'm gonna give you this data and you have to learn from it, but then I'm gonna hold this other data out, which is true data, again, but the model's never seen it, and now I'm gonna show it to you and see how correct you are. Well, if you don't do that properly, then you really fool yourself.

**Susan:** You can delude yourself so amazingly well. And the great thing about this particular trap is that there's so many variations to this trap. Let's go to the audiobook world and you're trying to do something like speech recognition. You have 500 audiobooks and 18 people reading those audiobooks. You could split your data one of two ways: off of audiobook or off of people.

**Scott:** Well there's 500 audiobooks, so ...

**Susan:** I can guarantee you if you do it off of audiobook you won't be able to use it in the real world. There's 500 audiobooks, but now you suddenly look a lot better than you actually are because you got the same person in your train versus validates.

**Scott:** It learned how that person spoke in the other books and even though it hadn't heard that exact audio, it's heard that voice before and it's learned a lot form that voice and so it can do a good job on it.

**Susan:** It's not just, hey: "I'm gonna randomly select." I have unfortunately learned this lesson on several times in my life. You want to know how the model performs in the wild. Real, wild data. This is a hard thing to just keep track of, especially in deep learning because it takes so long to do your experiments. It's easy to say what if I use this? What if I do this? What if I look over here? If you train just once on one of your validation data sets, you have a big problem. You can't undo that. If you've been training your model for the last three months and you then spend a day training on your validation data set, you might as well throw that validation data set away, put it in your training data set, or start over on the model.

**Susan:** Hopefully you had a checkpoint before you did that.

**Scott:** Or go back. Go back into a checkpoint.

**Susan:** You contaminate that model and you are done. And it can happen so many ways you don't even realize it. You get a new data set you didn't realize was actually derived data set and it has different IDs associated with the thing it was derived from and suddenly, wow, I'm doing really well on this, except for it's not real.

**Scott:** Training a model is not a reversible process. There's no undo button.

**Susan:** It's very incredibly easy to mess up your various sets of data, so treat them wisely. But, you know what's another great way to mess up training versus validate?

**Scott:** What?\r
## Overfit your hyper parameters.\r

**Scott:** Oh yeah, for sure. You're always picking how wide should it be, how deep should it be, how many input this do I need, how many that? What should my learning rate be? And all these different hyper parameters and then you try to use it on some wild data and things aren't working. Why aren't they working?

**Susan:** What happened? I ran 10,000 tests, each one tweaking it to be the best possible result on my validation. Why isn't that matching reality?

**Scott:** You're training yourself a little bit here. It isn't that the model got adjusted so much, it's that you are thinking: "Oh, what if I tune these knobs a little bit?" Now it does really well on your validation data set, but if you give it some real wild data, it doesn't do so well.

**Susan:** It's hard to grasp at, but the first time you really truly run into it and smacks you in the face, you definitely feel it hard. It's like that big gap between validation and reality is huge. There's still even more ways you can mess up your validation set. We could talk about this forever.

**Scott:** There's a good way to combat this, which is you have your large training data set, then you have a validation data set, then you have another validation data set, then another validation data set, then you at least have these backups. Hey, you can test over here, test over here, test over here, get your hyper parameters where you think you need them, and then test them on these other data sets that are maybe they're small too like your validation data set, but at least it's giving you a little cross check.

**Susan:** Do what you can. Also, on that note, grooming these sets as time goes on is important. Just like you're talking about setting aside a secret or whatever you wanna call these additional data sets.

**Scott:** Test validation secret.

**Susan:** There's a lot of different ways to split these up, but if you're getting data over time, make sure that you keep on adding to these sets over time, because data changes over time. The English language changes over time. All it takes is one popular new song and people shift their language a little bit. We keep going back to the language world because that's what we think about a lot, but this is also images. Just think about how cameras have changed, just the quality settings and all that. Your average distribution of colors are probably changing a little bit.

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976786/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/english-evolution_.jpg)

_Here we see how one verse of the Old Testament changed over time. By comparing the Old English (Anglo Saxon) and Middle English (Wycliffe's) versions you can see quite a bit of sound change as well as grammatical change. The King James version is radically different, showing the changes in culture that led to serious change in interpretation. The older two versions reflect a closer translation from the Latin bible._

**Scott:** If you take a picture on an iPhone now versus eight years ago it's gonna look vastly different.

**Susan:** Or the self driving world. If you did nothing but from the 80s, a whole bunch of video from the 80s, cars would've changed a lot, clothes would've changed a lot. Do you think that would all affect how well you can recognize everything out there? All this is talking about how you manage your data and the disconnect between the reality of the data you're training on and the current real world production environment you're gonna be on. That's the big mistake that a lot of people run into. You've been in this little locked in world and then suddenly reality is somehow different. That's because you're training and your validate probably aren't as good as representing the real world as you thought.

**Scott:** Some other good ones are you have to choose your hyper parameters, which is how big is your network, how deep is it, how wide is it? What learning rate are you going to use, et cetera? On the topic of learning rate, if you chose too high of a learning rate, your gradients get huge.\r
\r
## Choosing the wrong learning rate
**Susan:** Blow your weights.

**Scott:** It tries to learn too much too fast and it learns nothing.

**Susan:** There's so much that goes into hand tweaking all of that stuff. One of the first things that you need to do is do a quick exploration. Whatever mechanism that you do for that grid searcher or whatever, try to find those big hyperparameter sweet spots, but then at the same time, you've gotta fight that problem of overfitting your hyper parameters. It's a real challenge and it becomes a real pitfall that you do things like wrong learning rates and then you restart and this and that and the other, then suddenly at the end of it you have a set of hyper parameters that seem to fit and you've gotten into that overfit world. Minimize how often you change it.

**Scott:** Try to get into a good region and then don't change it too much and then rely on your data to do the talking of shaping where the model should go. You could have too low of a learning rate too, where it just takes way too long. We've talked about this before, where you're patience limited in deep learning because things take a while, there's a lot of parameters, it uses a lot of computational power, and if you're training models for days and it's just only getting better very slowly, maybe your learning rate needs to be jacked up again.

**Scott:** Again, that's part of that search.

**Susan:** That gets into one that I've re-learned recently, which is people pay a lot of attention to data, rightfully so. A lot of attention to model structure, rightfully so, but less attention to the optimizer that they use.

**Scott:** I completely agree.

**Susan:** There's just an epic difference out there based off that. It's hard to even begin with how important that selection is you forget about.

**Scott:** Heuristically, it's like, if I know what my error is and I know the errors I've been making in the past, how far should I project where I should go? You can think about this as a human too. If you're walking around in the day and you keep hearing the same wrong thing, over and over, and you're like, maybe I really need to go look into that and adjust it or something. It's weighting all of that. How much of the past should I keep for now and use in the future? How much should I throw away? There's a momentum side to that too. 

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976787/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/conifer-dawn-daylight-167698.jpg)

**Susan:** One way to think of it is this; going back to the classic forest problem. Dense forest and how do I get to low ground? The optimizer's are like how fast you should be walking at any one given time?

**Scott:** If it was all curvy right there, maybe you should do something else. But there are different programs there, basically, or different rules you should follow and that's what picking your optimizer is.

**Susan:** And how it adjusts that rate of walking as you go along. Those are big, huge, massive things there. What about the bottom end of a model? What kind of output errors have you run into here, Scott?

**Scott:** A lot.

**Susan:** I can give you my favorite one. If you got one right off the top, go for it.

**Scott:** Not outputting good results is the biggest. Outputting all zeros when you're loss function is all screwed up or whatever.

**Susan:** My personal favorite is outputting something that's useful in a training set, but not useful in the real world. Forgetting the difference between training and production. When you look at code and when you look at training models and stuff like that, you're going to take this thing, you're gonna wrap it in a loss function, you're going to do all these different things. I remember building my first models, when I actually tried to apply them to a real world problem, I realized that was completely not useful. The training set that I had been training against and the outputs I had been using there were giving me these great numbers of loss.

**Scott:** 99% accuracy.

**Susan:** And accuracy and I was like, oh, and I was tweaking the various setting and all that stuff. Then I disconnected the model from all that apparatus and tried to do something with it.

**Scott:** Yeah, feed it something real.

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976788/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/box-1.jpg)

_[Black box problem](https://en.wikipedia.org/wiki/Black_box)_

**Susan:** Suddenly I had no idea what it was actually doing. It just had this black box of data in, black box of data out, these metrics around it. You've always, always gotta keep in your mind that the whole point is not the training set, the whole point is not the accuracy number, the whole point is not all this apparatus over there, it's when you finally get to production. What is that output supposed to look like and how is it useful?

**Susan:** If you can't take whatever output it's got coming out there and make it useful, then you've gotta address that right away. It's like the water through the pipes that we were talking about there, 'cause water through the pipes on the training side and also water through the pipes on the production side.

**Scott:** Does it get sensible outputs in production-land as well?

**Susan:** Are you really able to take that 10,000 classes and do something with it or should have you stripped it down to those five because that's all you cared about in production?

**Scott:** I think some real nuts and bolts ones could help too. Like normalizing your input data.

**Susan:** Oh, yeah.\r
\r
## Not normalizing your input data
**Scott:** What is normalizing your input data mean? It's if you have an image, for example, and each pixel value goes from zero to 10,000, what scale should you keep it at? Should it be zero to 10,000 or should it be some other number? Typically machine learning algorithms are tuned to work well when you're talking about negative one to one or zero to one, or something like that. You should normalize your data so that zero to 10,000 now is like zero to one, or something like that.

**Susan:** It's staggering what normalization can do for you. You just think about humans. Think about looking around your environment. The first physical step of light going into your body goes through a normalization layer and that is what's your eye doing right now when it's stares into a light versus stares into a dark room?

**Scott:** It's getting smaller and bigger.

**Susan:** It shows you how important that step is that you physically do that. You do that with hearing too. It's a loud environment, what do we do? We put earmuffs on. We're trying to get all the sound normalized.

**Scott:** Or it's loud but it doesn't seem that loud. Hearing is logarithmic. There's tons of energy being pumped into your ear but it's only a little bit louder. One more on hearing. You've got two ears and the ability to swivel your head, when you listen to something, when you really wanna listen to something, you actually swivel your head and that helps your hearing. That physical motion of adjusting the two microphones in your body to be better aligned with [the source makes a big different to understanding](https://www.youtube.com/watch?v=Oai7HUqncAA&t=185). Don't forget that lesson when you're in the machine learning world. Normalize your data, don't forget those initial purely simple algorithmic filters you could apply to data beforehand.

**Scott:** We haven't discussed all that much, but essentially if you're Amazon Alexa or Google Home, or something like that, they usually have seven or eight microphones on them. But humans do pretty good with just two.

**Susan:** Sure, however humans have the ability to turn their heads.

**Scott:** Yeah we can move around. You have these infinite amount of microphones.

**Susan:** Yeah. We've got a lot of great things there.\r
\r
## More examples of common pitfalls\r



**Scott:** You have a deep learning model and in there are tons of parameters. Those parameters are initialized initially to what? What do I mean by initialize? I mean you fill up a bunch of matrices with numbers, what do you fill them with? Zeros? Is that a good answer?

**Susan:** Bad answer.

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976789/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/Wolfsgrube.jpg)

_[Pitfalls](https://en.wikipedia.org/wiki/Trapping_pit) were one method of hunting animals and defeating enemies in war. Now it's just a metaphor._

**Scott:** Very bad answer.

**Susan:** Very bad answer. As soon as you get to two layers everything goes out the window.

**Scott:** Do you fill them with all the same number? One?

**Susan:** Nope, it can't be the same number. Even just pure randomness is not good. We're not gonna get into the math and why, but look up some algorithms for how to fill up your parameters.

**Scott:** You want it round and peaked with some variance.

**Susan:** Based off of size, based off of number parameters.

**Scott:** But random still, but off a distribution.

**Susan:** Randomness is such an amazingly strong tool in the world. It's just crazy and that one example shows that you don't randomize things that literally will not learn.

**Scott:** You can stitch together networks or loss functions or pieces of code that other people have written and you use part of it, but you use it incorrectly. In other words, it should have a soft max that it's run through before it gets put in there, or it shouldn't. Do you strip those pieces off or not? This is an impedance mismatch.

**Susan:** To me this happens mostly going back to the training environment versus the production environment. You see this happen a lot. This is a basic one, luckily you fix it pretty quick just by changing that little piece of code. In training you may or may not need something at the end of your model that you'll strip off for production. Keeping that in mind will help save some debugging steps. Ask yourself: Is this really what I'm gonna use for production or was this here just for training? Forgetting that you've got dropout set wrong, or something like that.

**Scott:** You want it turned off for the evaluation inference probably.

**Susan:** You need to make sure your model's set for the right environment.

**Scott:** Drop out shuts off part of the deep learning brain basically. It's like oh, don't you think you might want that in there?

**Susan:** To get it in there if you've got a bunch of parameters. You can then randomly select a few of them to just ignore basically. The important thing about this is it tries to get the same level of output signal, so when you go to the production environment, you want those neurons to be there. You want those parameters to be useful, but if they were all turned on, suddenly your signal goes up, say, by 25% if you've got 25% drop out on there. You treat the evaluation side different than the train side. Some pretty important things to remember otherwise it's just not as good as it could be.

<iframe src="https://www.youtube.com/embed/KcrAkPNB8jc" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe>

_If you train your model based on speaker (narrator) then you have a narrator detector. You probably don't want that._

**Scott:** One of my favorite things to do though is overfitting. Which is normally a bad thing, but when you're in the water through the pipes stage, where hey, I've finally got my data going, I finally got my model built, I finally got some kind of output and I can verify that it's coming now. Now, take a really small training data set, maybe just one example or maybe 100, or maybe eight or whatever and train the model and make sure that you can very quickly get 100% accurate.

**Susan:** You wanna see that curve of learning.

**Scott:** Your error just drops like a rock because the model now has so many parameters it's able to just focus on getting those correct. All that it's doing at that point is memorizing, but if it can't memorize those eight things, it's probably not gonna do anything else.

**Susan:** That helps you prove that your loss function is working. It helps you prove you have some inputs and outputs there, at least in the right realm. It doesn't prove that you will have a good model structure.

**Scott:** No, you could have an awful model structure, but at least it has some hope.

**Susan:** That's your water through the pipes basic all the plumbing is involved here is working here. There's a ton of common, big, huge pitfalls. What's is the biggest one overall?

**Scott:** For me, I think the biggest one is like deep learning and hey, you can do anything. And it's like, okay cool, not anything, but there are a lot of things that you can do that you couldn't do before. Or it can do it better or something like that.\r
\r
They think, "I'm gonna go to TensorFlow and then I'm gonna solve my problem." If that's your thought process, you gotta back up a little bit and think we need to take baby steps here because you're not going to just go download a model or an example or train it with a couple days of toying around and get a real production thing done. It's just not gonna happen.\r
\r
Even from the engineering standpoint, you're not gonna do that. From the model building standpoint, you're not gonna get there basically unless it's something super simple you could've done with normal stats or machine learning.\r
\r
## The process side of the house is to me the biggest problem here.\r
\r

**Susan:** Did you analyze the problem correctly from the start? This is any engineering problem on the planet.

**Scott:** This takes budgeting the appropriate amount of time and the appropriate amount of time is at least days, and probably weeks, to get a good look at that problem. It might be months.

**Susan:** That might actually be the biggest pitfall in deep learning, is the assumption that it'll only be a day or two.

**Scott:** Yeah. Yeah. A day or two minimum. That happens frequently.

**Susan:** Oh, that's no problem. I'll have something in a week.

**Scott:** Yeah, "Oh weird. It's not doing what I thought it would do. But what if I just do this little trick?" Two days later: "Huh, okay," and then eight days later, "Huh."

**Susan:** It's a big time thing, that's for sure.\r`, "html": '<p><strong>Scott:</strong> Welcome to the AI Show. Today we\u2019re asking the question: What are the top mistakes in deep learning?</p>\n<p><strong>Susan:</strong> We\u2019ve got huge ones! We make them all the time.</p>\n<p><strong>Scott:</strong> The mistake is the rule in deep learning. You make nine mistakes and one maybe good move.</p>\n<p><strong>Susan:</strong> How are you gonna find new things if you don\u2019t make mistakes?</p>\n<p><strong>Scott:</strong> It\u2019s frontier tech, right?</p>\n<p><strong>Susan:</strong> There\u2019s just so many fun pitfalls you can fall into that everybody\u2019s fallen into.</p>\n<p><strong>Scott:</strong> What do you think the big areas are?</p>\n<p><strong>Susan:</strong> I think that there\u2019s really two major areas. There\u2019s just analyzing the problem; the basics of analyzing whatever problem you\u2019re going down and the pitfalls around there. Then there\u2019s the model and data. If you don\u2019t analyze your problem correctly then it really doesn\u2019t matter what you do with the model and data. You\u2019ve fallen off the tracks. Once you finally got a good understanding you can get down there and fall into brand new pitfalls. Have you fallen into any analysis problems there, Scott?</p>\n<p><strong>Scott:</strong> I\u2019d say there\u2019s too many to talk about, but yes. Analyze the problem, make sure your data is good, these are all good traps. Make sure your model is actually a model that we\u2019d actually be able to perform the task you care about and then in the end you actually have to train it too. Training has its own history, as well. Probably I can name some things that I\u2019ve done that are not good. Some of it just when you\u2019re doing deep learning you\u2019re doing programming. Programming is hard. Copy pasting, you\u2019re reusing old code, you\u2019re iterating as you go along and you\u2019re using the problems and errors that you\u2019re getting to help you along until you finally get water through the pipes.</p>\n<p><strong>Susan:</strong> That\u2019s really important: water through the pipes.\n<img src="https://res.cloudinary.com/deepgram/image/upload/v1661976785/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/business-equipment-factory-357440.jpg" alt="Alt"></p>\n<p><strong>Scott:</strong> But usually once you first get water through the pipes and you get your first result everything is totally wrong anyway, so you\u2019re backtracking. Okay, where did that mistake come in?</p>\n<p><strong>Susan:</strong> Comes out more mud than water.</p>\n<p><strong>Scott:</strong> How do you get the pieces to fit together in the model at all? How do you get even a few pieces of data into the right form to be jammed into the model? Then the model finally has to output something and its output doesn\u2019t make any sense at all. But biggest thing is trying to learn an impossible task.</p>\n<p><strong>Susan:</strong> I\u2019ve done that.</p>\n<h2 id="dont-try-to-do-something-that-is-impossible">Don\u2019t try to do something that is impossible.</h2>\n<p><strong>Susan:</strong> I\u2019ve a fairly good rule of thumb for that one. If you as a human looking at your training data can\u2019t learn the task, then I can pretty well guarantee you that no matter how good of a model builder you are it\u2019s probably not gonna be able to learn that task.</p>\n<p><strong>Scott:</strong> Even if you think you can learn it, but if you ask somebody else and they\u2019re like, no that\u2019s a totally different thing. This is something like summarizing a scientific article and its jammed full of information. People would summarize it in different ways. Maybe that\u2019s not an impossible task, but that\u2019s a very, very, very hard task.</p>\n<p><strong>Susan:</strong> The big point here is: pick battles that are winnable. Make sure you\u2019re doing the steps necessary to figure out that this is a winnable battle. Don\u2019t let yourself believe that magic black box that is deep learning will be able to learn anything so long as you throw in enough data at it.</p>\n<p><strong>Scott:</strong> There\u2019s so many ways that it can go wrong. You don\u2019t want the number one reason of just this isn\u2019t even possible to take you down. Pick something that maybe a human could do. This is a good rule of thumb. If a human could do it with maybe a second worth of thinking and come out with a reliable answer every time, maybe that\u2019s something a machine could do.</p>\n<p><strong>Susan:</strong> The second half of that is truly important. A reliable answer. What does that mean? An objective answer. Something that if you had ten humans and you asked them the same question, you would get probably nine of them at least agreeing. Because if you have five of them saying one thing and five of them saying another thing, even if they\u2019re super positive that\u2019s the right answer, a machine\u2019s gonna have a very hard time figuring out what side of those five to be on.</p>\n<p><strong>Scott:</strong> Yeah ask ten people what five times five is. Okay, maybe you can teach a machine how to do that. We know that for sure. But ask them what the meaning of life is or who won the argument.</p>\n<p><strong>Susan:</strong> Was that dress blue or was that dress \u2026 Well maybe that one a machine could figure out perfectly.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976785/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/The_Dress_-viral_phenomenon-.png" alt="Alt"></p>\n<h2 id="trying-deep-learning-first">Trying deep learning first.</h2>\n<p><strong>Susan:</strong> Deep learning thinks it\u2019s everything.</p>\n<p><strong>Scott:</strong> Pump the brakes here a little bit.</p>\n<p><strong>Susan:</strong> What is the simple approach? Also, you\u2019d be surprised, just the most basic of algorithms can solve some really, really hard problems, or what we think of as hard problems. Just add it all up and find the mean and suddenly you got 90% of your answer in a lot of cases. Don\u2019t skip to the most complex possible huge-chain answer that you can right off the bat.</p>\n<p><strong>Scott:</strong> You have to load up a ton of context and tools and the know-how in your brain and time and the iteration cycle on deep learning is so long that it\u2019s hard. You could be doing something with just regular stats or traditional tree based machine learning technique or something that would go through the data and you\u2019d have a result in five minutes. That often is better than waiting two days for deep learning. You want to get on that track, like the five minutes.</p>\n<p><strong>Susan:</strong> Even then, just doing a simple least squares fit. I\u2019ve done that so many times and that is an amazingly great, quick way. Say, hey, this is a tackle-able problem. Also, sometimes the answer is right where you need it to be.</p>\n<h2 id="dont-throw-away-your-standard-stats-and-machine-learning-techniques">Don\u2019t throw away your standard stats and machine learning techniques.</h2>\n<p><strong>Scott:</strong> Go to those first. If you think this isn\u2019t performing as well as it should and:</p>\n<ul>\n<li>I\u2019ve turned lots of knobs,</li>\n<li>I have a ton of data,</li>\n<li>I have the computing power,</li>\n<li>I have the time to go through and try to figure this out\r\nMaybe try deep learning then. And it\u2019s not impossible. What about data?</li>\n</ul>\n<p><strong>Susan:</strong> I\u2019m gonna tell an embarrassing story. I\u2019m going to hope most people have had the same embarrassing story. I started playing around with deep learning probably just for fun. Not deep learning, just machine learning in general, about 10 years ago, 15 years ago. I was playing around with little things and then I hit the stock market like everybody does.</p>\n<p><strong>Scott:</strong> Yeah, first thing. Oh, machine learning, here we come!</p>\n<p><strong>Susan:</strong> I\u2019m gonna predict the stock market! I started building my little model and man, it was accurate.</p>\n<p><strong>Scott:</strong> Super accurate?</p>\n<p><strong>Susan:</strong> It was crazy accurate. I was like, I\u2019m gonna be a millionaire! What did I not do properly?</p>\n<p><strong>Validation versus training.</strong></p>\n<h2 id="validation-versus-training">Validation versus training</h2>\n<p><strong>Scott:</strong> What you\u2019re trying to do is set up the problem, so I\u2019m gonna give you this data and you have to learn from it, but then I\u2019m gonna hold this other data out, which is true data, again, but the model\u2019s never seen it, and now I\u2019m gonna show it to you and see how correct you are. Well, if you don\u2019t do that properly, then you really fool yourself.</p>\n<p><strong>Susan:</strong> You can delude yourself so amazingly well. And the great thing about this particular trap is that there\u2019s so many variations to this trap. Let\u2019s go to the audiobook world and you\u2019re trying to do something like speech recognition. You have 500 audiobooks and 18 people reading those audiobooks. You could split your data one of two ways: off of audiobook or off of people.</p>\n<p><strong>Scott:</strong> Well there\u2019s 500 audiobooks, so \u2026</p>\n<p><strong>Susan:</strong> I can guarantee you if you do it off of audiobook you won\u2019t be able to use it in the real world. There\u2019s 500 audiobooks, but now you suddenly look a lot better than you actually are because you got the same person in your train versus validates.</p>\n<p><strong>Scott:</strong> It learned how that person spoke in the other books and even though it hadn\u2019t heard that exact audio, it\u2019s heard that voice before and it\u2019s learned a lot form that voice and so it can do a good job on it.</p>\n<p><strong>Susan:</strong> It\u2019s not just, hey: \u201CI\u2019m gonna randomly select.\u201D I have unfortunately learned this lesson on several times in my life. You want to know how the model performs in the wild. Real, wild data. This is a hard thing to just keep track of, especially in deep learning because it takes so long to do your experiments. It\u2019s easy to say what if I use this? What if I do this? What if I look over here? If you train just once on one of your validation data sets, you have a big problem. You can\u2019t undo that. If you\u2019ve been training your model for the last three months and you then spend a day training on your validation data set, you might as well throw that validation data set away, put it in your training data set, or start over on the model.</p>\n<p><strong>Susan:</strong> Hopefully you had a checkpoint before you did that.</p>\n<p><strong>Scott:</strong> Or go back. Go back into a checkpoint.</p>\n<p><strong>Susan:</strong> You contaminate that model and you are done. And it can happen so many ways you don\u2019t even realize it. You get a new data set you didn\u2019t realize was actually derived data set and it has different IDs associated with the thing it was derived from and suddenly, wow, I\u2019m doing really well on this, except for it\u2019s not real.</p>\n<p><strong>Scott:</strong> Training a model is not a reversible process. There\u2019s no undo button.</p>\n<p><strong>Susan:</strong> It\u2019s very incredibly easy to mess up your various sets of data, so treat them wisely. But, you know what\u2019s another great way to mess up training versus validate?</p>\n<p><strong>Scott:</strong> What?</p>\n<h2 id="overfit-your-hyper-parameters">Overfit your hyper parameters.</h2>\n<p><strong>Scott:</strong> Oh yeah, for sure. You\u2019re always picking how wide should it be, how deep should it be, how many input this do I need, how many that? What should my learning rate be? And all these different hyper parameters and then you try to use it on some wild data and things aren\u2019t working. Why aren\u2019t they working?</p>\n<p><strong>Susan:</strong> What happened? I ran 10,000 tests, each one tweaking it to be the best possible result on my validation. Why isn\u2019t that matching reality?</p>\n<p><strong>Scott:</strong> You\u2019re training yourself a little bit here. It isn\u2019t that the model got adjusted so much, it\u2019s that you are thinking: \u201COh, what if I tune these knobs a little bit?\u201D Now it does really well on your validation data set, but if you give it some real wild data, it doesn\u2019t do so well.</p>\n<p><strong>Susan:</strong> It\u2019s hard to grasp at, but the first time you really truly run into it and smacks you in the face, you definitely feel it hard. It\u2019s like that big gap between validation and reality is huge. There\u2019s still even more ways you can mess up your validation set. We could talk about this forever.</p>\n<p><strong>Scott:</strong> There\u2019s a good way to combat this, which is you have your large training data set, then you have a validation data set, then you have another validation data set, then another validation data set, then you at least have these backups. Hey, you can test over here, test over here, test over here, get your hyper parameters where you think you need them, and then test them on these other data sets that are maybe they\u2019re small too like your validation data set, but at least it\u2019s giving you a little cross check.</p>\n<p><strong>Susan:</strong> Do what you can. Also, on that note, grooming these sets as time goes on is important. Just like you\u2019re talking about setting aside a secret or whatever you wanna call these additional data sets.</p>\n<p><strong>Scott:</strong> Test validation secret.</p>\n<p><strong>Susan:</strong> There\u2019s a lot of different ways to split these up, but if you\u2019re getting data over time, make sure that you keep on adding to these sets over time, because data changes over time. The English language changes over time. All it takes is one popular new song and people shift their language a little bit. We keep going back to the language world because that\u2019s what we think about a lot, but this is also images. Just think about how cameras have changed, just the quality settings and all that. Your average distribution of colors are probably changing a little bit.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976786/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/english-evolution_.jpg" alt="Alt"></p>\n<p><em>Here we see how one verse of the Old Testament changed over time. By comparing the Old English (Anglo Saxon) and Middle English (Wycliffe\u2019s) versions you can see quite a bit of sound change as well as grammatical change. The King James version is radically different, showing the changes in culture that led to serious change in interpretation. The older two versions reflect a closer translation from the Latin bible.</em></p>\n<p><strong>Scott:</strong> If you take a picture on an iPhone now versus eight years ago it\u2019s gonna look vastly different.</p>\n<p><strong>Susan:</strong> Or the self driving world. If you did nothing but from the 80s, a whole bunch of video from the 80s, cars would\u2019ve changed a lot, clothes would\u2019ve changed a lot. Do you think that would all affect how well you can recognize everything out there? All this is talking about how you manage your data and the disconnect between the reality of the data you\u2019re training on and the current real world production environment you\u2019re gonna be on. That\u2019s the big mistake that a lot of people run into. You\u2019ve been in this little locked in world and then suddenly reality is somehow different. That\u2019s because you\u2019re training and your validate probably aren\u2019t as good as representing the real world as you thought.</p>\n<p><strong>Scott:</strong> Some other good ones are you have to choose your hyper parameters, which is how big is your network, how deep is it, how wide is it? What learning rate are you going to use, et cetera? On the topic of learning rate, if you chose too high of a learning rate, your gradients get huge.</p>\n<h2 id="choosing-the-wrong-learning-rate">Choosing the wrong learning rate</h2>\n<p><strong>Susan:</strong> Blow your weights.</p>\n<p><strong>Scott:</strong> It tries to learn too much too fast and it learns nothing.</p>\n<p><strong>Susan:</strong> There\u2019s so much that goes into hand tweaking all of that stuff. One of the first things that you need to do is do a quick exploration. Whatever mechanism that you do for that grid searcher or whatever, try to find those big hyperparameter sweet spots, but then at the same time, you\u2019ve gotta fight that problem of overfitting your hyper parameters. It\u2019s a real challenge and it becomes a real pitfall that you do things like wrong learning rates and then you restart and this and that and the other, then suddenly at the end of it you have a set of hyper parameters that seem to fit and you\u2019ve gotten into that overfit world. Minimize how often you change it.</p>\n<p><strong>Scott:</strong> Try to get into a good region and then don\u2019t change it too much and then rely on your data to do the talking of shaping where the model should go. You could have too low of a learning rate too, where it just takes way too long. We\u2019ve talked about this before, where you\u2019re patience limited in deep learning because things take a while, there\u2019s a lot of parameters, it uses a lot of computational power, and if you\u2019re training models for days and it\u2019s just only getting better very slowly, maybe your learning rate needs to be jacked up again.</p>\n<p><strong>Scott:</strong> Again, that\u2019s part of that search.</p>\n<p><strong>Susan:</strong> That gets into one that I\u2019ve re-learned recently, which is people pay a lot of attention to data, rightfully so. A lot of attention to model structure, rightfully so, but less attention to the optimizer that they use.</p>\n<p><strong>Scott:</strong> I completely agree.</p>\n<p><strong>Susan:</strong> There\u2019s just an epic difference out there based off that. It\u2019s hard to even begin with how important that selection is you forget about.</p>\n<p><strong>Scott:</strong> Heuristically, it\u2019s like, if I know what my error is and I know the errors I\u2019ve been making in the past, how far should I project where I should go? You can think about this as a human too. If you\u2019re walking around in the day and you keep hearing the same wrong thing, over and over, and you\u2019re like, maybe I really need to go look into that and adjust it or something. It\u2019s weighting all of that. How much of the past should I keep for now and use in the future? How much should I throw away? There\u2019s a momentum side to that too.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976787/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/conifer-dawn-daylight-167698.jpg" alt="Alt"></p>\n<p><strong>Susan:</strong> One way to think of it is this; going back to the classic forest problem. Dense forest and how do I get to low ground? The optimizer\u2019s are like how fast you should be walking at any one given time?</p>\n<p><strong>Scott:</strong> If it was all curvy right there, maybe you should do something else. But there are different programs there, basically, or different rules you should follow and that\u2019s what picking your optimizer is.</p>\n<p><strong>Susan:</strong> And how it adjusts that rate of walking as you go along. Those are big, huge, massive things there. What about the bottom end of a model? What kind of output errors have you run into here, Scott?</p>\n<p><strong>Scott:</strong> A lot.</p>\n<p><strong>Susan:</strong> I can give you my favorite one. If you got one right off the top, go for it.</p>\n<p><strong>Scott:</strong> Not outputting good results is the biggest. Outputting all zeros when you\u2019re loss function is all screwed up or whatever.</p>\n<p><strong>Susan:</strong> My personal favorite is outputting something that\u2019s useful in a training set, but not useful in the real world. Forgetting the difference between training and production. When you look at code and when you look at training models and stuff like that, you\u2019re going to take this thing, you\u2019re gonna wrap it in a loss function, you\u2019re going to do all these different things. I remember building my first models, when I actually tried to apply them to a real world problem, I realized that was completely not useful. The training set that I had been training against and the outputs I had been using there were giving me these great numbers of loss.</p>\n<p><strong>Scott:</strong> 99% accuracy.</p>\n<p><strong>Susan:</strong> And accuracy and I was like, oh, and I was tweaking the various setting and all that stuff. Then I disconnected the model from all that apparatus and tried to do something with it.</p>\n<p><strong>Scott:</strong> Yeah, feed it something real.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976788/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/box-1.jpg" alt="Alt"></p>\n<p><em><a href="https://en.wikipedia.org/wiki/Black_box">Black box problem</a></em></p>\n<p><strong>Susan:</strong> Suddenly I had no idea what it was actually doing. It just had this black box of data in, black box of data out, these metrics around it. You\u2019ve always, always gotta keep in your mind that the whole point is not the training set, the whole point is not the accuracy number, the whole point is not all this apparatus over there, it\u2019s when you finally get to production. What is that output supposed to look like and how is it useful?</p>\n<p><strong>Susan:</strong> If you can\u2019t take whatever output it\u2019s got coming out there and make it useful, then you\u2019ve gotta address that right away. It\u2019s like the water through the pipes that we were talking about there, \u2018cause water through the pipes on the training side and also water through the pipes on the production side.</p>\n<p><strong>Scott:</strong> Does it get sensible outputs in production-land as well?</p>\n<p><strong>Susan:</strong> Are you really able to take that 10,000 classes and do something with it or should have you stripped it down to those five because that\u2019s all you cared about in production?</p>\n<p><strong>Scott:</strong> I think some real nuts and bolts ones could help too. Like normalizing your input data.</p>\n<p><strong>Susan:</strong> Oh, yeah.</p>\n<h2 id="not-normalizing-your-input-data">Not normalizing your input data</h2>\n<p><strong>Scott:</strong> What is normalizing your input data mean? It\u2019s if you have an image, for example, and each pixel value goes from zero to 10,000, what scale should you keep it at? Should it be zero to 10,000 or should it be some other number? Typically machine learning algorithms are tuned to work well when you\u2019re talking about negative one to one or zero to one, or something like that. You should normalize your data so that zero to 10,000 now is like zero to one, or something like that.</p>\n<p><strong>Susan:</strong> It\u2019s staggering what normalization can do for you. You just think about humans. Think about looking around your environment. The first physical step of light going into your body goes through a normalization layer and that is what\u2019s your eye doing right now when it\u2019s stares into a light versus stares into a dark room?</p>\n<p><strong>Scott:</strong> It\u2019s getting smaller and bigger.</p>\n<p><strong>Susan:</strong> It shows you how important that step is that you physically do that. You do that with hearing too. It\u2019s a loud environment, what do we do? We put earmuffs on. We\u2019re trying to get all the sound normalized.</p>\n<p><strong>Scott:</strong> Or it\u2019s loud but it doesn\u2019t seem that loud. Hearing is logarithmic. There\u2019s tons of energy being pumped into your ear but it\u2019s only a little bit louder. One more on hearing. You\u2019ve got two ears and the ability to swivel your head, when you listen to something, when you really wanna listen to something, you actually swivel your head and that helps your hearing. That physical motion of adjusting the two microphones in your body to be better aligned with <a href="https://www.youtube.com/watch?v=Oai7HUqncAA&#x26;t=185">the source makes a big different to understanding</a>. Don\u2019t forget that lesson when you\u2019re in the machine learning world. Normalize your data, don\u2019t forget those initial purely simple algorithmic filters you could apply to data beforehand.</p>\n<p><strong>Scott:</strong> We haven\u2019t discussed all that much, but essentially if you\u2019re Amazon Alexa or Google Home, or something like that, they usually have seven or eight microphones on them. But humans do pretty good with just two.</p>\n<p><strong>Susan:</strong> Sure, however humans have the ability to turn their heads.</p>\n<p><strong>Scott:</strong> Yeah we can move around. You have these infinite amount of microphones.</p>\n<p><strong>Susan:</strong> Yeah. We\u2019ve got a lot of great things there.</p>\n<h2 id="more-examples-of-common-pitfalls">More examples of common pitfalls</h2>\n<p><strong>Scott:</strong> You have a deep learning model and in there are tons of parameters. Those parameters are initialized initially to what? What do I mean by initialize? I mean you fill up a bunch of matrices with numbers, what do you fill them with? Zeros? Is that a good answer?</p>\n<p><strong>Susan:</strong> Bad answer.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976789/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/Wolfsgrube.jpg" alt="Alt"></p>\n<p><em><a href="https://en.wikipedia.org/wiki/Trapping_pit">Pitfalls</a> were one method of hunting animals and defeating enemies in war. Now it\u2019s just a metaphor.</em></p>\n<p><strong>Scott:</strong> Very bad answer.</p>\n<p><strong>Susan:</strong> Very bad answer. As soon as you get to two layers everything goes out the window.</p>\n<p><strong>Scott:</strong> Do you fill them with all the same number? One?</p>\n<p><strong>Susan:</strong> Nope, it can\u2019t be the same number. Even just pure randomness is not good. We\u2019re not gonna get into the math and why, but look up some algorithms for how to fill up your parameters.</p>\n<p><strong>Scott:</strong> You want it round and peaked with some variance.</p>\n<p><strong>Susan:</strong> Based off of size, based off of number parameters.</p>\n<p><strong>Scott:</strong> But random still, but off a distribution.</p>\n<p><strong>Susan:</strong> Randomness is such an amazingly strong tool in the world. It\u2019s just crazy and that one example shows that you don\u2019t randomize things that literally will not learn.</p>\n<p><strong>Scott:</strong> You can stitch together networks or loss functions or pieces of code that other people have written and you use part of it, but you use it incorrectly. In other words, it should have a soft max that it\u2019s run through before it gets put in there, or it shouldn\u2019t. Do you strip those pieces off or not? This is an impedance mismatch.</p>\n<p><strong>Susan:</strong> To me this happens mostly going back to the training environment versus the production environment. You see this happen a lot. This is a basic one, luckily you fix it pretty quick just by changing that little piece of code. In training you may or may not need something at the end of your model that you\u2019ll strip off for production. Keeping that in mind will help save some debugging steps. Ask yourself: Is this really what I\u2019m gonna use for production or was this here just for training? Forgetting that you\u2019ve got dropout set wrong, or something like that.</p>\n<p><strong>Scott:</strong> You want it turned off for the evaluation inference probably.</p>\n<p><strong>Susan:</strong> You need to make sure your model\u2019s set for the right environment.</p>\n<p><strong>Scott:</strong> Drop out shuts off part of the deep learning brain basically. It\u2019s like oh, don\u2019t you think you might want that in there?</p>\n<p><strong>Susan:</strong> To get it in there if you\u2019ve got a bunch of parameters. You can then randomly select a few of them to just ignore basically. The important thing about this is it tries to get the same level of output signal, so when you go to the production environment, you want those neurons to be there. You want those parameters to be useful, but if they were all turned on, suddenly your signal goes up, say, by 25% if you\u2019ve got 25% drop out on there. You treat the evaluation side different than the train side. Some pretty important things to remember otherwise it\u2019s just not as good as it could be.</p>\n<iframe src="https://www.youtube.com/embed/KcrAkPNB8jc" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen" />\n<p><em>If you train your model based on speaker (narrator) then you have a narrator detector. You probably don\u2019t want that.</em></p>\n<p><strong>Scott:</strong> One of my favorite things to do though is overfitting. Which is normally a bad thing, but when you\u2019re in the water through the pipes stage, where hey, I\u2019ve finally got my data going, I finally got my model built, I finally got some kind of output and I can verify that it\u2019s coming now. Now, take a really small training data set, maybe just one example or maybe 100, or maybe eight or whatever and train the model and make sure that you can very quickly get 100% accurate.</p>\n<p><strong>Susan:</strong> You wanna see that curve of learning.</p>\n<p><strong>Scott:</strong> Your error just drops like a rock because the model now has so many parameters it\u2019s able to just focus on getting those correct. All that it\u2019s doing at that point is memorizing, but if it can\u2019t memorize those eight things, it\u2019s probably not gonna do anything else.</p>\n<p><strong>Susan:</strong> That helps you prove that your loss function is working. It helps you prove you have some inputs and outputs there, at least in the right realm. It doesn\u2019t prove that you will have a good model structure.</p>\n<p><strong>Scott:</strong> No, you could have an awful model structure, but at least it has some hope.</p>\n<p><strong>Susan:</strong> That\u2019s your water through the pipes basic all the plumbing is involved here is working here. There\u2019s a ton of common, big, huge pitfalls. What\u2019s is the biggest one overall?</p>\n<p><strong>Scott:</strong> For me, I think the biggest one is like deep learning and hey, you can do anything. And it\u2019s like, okay cool, not anything, but there are a lot of things that you can do that you couldn\u2019t do before. Or it can do it better or something like that.</p>\n<p>They think, \u201CI\u2019m gonna go to TensorFlow and then I\u2019m gonna solve my problem.\u201D If that\u2019s your thought process, you gotta back up a little bit and think we need to take baby steps here because you\u2019re not going to just go download a model or an example or train it with a couple days of toying around and get a real production thing done. It\u2019s just not gonna happen.</p>\n<p>Even from the engineering standpoint, you\u2019re not gonna do that. From the model building standpoint, you\u2019re not gonna get there basically unless it\u2019s something super simple you could\u2019ve done with normal stats or machine learning.</p>\n<h2 id="the-process-side-of-the-house-is-to-me-the-biggest-problem-here">The process side of the house is to me the biggest problem here.</h2>\n<p><strong>Susan:</strong> Did you analyze the problem correctly from the start? This is any engineering problem on the planet.</p>\n<p><strong>Scott:</strong> This takes budgeting the appropriate amount of time and the appropriate amount of time is at least days, and probably weeks, to get a good look at that problem. It might be months.</p>\n<p><strong>Susan:</strong> That might actually be the biggest pitfall in deep learning, is the assumption that it\u2019ll only be a day or two.</p>\n<p><strong>Scott:</strong> Yeah. Yeah. A day or two minimum. That happens frequently.</p>\n<p><strong>Susan:</strong> Oh, that\u2019s no problem. I\u2019ll have something in a week.</p>\n<p><strong>Scott:</strong> Yeah, \u201COh weird. It\u2019s not doing what I thought it would do. But what if I just do this little trick?\u201D Two days later: \u201CHuh, okay,\u201D and then eight days later, \u201CHuh.\u201D</p>\n<p><strong>Susan:</strong> It\u2019s a big time thing, that\u2019s for sure.</p>' }, "file": "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-are-the-top-mistakes-in-deep-learning-ai-show/index.md" };
function rawContent() {
  return `**Scott:** Welcome to the AI Show. Today we're asking the question: What are the top mistakes in deep learning?

**Susan:** We've got huge ones! We make them all the time.

**Scott:** The mistake is the rule in deep learning. You make nine mistakes and one maybe good move.

**Susan:** How are you gonna find new things if you don't make mistakes?

**Scott:** It's frontier tech, right?

**Susan:** There's just so many fun pitfalls you can fall into that everybody's fallen into.

**Scott:** What do you think the big areas are?

**Susan:** I think that there's really two major areas. There's just analyzing the problem; the basics of analyzing whatever problem you're going down and the pitfalls around there. Then there's the model and data. If you don't analyze your problem correctly then it really doesn't matter what you do with the model and data. You've fallen off the tracks. Once you finally got a good understanding you can get down there and fall into brand new pitfalls. Have you fallen into any analysis problems there, Scott?

**Scott:** I'd say there's too many to talk about, but yes. Analyze the problem, make sure your data is good, these are all good traps. Make sure your model is actually a model that we'd actually be able to perform the task you care about and then in the end you actually have to train it too. Training has its own history, as well. Probably I can name some things that I've done that are not good. Some of it just when you're doing deep learning you're doing programming. Programming is hard. Copy pasting, you're reusing old code, you're iterating as you go along and you're using the problems and errors that you're getting to help you along until you finally get water through the pipes.

**Susan:** That's really important: water through the pipes.
![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976785/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/business-equipment-factory-357440.jpg)

**Scott:** But usually once you first get water through the pipes and you get your first result everything is totally wrong anyway, so you're backtracking. Okay, where did that mistake come in?

**Susan:** Comes out more mud than water.

**Scott:** How do you get the pieces to fit together in the model at all? How do you get even a few pieces of data into the right form to be jammed into the model? Then the model finally has to output something and its output doesn't make any sense at all. But biggest thing is trying to learn an impossible task.

**Susan:** I've done that.\r
## Don't try to do something that is impossible.\r

**Susan:** I've a fairly good rule of thumb for that one. If you as a human looking at your training data can't learn the task, then I can pretty well guarantee you that no matter how good of a model builder you are it's probably not gonna be able to learn that task.

**Scott:** Even if you think you can learn it, but if you ask somebody else and they're like, no that's a totally different thing. This is something like summarizing a scientific article and its jammed full of information. People would summarize it in different ways. Maybe that's not an impossible task, but that's a very, very, very hard task.

**Susan:** The big point here is: pick battles that are winnable. Make sure you're doing the steps necessary to figure out that this is a winnable battle. Don't let yourself believe that magic black box that is deep learning will be able to learn anything so long as you throw in enough data at it.\r

**Scott:** There's so many ways that it can go wrong. You don't want the number one reason of just this isn't even possible to take you down. Pick something that maybe a human could do. This is a good rule of thumb. If a human could do it with maybe a second worth of thinking and come out with a reliable answer every time, maybe that's something a machine could do.

**Susan:** The second half of that is truly important. A reliable answer. What does that mean? An objective answer. Something that if you had ten humans and you asked them the same question, you would get probably nine of them at least agreeing. Because if you have five of them saying one thing and five of them saying another thing, even if they're super positive that's the right answer, a machine's gonna have a very hard time figuring out what side of those five to be on.

**Scott:** Yeah ask ten people what five times five is. Okay, maybe you can teach a machine how to do that. We know that for sure. But ask them what the meaning of life is or who won the argument.

**Susan:** Was that dress blue or was that dress ... Well maybe that one a machine could figure out perfectly.

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976785/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/The_Dress_-viral_phenomenon-.png)\r

## Trying deep learning first.\r

**Susan:** Deep learning thinks it's everything.

**Scott:** Pump the brakes here a little bit.

**Susan:** What is the simple approach? Also, you'd be surprised, just the most basic of algorithms can solve some really, really hard problems, or what we think of as hard problems. Just add it all up and find the mean and suddenly you got 90% of your answer in a lot of cases. Don't skip to the most complex possible huge-chain answer that you can right off the bat.

**Scott:** You have to load up a ton of context and tools and the know-how in your brain and time and the iteration cycle on deep learning is so long that it's hard. You could be doing something with just regular stats or traditional tree based machine learning technique or something that would go through the data and you'd have a result in five minutes. That often is better than waiting two days for deep learning. You want to get on that track, like the five minutes.

**Susan:** Even then, just doing a simple least squares fit. I've done that so many times and that is an amazingly great, quick way. Say, hey, this is a tackle-able problem. Also, sometimes the answer is right where you need it to be.\r
## Don't throw away your standard stats and machine learning techniques.\r

**Scott:** Go to those first. If you think this isn't performing as well as it should and:\r
*   I've turned lots of knobs,
*   I have a ton of data,\r
*   I have the computing power,\r
*   I have the time to go through and try to figure this out\r
Maybe try deep learning then. And it's not impossible. What about data?

**Susan:** I'm gonna tell an embarrassing story. I'm going to hope most people have had the same embarrassing story. I started playing around with deep learning probably just for fun. Not deep learning, just machine learning in general, about 10 years ago, 15 years ago. I was playing around with little things and then I hit the stock market like everybody does.

**Scott:** Yeah, first thing. Oh, machine learning, here we come!

**Susan:** I'm gonna predict the stock market! I started building my little model and man, it was accurate.

**Scott:** Super accurate?

**Susan:** It was crazy accurate. I was like, I'm gonna be a millionaire! What did I not do properly?

**Validation versus training.**\r
## Validation versus training\r

**Scott:** What you're trying to do is set up the problem, so I'm gonna give you this data and you have to learn from it, but then I'm gonna hold this other data out, which is true data, again, but the model's never seen it, and now I'm gonna show it to you and see how correct you are. Well, if you don't do that properly, then you really fool yourself.

**Susan:** You can delude yourself so amazingly well. And the great thing about this particular trap is that there's so many variations to this trap. Let's go to the audiobook world and you're trying to do something like speech recognition. You have 500 audiobooks and 18 people reading those audiobooks. You could split your data one of two ways: off of audiobook or off of people.

**Scott:** Well there's 500 audiobooks, so ...

**Susan:** I can guarantee you if you do it off of audiobook you won't be able to use it in the real world. There's 500 audiobooks, but now you suddenly look a lot better than you actually are because you got the same person in your train versus validates.

**Scott:** It learned how that person spoke in the other books and even though it hadn't heard that exact audio, it's heard that voice before and it's learned a lot form that voice and so it can do a good job on it.

**Susan:** It's not just, hey: "I'm gonna randomly select." I have unfortunately learned this lesson on several times in my life. You want to know how the model performs in the wild. Real, wild data. This is a hard thing to just keep track of, especially in deep learning because it takes so long to do your experiments. It's easy to say what if I use this? What if I do this? What if I look over here? If you train just once on one of your validation data sets, you have a big problem. You can't undo that. If you've been training your model for the last three months and you then spend a day training on your validation data set, you might as well throw that validation data set away, put it in your training data set, or start over on the model.

**Susan:** Hopefully you had a checkpoint before you did that.

**Scott:** Or go back. Go back into a checkpoint.

**Susan:** You contaminate that model and you are done. And it can happen so many ways you don't even realize it. You get a new data set you didn't realize was actually derived data set and it has different IDs associated with the thing it was derived from and suddenly, wow, I'm doing really well on this, except for it's not real.

**Scott:** Training a model is not a reversible process. There's no undo button.

**Susan:** It's very incredibly easy to mess up your various sets of data, so treat them wisely. But, you know what's another great way to mess up training versus validate?

**Scott:** What?\r
## Overfit your hyper parameters.\r

**Scott:** Oh yeah, for sure. You're always picking how wide should it be, how deep should it be, how many input this do I need, how many that? What should my learning rate be? And all these different hyper parameters and then you try to use it on some wild data and things aren't working. Why aren't they working?

**Susan:** What happened? I ran 10,000 tests, each one tweaking it to be the best possible result on my validation. Why isn't that matching reality?

**Scott:** You're training yourself a little bit here. It isn't that the model got adjusted so much, it's that you are thinking: "Oh, what if I tune these knobs a little bit?" Now it does really well on your validation data set, but if you give it some real wild data, it doesn't do so well.

**Susan:** It's hard to grasp at, but the first time you really truly run into it and smacks you in the face, you definitely feel it hard. It's like that big gap between validation and reality is huge. There's still even more ways you can mess up your validation set. We could talk about this forever.

**Scott:** There's a good way to combat this, which is you have your large training data set, then you have a validation data set, then you have another validation data set, then another validation data set, then you at least have these backups. Hey, you can test over here, test over here, test over here, get your hyper parameters where you think you need them, and then test them on these other data sets that are maybe they're small too like your validation data set, but at least it's giving you a little cross check.

**Susan:** Do what you can. Also, on that note, grooming these sets as time goes on is important. Just like you're talking about setting aside a secret or whatever you wanna call these additional data sets.

**Scott:** Test validation secret.

**Susan:** There's a lot of different ways to split these up, but if you're getting data over time, make sure that you keep on adding to these sets over time, because data changes over time. The English language changes over time. All it takes is one popular new song and people shift their language a little bit. We keep going back to the language world because that's what we think about a lot, but this is also images. Just think about how cameras have changed, just the quality settings and all that. Your average distribution of colors are probably changing a little bit.

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976786/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/english-evolution_.jpg)

_Here we see how one verse of the Old Testament changed over time. By comparing the Old English (Anglo Saxon) and Middle English (Wycliffe's) versions you can see quite a bit of sound change as well as grammatical change. The King James version is radically different, showing the changes in culture that led to serious change in interpretation. The older two versions reflect a closer translation from the Latin bible._

**Scott:** If you take a picture on an iPhone now versus eight years ago it's gonna look vastly different.

**Susan:** Or the self driving world. If you did nothing but from the 80s, a whole bunch of video from the 80s, cars would've changed a lot, clothes would've changed a lot. Do you think that would all affect how well you can recognize everything out there? All this is talking about how you manage your data and the disconnect between the reality of the data you're training on and the current real world production environment you're gonna be on. That's the big mistake that a lot of people run into. You've been in this little locked in world and then suddenly reality is somehow different. That's because you're training and your validate probably aren't as good as representing the real world as you thought.

**Scott:** Some other good ones are you have to choose your hyper parameters, which is how big is your network, how deep is it, how wide is it? What learning rate are you going to use, et cetera? On the topic of learning rate, if you chose too high of a learning rate, your gradients get huge.\r
\r
## Choosing the wrong learning rate
**Susan:** Blow your weights.

**Scott:** It tries to learn too much too fast and it learns nothing.

**Susan:** There's so much that goes into hand tweaking all of that stuff. One of the first things that you need to do is do a quick exploration. Whatever mechanism that you do for that grid searcher or whatever, try to find those big hyperparameter sweet spots, but then at the same time, you've gotta fight that problem of overfitting your hyper parameters. It's a real challenge and it becomes a real pitfall that you do things like wrong learning rates and then you restart and this and that and the other, then suddenly at the end of it you have a set of hyper parameters that seem to fit and you've gotten into that overfit world. Minimize how often you change it.

**Scott:** Try to get into a good region and then don't change it too much and then rely on your data to do the talking of shaping where the model should go. You could have too low of a learning rate too, where it just takes way too long. We've talked about this before, where you're patience limited in deep learning because things take a while, there's a lot of parameters, it uses a lot of computational power, and if you're training models for days and it's just only getting better very slowly, maybe your learning rate needs to be jacked up again.

**Scott:** Again, that's part of that search.

**Susan:** That gets into one that I've re-learned recently, which is people pay a lot of attention to data, rightfully so. A lot of attention to model structure, rightfully so, but less attention to the optimizer that they use.

**Scott:** I completely agree.

**Susan:** There's just an epic difference out there based off that. It's hard to even begin with how important that selection is you forget about.

**Scott:** Heuristically, it's like, if I know what my error is and I know the errors I've been making in the past, how far should I project where I should go? You can think about this as a human too. If you're walking around in the day and you keep hearing the same wrong thing, over and over, and you're like, maybe I really need to go look into that and adjust it or something. It's weighting all of that. How much of the past should I keep for now and use in the future? How much should I throw away? There's a momentum side to that too. 

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976787/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/conifer-dawn-daylight-167698.jpg)

**Susan:** One way to think of it is this; going back to the classic forest problem. Dense forest and how do I get to low ground? The optimizer's are like how fast you should be walking at any one given time?

**Scott:** If it was all curvy right there, maybe you should do something else. But there are different programs there, basically, or different rules you should follow and that's what picking your optimizer is.

**Susan:** And how it adjusts that rate of walking as you go along. Those are big, huge, massive things there. What about the bottom end of a model? What kind of output errors have you run into here, Scott?

**Scott:** A lot.

**Susan:** I can give you my favorite one. If you got one right off the top, go for it.

**Scott:** Not outputting good results is the biggest. Outputting all zeros when you're loss function is all screwed up or whatever.

**Susan:** My personal favorite is outputting something that's useful in a training set, but not useful in the real world. Forgetting the difference between training and production. When you look at code and when you look at training models and stuff like that, you're going to take this thing, you're gonna wrap it in a loss function, you're going to do all these different things. I remember building my first models, when I actually tried to apply them to a real world problem, I realized that was completely not useful. The training set that I had been training against and the outputs I had been using there were giving me these great numbers of loss.

**Scott:** 99% accuracy.

**Susan:** And accuracy and I was like, oh, and I was tweaking the various setting and all that stuff. Then I disconnected the model from all that apparatus and tried to do something with it.

**Scott:** Yeah, feed it something real.

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976788/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/box-1.jpg)

_[Black box problem](https://en.wikipedia.org/wiki/Black_box)_

**Susan:** Suddenly I had no idea what it was actually doing. It just had this black box of data in, black box of data out, these metrics around it. You've always, always gotta keep in your mind that the whole point is not the training set, the whole point is not the accuracy number, the whole point is not all this apparatus over there, it's when you finally get to production. What is that output supposed to look like and how is it useful?

**Susan:** If you can't take whatever output it's got coming out there and make it useful, then you've gotta address that right away. It's like the water through the pipes that we were talking about there, 'cause water through the pipes on the training side and also water through the pipes on the production side.

**Scott:** Does it get sensible outputs in production-land as well?

**Susan:** Are you really able to take that 10,000 classes and do something with it or should have you stripped it down to those five because that's all you cared about in production?

**Scott:** I think some real nuts and bolts ones could help too. Like normalizing your input data.

**Susan:** Oh, yeah.\r
\r
## Not normalizing your input data
**Scott:** What is normalizing your input data mean? It's if you have an image, for example, and each pixel value goes from zero to 10,000, what scale should you keep it at? Should it be zero to 10,000 or should it be some other number? Typically machine learning algorithms are tuned to work well when you're talking about negative one to one or zero to one, or something like that. You should normalize your data so that zero to 10,000 now is like zero to one, or something like that.

**Susan:** It's staggering what normalization can do for you. You just think about humans. Think about looking around your environment. The first physical step of light going into your body goes through a normalization layer and that is what's your eye doing right now when it's stares into a light versus stares into a dark room?

**Scott:** It's getting smaller and bigger.

**Susan:** It shows you how important that step is that you physically do that. You do that with hearing too. It's a loud environment, what do we do? We put earmuffs on. We're trying to get all the sound normalized.

**Scott:** Or it's loud but it doesn't seem that loud. Hearing is logarithmic. There's tons of energy being pumped into your ear but it's only a little bit louder. One more on hearing. You've got two ears and the ability to swivel your head, when you listen to something, when you really wanna listen to something, you actually swivel your head and that helps your hearing. That physical motion of adjusting the two microphones in your body to be better aligned with [the source makes a big different to understanding](https://www.youtube.com/watch?v=Oai7HUqncAA&t=185). Don't forget that lesson when you're in the machine learning world. Normalize your data, don't forget those initial purely simple algorithmic filters you could apply to data beforehand.

**Scott:** We haven't discussed all that much, but essentially if you're Amazon Alexa or Google Home, or something like that, they usually have seven or eight microphones on them. But humans do pretty good with just two.

**Susan:** Sure, however humans have the ability to turn their heads.

**Scott:** Yeah we can move around. You have these infinite amount of microphones.

**Susan:** Yeah. We've got a lot of great things there.\r
\r
## More examples of common pitfalls\r



**Scott:** You have a deep learning model and in there are tons of parameters. Those parameters are initialized initially to what? What do I mean by initialize? I mean you fill up a bunch of matrices with numbers, what do you fill them with? Zeros? Is that a good answer?

**Susan:** Bad answer.

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976789/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/Wolfsgrube.jpg)

_[Pitfalls](https://en.wikipedia.org/wiki/Trapping_pit) were one method of hunting animals and defeating enemies in war. Now it's just a metaphor._

**Scott:** Very bad answer.

**Susan:** Very bad answer. As soon as you get to two layers everything goes out the window.

**Scott:** Do you fill them with all the same number? One?

**Susan:** Nope, it can't be the same number. Even just pure randomness is not good. We're not gonna get into the math and why, but look up some algorithms for how to fill up your parameters.

**Scott:** You want it round and peaked with some variance.

**Susan:** Based off of size, based off of number parameters.

**Scott:** But random still, but off a distribution.

**Susan:** Randomness is such an amazingly strong tool in the world. It's just crazy and that one example shows that you don't randomize things that literally will not learn.

**Scott:** You can stitch together networks or loss functions or pieces of code that other people have written and you use part of it, but you use it incorrectly. In other words, it should have a soft max that it's run through before it gets put in there, or it shouldn't. Do you strip those pieces off or not? This is an impedance mismatch.

**Susan:** To me this happens mostly going back to the training environment versus the production environment. You see this happen a lot. This is a basic one, luckily you fix it pretty quick just by changing that little piece of code. In training you may or may not need something at the end of your model that you'll strip off for production. Keeping that in mind will help save some debugging steps. Ask yourself: Is this really what I'm gonna use for production or was this here just for training? Forgetting that you've got dropout set wrong, or something like that.

**Scott:** You want it turned off for the evaluation inference probably.

**Susan:** You need to make sure your model's set for the right environment.

**Scott:** Drop out shuts off part of the deep learning brain basically. It's like oh, don't you think you might want that in there?

**Susan:** To get it in there if you've got a bunch of parameters. You can then randomly select a few of them to just ignore basically. The important thing about this is it tries to get the same level of output signal, so when you go to the production environment, you want those neurons to be there. You want those parameters to be useful, but if they were all turned on, suddenly your signal goes up, say, by 25% if you've got 25% drop out on there. You treat the evaluation side different than the train side. Some pretty important things to remember otherwise it's just not as good as it could be.

<iframe src="https://www.youtube.com/embed/KcrAkPNB8jc" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe>

_If you train your model based on speaker (narrator) then you have a narrator detector. You probably don't want that._

**Scott:** One of my favorite things to do though is overfitting. Which is normally a bad thing, but when you're in the water through the pipes stage, where hey, I've finally got my data going, I finally got my model built, I finally got some kind of output and I can verify that it's coming now. Now, take a really small training data set, maybe just one example or maybe 100, or maybe eight or whatever and train the model and make sure that you can very quickly get 100% accurate.

**Susan:** You wanna see that curve of learning.

**Scott:** Your error just drops like a rock because the model now has so many parameters it's able to just focus on getting those correct. All that it's doing at that point is memorizing, but if it can't memorize those eight things, it's probably not gonna do anything else.

**Susan:** That helps you prove that your loss function is working. It helps you prove you have some inputs and outputs there, at least in the right realm. It doesn't prove that you will have a good model structure.

**Scott:** No, you could have an awful model structure, but at least it has some hope.

**Susan:** That's your water through the pipes basic all the plumbing is involved here is working here. There's a ton of common, big, huge pitfalls. What's is the biggest one overall?

**Scott:** For me, I think the biggest one is like deep learning and hey, you can do anything. And it's like, okay cool, not anything, but there are a lot of things that you can do that you couldn't do before. Or it can do it better or something like that.\r
\r
They think, "I'm gonna go to TensorFlow and then I'm gonna solve my problem." If that's your thought process, you gotta back up a little bit and think we need to take baby steps here because you're not going to just go download a model or an example or train it with a couple days of toying around and get a real production thing done. It's just not gonna happen.\r
\r
Even from the engineering standpoint, you're not gonna do that. From the model building standpoint, you're not gonna get there basically unless it's something super simple you could've done with normal stats or machine learning.\r
\r
## The process side of the house is to me the biggest problem here.\r
\r

**Susan:** Did you analyze the problem correctly from the start? This is any engineering problem on the planet.

**Scott:** This takes budgeting the appropriate amount of time and the appropriate amount of time is at least days, and probably weeks, to get a good look at that problem. It might be months.

**Susan:** That might actually be the biggest pitfall in deep learning, is the assumption that it'll only be a day or two.

**Scott:** Yeah. Yeah. A day or two minimum. That happens frequently.

**Susan:** Oh, that's no problem. I'll have something in a week.

**Scott:** Yeah, "Oh weird. It's not doing what I thought it would do. But what if I just do this little trick?" Two days later: "Huh, okay," and then eight days later, "Huh."

**Susan:** It's a big time thing, that's for sure.\r`;
}
function compiledContent() {
  return '<p><strong>Scott:</strong> Welcome to the AI Show. Today we\u2019re asking the question: What are the top mistakes in deep learning?</p>\n<p><strong>Susan:</strong> We\u2019ve got huge ones! We make them all the time.</p>\n<p><strong>Scott:</strong> The mistake is the rule in deep learning. You make nine mistakes and one maybe good move.</p>\n<p><strong>Susan:</strong> How are you gonna find new things if you don\u2019t make mistakes?</p>\n<p><strong>Scott:</strong> It\u2019s frontier tech, right?</p>\n<p><strong>Susan:</strong> There\u2019s just so many fun pitfalls you can fall into that everybody\u2019s fallen into.</p>\n<p><strong>Scott:</strong> What do you think the big areas are?</p>\n<p><strong>Susan:</strong> I think that there\u2019s really two major areas. There\u2019s just analyzing the problem; the basics of analyzing whatever problem you\u2019re going down and the pitfalls around there. Then there\u2019s the model and data. If you don\u2019t analyze your problem correctly then it really doesn\u2019t matter what you do with the model and data. You\u2019ve fallen off the tracks. Once you finally got a good understanding you can get down there and fall into brand new pitfalls. Have you fallen into any analysis problems there, Scott?</p>\n<p><strong>Scott:</strong> I\u2019d say there\u2019s too many to talk about, but yes. Analyze the problem, make sure your data is good, these are all good traps. Make sure your model is actually a model that we\u2019d actually be able to perform the task you care about and then in the end you actually have to train it too. Training has its own history, as well. Probably I can name some things that I\u2019ve done that are not good. Some of it just when you\u2019re doing deep learning you\u2019re doing programming. Programming is hard. Copy pasting, you\u2019re reusing old code, you\u2019re iterating as you go along and you\u2019re using the problems and errors that you\u2019re getting to help you along until you finally get water through the pipes.</p>\n<p><strong>Susan:</strong> That\u2019s really important: water through the pipes.\n<img src="https://res.cloudinary.com/deepgram/image/upload/v1661976785/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/business-equipment-factory-357440.jpg" alt="Alt"></p>\n<p><strong>Scott:</strong> But usually once you first get water through the pipes and you get your first result everything is totally wrong anyway, so you\u2019re backtracking. Okay, where did that mistake come in?</p>\n<p><strong>Susan:</strong> Comes out more mud than water.</p>\n<p><strong>Scott:</strong> How do you get the pieces to fit together in the model at all? How do you get even a few pieces of data into the right form to be jammed into the model? Then the model finally has to output something and its output doesn\u2019t make any sense at all. But biggest thing is trying to learn an impossible task.</p>\n<p><strong>Susan:</strong> I\u2019ve done that.</p>\n<h2 id="dont-try-to-do-something-that-is-impossible">Don\u2019t try to do something that is impossible.</h2>\n<p><strong>Susan:</strong> I\u2019ve a fairly good rule of thumb for that one. If you as a human looking at your training data can\u2019t learn the task, then I can pretty well guarantee you that no matter how good of a model builder you are it\u2019s probably not gonna be able to learn that task.</p>\n<p><strong>Scott:</strong> Even if you think you can learn it, but if you ask somebody else and they\u2019re like, no that\u2019s a totally different thing. This is something like summarizing a scientific article and its jammed full of information. People would summarize it in different ways. Maybe that\u2019s not an impossible task, but that\u2019s a very, very, very hard task.</p>\n<p><strong>Susan:</strong> The big point here is: pick battles that are winnable. Make sure you\u2019re doing the steps necessary to figure out that this is a winnable battle. Don\u2019t let yourself believe that magic black box that is deep learning will be able to learn anything so long as you throw in enough data at it.</p>\n<p><strong>Scott:</strong> There\u2019s so many ways that it can go wrong. You don\u2019t want the number one reason of just this isn\u2019t even possible to take you down. Pick something that maybe a human could do. This is a good rule of thumb. If a human could do it with maybe a second worth of thinking and come out with a reliable answer every time, maybe that\u2019s something a machine could do.</p>\n<p><strong>Susan:</strong> The second half of that is truly important. A reliable answer. What does that mean? An objective answer. Something that if you had ten humans and you asked them the same question, you would get probably nine of them at least agreeing. Because if you have five of them saying one thing and five of them saying another thing, even if they\u2019re super positive that\u2019s the right answer, a machine\u2019s gonna have a very hard time figuring out what side of those five to be on.</p>\n<p><strong>Scott:</strong> Yeah ask ten people what five times five is. Okay, maybe you can teach a machine how to do that. We know that for sure. But ask them what the meaning of life is or who won the argument.</p>\n<p><strong>Susan:</strong> Was that dress blue or was that dress \u2026 Well maybe that one a machine could figure out perfectly.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976785/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/The_Dress_-viral_phenomenon-.png" alt="Alt"></p>\n<h2 id="trying-deep-learning-first">Trying deep learning first.</h2>\n<p><strong>Susan:</strong> Deep learning thinks it\u2019s everything.</p>\n<p><strong>Scott:</strong> Pump the brakes here a little bit.</p>\n<p><strong>Susan:</strong> What is the simple approach? Also, you\u2019d be surprised, just the most basic of algorithms can solve some really, really hard problems, or what we think of as hard problems. Just add it all up and find the mean and suddenly you got 90% of your answer in a lot of cases. Don\u2019t skip to the most complex possible huge-chain answer that you can right off the bat.</p>\n<p><strong>Scott:</strong> You have to load up a ton of context and tools and the know-how in your brain and time and the iteration cycle on deep learning is so long that it\u2019s hard. You could be doing something with just regular stats or traditional tree based machine learning technique or something that would go through the data and you\u2019d have a result in five minutes. That often is better than waiting two days for deep learning. You want to get on that track, like the five minutes.</p>\n<p><strong>Susan:</strong> Even then, just doing a simple least squares fit. I\u2019ve done that so many times and that is an amazingly great, quick way. Say, hey, this is a tackle-able problem. Also, sometimes the answer is right where you need it to be.</p>\n<h2 id="dont-throw-away-your-standard-stats-and-machine-learning-techniques">Don\u2019t throw away your standard stats and machine learning techniques.</h2>\n<p><strong>Scott:</strong> Go to those first. If you think this isn\u2019t performing as well as it should and:</p>\n<ul>\n<li>I\u2019ve turned lots of knobs,</li>\n<li>I have a ton of data,</li>\n<li>I have the computing power,</li>\n<li>I have the time to go through and try to figure this out\r\nMaybe try deep learning then. And it\u2019s not impossible. What about data?</li>\n</ul>\n<p><strong>Susan:</strong> I\u2019m gonna tell an embarrassing story. I\u2019m going to hope most people have had the same embarrassing story. I started playing around with deep learning probably just for fun. Not deep learning, just machine learning in general, about 10 years ago, 15 years ago. I was playing around with little things and then I hit the stock market like everybody does.</p>\n<p><strong>Scott:</strong> Yeah, first thing. Oh, machine learning, here we come!</p>\n<p><strong>Susan:</strong> I\u2019m gonna predict the stock market! I started building my little model and man, it was accurate.</p>\n<p><strong>Scott:</strong> Super accurate?</p>\n<p><strong>Susan:</strong> It was crazy accurate. I was like, I\u2019m gonna be a millionaire! What did I not do properly?</p>\n<p><strong>Validation versus training.</strong></p>\n<h2 id="validation-versus-training">Validation versus training</h2>\n<p><strong>Scott:</strong> What you\u2019re trying to do is set up the problem, so I\u2019m gonna give you this data and you have to learn from it, but then I\u2019m gonna hold this other data out, which is true data, again, but the model\u2019s never seen it, and now I\u2019m gonna show it to you and see how correct you are. Well, if you don\u2019t do that properly, then you really fool yourself.</p>\n<p><strong>Susan:</strong> You can delude yourself so amazingly well. And the great thing about this particular trap is that there\u2019s so many variations to this trap. Let\u2019s go to the audiobook world and you\u2019re trying to do something like speech recognition. You have 500 audiobooks and 18 people reading those audiobooks. You could split your data one of two ways: off of audiobook or off of people.</p>\n<p><strong>Scott:</strong> Well there\u2019s 500 audiobooks, so \u2026</p>\n<p><strong>Susan:</strong> I can guarantee you if you do it off of audiobook you won\u2019t be able to use it in the real world. There\u2019s 500 audiobooks, but now you suddenly look a lot better than you actually are because you got the same person in your train versus validates.</p>\n<p><strong>Scott:</strong> It learned how that person spoke in the other books and even though it hadn\u2019t heard that exact audio, it\u2019s heard that voice before and it\u2019s learned a lot form that voice and so it can do a good job on it.</p>\n<p><strong>Susan:</strong> It\u2019s not just, hey: \u201CI\u2019m gonna randomly select.\u201D I have unfortunately learned this lesson on several times in my life. You want to know how the model performs in the wild. Real, wild data. This is a hard thing to just keep track of, especially in deep learning because it takes so long to do your experiments. It\u2019s easy to say what if I use this? What if I do this? What if I look over here? If you train just once on one of your validation data sets, you have a big problem. You can\u2019t undo that. If you\u2019ve been training your model for the last three months and you then spend a day training on your validation data set, you might as well throw that validation data set away, put it in your training data set, or start over on the model.</p>\n<p><strong>Susan:</strong> Hopefully you had a checkpoint before you did that.</p>\n<p><strong>Scott:</strong> Or go back. Go back into a checkpoint.</p>\n<p><strong>Susan:</strong> You contaminate that model and you are done. And it can happen so many ways you don\u2019t even realize it. You get a new data set you didn\u2019t realize was actually derived data set and it has different IDs associated with the thing it was derived from and suddenly, wow, I\u2019m doing really well on this, except for it\u2019s not real.</p>\n<p><strong>Scott:</strong> Training a model is not a reversible process. There\u2019s no undo button.</p>\n<p><strong>Susan:</strong> It\u2019s very incredibly easy to mess up your various sets of data, so treat them wisely. But, you know what\u2019s another great way to mess up training versus validate?</p>\n<p><strong>Scott:</strong> What?</p>\n<h2 id="overfit-your-hyper-parameters">Overfit your hyper parameters.</h2>\n<p><strong>Scott:</strong> Oh yeah, for sure. You\u2019re always picking how wide should it be, how deep should it be, how many input this do I need, how many that? What should my learning rate be? And all these different hyper parameters and then you try to use it on some wild data and things aren\u2019t working. Why aren\u2019t they working?</p>\n<p><strong>Susan:</strong> What happened? I ran 10,000 tests, each one tweaking it to be the best possible result on my validation. Why isn\u2019t that matching reality?</p>\n<p><strong>Scott:</strong> You\u2019re training yourself a little bit here. It isn\u2019t that the model got adjusted so much, it\u2019s that you are thinking: \u201COh, what if I tune these knobs a little bit?\u201D Now it does really well on your validation data set, but if you give it some real wild data, it doesn\u2019t do so well.</p>\n<p><strong>Susan:</strong> It\u2019s hard to grasp at, but the first time you really truly run into it and smacks you in the face, you definitely feel it hard. It\u2019s like that big gap between validation and reality is huge. There\u2019s still even more ways you can mess up your validation set. We could talk about this forever.</p>\n<p><strong>Scott:</strong> There\u2019s a good way to combat this, which is you have your large training data set, then you have a validation data set, then you have another validation data set, then another validation data set, then you at least have these backups. Hey, you can test over here, test over here, test over here, get your hyper parameters where you think you need them, and then test them on these other data sets that are maybe they\u2019re small too like your validation data set, but at least it\u2019s giving you a little cross check.</p>\n<p><strong>Susan:</strong> Do what you can. Also, on that note, grooming these sets as time goes on is important. Just like you\u2019re talking about setting aside a secret or whatever you wanna call these additional data sets.</p>\n<p><strong>Scott:</strong> Test validation secret.</p>\n<p><strong>Susan:</strong> There\u2019s a lot of different ways to split these up, but if you\u2019re getting data over time, make sure that you keep on adding to these sets over time, because data changes over time. The English language changes over time. All it takes is one popular new song and people shift their language a little bit. We keep going back to the language world because that\u2019s what we think about a lot, but this is also images. Just think about how cameras have changed, just the quality settings and all that. Your average distribution of colors are probably changing a little bit.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976786/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/english-evolution_.jpg" alt="Alt"></p>\n<p><em>Here we see how one verse of the Old Testament changed over time. By comparing the Old English (Anglo Saxon) and Middle English (Wycliffe\u2019s) versions you can see quite a bit of sound change as well as grammatical change. The King James version is radically different, showing the changes in culture that led to serious change in interpretation. The older two versions reflect a closer translation from the Latin bible.</em></p>\n<p><strong>Scott:</strong> If you take a picture on an iPhone now versus eight years ago it\u2019s gonna look vastly different.</p>\n<p><strong>Susan:</strong> Or the self driving world. If you did nothing but from the 80s, a whole bunch of video from the 80s, cars would\u2019ve changed a lot, clothes would\u2019ve changed a lot. Do you think that would all affect how well you can recognize everything out there? All this is talking about how you manage your data and the disconnect between the reality of the data you\u2019re training on and the current real world production environment you\u2019re gonna be on. That\u2019s the big mistake that a lot of people run into. You\u2019ve been in this little locked in world and then suddenly reality is somehow different. That\u2019s because you\u2019re training and your validate probably aren\u2019t as good as representing the real world as you thought.</p>\n<p><strong>Scott:</strong> Some other good ones are you have to choose your hyper parameters, which is how big is your network, how deep is it, how wide is it? What learning rate are you going to use, et cetera? On the topic of learning rate, if you chose too high of a learning rate, your gradients get huge.</p>\n<h2 id="choosing-the-wrong-learning-rate">Choosing the wrong learning rate</h2>\n<p><strong>Susan:</strong> Blow your weights.</p>\n<p><strong>Scott:</strong> It tries to learn too much too fast and it learns nothing.</p>\n<p><strong>Susan:</strong> There\u2019s so much that goes into hand tweaking all of that stuff. One of the first things that you need to do is do a quick exploration. Whatever mechanism that you do for that grid searcher or whatever, try to find those big hyperparameter sweet spots, but then at the same time, you\u2019ve gotta fight that problem of overfitting your hyper parameters. It\u2019s a real challenge and it becomes a real pitfall that you do things like wrong learning rates and then you restart and this and that and the other, then suddenly at the end of it you have a set of hyper parameters that seem to fit and you\u2019ve gotten into that overfit world. Minimize how often you change it.</p>\n<p><strong>Scott:</strong> Try to get into a good region and then don\u2019t change it too much and then rely on your data to do the talking of shaping where the model should go. You could have too low of a learning rate too, where it just takes way too long. We\u2019ve talked about this before, where you\u2019re patience limited in deep learning because things take a while, there\u2019s a lot of parameters, it uses a lot of computational power, and if you\u2019re training models for days and it\u2019s just only getting better very slowly, maybe your learning rate needs to be jacked up again.</p>\n<p><strong>Scott:</strong> Again, that\u2019s part of that search.</p>\n<p><strong>Susan:</strong> That gets into one that I\u2019ve re-learned recently, which is people pay a lot of attention to data, rightfully so. A lot of attention to model structure, rightfully so, but less attention to the optimizer that they use.</p>\n<p><strong>Scott:</strong> I completely agree.</p>\n<p><strong>Susan:</strong> There\u2019s just an epic difference out there based off that. It\u2019s hard to even begin with how important that selection is you forget about.</p>\n<p><strong>Scott:</strong> Heuristically, it\u2019s like, if I know what my error is and I know the errors I\u2019ve been making in the past, how far should I project where I should go? You can think about this as a human too. If you\u2019re walking around in the day and you keep hearing the same wrong thing, over and over, and you\u2019re like, maybe I really need to go look into that and adjust it or something. It\u2019s weighting all of that. How much of the past should I keep for now and use in the future? How much should I throw away? There\u2019s a momentum side to that too.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976787/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/conifer-dawn-daylight-167698.jpg" alt="Alt"></p>\n<p><strong>Susan:</strong> One way to think of it is this; going back to the classic forest problem. Dense forest and how do I get to low ground? The optimizer\u2019s are like how fast you should be walking at any one given time?</p>\n<p><strong>Scott:</strong> If it was all curvy right there, maybe you should do something else. But there are different programs there, basically, or different rules you should follow and that\u2019s what picking your optimizer is.</p>\n<p><strong>Susan:</strong> And how it adjusts that rate of walking as you go along. Those are big, huge, massive things there. What about the bottom end of a model? What kind of output errors have you run into here, Scott?</p>\n<p><strong>Scott:</strong> A lot.</p>\n<p><strong>Susan:</strong> I can give you my favorite one. If you got one right off the top, go for it.</p>\n<p><strong>Scott:</strong> Not outputting good results is the biggest. Outputting all zeros when you\u2019re loss function is all screwed up or whatever.</p>\n<p><strong>Susan:</strong> My personal favorite is outputting something that\u2019s useful in a training set, but not useful in the real world. Forgetting the difference between training and production. When you look at code and when you look at training models and stuff like that, you\u2019re going to take this thing, you\u2019re gonna wrap it in a loss function, you\u2019re going to do all these different things. I remember building my first models, when I actually tried to apply them to a real world problem, I realized that was completely not useful. The training set that I had been training against and the outputs I had been using there were giving me these great numbers of loss.</p>\n<p><strong>Scott:</strong> 99% accuracy.</p>\n<p><strong>Susan:</strong> And accuracy and I was like, oh, and I was tweaking the various setting and all that stuff. Then I disconnected the model from all that apparatus and tried to do something with it.</p>\n<p><strong>Scott:</strong> Yeah, feed it something real.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976788/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/box-1.jpg" alt="Alt"></p>\n<p><em><a href="https://en.wikipedia.org/wiki/Black_box">Black box problem</a></em></p>\n<p><strong>Susan:</strong> Suddenly I had no idea what it was actually doing. It just had this black box of data in, black box of data out, these metrics around it. You\u2019ve always, always gotta keep in your mind that the whole point is not the training set, the whole point is not the accuracy number, the whole point is not all this apparatus over there, it\u2019s when you finally get to production. What is that output supposed to look like and how is it useful?</p>\n<p><strong>Susan:</strong> If you can\u2019t take whatever output it\u2019s got coming out there and make it useful, then you\u2019ve gotta address that right away. It\u2019s like the water through the pipes that we were talking about there, \u2018cause water through the pipes on the training side and also water through the pipes on the production side.</p>\n<p><strong>Scott:</strong> Does it get sensible outputs in production-land as well?</p>\n<p><strong>Susan:</strong> Are you really able to take that 10,000 classes and do something with it or should have you stripped it down to those five because that\u2019s all you cared about in production?</p>\n<p><strong>Scott:</strong> I think some real nuts and bolts ones could help too. Like normalizing your input data.</p>\n<p><strong>Susan:</strong> Oh, yeah.</p>\n<h2 id="not-normalizing-your-input-data">Not normalizing your input data</h2>\n<p><strong>Scott:</strong> What is normalizing your input data mean? It\u2019s if you have an image, for example, and each pixel value goes from zero to 10,000, what scale should you keep it at? Should it be zero to 10,000 or should it be some other number? Typically machine learning algorithms are tuned to work well when you\u2019re talking about negative one to one or zero to one, or something like that. You should normalize your data so that zero to 10,000 now is like zero to one, or something like that.</p>\n<p><strong>Susan:</strong> It\u2019s staggering what normalization can do for you. You just think about humans. Think about looking around your environment. The first physical step of light going into your body goes through a normalization layer and that is what\u2019s your eye doing right now when it\u2019s stares into a light versus stares into a dark room?</p>\n<p><strong>Scott:</strong> It\u2019s getting smaller and bigger.</p>\n<p><strong>Susan:</strong> It shows you how important that step is that you physically do that. You do that with hearing too. It\u2019s a loud environment, what do we do? We put earmuffs on. We\u2019re trying to get all the sound normalized.</p>\n<p><strong>Scott:</strong> Or it\u2019s loud but it doesn\u2019t seem that loud. Hearing is logarithmic. There\u2019s tons of energy being pumped into your ear but it\u2019s only a little bit louder. One more on hearing. You\u2019ve got two ears and the ability to swivel your head, when you listen to something, when you really wanna listen to something, you actually swivel your head and that helps your hearing. That physical motion of adjusting the two microphones in your body to be better aligned with <a href="https://www.youtube.com/watch?v=Oai7HUqncAA&#x26;t=185">the source makes a big different to understanding</a>. Don\u2019t forget that lesson when you\u2019re in the machine learning world. Normalize your data, don\u2019t forget those initial purely simple algorithmic filters you could apply to data beforehand.</p>\n<p><strong>Scott:</strong> We haven\u2019t discussed all that much, but essentially if you\u2019re Amazon Alexa or Google Home, or something like that, they usually have seven or eight microphones on them. But humans do pretty good with just two.</p>\n<p><strong>Susan:</strong> Sure, however humans have the ability to turn their heads.</p>\n<p><strong>Scott:</strong> Yeah we can move around. You have these infinite amount of microphones.</p>\n<p><strong>Susan:</strong> Yeah. We\u2019ve got a lot of great things there.</p>\n<h2 id="more-examples-of-common-pitfalls">More examples of common pitfalls</h2>\n<p><strong>Scott:</strong> You have a deep learning model and in there are tons of parameters. Those parameters are initialized initially to what? What do I mean by initialize? I mean you fill up a bunch of matrices with numbers, what do you fill them with? Zeros? Is that a good answer?</p>\n<p><strong>Susan:</strong> Bad answer.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976789/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/Wolfsgrube.jpg" alt="Alt"></p>\n<p><em><a href="https://en.wikipedia.org/wiki/Trapping_pit">Pitfalls</a> were one method of hunting animals and defeating enemies in war. Now it\u2019s just a metaphor.</em></p>\n<p><strong>Scott:</strong> Very bad answer.</p>\n<p><strong>Susan:</strong> Very bad answer. As soon as you get to two layers everything goes out the window.</p>\n<p><strong>Scott:</strong> Do you fill them with all the same number? One?</p>\n<p><strong>Susan:</strong> Nope, it can\u2019t be the same number. Even just pure randomness is not good. We\u2019re not gonna get into the math and why, but look up some algorithms for how to fill up your parameters.</p>\n<p><strong>Scott:</strong> You want it round and peaked with some variance.</p>\n<p><strong>Susan:</strong> Based off of size, based off of number parameters.</p>\n<p><strong>Scott:</strong> But random still, but off a distribution.</p>\n<p><strong>Susan:</strong> Randomness is such an amazingly strong tool in the world. It\u2019s just crazy and that one example shows that you don\u2019t randomize things that literally will not learn.</p>\n<p><strong>Scott:</strong> You can stitch together networks or loss functions or pieces of code that other people have written and you use part of it, but you use it incorrectly. In other words, it should have a soft max that it\u2019s run through before it gets put in there, or it shouldn\u2019t. Do you strip those pieces off or not? This is an impedance mismatch.</p>\n<p><strong>Susan:</strong> To me this happens mostly going back to the training environment versus the production environment. You see this happen a lot. This is a basic one, luckily you fix it pretty quick just by changing that little piece of code. In training you may or may not need something at the end of your model that you\u2019ll strip off for production. Keeping that in mind will help save some debugging steps. Ask yourself: Is this really what I\u2019m gonna use for production or was this here just for training? Forgetting that you\u2019ve got dropout set wrong, or something like that.</p>\n<p><strong>Scott:</strong> You want it turned off for the evaluation inference probably.</p>\n<p><strong>Susan:</strong> You need to make sure your model\u2019s set for the right environment.</p>\n<p><strong>Scott:</strong> Drop out shuts off part of the deep learning brain basically. It\u2019s like oh, don\u2019t you think you might want that in there?</p>\n<p><strong>Susan:</strong> To get it in there if you\u2019ve got a bunch of parameters. You can then randomly select a few of them to just ignore basically. The important thing about this is it tries to get the same level of output signal, so when you go to the production environment, you want those neurons to be there. You want those parameters to be useful, but if they were all turned on, suddenly your signal goes up, say, by 25% if you\u2019ve got 25% drop out on there. You treat the evaluation side different than the train side. Some pretty important things to remember otherwise it\u2019s just not as good as it could be.</p>\n<iframe src="https://www.youtube.com/embed/KcrAkPNB8jc" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen" />\n<p><em>If you train your model based on speaker (narrator) then you have a narrator detector. You probably don\u2019t want that.</em></p>\n<p><strong>Scott:</strong> One of my favorite things to do though is overfitting. Which is normally a bad thing, but when you\u2019re in the water through the pipes stage, where hey, I\u2019ve finally got my data going, I finally got my model built, I finally got some kind of output and I can verify that it\u2019s coming now. Now, take a really small training data set, maybe just one example or maybe 100, or maybe eight or whatever and train the model and make sure that you can very quickly get 100% accurate.</p>\n<p><strong>Susan:</strong> You wanna see that curve of learning.</p>\n<p><strong>Scott:</strong> Your error just drops like a rock because the model now has so many parameters it\u2019s able to just focus on getting those correct. All that it\u2019s doing at that point is memorizing, but if it can\u2019t memorize those eight things, it\u2019s probably not gonna do anything else.</p>\n<p><strong>Susan:</strong> That helps you prove that your loss function is working. It helps you prove you have some inputs and outputs there, at least in the right realm. It doesn\u2019t prove that you will have a good model structure.</p>\n<p><strong>Scott:</strong> No, you could have an awful model structure, but at least it has some hope.</p>\n<p><strong>Susan:</strong> That\u2019s your water through the pipes basic all the plumbing is involved here is working here. There\u2019s a ton of common, big, huge pitfalls. What\u2019s is the biggest one overall?</p>\n<p><strong>Scott:</strong> For me, I think the biggest one is like deep learning and hey, you can do anything. And it\u2019s like, okay cool, not anything, but there are a lot of things that you can do that you couldn\u2019t do before. Or it can do it better or something like that.</p>\n<p>They think, \u201CI\u2019m gonna go to TensorFlow and then I\u2019m gonna solve my problem.\u201D If that\u2019s your thought process, you gotta back up a little bit and think we need to take baby steps here because you\u2019re not going to just go download a model or an example or train it with a couple days of toying around and get a real production thing done. It\u2019s just not gonna happen.</p>\n<p>Even from the engineering standpoint, you\u2019re not gonna do that. From the model building standpoint, you\u2019re not gonna get there basically unless it\u2019s something super simple you could\u2019ve done with normal stats or machine learning.</p>\n<h2 id="the-process-side-of-the-house-is-to-me-the-biggest-problem-here">The process side of the house is to me the biggest problem here.</h2>\n<p><strong>Susan:</strong> Did you analyze the problem correctly from the start? This is any engineering problem on the planet.</p>\n<p><strong>Scott:</strong> This takes budgeting the appropriate amount of time and the appropriate amount of time is at least days, and probably weeks, to get a good look at that problem. It might be months.</p>\n<p><strong>Susan:</strong> That might actually be the biggest pitfall in deep learning, is the assumption that it\u2019ll only be a day or two.</p>\n<p><strong>Scott:</strong> Yeah. Yeah. A day or two minimum. That happens frequently.</p>\n<p><strong>Susan:</strong> Oh, that\u2019s no problem. I\u2019ll have something in a week.</p>\n<p><strong>Scott:</strong> Yeah, \u201COh weird. It\u2019s not doing what I thought it would do. But what if I just do this little trick?\u201D Two days later: \u201CHuh, okay,\u201D and then eight days later, \u201CHuh.\u201D</p>\n<p><strong>Susan:</strong> It\u2019s a big time thing, that\u2019s for sure.</p>';
}
const $$Astro = createAstro("/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-are-the-top-mistakes-in-deep-learning-ai-show/index.md", "https://blog.deepgram.com/", "file:///Users/sandrarodgers/web-next/blog/");
const $$Index = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro, $$props, $$slots);
  Astro2.self = $$Index;
  new Slugger();
  return renderTemplate`<head>${renderHead($$result)}</head><p><strong>Scott:</strong> Welcome to the AI Show. Today we’re asking the question: What are the top mistakes in deep learning?</p>
<p><strong>Susan:</strong> We’ve got huge ones! We make them all the time.</p>
<p><strong>Scott:</strong> The mistake is the rule in deep learning. You make nine mistakes and one maybe good move.</p>
<p><strong>Susan:</strong> How are you gonna find new things if you don’t make mistakes?</p>
<p><strong>Scott:</strong> It’s frontier tech, right?</p>
<p><strong>Susan:</strong> There’s just so many fun pitfalls you can fall into that everybody’s fallen into.</p>
<p><strong>Scott:</strong> What do you think the big areas are?</p>
<p><strong>Susan:</strong> I think that there’s really two major areas. There’s just analyzing the problem; the basics of analyzing whatever problem you’re going down and the pitfalls around there. Then there’s the model and data. If you don’t analyze your problem correctly then it really doesn’t matter what you do with the model and data. You’ve fallen off the tracks. Once you finally got a good understanding you can get down there and fall into brand new pitfalls. Have you fallen into any analysis problems there, Scott?</p>
<p><strong>Scott:</strong> I’d say there’s too many to talk about, but yes. Analyze the problem, make sure your data is good, these are all good traps. Make sure your model is actually a model that we’d actually be able to perform the task you care about and then in the end you actually have to train it too. Training has its own history, as well. Probably I can name some things that I’ve done that are not good. Some of it just when you’re doing deep learning you’re doing programming. Programming is hard. Copy pasting, you’re reusing old code, you’re iterating as you go along and you’re using the problems and errors that you’re getting to help you along until you finally get water through the pipes.</p>
<p><strong>Susan:</strong> That’s really important: water through the pipes.
<img src="https://res.cloudinary.com/deepgram/image/upload/v1661976785/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/business-equipment-factory-357440.jpg" alt="Alt"></p>
<p><strong>Scott:</strong> But usually once you first get water through the pipes and you get your first result everything is totally wrong anyway, so you’re backtracking. Okay, where did that mistake come in?</p>
<p><strong>Susan:</strong> Comes out more mud than water.</p>
<p><strong>Scott:</strong> How do you get the pieces to fit together in the model at all? How do you get even a few pieces of data into the right form to be jammed into the model? Then the model finally has to output something and its output doesn’t make any sense at all. But biggest thing is trying to learn an impossible task.</p>
<p><strong>Susan:</strong> I’ve done that.</p>
<h2 id="dont-try-to-do-something-that-is-impossible">Don’t try to do something that is impossible.</h2>
<p><strong>Susan:</strong> I’ve a fairly good rule of thumb for that one. If you as a human looking at your training data can’t learn the task, then I can pretty well guarantee you that no matter how good of a model builder you are it’s probably not gonna be able to learn that task.</p>
<p><strong>Scott:</strong> Even if you think you can learn it, but if you ask somebody else and they’re like, no that’s a totally different thing. This is something like summarizing a scientific article and its jammed full of information. People would summarize it in different ways. Maybe that’s not an impossible task, but that’s a very, very, very hard task.</p>
<p><strong>Susan:</strong> The big point here is: pick battles that are winnable. Make sure you’re doing the steps necessary to figure out that this is a winnable battle. Don’t let yourself believe that magic black box that is deep learning will be able to learn anything so long as you throw in enough data at it.</p>
<p><strong>Scott:</strong> There’s so many ways that it can go wrong. You don’t want the number one reason of just this isn’t even possible to take you down. Pick something that maybe a human could do. This is a good rule of thumb. If a human could do it with maybe a second worth of thinking and come out with a reliable answer every time, maybe that’s something a machine could do.</p>
<p><strong>Susan:</strong> The second half of that is truly important. A reliable answer. What does that mean? An objective answer. Something that if you had ten humans and you asked them the same question, you would get probably nine of them at least agreeing. Because if you have five of them saying one thing and five of them saying another thing, even if they’re super positive that’s the right answer, a machine’s gonna have a very hard time figuring out what side of those five to be on.</p>
<p><strong>Scott:</strong> Yeah ask ten people what five times five is. Okay, maybe you can teach a machine how to do that. We know that for sure. But ask them what the meaning of life is or who won the argument.</p>
<p><strong>Susan:</strong> Was that dress blue or was that dress … Well maybe that one a machine could figure out perfectly.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976785/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/The_Dress_-viral_phenomenon-.png" alt="Alt"></p>
<h2 id="trying-deep-learning-first">Trying deep learning first.</h2>
<p><strong>Susan:</strong> Deep learning thinks it’s everything.</p>
<p><strong>Scott:</strong> Pump the brakes here a little bit.</p>
<p><strong>Susan:</strong> What is the simple approach? Also, you’d be surprised, just the most basic of algorithms can solve some really, really hard problems, or what we think of as hard problems. Just add it all up and find the mean and suddenly you got 90% of your answer in a lot of cases. Don’t skip to the most complex possible huge-chain answer that you can right off the bat.</p>
<p><strong>Scott:</strong> You have to load up a ton of context and tools and the know-how in your brain and time and the iteration cycle on deep learning is so long that it’s hard. You could be doing something with just regular stats or traditional tree based machine learning technique or something that would go through the data and you’d have a result in five minutes. That often is better than waiting two days for deep learning. You want to get on that track, like the five minutes.</p>
<p><strong>Susan:</strong> Even then, just doing a simple least squares fit. I’ve done that so many times and that is an amazingly great, quick way. Say, hey, this is a tackle-able problem. Also, sometimes the answer is right where you need it to be.</p>
<h2 id="dont-throw-away-your-standard-stats-and-machine-learning-techniques">Don’t throw away your standard stats and machine learning techniques.</h2>
<p><strong>Scott:</strong> Go to those first. If you think this isn’t performing as well as it should and:</p>
<ul>
<li>I’ve turned lots of knobs,</li>
<li>I have a ton of data,</li>
<li>I have the computing power,</li>
<li>I have the time to go through and try to figure this out
Maybe try deep learning then. And it’s not impossible. What about data?</li>
</ul>
<p><strong>Susan:</strong> I’m gonna tell an embarrassing story. I’m going to hope most people have had the same embarrassing story. I started playing around with deep learning probably just for fun. Not deep learning, just machine learning in general, about 10 years ago, 15 years ago. I was playing around with little things and then I hit the stock market like everybody does.</p>
<p><strong>Scott:</strong> Yeah, first thing. Oh, machine learning, here we come!</p>
<p><strong>Susan:</strong> I’m gonna predict the stock market! I started building my little model and man, it was accurate.</p>
<p><strong>Scott:</strong> Super accurate?</p>
<p><strong>Susan:</strong> It was crazy accurate. I was like, I’m gonna be a millionaire! What did I not do properly?</p>
<p><strong>Validation versus training.</strong></p>
<h2 id="validation-versus-training">Validation versus training</h2>
<p><strong>Scott:</strong> What you’re trying to do is set up the problem, so I’m gonna give you this data and you have to learn from it, but then I’m gonna hold this other data out, which is true data, again, but the model’s never seen it, and now I’m gonna show it to you and see how correct you are. Well, if you don’t do that properly, then you really fool yourself.</p>
<p><strong>Susan:</strong> You can delude yourself so amazingly well. And the great thing about this particular trap is that there’s so many variations to this trap. Let’s go to the audiobook world and you’re trying to do something like speech recognition. You have 500 audiobooks and 18 people reading those audiobooks. You could split your data one of two ways: off of audiobook or off of people.</p>
<p><strong>Scott:</strong> Well there’s 500 audiobooks, so …</p>
<p><strong>Susan:</strong> I can guarantee you if you do it off of audiobook you won’t be able to use it in the real world. There’s 500 audiobooks, but now you suddenly look a lot better than you actually are because you got the same person in your train versus validates.</p>
<p><strong>Scott:</strong> It learned how that person spoke in the other books and even though it hadn’t heard that exact audio, it’s heard that voice before and it’s learned a lot form that voice and so it can do a good job on it.</p>
<p><strong>Susan:</strong> It’s not just, hey: “I’m gonna randomly select.” I have unfortunately learned this lesson on several times in my life. You want to know how the model performs in the wild. Real, wild data. This is a hard thing to just keep track of, especially in deep learning because it takes so long to do your experiments. It’s easy to say what if I use this? What if I do this? What if I look over here? If you train just once on one of your validation data sets, you have a big problem. You can’t undo that. If you’ve been training your model for the last three months and you then spend a day training on your validation data set, you might as well throw that validation data set away, put it in your training data set, or start over on the model.</p>
<p><strong>Susan:</strong> Hopefully you had a checkpoint before you did that.</p>
<p><strong>Scott:</strong> Or go back. Go back into a checkpoint.</p>
<p><strong>Susan:</strong> You contaminate that model and you are done. And it can happen so many ways you don’t even realize it. You get a new data set you didn’t realize was actually derived data set and it has different IDs associated with the thing it was derived from and suddenly, wow, I’m doing really well on this, except for it’s not real.</p>
<p><strong>Scott:</strong> Training a model is not a reversible process. There’s no undo button.</p>
<p><strong>Susan:</strong> It’s very incredibly easy to mess up your various sets of data, so treat them wisely. But, you know what’s another great way to mess up training versus validate?</p>
<p><strong>Scott:</strong> What?</p>
<h2 id="overfit-your-hyper-parameters">Overfit your hyper parameters.</h2>
<p><strong>Scott:</strong> Oh yeah, for sure. You’re always picking how wide should it be, how deep should it be, how many input this do I need, how many that? What should my learning rate be? And all these different hyper parameters and then you try to use it on some wild data and things aren’t working. Why aren’t they working?</p>
<p><strong>Susan:</strong> What happened? I ran 10,000 tests, each one tweaking it to be the best possible result on my validation. Why isn’t that matching reality?</p>
<p><strong>Scott:</strong> You’re training yourself a little bit here. It isn’t that the model got adjusted so much, it’s that you are thinking: “Oh, what if I tune these knobs a little bit?” Now it does really well on your validation data set, but if you give it some real wild data, it doesn’t do so well.</p>
<p><strong>Susan:</strong> It’s hard to grasp at, but the first time you really truly run into it and smacks you in the face, you definitely feel it hard. It’s like that big gap between validation and reality is huge. There’s still even more ways you can mess up your validation set. We could talk about this forever.</p>
<p><strong>Scott:</strong> There’s a good way to combat this, which is you have your large training data set, then you have a validation data set, then you have another validation data set, then another validation data set, then you at least have these backups. Hey, you can test over here, test over here, test over here, get your hyper parameters where you think you need them, and then test them on these other data sets that are maybe they’re small too like your validation data set, but at least it’s giving you a little cross check.</p>
<p><strong>Susan:</strong> Do what you can. Also, on that note, grooming these sets as time goes on is important. Just like you’re talking about setting aside a secret or whatever you wanna call these additional data sets.</p>
<p><strong>Scott:</strong> Test validation secret.</p>
<p><strong>Susan:</strong> There’s a lot of different ways to split these up, but if you’re getting data over time, make sure that you keep on adding to these sets over time, because data changes over time. The English language changes over time. All it takes is one popular new song and people shift their language a little bit. We keep going back to the language world because that’s what we think about a lot, but this is also images. Just think about how cameras have changed, just the quality settings and all that. Your average distribution of colors are probably changing a little bit.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976786/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/english-evolution_.jpg" alt="Alt"></p>
<p><em>Here we see how one verse of the Old Testament changed over time. By comparing the Old English (Anglo Saxon) and Middle English (Wycliffe’s) versions you can see quite a bit of sound change as well as grammatical change. The King James version is radically different, showing the changes in culture that led to serious change in interpretation. The older two versions reflect a closer translation from the Latin bible.</em></p>
<p><strong>Scott:</strong> If you take a picture on an iPhone now versus eight years ago it’s gonna look vastly different.</p>
<p><strong>Susan:</strong> Or the self driving world. If you did nothing but from the 80s, a whole bunch of video from the 80s, cars would’ve changed a lot, clothes would’ve changed a lot. Do you think that would all affect how well you can recognize everything out there? All this is talking about how you manage your data and the disconnect between the reality of the data you’re training on and the current real world production environment you’re gonna be on. That’s the big mistake that a lot of people run into. You’ve been in this little locked in world and then suddenly reality is somehow different. That’s because you’re training and your validate probably aren’t as good as representing the real world as you thought.</p>
<p><strong>Scott:</strong> Some other good ones are you have to choose your hyper parameters, which is how big is your network, how deep is it, how wide is it? What learning rate are you going to use, et cetera? On the topic of learning rate, if you chose too high of a learning rate, your gradients get huge.</p>
<h2 id="choosing-the-wrong-learning-rate">Choosing the wrong learning rate</h2>
<p><strong>Susan:</strong> Blow your weights.</p>
<p><strong>Scott:</strong> It tries to learn too much too fast and it learns nothing.</p>
<p><strong>Susan:</strong> There’s so much that goes into hand tweaking all of that stuff. One of the first things that you need to do is do a quick exploration. Whatever mechanism that you do for that grid searcher or whatever, try to find those big hyperparameter sweet spots, but then at the same time, you’ve gotta fight that problem of overfitting your hyper parameters. It’s a real challenge and it becomes a real pitfall that you do things like wrong learning rates and then you restart and this and that and the other, then suddenly at the end of it you have a set of hyper parameters that seem to fit and you’ve gotten into that overfit world. Minimize how often you change it.</p>
<p><strong>Scott:</strong> Try to get into a good region and then don’t change it too much and then rely on your data to do the talking of shaping where the model should go. You could have too low of a learning rate too, where it just takes way too long. We’ve talked about this before, where you’re patience limited in deep learning because things take a while, there’s a lot of parameters, it uses a lot of computational power, and if you’re training models for days and it’s just only getting better very slowly, maybe your learning rate needs to be jacked up again.</p>
<p><strong>Scott:</strong> Again, that’s part of that search.</p>
<p><strong>Susan:</strong> That gets into one that I’ve re-learned recently, which is people pay a lot of attention to data, rightfully so. A lot of attention to model structure, rightfully so, but less attention to the optimizer that they use.</p>
<p><strong>Scott:</strong> I completely agree.</p>
<p><strong>Susan:</strong> There’s just an epic difference out there based off that. It’s hard to even begin with how important that selection is you forget about.</p>
<p><strong>Scott:</strong> Heuristically, it’s like, if I know what my error is and I know the errors I’ve been making in the past, how far should I project where I should go? You can think about this as a human too. If you’re walking around in the day and you keep hearing the same wrong thing, over and over, and you’re like, maybe I really need to go look into that and adjust it or something. It’s weighting all of that. How much of the past should I keep for now and use in the future? How much should I throw away? There’s a momentum side to that too.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976787/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/conifer-dawn-daylight-167698.jpg" alt="Alt"></p>
<p><strong>Susan:</strong> One way to think of it is this; going back to the classic forest problem. Dense forest and how do I get to low ground? The optimizer’s are like how fast you should be walking at any one given time?</p>
<p><strong>Scott:</strong> If it was all curvy right there, maybe you should do something else. But there are different programs there, basically, or different rules you should follow and that’s what picking your optimizer is.</p>
<p><strong>Susan:</strong> And how it adjusts that rate of walking as you go along. Those are big, huge, massive things there. What about the bottom end of a model? What kind of output errors have you run into here, Scott?</p>
<p><strong>Scott:</strong> A lot.</p>
<p><strong>Susan:</strong> I can give you my favorite one. If you got one right off the top, go for it.</p>
<p><strong>Scott:</strong> Not outputting good results is the biggest. Outputting all zeros when you’re loss function is all screwed up or whatever.</p>
<p><strong>Susan:</strong> My personal favorite is outputting something that’s useful in a training set, but not useful in the real world. Forgetting the difference between training and production. When you look at code and when you look at training models and stuff like that, you’re going to take this thing, you’re gonna wrap it in a loss function, you’re going to do all these different things. I remember building my first models, when I actually tried to apply them to a real world problem, I realized that was completely not useful. The training set that I had been training against and the outputs I had been using there were giving me these great numbers of loss.</p>
<p><strong>Scott:</strong> 99% accuracy.</p>
<p><strong>Susan:</strong> And accuracy and I was like, oh, and I was tweaking the various setting and all that stuff. Then I disconnected the model from all that apparatus and tried to do something with it.</p>
<p><strong>Scott:</strong> Yeah, feed it something real.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976788/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/box-1.jpg" alt="Alt"></p>
<p><em><a href="https://en.wikipedia.org/wiki/Black_box">Black box problem</a></em></p>
<p><strong>Susan:</strong> Suddenly I had no idea what it was actually doing. It just had this black box of data in, black box of data out, these metrics around it. You’ve always, always gotta keep in your mind that the whole point is not the training set, the whole point is not the accuracy number, the whole point is not all this apparatus over there, it’s when you finally get to production. What is that output supposed to look like and how is it useful?</p>
<p><strong>Susan:</strong> If you can’t take whatever output it’s got coming out there and make it useful, then you’ve gotta address that right away. It’s like the water through the pipes that we were talking about there, ‘cause water through the pipes on the training side and also water through the pipes on the production side.</p>
<p><strong>Scott:</strong> Does it get sensible outputs in production-land as well?</p>
<p><strong>Susan:</strong> Are you really able to take that 10,000 classes and do something with it or should have you stripped it down to those five because that’s all you cared about in production?</p>
<p><strong>Scott:</strong> I think some real nuts and bolts ones could help too. Like normalizing your input data.</p>
<p><strong>Susan:</strong> Oh, yeah.</p>
<h2 id="not-normalizing-your-input-data">Not normalizing your input data</h2>
<p><strong>Scott:</strong> What is normalizing your input data mean? It’s if you have an image, for example, and each pixel value goes from zero to 10,000, what scale should you keep it at? Should it be zero to 10,000 or should it be some other number? Typically machine learning algorithms are tuned to work well when you’re talking about negative one to one or zero to one, or something like that. You should normalize your data so that zero to 10,000 now is like zero to one, or something like that.</p>
<p><strong>Susan:</strong> It’s staggering what normalization can do for you. You just think about humans. Think about looking around your environment. The first physical step of light going into your body goes through a normalization layer and that is what’s your eye doing right now when it’s stares into a light versus stares into a dark room?</p>
<p><strong>Scott:</strong> It’s getting smaller and bigger.</p>
<p><strong>Susan:</strong> It shows you how important that step is that you physically do that. You do that with hearing too. It’s a loud environment, what do we do? We put earmuffs on. We’re trying to get all the sound normalized.</p>
<p><strong>Scott:</strong> Or it’s loud but it doesn’t seem that loud. Hearing is logarithmic. There’s tons of energy being pumped into your ear but it’s only a little bit louder. One more on hearing. You’ve got two ears and the ability to swivel your head, when you listen to something, when you really wanna listen to something, you actually swivel your head and that helps your hearing. That physical motion of adjusting the two microphones in your body to be better aligned with <a href="https://www.youtube.com/watch?v=Oai7HUqncAA&t=185">the source makes a big different to understanding</a>. Don’t forget that lesson when you’re in the machine learning world. Normalize your data, don’t forget those initial purely simple algorithmic filters you could apply to data beforehand.</p>
<p><strong>Scott:</strong> We haven’t discussed all that much, but essentially if you’re Amazon Alexa or Google Home, or something like that, they usually have seven or eight microphones on them. But humans do pretty good with just two.</p>
<p><strong>Susan:</strong> Sure, however humans have the ability to turn their heads.</p>
<p><strong>Scott:</strong> Yeah we can move around. You have these infinite amount of microphones.</p>
<p><strong>Susan:</strong> Yeah. We’ve got a lot of great things there.</p>
<h2 id="more-examples-of-common-pitfalls">More examples of common pitfalls</h2>
<p><strong>Scott:</strong> You have a deep learning model and in there are tons of parameters. Those parameters are initialized initially to what? What do I mean by initialize? I mean you fill up a bunch of matrices with numbers, what do you fill them with? Zeros? Is that a good answer?</p>
<p><strong>Susan:</strong> Bad answer.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976789/blog/what-are-the-top-mistakes-in-deep-learning-ai-show/Wolfsgrube.jpg" alt="Alt"></p>
<p><em><a href="https://en.wikipedia.org/wiki/Trapping_pit">Pitfalls</a> were one method of hunting animals and defeating enemies in war. Now it’s just a metaphor.</em></p>
<p><strong>Scott:</strong> Very bad answer.</p>
<p><strong>Susan:</strong> Very bad answer. As soon as you get to two layers everything goes out the window.</p>
<p><strong>Scott:</strong> Do you fill them with all the same number? One?</p>
<p><strong>Susan:</strong> Nope, it can’t be the same number. Even just pure randomness is not good. We’re not gonna get into the math and why, but look up some algorithms for how to fill up your parameters.</p>
<p><strong>Scott:</strong> You want it round and peaked with some variance.</p>
<p><strong>Susan:</strong> Based off of size, based off of number parameters.</p>
<p><strong>Scott:</strong> But random still, but off a distribution.</p>
<p><strong>Susan:</strong> Randomness is such an amazingly strong tool in the world. It’s just crazy and that one example shows that you don’t randomize things that literally will not learn.</p>
<p><strong>Scott:</strong> You can stitch together networks or loss functions or pieces of code that other people have written and you use part of it, but you use it incorrectly. In other words, it should have a soft max that it’s run through before it gets put in there, or it shouldn’t. Do you strip those pieces off or not? This is an impedance mismatch.</p>
<p><strong>Susan:</strong> To me this happens mostly going back to the training environment versus the production environment. You see this happen a lot. This is a basic one, luckily you fix it pretty quick just by changing that little piece of code. In training you may or may not need something at the end of your model that you’ll strip off for production. Keeping that in mind will help save some debugging steps. Ask yourself: Is this really what I’m gonna use for production or was this here just for training? Forgetting that you’ve got dropout set wrong, or something like that.</p>
<p><strong>Scott:</strong> You want it turned off for the evaluation inference probably.</p>
<p><strong>Susan:</strong> You need to make sure your model’s set for the right environment.</p>
<p><strong>Scott:</strong> Drop out shuts off part of the deep learning brain basically. It’s like oh, don’t you think you might want that in there?</p>
<p><strong>Susan:</strong> To get it in there if you’ve got a bunch of parameters. You can then randomly select a few of them to just ignore basically. The important thing about this is it tries to get the same level of output signal, so when you go to the production environment, you want those neurons to be there. You want those parameters to be useful, but if they were all turned on, suddenly your signal goes up, say, by 25% if you’ve got 25% drop out on there. You treat the evaluation side different than the train side. Some pretty important things to remember otherwise it’s just not as good as it could be.</p>
<iframe src="https://www.youtube.com/embed/KcrAkPNB8jc" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe>
<p><em>If you train your model based on speaker (narrator) then you have a narrator detector. You probably don’t want that.</em></p>
<p><strong>Scott:</strong> One of my favorite things to do though is overfitting. Which is normally a bad thing, but when you’re in the water through the pipes stage, where hey, I’ve finally got my data going, I finally got my model built, I finally got some kind of output and I can verify that it’s coming now. Now, take a really small training data set, maybe just one example or maybe 100, or maybe eight or whatever and train the model and make sure that you can very quickly get 100% accurate.</p>
<p><strong>Susan:</strong> You wanna see that curve of learning.</p>
<p><strong>Scott:</strong> Your error just drops like a rock because the model now has so many parameters it’s able to just focus on getting those correct. All that it’s doing at that point is memorizing, but if it can’t memorize those eight things, it’s probably not gonna do anything else.</p>
<p><strong>Susan:</strong> That helps you prove that your loss function is working. It helps you prove you have some inputs and outputs there, at least in the right realm. It doesn’t prove that you will have a good model structure.</p>
<p><strong>Scott:</strong> No, you could have an awful model structure, but at least it has some hope.</p>
<p><strong>Susan:</strong> That’s your water through the pipes basic all the plumbing is involved here is working here. There’s a ton of common, big, huge pitfalls. What’s is the biggest one overall?</p>
<p><strong>Scott:</strong> For me, I think the biggest one is like deep learning and hey, you can do anything. And it’s like, okay cool, not anything, but there are a lot of things that you can do that you couldn’t do before. Or it can do it better or something like that.</p>
<p>They think, “I’m gonna go to TensorFlow and then I’m gonna solve my problem.” If that’s your thought process, you gotta back up a little bit and think we need to take baby steps here because you’re not going to just go download a model or an example or train it with a couple days of toying around and get a real production thing done. It’s just not gonna happen.</p>
<p>Even from the engineering standpoint, you’re not gonna do that. From the model building standpoint, you’re not gonna get there basically unless it’s something super simple you could’ve done with normal stats or machine learning.</p>
<h2 id="the-process-side-of-the-house-is-to-me-the-biggest-problem-here">The process side of the house is to me the biggest problem here.</h2>
<p><strong>Susan:</strong> Did you analyze the problem correctly from the start? This is any engineering problem on the planet.</p>
<p><strong>Scott:</strong> This takes budgeting the appropriate amount of time and the appropriate amount of time is at least days, and probably weeks, to get a good look at that problem. It might be months.</p>
<p><strong>Susan:</strong> That might actually be the biggest pitfall in deep learning, is the assumption that it’ll only be a day or two.</p>
<p><strong>Scott:</strong> Yeah. Yeah. A day or two minimum. That happens frequently.</p>
<p><strong>Susan:</strong> Oh, that’s no problem. I’ll have something in a week.</p>
<p><strong>Scott:</strong> Yeah, “Oh weird. It’s not doing what I thought it would do. But what if I just do this little trick?” Two days later: “Huh, okay,” and then eight days later, “Huh.”</p>
<p><strong>Susan:</strong> It’s a big time thing, that’s for sure.</p>`;
}, "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-are-the-top-mistakes-in-deep-learning-ai-show/index.md");

export { compiledContent, $$Index as default, frontmatter, metadata, rawContent };
