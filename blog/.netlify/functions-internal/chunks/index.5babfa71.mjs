import { c as createAstro, a as createComponent, r as renderTemplate, b as renderHead } from '../entry.mjs';
import Slugger from 'github-slugger';
import '@astrojs/netlify/netlify-functions.js';
import 'preact';
import 'preact-render-to-string';
import 'vue';
import 'vue/server-renderer';
import 'html-escaper';
import 'node-html-parser';
import 'axios';
/* empty css                           *//* empty css                           *//* empty css                           */import '@storyblok/js';
/* empty css                           *//* empty css                          */import 'clone-deep';
import 'slugify';
import 'shiki';
/* empty css                           */import 'camelcase';
import '@astrojs/rss';
/* empty css                           */import 'mime';
import 'cookie';
import 'kleur/colors';
import 'string-width';
import 'path-browserify';
import 'path-to-regexp';

const metadata = { "headings": [{ "depth": 2, "slug": "what-is-deepspeech", "text": "What is DeepSpeech?" }, { "depth": 2, "slug": "set-up-for-local-speech-to-text-with-deepspeech", "text": "Set Up for Local Speech to Text with DeepSpeech" }, { "depth": 2, "slug": "file-handler-for-deepspeech-speech-transcription", "text": "File Handler for DeepSpeech Speech Transcription" }, { "depth": 3, "slug": "reading-audio-data-from-a-wav-file", "text": "Reading Audio Data from a WAV file" }, { "depth": 3, "slug": "writing-audio-data-to-a-wav-file", "text": "Writing Audio Data to a WAV file" }, { "depth": 3, "slug": "creating-frames-of-audio-data-for-deepspeech-to-transcribe", "text": "Creating Frames of Audio Data for DeepSpeech to Transcribe" }, { "depth": 3, "slug": "collecting-voice-activated-frames-for-speech-to-text-with-deepspeech", "text": "Collecting Voice Activated Frames for Speech to Text with DeepSpeech" }, { "depth": 2, "slug": "transcribe-speech-to-text-for-wav-file-with-deepspeech", "text": "Transcribe Speech to Text for WAV file with DeepSpeech" }, { "depth": 3, "slug": "pick-which-deepspeech-model-to-use", "text": "Pick Which DeepSpeech Model to Use" }, { "depth": 3, "slug": "speech-to-text-on-an-audio-file-with-deepspeech", "text": "Speech to Text on an Audio File with DeepSpeech" }, { "depth": 3, "slug": "deepspeech-model-graph-creator-function", "text": "DeepSpeech Model Graph Creator Function" }, { "depth": 3, "slug": "voice-activation-detection-to-create-segments-for-speech-to-text", "text": "Voice Activation Detection to Create Segments for Speech to Text" }, { "depth": 2, "slug": "deepspeech-cli-for-real-time-and-asynchronous-speech-to-text", "text": "DeepSpeech CLI for Real Time and Asynchronous Speech to Text" }, { "depth": 3, "slug": "reading-arguments-for-deepspeech-speech-to-text", "text": "Reading Arguments for DeepSpeech Speech to Text" }, { "depth": 3, "slug": "using-deepspeech-for-real-time-or-asynchronous-speech-recognition", "text": "Using DeepSpeech for Real Time or Asynchronous Speech Recognition" }, { "depth": 2, "slug": "summary", "text": "Summary" }], "source": "\r\nNo, we\u2019re not talking about you Cthulhu. This is a different type of DeepSpeech. The DeepSpeech we\u2019re talking about today is a Python speech to text library. Speech to text is part of [Natural Language Processing (NLP)](https://pythonalgos.com/?p=1436). Automated speech recognition, or ASR, started out as an offshoot of NLP in the 1990s.\r\n\r\nToday, there are tons of audio libraries that can help you [manipulate audio data](https://blog.deepgram.com/best-python-audio-manipulation-tools/) such as DeepSpeech and [PyTorch](https://blog.deepgram.com/pytorch-intro-with-torchaudio/). In this post, we will be using DeepSpeech to do both asynchronous and real time speech transcription. We will cover:\r\n\r\n*   [What is DeepSpeech?](#what-is-deepspeech)\r\n*   [Set Up for Local Speech to Text with DeepSpeech](#set-up-for-local-speech-to-text-with-deepspeech)\r\n*   [File Handler for DeepSpeech Speech Transcription](#file-handler-for-deepspeech-speech-transcription)\r\n*   [Transcribe Speech to Text for WAV file with DeepSpeech](#transcribe-speech-to-text-for-wav-file-with-deepspeech)\r\n*   [DeepSpeech CLI for Real-Time and Asynchronous Speech to Text](#deepspeech-cli-for-real-time-and-asynchronous-speech-to-text)\r\n*   [Summary](#summary)\r\n\r\n## What is DeepSpeech?\r\n\r\nDeepSpeech is an open source Python library that enables us to build automatic speech recognition systems. It is based on Baidu\u2019s 2014 paper titled [Deep Speech: Scaling up end-to-end speech recognition](https://arxiv.org/abs/1412.5567).\r\n\r\nThe initial proposal for Deep Speech was simple - let\u2019s create a speech recognition system based entirely off of deep learning. The paper describes a solution using RNNs trained with multiple GPUs with no concept of phonemes. The authors, Hannun et al., show that their solution also outperformed the existing solutions at the time and was more robust to background noise without a need for filtering.\r\n\r\nSince then, Mozilla has been the one in charge of maintaining the open source Python package for DeepSpeech. Before moving on, it\u2019s important to note that DeepSpeech is not yet compatible with Python 3.10 nor some more recent versions of \\*nix kernels. I suggest using a virtual machine or Docker container to develop with DeepSpeech on unsupported OSes.\r\n\r\n## Set Up for Local Speech to Text with DeepSpeech\r\n\r\nTo use DeepSpeech, we have to install a few libraries. We need `deepspeech`, `numpy`, and `webrtcvad`. We can install all of these by running `pip install deepspeech numpy webrtcvad`. The `webrtcvad` library is the voice activity detection (VAD) library developed by Google for WebRTC (real time communication).\r\n\r\nFor the asynchronous transcription, we\u2019re going to need three files. One file to handle interaction with WAV data, one file to transcribe speech to text on a WAV file, and one to use these two in the command line. We will also be using a pretrained DeepSpeech model and scorer. You can download their model by running the following lines in your terminal:\r\n\r\n```bash\r\nwget https://github.com/mozilla/DeepSpeech/releases/download/v0.9.3/deepspeech-0.9.3-models.pbmm\r\nwget https://github.com/mozilla/DeepSpeech/releases/download/v0.9.3/deepspeech-0.9.3-models.scorer\r\n```\r\n\r\n## File Handler for DeepSpeech Speech Transcription\r\n\r\nThe first file we create is the WAV handling file. This file should be named something like `wav_handler.py`. We import three built-in libraries to do this, `wave`, `collections`, and `contextlib`. We create four functions and one class. We need one function each to read WAV files, write WAV files, create Frames, and detect voice activity. Our one class represents individual frames in the WAV file.\r\n\r\n### Reading Audio Data from a WAV file\r\n\r\nLet\u2019s start with creating a function to read WAV files. This function will take an input, this input is the path to a WAV file. The file will use the `contextlib` library to open the WAV file and read in the contents as bytes. Next, we run multiple asserts on the WAV file - it must have one channel, have a sample width of 2, have a sample rate of 8, 16, or 32 kHz.\r\n\r\nOnce we have asserted that the WAV file is in the right format for processing, we extract the frames. Next, we extract the pcm data from the frames and the duration from the metadata. Finally, we return the PCM data, the sample rate, and the duration.\r\n\r\n```py\r\nimport collections\r\nimport contextlib\r\nimport wave\r\n\r\n\"\"\"Reads a .wav file.\r\nInput: path to a .wav file\r\nOutput: tuple of pcm data, sample rate, and duration\r\n\"\"\"\r\ndef read_wave(path):\r\n   with contextlib.closing(wave.open(path, 'rb')) as wf:\r\n       num_channels = wf.getnchannels()\r\n       assert num_channels == 1\r\n       sample_width = wf.getsampwidth()\r\n       assert sample_width == 2\r\n       sample_rate = wf.getframerate()\r\n       assert sample_rate in (8000, 16000, 32000)\r\n       frames = wf.getnframes()\r\n       pcm_data = wf.readframes(frames)\r\n       duration = frames / sample_rate\r\n       return pcm_data, sample_rate, duration\r\n```\r\n\r\n### Writing Audio Data to a WAV file\r\n\r\nNow let\u2019s create the function to write audio data to a WAV file. This function requires three parameters, the path to a file to write to, the audio data, and the sample rate. This function writes a WAV file in the same way that the read function asserts its parameters. All we do is here is set the channels, sample width, and frame rate and then write the audio frames.\r\n\r\n```py\r\n\"\"\"Writes a .wav file.\r\nInput: path to new .wav file, PCM audio data, and sample rate.\r\nOutput: a .wav file\r\n\"\"\"\r\ndef write_wave(path, audio, sample_rate):\r\n   with contextlib.closing(wave.open(path, 'wb')) as wf:\r\n       wf.setnchannels(1)\r\n       wf.setsampwidth(2)\r\n       wf.setframerate(sample_rate)\r\n       wf.writeframes(audio)\r\n```\r\n\r\n### Creating Frames of Audio Data for DeepSpeech to Transcribe\r\n\r\nWe\u2019re going to create a class called `Frame` to hold some information to represent our audio data and make it easier to handle. This object requires three parameters to be created: the bytes, the timestamp in the audio file, and the duration of the `Frame`.\r\n\r\nWe also need to create a function to create frames. You can think of this function as a frame [generator](https://docs.google.com/document/d/1JwfuaBrao_pZjLBy6isPdqv_hndQnetCJ8MjYWlLtlI/edit?pli=1\\&disco=AAAAddu7kIo) or a frame factory that returns an iterator. This function requires three parameters: the frame duration in milliseconds, the audio data, and the sample rate.\r\n\r\nThis function starts by deriving an interval of frames from the passed in sample rate and frame duration in milliseconds. We start at an offset and timestamp of 0. We also create a duration constant equal to the number of frames in a second.\r\n\r\nWhile the current offset can be incremented by the interval constant and be within the number of frames of the audio, we generate a `Frame` for each interval and then increment the timestamp and offset appropriately.\r\n\r\n```py\r\n\"\"\"Represents a \"frame\" of audio data.\r\nRequires the number of byes, the timestamp of the frame, and the duration on init\"\"\"\r\nclass Frame(object):\r\n   def __init__(self, bytes, timestamp, duration):\r\n       self.bytes = bytes\r\n       self.timestamp = timestamp\r\n       self.duration = duration\r\n\r\n\"\"\"Generates audio frames from PCM audio data.\r\nInput: the desired frame duration in milliseconds, the PCM data, and\r\nthe sample rate.\r\nYields/Generates: Frames of the requested duration.\r\n\"\"\"\r\ndef frame_generator(frame_duration_ms, audio, sample_rate):\r\n   n = int(sample_rate * (frame_duration_ms / 1000.0) * 2)\r\n   offset = 0\r\n   timestamp = 0.0\r\n   duration = (float(n) / sample_rate) / 2.0\r\n   while offset + n < len(audio):\r\n       yield Frame(audio[offset:offset + n], timestamp, duration)\r\n       timestamp += duration\r\n       offset += n\r\n```\r\n\r\n### Collecting Voice Activated Frames for Speech to Text with DeepSpeech\r\n\r\nNext, let\u2019s create a function to collect all the frames that contain voice. This function requires a sample rate, the frame duration in milliseconds, the padding duration in milliseconds, a voice activation detector (VAD) from `webrtcvad`, and the audio data frames.\r\n\r\nThe VAD algorithm uses a padded ring buffer and checks to see what percentage of the frames in the window are voiced. When the window reaches a 90% voiced frame rate, the VAD triggers and begins yielding audio frames. While generating frames, if the percentage of voiced audio data in the frame drops below 10%, it will stop generating frames.\r\n\r\n```py\r\n\"\"\"Filters out non-voiced audio frames.\r\nGiven a webrtcvad.Vad and a source of audio frames, yields only\r\nthe voiced audio.\r\nArguments:\r\nsample_rate - The audio sample rate, in Hz.\r\nframe_duration_ms - The frame duration in milliseconds.\r\npadding_duration_ms - The amount to pad the window, in milliseconds.\r\nvad - An instance of webrtcvad.Vad.\r\nframes - a source of audio frames (sequence or generator).\r\nReturns: A generator that yields PCM audio data.\r\n\"\"\"\r\ndef vad_collector(sample_rate, frame_duration_ms,\r\n                 padding_duration_ms, vad, frames):\r\n   num_padding_frames = int(padding_duration_ms / frame_duration_ms)\r\n   # We use a deque for our sliding window/ring buffer.\r\n   ring_buffer = collections.deque(maxlen=num_padding_frames)\r\n   # We have two states: TRIGGERED and NOTTRIGGERED. We start in the\r\n   # NOTTRIGGERED state.\r\n   triggered = False\r\n\r\n   voiced_frames = []\r\n   for frame in frames:\r\n       is_speech = vad.is_speech(frame.bytes, sample_rate)\r\n\r\n       if not triggered:\r\n           ring_buffer.append((frame, is_speech))\r\n           num_voiced = len([f for f, speech in ring_buffer if speech])\r\n           # If we're NOTTRIGGERED and more than 90% of the frames in\r\n           # the ring buffer are voiced frames, then enter the\r\n           # TRIGGERED state.\r\n           if num_voiced > 0.9 * ring_buffer.maxlen:\r\n               triggered = True\r\n               # We want to yield all the audio we see from now until\r\n               # we are NOTTRIGGERED, but we have to start with the\r\n               # audio that's already in the ring buffer.\r\n               for f, s in ring_buffer:\r\n                   voiced_frames.append(f)\r\n               ring_buffer.clear()\r\n       else:\r\n           # We're in the TRIGGERED state, so collect the audio data\r\n           # and add it to the ring buffer.\r\n           voiced_frames.append(frame)\r\n           ring_buffer.append((frame, is_speech))\r\n           num_unvoiced = len([f for f, speech in ring_buffer if not speech])\r\n           # If more than 90% of the frames in the ring buffer are\r\n           # unvoiced, then enter NOTTRIGGERED and yield whatever\r\n           # audio we've collected.\r\n           if num_unvoiced > 0.9 * ring_buffer.maxlen:\r\n               triggered = False\r\n               yield b''.join([f.bytes for f in voiced_frames])\r\n               ring_buffer.clear()\r\n               voiced_frames = []\r\n   if triggered:\r\n       pass\r\n   # If we have any leftover voiced audio when we run out of input,\r\n   # yield it.\r\n   if voiced_frames:\r\n       yield b''.join([f.bytes for f in voiced_frames])\r\n```\r\n\r\n## Transcribe Speech to Text for WAV file with DeepSpeech\r\n\r\nWe\u2019re going to create a new file for this section. This file should be named something like `wav_transcriber.py`. This layer completely abstracts out WAV handling from the CLI (which we create below). We use these functions to call DeepSpeech on the audio data and transcribe it.\r\n\r\n### Pick Which DeepSpeech Model to Use\r\n\r\nThe first function we create in this file is the function to load up the model and scorer for DeepSpeech to run speech to text with. This function takes two parameters, the models graph, which we create a function to produce below, and the path to the scorer file. All it does is load the model from the graph and enable the use of the scorer. This function returns a DeepSpeech object.\r\n\r\n```py\r\nimport glob\r\nimport webrtcvad\r\nimport logging\r\nimport wav_handler\r\nfrom deepspeech import Model\r\nfrom timeit import default_timer as timer\r\n\r\n'''\r\nLoad the pre-trained model into the memory\r\n@param models: Output Graph Protocol Buffer file\r\n@param scorer: Scorer file\r\n@Retval\r\nReturns a DeepSpeech Object\r\n'''\r\ndef load_model(models, scorer):\r\n   ds = Model(models)\r\n   ds.enableExternalScorer(scorer)\r\n   return ds\r\n```\r\n\r\n### Speech to Text on an Audio File with DeepSpeech\r\n\r\nThis function is the one that does the actual speech recognition. It takes three inputs, a DeepSpeech model, the audio data, and the sample rate.\r\n\r\nWe begin by setting the time to 0 and calculating the length of the audio. All we really have to do is call the DeepSpeech model\u2019s `stt` function to do our own `stt` function. We pass the audio file to the `stt` function and return the output.\r\n\r\n```py\r\n'''\r\nRun Inference on input audio file\r\n@param ds: Deepspeech object\r\n@param audio: Input audio for running inference on\r\n@param fs: Sample rate of the input audio file\r\n@Retval:\r\nReturns a list [Inference, Inference Time, Audio Length]\r\n'''\r\ndef stt(ds, audio, fs):\r\n   inference_time = 0.0\r\n   audio_length = len(audio) * (1 / fs)\r\n\r\n   # Run Deepspeech\r\n   output = ds.stt(audio)\r\n\r\n   return output\r\n```\r\n\r\n### DeepSpeech Model Graph Creator Function\r\n\r\nThis is the function that creates the model graph for the `load_model` function we created a couple sections above. This function takes the path to a directory. From that directory, it looks for files with the DeepSpeech model extension, `pbmm` and the DeepSpeech scorer file extension, `.scorer`. Then, it returns both of those values.\r\n\r\n```py\r\n'''\r\nResolve directory path for the models and fetch each of them.\r\n@param dirName: Path to the directory containing pre-trained models\r\n@Retval:\r\nRetunns a tuple containing each of the model files (pb, scorer)\r\n'''\r\ndef resolve_models(dirName):\r\n   pb = glob.glob(dirName + \"/*.pbmm\")[0]\r\n   logging.debug(\"Found Model: %s\" % pb)\r\n\r\n   scorer = glob.glob(dirName + \"/*.scorer\")[0]\r\n   logging.debug(\"Found scorer: %s\" % scorer)\r\n\r\n   return pb, scorer\r\n```\r\n\r\n### Voice Activation Detection to Create Segments for Speech to Text\r\n\r\nThe last function in our WAV transcription file generates segments of text that contain voice. We use the WAV handler file we created earlier and `webrtcvad` to do the heavy lifting. This function requires two parameters: a WAV file and an integer value from 0 to 3 representing how aggressively we want to filter out non-voice activity.\r\n\r\nWe call the `read_wave` function from the `wav_handler.py` file we created earlier and imported above to get the audio data, sample rate, and audio length. We then assert that the sample rate is 16kHz before moving on to create a VAD object. Next, we call the frame generator from `wav_handler`.\r\n\r\nWe convert the generated iterator to a list which we pass to the `vad_collector` function from `wav_handler` along with the sample rate, frame duration (30 ms), padding duration (300 ms), and VAD object. Finally, we return the collected VAD segments along with the sample rate and audio length.\r\n\r\n```py\r\n'''\r\nGenerate VAD segments. Filters out non-voiced audio frames.\r\n@param waveFile: Input wav file to run VAD on.0\r\n@Retval:\r\nReturns tuple of\r\n   segments: a bytearray of multiple smaller audio frames\r\n             (The longer audio split into mutiple smaller one's)\r\n   sample_rate: Sample rate of the input audio file\r\n   audio_length: Duraton of the input audio file\r\n'''\r\ndef vad_segment_generator(wavFile, aggressiveness):\r\n   audio, sample_rate, audio_length = wav_handler.read_wave(wavFile)\r\n   assert sample_rate == 16000, \"Only 16000Hz input WAV files are supported for now!\"\r\n   vad = webrtcvad.Vad(int(aggressiveness))\r\n   frames = wav_handler.frame_generator(30, audio, sample_rate)\r\n   frames = list(frames)\r\n   segments = wav_handler.vad_collector(sample_rate, 30, 300, vad, frames)\r\n\r\n   return segments, sample_rate, audio_length\r\n```\r\n\r\n## DeepSpeech CLI for Real Time and Asynchronous Speech to Text\r\n\r\nEverything is set up to transcribe audio data with DeepSpeech via pretrained models. Now, let\u2019s look at how to turn the functionality we created above into a command line interface for real time and asynchronous speech to text. We start by importing a bunch of libraries for operating with the command line - `sys`, `os`, `logging`, `argparse`, `subprocess`, and `shlex`. We also need to import `numpy` and the `wav_transcriber` we made above to work with the audio data.\r\n\r\n### Reading Arguments for DeepSpeech Speech to Text\r\n\r\nWe create a main function that takes one parameter - `args`. These are the arguments passed in through the command line. We use the `argparse` libraries to parse the arguments sent in. We also create helpful tips on how to use each one.\r\n\r\nWe use `aggressive` to determine how aggressively we want to filter. `audio` directs us to the audio file path. `model` points us to the directory containing the model and scorer. Finally, `stream` dictates whether or not we are streaming audio. Neither `stream` nor `audio` is required, but one or the other must be present.\r\n\r\n```py\r\nimport sys\r\nimport os\r\nimport logging\r\nimport argparse\r\nimport subprocess\r\nimport shlex\r\nimport numpy as np\r\nimport wav_transcriber\r\n\r\n# Debug helpers\r\nlogging.basicConfig(stream=sys.stderr, level=logging.DEBUG)\r\n\r\ndef main(args):\r\n   parser = argparse.ArgumentParser(description='Transcribe long audio files using webRTC VAD or use the streaming interface')\r\n   parser.add_argument('--aggressive', type=int, choices=range(4), required=False,\r\n                       help='Determines how aggressive filtering out non-speech is. (Interger between 0-3)')\r\n   parser.add_argument('--audio', required=False,\r\n                       help='Path to the audio file to run (WAV format)')\r\n   parser.add_argument('--model', required=True,\r\n                       help='Path to directory that contains all model files (output_graph and scorer)')\r\n   parser.add_argument('--stream', required=False, action='store_true',\r\n                       help='To use deepspeech streaming interface')\r\n   args = parser.parse_args()\r\n   if args.stream is True:\r\n       print(\"Opening mic for streaming\")\r\n   elif args.audio is not None:\r\n       logging.debug(\"Transcribing audio file @ %s\" % args.audio)\r\n   else:\r\n       parser.print_help()\r\n       parser.exit()\r\n```\r\n\r\n### Using DeepSpeech for Real Time or Asynchronous Speech Recognition\r\n\r\nThis is still inside the main function we started above. Once we parse all the arguments, we load up DeepSpeech. First, we get the directory containing the models. Next, we call the `wav_transcriber` to resolve and load the models.\r\n\r\nIf we pass the path to an audio data file in the command line, then we will run asynchronous speech recognition. The first thing we do for that is call the VAD segment generator to generate the VAD segments and get the sample rate and audio length. Next, we open up a text file to transcribe to.\r\n\r\nFor each of the enumerated segments, we will process each chunk by using `numpy` to pull the segment from the buffer and the speech to text function from `wav_transcriber` to do the speech to text functionality. We write to the text file until we run out of audio segments.\r\n\r\nIf we pass stream instead of audio, then we open up the mic to stream audio data in. If you don\u2019t need real time automatic speech recognition, then you can ignore this part. First, we have to spin up a subprocess to open up a mic to stream in real time just like we did with PyTorch local speech recognition.\r\n\r\nWe use the `subprocess` and `shlex` libraries to open the mic to stream voice audio until we shut it down. The model will read 512 bytes of audio data at a time and transcribe it.\r\n\r\n```py\r\n# Point to a path containing the pre-trained models & resolve ~ if used\r\ndirName = os.path.expanduser(args.model)\r\n\r\n# Resolve all the paths of model files\r\noutput_graph, scorer = wav_transcriber.resolve_models(dirName)\r\n\r\n# Load output_graph, alpahbet and scorer\r\nmodel_retval = wav_transcriber.load_model(output_graph, scorer)\r\n\r\nif args.audio is not None:\r\n    # Run VAD on the input file\r\n    waveFile = args.audio\r\n    segments, sample_rate, audio_length = wav_transcriber.vad_segment_generator(waveFile, args.aggressive)\r\n    f = open(waveFile.rstrip(\".wav\") + \".txt\", 'w')\r\n    logging.debug(\"Saving Transcript @: %s\" % waveFile.rstrip(\".wav\") + \".txt\")\r\n\r\n    for i, segment in enumerate(segments):\r\n        # Run deepspeech on the chunk that just completed VAD\r\n        logging.debug(\"Processing chunk %002d\" % (i,))\r\n        audio = np.frombuffer(segment, dtype=np.int16)\r\n        output = wav_transcriber.stt(model_retval, audio, sample_rate)\r\n        logging.debug(\"Transcript: %s\" % output)\r\n\r\n        f.write(output + \" \")\r\n\r\n    # Summary of the files processed\r\n    f.close()\r\n\r\nelse:\r\n    sctx = model_retval.createStream()\r\n    subproc = subprocess.Popen(shlex.split('rec -q -V0 -e signed -L -c 1 -b 16 -r 16k -t raw - gain -2'),\r\n                                stdout=subprocess.PIPE,\r\n                                bufsize=0)\r\n    print('You can start speaking now. Press Control-C to stop recording.')\r\n\r\n    try:\r\n        while True:\r\n            data = subproc.stdout.read(512)\r\n            sctx.feedAudioContent(np.frombuffer(data, np.int16))\r\n    except KeyboardInterrupt:\r\n        print('Transcription: ', sctx.finishStream())\r\n        subproc.terminate()\r\n        subproc.wait()\r\n\r\nif __name__ == '__main__':\r\n   main(sys.argv[1:])\r\n```\r\n\r\n## Summary\r\n\r\nWe started this post out with a high level view of DeepSpeech, an open source speech recognition software. It was inspired by a 2014 paper from Baidu and is currently maintained by Mozilla.\r\n\r\nAfter a basic introduction, we stepped into a guide on how to use DeepSpeech to locally transcribe speech to text. While it may have been possible to create all of this code in one document, we opted for a modular approach with principles of software engineering in mind.\r\n\r\nWe created three modules. One to handle WAV files, which are the audio data files that we can use DeepSpeech to transcribe. One to transcribe from WAV files, and one more file to create a command line interface to use DeepSpeech. Our CLI allows us to pass in options to pick if we want to do real time speech recognition or run speech recognition on an existing WAV audio file.\r\n\r\n        ", "html": '<p>No, we\u2019re not talking about you Cthulhu. This is a different type of DeepSpeech. The DeepSpeech we\u2019re talking about today is a Python speech to text library. Speech to text is part of <a href="https://pythonalgos.com/?p=1436">Natural Language Processing (NLP)</a>. Automated speech recognition, or ASR, started out as an offshoot of NLP in the 1990s.</p>\n<p>Today, there are tons of audio libraries that can help you <a href="https://blog.deepgram.com/best-python-audio-manipulation-tools/">manipulate audio data</a> such as DeepSpeech and <a href="https://blog.deepgram.com/pytorch-intro-with-torchaudio/">PyTorch</a>. In this post, we will be using DeepSpeech to do both asynchronous and real time speech transcription. We will cover:</p>\n<ul>\n<li><a href="#what-is-deepspeech">What is DeepSpeech?</a></li>\n<li><a href="#set-up-for-local-speech-to-text-with-deepspeech">Set Up for Local Speech to Text with DeepSpeech</a></li>\n<li><a href="#file-handler-for-deepspeech-speech-transcription">File Handler for DeepSpeech Speech Transcription</a></li>\n<li><a href="#transcribe-speech-to-text-for-wav-file-with-deepspeech">Transcribe Speech to Text for WAV file with DeepSpeech</a></li>\n<li><a href="#deepspeech-cli-for-real-time-and-asynchronous-speech-to-text">DeepSpeech CLI for Real-Time and Asynchronous Speech to Text</a></li>\n<li><a href="#summary">Summary</a></li>\n</ul>\n<h2 id="what-is-deepspeech">What is DeepSpeech?</h2>\n<p>DeepSpeech is an open source Python library that enables us to build automatic speech recognition systems. It is based on Baidu\u2019s 2014 paper titled <a href="https://arxiv.org/abs/1412.5567">Deep Speech: Scaling up end-to-end speech recognition</a>.</p>\n<p>The initial proposal for Deep Speech was simple - let\u2019s create a speech recognition system based entirely off of deep learning. The paper describes a solution using RNNs trained with multiple GPUs with no concept of phonemes. The authors, Hannun et al., show that their solution also outperformed the existing solutions at the time and was more robust to background noise without a need for filtering.</p>\n<p>Since then, Mozilla has been the one in charge of maintaining the open source Python package for DeepSpeech. Before moving on, it\u2019s important to note that DeepSpeech is not yet compatible with Python 3.10 nor some more recent versions of *nix kernels. I suggest using a virtual machine or Docker container to develop with DeepSpeech on unsupported OSes.</p>\n<h2 id="set-up-for-local-speech-to-text-with-deepspeech">Set Up for Local Speech to Text with DeepSpeech</h2>\n<p>To use DeepSpeech, we have to install a few libraries. We need <code is:raw>deepspeech</code>, <code is:raw>numpy</code>, and <code is:raw>webrtcvad</code>. We can install all of these by running <code is:raw>pip install deepspeech numpy webrtcvad</code>. The <code is:raw>webrtcvad</code> library is the voice activity detection (VAD) library developed by Google for WebRTC (real time communication).</p>\n<p>For the asynchronous transcription, we\u2019re going to need three files. One file to handle interaction with WAV data, one file to transcribe speech to text on a WAV file, and one to use these two in the command line. We will also be using a pretrained DeepSpeech model and scorer. You can download their model by running the following lines in your terminal:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">wget https://github.com/mozilla/DeepSpeech/releases/download/v0.9.3/deepspeech-0.9.3-models.pbmm</span></span>\n<span class="line"><span style="color: #C9D1D9">wget https://github.com/mozilla/DeepSpeech/releases/download/v0.9.3/deepspeech-0.9.3-models.scorer</span></span></code></pre>\n<h2 id="file-handler-for-deepspeech-speech-transcription">File Handler for DeepSpeech Speech Transcription</h2>\n<p>The first file we create is the WAV handling file. This file should be named something like <code is:raw>wav_handler.py</code>. We import three built-in libraries to do this, <code is:raw>wave</code>, <code is:raw>collections</code>, and <code is:raw>contextlib</code>. We create four functions and one class. We need one function each to read WAV files, write WAV files, create Frames, and detect voice activity. Our one class represents individual frames in the WAV file.</p>\n<h3 id="reading-audio-data-from-a-wav-file">Reading Audio Data from a WAV file</h3>\n<p>Let\u2019s start with creating a function to read WAV files. This function will take an input, this input is the path to a WAV file. The file will use the <code is:raw>contextlib</code> library to open the WAV file and read in the contents as bytes. Next, we run multiple asserts on the WAV file - it must have one channel, have a sample width of 2, have a sample rate of 8, 16, or 32 kHz.</p>\n<p>Once we have asserted that the WAV file is in the right format for processing, we extract the frames. Next, we extract the pcm data from the frames and the duration from the metadata. Finally, we return the PCM data, the sample rate, and the duration.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> collections</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> contextlib</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> wave</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;Reads a .wav file.</span></span>\n<span class="line"><span style="color: #A5D6FF">Input: path to a .wav file</span></span>\n<span class="line"><span style="color: #A5D6FF">Output: tuple of pcm data, sample rate, and duration</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;</span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">read_wave</span><span style="color: #C9D1D9">(path):</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">with</span><span style="color: #C9D1D9"> contextlib.closing(wave.open(path, </span><span style="color: #A5D6FF">&#39;rb&#39;</span><span style="color: #C9D1D9">)) </span><span style="color: #FF7B72">as</span><span style="color: #C9D1D9"> wf:</span></span>\n<span class="line"><span style="color: #C9D1D9">       num_channels </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wf.getnchannels()</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">assert</span><span style="color: #C9D1D9"> num_channels </span><span style="color: #FF7B72">==</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">1</span></span>\n<span class="line"><span style="color: #C9D1D9">       sample_width </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wf.getsampwidth()</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">assert</span><span style="color: #C9D1D9"> sample_width </span><span style="color: #FF7B72">==</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">2</span></span>\n<span class="line"><span style="color: #C9D1D9">       sample_rate </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wf.getframerate()</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">assert</span><span style="color: #C9D1D9"> sample_rate </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> (</span><span style="color: #79C0FF">8000</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">16000</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">32000</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">       frames </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wf.getnframes()</span></span>\n<span class="line"><span style="color: #C9D1D9">       pcm_data </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wf.readframes(frames)</span></span>\n<span class="line"><span style="color: #C9D1D9">       duration </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> frames </span><span style="color: #FF7B72">/</span><span style="color: #C9D1D9"> sample_rate</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> pcm_data, sample_rate, duration</span></span></code></pre>\n<h3 id="writing-audio-data-to-a-wav-file">Writing Audio Data to a WAV file</h3>\n<p>Now let\u2019s create the function to write audio data to a WAV file. This function requires three parameters, the path to a file to write to, the audio data, and the sample rate. This function writes a WAV file in the same way that the read function asserts its parameters. All we do is here is set the channels, sample width, and frame rate and then write the audio frames.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;Writes a .wav file.</span></span>\n<span class="line"><span style="color: #A5D6FF">Input: path to new .wav file, PCM audio data, and sample rate.</span></span>\n<span class="line"><span style="color: #A5D6FF">Output: a .wav file</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;</span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">write_wave</span><span style="color: #C9D1D9">(path, audio, sample_rate):</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">with</span><span style="color: #C9D1D9"> contextlib.closing(wave.open(path, </span><span style="color: #A5D6FF">&#39;wb&#39;</span><span style="color: #C9D1D9">)) </span><span style="color: #FF7B72">as</span><span style="color: #C9D1D9"> wf:</span></span>\n<span class="line"><span style="color: #C9D1D9">       wf.setnchannels(</span><span style="color: #79C0FF">1</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">       wf.setsampwidth(</span><span style="color: #79C0FF">2</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">       wf.setframerate(sample_rate)</span></span>\n<span class="line"><span style="color: #C9D1D9">       wf.writeframes(audio)</span></span></code></pre>\n<h3 id="creating-frames-of-audio-data-for-deepspeech-to-transcribe">Creating Frames of Audio Data for DeepSpeech to Transcribe</h3>\n<p>We\u2019re going to create a class called <code is:raw>Frame</code> to hold some information to represent our audio data and make it easier to handle. This object requires three parameters to be created: the bytes, the timestamp in the audio file, and the duration of the <code is:raw>Frame</code>.</p>\n<p>We also need to create a function to create frames. You can think of this function as a frame <a href="https://docs.google.com/document/d/1JwfuaBrao_pZjLBy6isPdqv_hndQnetCJ8MjYWlLtlI/edit?pli=1&#x26;disco=AAAAddu7kIo">generator</a> or a frame factory that returns an iterator. This function requires three parameters: the frame duration in milliseconds, the audio data, and the sample rate.</p>\n<p>This function starts by deriving an interval of frames from the passed in sample rate and frame duration in milliseconds. We start at an offset and timestamp of 0. We also create a duration constant equal to the number of frames in a second.</p>\n<p>While the current offset can be incremented by the interval constant and be within the number of frames of the audio, we generate a <code is:raw>Frame</code> for each interval and then increment the timestamp and offset appropriately.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;Represents a &quot;frame&quot; of audio data.</span></span>\n<span class="line"><span style="color: #A5D6FF">Requires the number of byes, the timestamp of the frame, and the duration on init&quot;&quot;&quot;</span></span>\n<span class="line"><span style="color: #FF7B72">class</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">Frame</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">object</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__init__</span><span style="color: #C9D1D9">(self, bytes, timestamp, duration):</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.bytes </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">bytes</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.timestamp </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> timestamp</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.duration </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> duration</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;Generates audio frames from PCM audio data.</span></span>\n<span class="line"><span style="color: #A5D6FF">Input: the desired frame duration in milliseconds, the PCM data, and</span></span>\n<span class="line"><span style="color: #A5D6FF">the sample rate.</span></span>\n<span class="line"><span style="color: #A5D6FF">Yields/Generates: Frames of the requested duration.</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;</span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">frame_generator</span><span style="color: #C9D1D9">(frame_duration_ms, audio, sample_rate):</span></span>\n<span class="line"><span style="color: #C9D1D9">   n </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">(sample_rate </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> (frame_duration_ms </span><span style="color: #FF7B72">/</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">1000.0</span><span style="color: #C9D1D9">) </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">2</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   offset </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">0</span></span>\n<span class="line"><span style="color: #C9D1D9">   timestamp </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">0.0</span></span>\n<span class="line"><span style="color: #C9D1D9">   duration </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> (</span><span style="color: #79C0FF">float</span><span style="color: #C9D1D9">(n) </span><span style="color: #FF7B72">/</span><span style="color: #C9D1D9"> sample_rate) </span><span style="color: #FF7B72">/</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">2.0</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">while</span><span style="color: #C9D1D9"> offset </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> n </span><span style="color: #FF7B72">&lt;</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">len</span><span style="color: #C9D1D9">(audio):</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">yield</span><span style="color: #C9D1D9"> Frame(audio[offset:offset </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> n], timestamp, duration)</span></span>\n<span class="line"><span style="color: #C9D1D9">       timestamp </span><span style="color: #FF7B72">+=</span><span style="color: #C9D1D9"> duration</span></span>\n<span class="line"><span style="color: #C9D1D9">       offset </span><span style="color: #FF7B72">+=</span><span style="color: #C9D1D9"> n</span></span></code></pre>\n<h3 id="collecting-voice-activated-frames-for-speech-to-text-with-deepspeech">Collecting Voice Activated Frames for Speech to Text with DeepSpeech</h3>\n<p>Next, let\u2019s create a function to collect all the frames that contain voice. This function requires a sample rate, the frame duration in milliseconds, the padding duration in milliseconds, a voice activation detector (VAD) from <code is:raw>webrtcvad</code>, and the audio data frames.</p>\n<p>The VAD algorithm uses a padded ring buffer and checks to see what percentage of the frames in the window are voiced. When the window reaches a 90% voiced frame rate, the VAD triggers and begins yielding audio frames. While generating frames, if the percentage of voiced audio data in the frame drops below 10%, it will stop generating frames.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;Filters out non-voiced audio frames.</span></span>\n<span class="line"><span style="color: #A5D6FF">Given a webrtcvad.Vad and a source of audio frames, yields only</span></span>\n<span class="line"><span style="color: #A5D6FF">the voiced audio.</span></span>\n<span class="line"><span style="color: #A5D6FF">Arguments:</span></span>\n<span class="line"><span style="color: #A5D6FF">sample_rate - The audio sample rate, in Hz.</span></span>\n<span class="line"><span style="color: #A5D6FF">frame_duration_ms - The frame duration in milliseconds.</span></span>\n<span class="line"><span style="color: #A5D6FF">padding_duration_ms - The amount to pad the window, in milliseconds.</span></span>\n<span class="line"><span style="color: #A5D6FF">vad - An instance of webrtcvad.Vad.</span></span>\n<span class="line"><span style="color: #A5D6FF">frames - a source of audio frames (sequence or generator).</span></span>\n<span class="line"><span style="color: #A5D6FF">Returns: A generator that yields PCM audio data.</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;</span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">vad_collector</span><span style="color: #C9D1D9">(sample_rate, frame_duration_ms,</span></span>\n<span class="line"><span style="color: #C9D1D9">                 padding_duration_ms, vad, frames):</span></span>\n<span class="line"><span style="color: #C9D1D9">   num_padding_frames </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">(padding_duration_ms </span><span style="color: #FF7B72">/</span><span style="color: #C9D1D9"> frame_duration_ms)</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #8B949E"># We use a deque for our sliding window/ring buffer.</span></span>\n<span class="line"><span style="color: #C9D1D9">   ring_buffer </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> collections.deque(</span><span style="color: #FFA657">maxlen</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">num_padding_frames)</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #8B949E"># We have two states: TRIGGERED and NOTTRIGGERED. We start in the</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #8B949E"># NOTTRIGGERED state.</span></span>\n<span class="line"><span style="color: #C9D1D9">   triggered </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">False</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">   voiced_frames </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> []</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> frame </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> frames:</span></span>\n<span class="line"><span style="color: #C9D1D9">       is_speech </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> vad.is_speech(frame.bytes, sample_rate)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">not</span><span style="color: #C9D1D9"> triggered:</span></span>\n<span class="line"><span style="color: #C9D1D9">           ring_buffer.append((frame, is_speech))</span></span>\n<span class="line"><span style="color: #C9D1D9">           num_voiced </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">len</span><span style="color: #C9D1D9">([f </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> f, speech </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> ring_buffer </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> speech])</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #8B949E"># If we&#39;re NOTTRIGGERED and more than 90% of the frames in</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #8B949E"># the ring buffer are voiced frames, then enter the</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #8B949E"># TRIGGERED state.</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> num_voiced </span><span style="color: #FF7B72">&gt;</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">0.9</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> ring_buffer.maxlen:</span></span>\n<span class="line"><span style="color: #C9D1D9">               triggered </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">True</span></span>\n<span class="line"><span style="color: #C9D1D9">               </span><span style="color: #8B949E"># We want to yield all the audio we see from now until</span></span>\n<span class="line"><span style="color: #C9D1D9">               </span><span style="color: #8B949E"># we are NOTTRIGGERED, but we have to start with the</span></span>\n<span class="line"><span style="color: #C9D1D9">               </span><span style="color: #8B949E"># audio that&#39;s already in the ring buffer.</span></span>\n<span class="line"><span style="color: #C9D1D9">               </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> f, s </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> ring_buffer:</span></span>\n<span class="line"><span style="color: #C9D1D9">                   voiced_frames.append(f)</span></span>\n<span class="line"><span style="color: #C9D1D9">               ring_buffer.clear()</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">else</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #8B949E"># We&#39;re in the TRIGGERED state, so collect the audio data</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #8B949E"># and add it to the ring buffer.</span></span>\n<span class="line"><span style="color: #C9D1D9">           voiced_frames.append(frame)</span></span>\n<span class="line"><span style="color: #C9D1D9">           ring_buffer.append((frame, is_speech))</span></span>\n<span class="line"><span style="color: #C9D1D9">           num_unvoiced </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">len</span><span style="color: #C9D1D9">([f </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> f, speech </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> ring_buffer </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">not</span><span style="color: #C9D1D9"> speech])</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #8B949E"># If more than 90% of the frames in the ring buffer are</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #8B949E"># unvoiced, then enter NOTTRIGGERED and yield whatever</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #8B949E"># audio we&#39;ve collected.</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> num_unvoiced </span><span style="color: #FF7B72">&gt;</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">0.9</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> ring_buffer.maxlen:</span></span>\n<span class="line"><span style="color: #C9D1D9">               triggered </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">False</span></span>\n<span class="line"><span style="color: #C9D1D9">               </span><span style="color: #FF7B72">yield</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">b</span><span style="color: #A5D6FF">&#39;&#39;</span><span style="color: #C9D1D9">.join([f.bytes </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> f </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> voiced_frames])</span></span>\n<span class="line"><span style="color: #C9D1D9">               ring_buffer.clear()</span></span>\n<span class="line"><span style="color: #C9D1D9">               voiced_frames </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> []</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> triggered:</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">pass</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #8B949E"># If we have any leftover voiced audio when we run out of input,</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #8B949E"># yield it.</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> voiced_frames:</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">yield</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">b</span><span style="color: #A5D6FF">&#39;&#39;</span><span style="color: #C9D1D9">.join([f.bytes </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> f </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> voiced_frames])</span></span></code></pre>\n<h2 id="transcribe-speech-to-text-for-wav-file-with-deepspeech">Transcribe Speech to Text for WAV file with DeepSpeech</h2>\n<p>We\u2019re going to create a new file for this section. This file should be named something like <code is:raw>wav_transcriber.py</code>. This layer completely abstracts out WAV handling from the CLI (which we create below). We use these functions to call DeepSpeech on the audio data and transcribe it.</p>\n<h3 id="pick-which-deepspeech-model-to-use">Pick Which DeepSpeech Model to Use</h3>\n<p>The first function we create in this file is the function to load up the model and scorer for DeepSpeech to run speech to text with. This function takes two parameters, the models graph, which we create a function to produce below, and the path to the scorer file. All it does is load the model from the graph and enable the use of the scorer. This function returns a DeepSpeech object.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> glob</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> webrtcvad</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> logging</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> wav_handler</span></span>\n<span class="line"><span style="color: #FF7B72">from</span><span style="color: #C9D1D9"> deepspeech </span><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> Model</span></span>\n<span class="line"><span style="color: #FF7B72">from</span><span style="color: #C9D1D9"> timeit </span><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> default_timer </span><span style="color: #FF7B72">as</span><span style="color: #C9D1D9"> timer</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #A5D6FF">&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #A5D6FF">Load the pre-trained model into the memory</span></span>\n<span class="line"><span style="color: #A5D6FF">@param models: Output Graph Protocol Buffer file</span></span>\n<span class="line"><span style="color: #A5D6FF">@param scorer: Scorer file</span></span>\n<span class="line"><span style="color: #A5D6FF">@Retval</span></span>\n<span class="line"><span style="color: #A5D6FF">Returns a DeepSpeech Object</span></span>\n<span class="line"><span style="color: #A5D6FF">&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">load_model</span><span style="color: #C9D1D9">(models, scorer):</span></span>\n<span class="line"><span style="color: #C9D1D9">   ds </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> Model(models)</span></span>\n<span class="line"><span style="color: #C9D1D9">   ds.enableExternalScorer(scorer)</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> ds</span></span></code></pre>\n<h3 id="speech-to-text-on-an-audio-file-with-deepspeech">Speech to Text on an Audio File with DeepSpeech</h3>\n<p>This function is the one that does the actual speech recognition. It takes three inputs, a DeepSpeech model, the audio data, and the sample rate.</p>\n<p>We begin by setting the time to 0 and calculating the length of the audio. All we really have to do is call the DeepSpeech model\u2019s <code is:raw>stt</code> function to do our own <code is:raw>stt</code> function. We pass the audio file to the <code is:raw>stt</code> function and return the output.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #A5D6FF">&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #A5D6FF">Run Inference on input audio file</span></span>\n<span class="line"><span style="color: #A5D6FF">@param ds: Deepspeech object</span></span>\n<span class="line"><span style="color: #A5D6FF">@param audio: Input audio for running inference on</span></span>\n<span class="line"><span style="color: #A5D6FF">@param fs: Sample rate of the input audio file</span></span>\n<span class="line"><span style="color: #A5D6FF">@Retval:</span></span>\n<span class="line"><span style="color: #A5D6FF">Returns a list [Inference, Inference Time, Audio Length]</span></span>\n<span class="line"><span style="color: #A5D6FF">&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">stt</span><span style="color: #C9D1D9">(ds, audio, fs):</span></span>\n<span class="line"><span style="color: #C9D1D9">   inference_time </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">0.0</span></span>\n<span class="line"><span style="color: #C9D1D9">   audio_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">len</span><span style="color: #C9D1D9">(audio) </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> (</span><span style="color: #79C0FF">1</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">/</span><span style="color: #C9D1D9"> fs)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #8B949E"># Run Deepspeech</span></span>\n<span class="line"><span style="color: #C9D1D9">   output </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> ds.stt(audio)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> output</span></span></code></pre>\n<h3 id="deepspeech-model-graph-creator-function">DeepSpeech Model Graph Creator Function</h3>\n<p>This is the function that creates the model graph for the <code is:raw>load_model</code> function we created a couple sections above. This function takes the path to a directory. From that directory, it looks for files with the DeepSpeech model extension, <code is:raw>pbmm</code> and the DeepSpeech scorer file extension, <code is:raw>.scorer</code>. Then, it returns both of those values.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #A5D6FF">&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #A5D6FF">Resolve directory path for the models and fetch each of them.</span></span>\n<span class="line"><span style="color: #A5D6FF">@param dirName: Path to the directory containing pre-trained models</span></span>\n<span class="line"><span style="color: #A5D6FF">@Retval:</span></span>\n<span class="line"><span style="color: #A5D6FF">Retunns a tuple containing each of the model files (pb, scorer)</span></span>\n<span class="line"><span style="color: #A5D6FF">&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">resolve_models</span><span style="color: #C9D1D9">(dirName):</span></span>\n<span class="line"><span style="color: #C9D1D9">   pb </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> glob.glob(dirName </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;/*.pbmm&quot;</span><span style="color: #C9D1D9">)[</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">]</span></span>\n<span class="line"><span style="color: #C9D1D9">   logging.debug(</span><span style="color: #A5D6FF">&quot;Found Model: </span><span style="color: #79C0FF">%s</span><span style="color: #A5D6FF">&quot;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">%</span><span style="color: #C9D1D9"> pb)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">   scorer </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> glob.glob(dirName </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;/*.scorer&quot;</span><span style="color: #C9D1D9">)[</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">]</span></span>\n<span class="line"><span style="color: #C9D1D9">   logging.debug(</span><span style="color: #A5D6FF">&quot;Found scorer: </span><span style="color: #79C0FF">%s</span><span style="color: #A5D6FF">&quot;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">%</span><span style="color: #C9D1D9"> scorer)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> pb, scorer</span></span></code></pre>\n<h3 id="voice-activation-detection-to-create-segments-for-speech-to-text">Voice Activation Detection to Create Segments for Speech to Text</h3>\n<p>The last function in our WAV transcription file generates segments of text that contain voice. We use the WAV handler file we created earlier and <code is:raw>webrtcvad</code> to do the heavy lifting. This function requires two parameters: a WAV file and an integer value from 0 to 3 representing how aggressively we want to filter out non-voice activity.</p>\n<p>We call the <code is:raw>read_wave</code> function from the <code is:raw>wav_handler.py</code> file we created earlier and imported above to get the audio data, sample rate, and audio length. We then assert that the sample rate is 16kHz before moving on to create a VAD object. Next, we call the frame generator from <code is:raw>wav_handler</code>.</p>\n<p>We convert the generated iterator to a list which we pass to the <code is:raw>vad_collector</code> function from <code is:raw>wav_handler</code> along with the sample rate, frame duration (30 ms), padding duration (300 ms), and VAD object. Finally, we return the collected VAD segments along with the sample rate and audio length.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #A5D6FF">&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #A5D6FF">Generate VAD segments. Filters out non-voiced audio frames.</span></span>\n<span class="line"><span style="color: #A5D6FF">@param waveFile: Input wav file to run VAD on.0</span></span>\n<span class="line"><span style="color: #A5D6FF">@Retval:</span></span>\n<span class="line"><span style="color: #A5D6FF">Returns tuple of</span></span>\n<span class="line"><span style="color: #A5D6FF">   segments: a bytearray of multiple smaller audio frames</span></span>\n<span class="line"><span style="color: #A5D6FF">             (The longer audio split into mutiple smaller one&#39;s)</span></span>\n<span class="line"><span style="color: #A5D6FF">   sample_rate: Sample rate of the input audio file</span></span>\n<span class="line"><span style="color: #A5D6FF">   audio_length: Duraton of the input audio file</span></span>\n<span class="line"><span style="color: #A5D6FF">&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">vad_segment_generator</span><span style="color: #C9D1D9">(wavFile, aggressiveness):</span></span>\n<span class="line"><span style="color: #C9D1D9">   audio, sample_rate, audio_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wav_handler.read_wave(wavFile)</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">assert</span><span style="color: #C9D1D9"> sample_rate </span><span style="color: #FF7B72">==</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">16000</span><span style="color: #C9D1D9">, </span><span style="color: #A5D6FF">&quot;Only 16000Hz input WAV files are supported for now!&quot;</span></span>\n<span class="line"><span style="color: #C9D1D9">   vad </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> webrtcvad.Vad(</span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">(aggressiveness))</span></span>\n<span class="line"><span style="color: #C9D1D9">   frames </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wav_handler.frame_generator(</span><span style="color: #79C0FF">30</span><span style="color: #C9D1D9">, audio, sample_rate)</span></span>\n<span class="line"><span style="color: #C9D1D9">   frames </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">list</span><span style="color: #C9D1D9">(frames)</span></span>\n<span class="line"><span style="color: #C9D1D9">   segments </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wav_handler.vad_collector(sample_rate, </span><span style="color: #79C0FF">30</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">300</span><span style="color: #C9D1D9">, vad, frames)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> segments, sample_rate, audio_length</span></span></code></pre>\n<h2 id="deepspeech-cli-for-real-time-and-asynchronous-speech-to-text">DeepSpeech CLI for Real Time and Asynchronous Speech to Text</h2>\n<p>Everything is set up to transcribe audio data with DeepSpeech via pretrained models. Now, let\u2019s look at how to turn the functionality we created above into a command line interface for real time and asynchronous speech to text. We start by importing a bunch of libraries for operating with the command line - <code is:raw>sys</code>, <code is:raw>os</code>, <code is:raw>logging</code>, <code is:raw>argparse</code>, <code is:raw>subprocess</code>, and <code is:raw>shlex</code>. We also need to import <code is:raw>numpy</code> and the <code is:raw>wav_transcriber</code> we made above to work with the audio data.</p>\n<h3 id="reading-arguments-for-deepspeech-speech-to-text">Reading Arguments for DeepSpeech Speech to Text</h3>\n<p>We create a main function that takes one parameter - <code is:raw>args</code>. These are the arguments passed in through the command line. We use the <code is:raw>argparse</code> libraries to parse the arguments sent in. We also create helpful tips on how to use each one.</p>\n<p>We use <code is:raw>aggressive</code> to determine how aggressively we want to filter. <code is:raw>audio</code> directs us to the audio file path. <code is:raw>model</code> points us to the directory containing the model and scorer. Finally, <code is:raw>stream</code> dictates whether or not we are streaming audio. Neither <code is:raw>stream</code> nor <code is:raw>audio</code> is required, but one or the other must be present.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> sys</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> os</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> logging</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> argparse</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> subprocess</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> shlex</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> numpy </span><span style="color: #FF7B72">as</span><span style="color: #C9D1D9"> np</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> wav_transcriber</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #8B949E"># Debug helpers</span></span>\n<span class="line"><span style="color: #C9D1D9">logging.basicConfig(</span><span style="color: #FFA657">stream</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">sys.stderr, </span><span style="color: #FFA657">level</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">logging.</span><span style="color: #79C0FF">DEBUG</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">main</span><span style="color: #C9D1D9">(args):</span></span>\n<span class="line"><span style="color: #C9D1D9">   parser </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> argparse.ArgumentParser(</span><span style="color: #FFA657">description</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&#39;Transcribe long audio files using webRTC VAD or use the streaming interface&#39;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   parser.add_argument(</span><span style="color: #A5D6FF">&#39;--aggressive&#39;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">type</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">choices</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">range</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">4</span><span style="color: #C9D1D9">), </span><span style="color: #FFA657">required</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">False</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">                       </span><span style="color: #FFA657">help</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&#39;Determines how aggressive filtering out non-speech is. (Interger between 0-3)&#39;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   parser.add_argument(</span><span style="color: #A5D6FF">&#39;--audio&#39;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">required</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">False</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">                       </span><span style="color: #FFA657">help</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&#39;Path to the audio file to run (WAV format)&#39;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   parser.add_argument(</span><span style="color: #A5D6FF">&#39;--model&#39;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">required</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">True</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">                       </span><span style="color: #FFA657">help</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&#39;Path to directory that contains all model files (output_graph and scorer)&#39;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   parser.add_argument(</span><span style="color: #A5D6FF">&#39;--stream&#39;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">required</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">False</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">action</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&#39;store_true&#39;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">                       </span><span style="color: #FFA657">help</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&#39;To use deepspeech streaming interface&#39;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   args </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> parser.parse_args()</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> args.stream </span><span style="color: #FF7B72">is</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">True</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(</span><span style="color: #A5D6FF">&quot;Opening mic for streaming&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">elif</span><span style="color: #C9D1D9"> args.audio </span><span style="color: #FF7B72">is</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">not</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">None</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">       logging.debug(</span><span style="color: #A5D6FF">&quot;Transcribing audio file @ </span><span style="color: #79C0FF">%s</span><span style="color: #A5D6FF">&quot;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">%</span><span style="color: #C9D1D9"> args.audio)</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">else</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">       parser.print_help()</span></span>\n<span class="line"><span style="color: #C9D1D9">       parser.exit()</span></span></code></pre>\n<h3 id="using-deepspeech-for-real-time-or-asynchronous-speech-recognition">Using DeepSpeech for Real Time or Asynchronous Speech Recognition</h3>\n<p>This is still inside the main function we started above. Once we parse all the arguments, we load up DeepSpeech. First, we get the directory containing the models. Next, we call the <code is:raw>wav_transcriber</code> to resolve and load the models.</p>\n<p>If we pass the path to an audio data file in the command line, then we will run asynchronous speech recognition. The first thing we do for that is call the VAD segment generator to generate the VAD segments and get the sample rate and audio length. Next, we open up a text file to transcribe to.</p>\n<p>For each of the enumerated segments, we will process each chunk by using <code is:raw>numpy</code> to pull the segment from the buffer and the speech to text function from <code is:raw>wav_transcriber</code> to do the speech to text functionality. We write to the text file until we run out of audio segments.</p>\n<p>If we pass stream instead of audio, then we open up the mic to stream audio data in. If you don\u2019t need real time automatic speech recognition, then you can ignore this part. First, we have to spin up a subprocess to open up a mic to stream in real time just like we did with PyTorch local speech recognition.</p>\n<p>We use the <code is:raw>subprocess</code> and <code is:raw>shlex</code> libraries to open the mic to stream voice audio until we shut it down. The model will read 512 bytes of audio data at a time and transcribe it.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #8B949E"># Point to a path containing the pre-trained models &amp; resolve ~ if used</span></span>\n<span class="line"><span style="color: #C9D1D9">dirName </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> os.path.expanduser(args.model)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #8B949E"># Resolve all the paths of model files</span></span>\n<span class="line"><span style="color: #C9D1D9">output_graph, scorer </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wav_transcriber.resolve_models(dirName)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #8B949E"># Load output_graph, alpahbet and scorer</span></span>\n<span class="line"><span style="color: #C9D1D9">model_retval </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wav_transcriber.load_model(output_graph, scorer)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> args.audio </span><span style="color: #FF7B72">is</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">not</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">None</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">    </span><span style="color: #8B949E"># Run VAD on the input file</span></span>\n<span class="line"><span style="color: #C9D1D9">    waveFile </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> args.audio</span></span>\n<span class="line"><span style="color: #C9D1D9">    segments, sample_rate, audio_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wav_transcriber.vad_segment_generator(waveFile, args.aggressive)</span></span>\n<span class="line"><span style="color: #C9D1D9">    f </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">open</span><span style="color: #C9D1D9">(waveFile.rstrip(</span><span style="color: #A5D6FF">&quot;.wav&quot;</span><span style="color: #C9D1D9">) </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;.txt&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #A5D6FF">&#39;w&#39;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">    logging.debug(</span><span style="color: #A5D6FF">&quot;Saving Transcript @: </span><span style="color: #79C0FF">%s</span><span style="color: #A5D6FF">&quot;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">%</span><span style="color: #C9D1D9"> waveFile.rstrip(</span><span style="color: #A5D6FF">&quot;.wav&quot;</span><span style="color: #C9D1D9">) </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;.txt&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">    </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> i, segment </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">enumerate</span><span style="color: #C9D1D9">(segments):</span></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #8B949E"># Run deepspeech on the chunk that just completed VAD</span></span>\n<span class="line"><span style="color: #C9D1D9">        logging.debug(</span><span style="color: #A5D6FF">&quot;Processing chunk </span><span style="color: #79C0FF">%002d</span><span style="color: #A5D6FF">&quot;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">%</span><span style="color: #C9D1D9"> (i,))</span></span>\n<span class="line"><span style="color: #C9D1D9">        audio </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> np.frombuffer(segment, </span><span style="color: #FFA657">dtype</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">np.int16)</span></span>\n<span class="line"><span style="color: #C9D1D9">        output </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wav_transcriber.stt(model_retval, audio, sample_rate)</span></span>\n<span class="line"><span style="color: #C9D1D9">        logging.debug(</span><span style="color: #A5D6FF">&quot;Transcript: </span><span style="color: #79C0FF">%s</span><span style="color: #A5D6FF">&quot;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">%</span><span style="color: #C9D1D9"> output)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">        f.write(output </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot; &quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">    </span><span style="color: #8B949E"># Summary of the files processed</span></span>\n<span class="line"><span style="color: #C9D1D9">    f.close()</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #FF7B72">else</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">    sctx </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> model_retval.createStream()</span></span>\n<span class="line"><span style="color: #C9D1D9">    subproc </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> subprocess.Popen(shlex.split(</span><span style="color: #A5D6FF">&#39;rec -q -V0 -e signed -L -c 1 -b 16 -r 16k -t raw - gain -2&#39;</span><span style="color: #C9D1D9">),</span></span>\n<span class="line"><span style="color: #C9D1D9">                                </span><span style="color: #FFA657">stdout</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">subprocess.</span><span style="color: #79C0FF">PIPE</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">                                </span><span style="color: #FFA657">bufsize</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">    </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(</span><span style="color: #A5D6FF">&#39;You can start speaking now. Press Control-C to stop recording.&#39;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">    </span><span style="color: #FF7B72">try</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #FF7B72">while</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">True</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">            data </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> subproc.stdout.read(</span><span style="color: #79C0FF">512</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">            sctx.feedAudioContent(np.frombuffer(data, np.int16))</span></span>\n<span class="line"><span style="color: #C9D1D9">    </span><span style="color: #FF7B72">except</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">KeyboardInterrupt</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(</span><span style="color: #A5D6FF">&#39;Transcription: &#39;</span><span style="color: #C9D1D9">, sctx.finishStream())</span></span>\n<span class="line"><span style="color: #C9D1D9">        subproc.terminate()</span></span>\n<span class="line"><span style="color: #C9D1D9">        subproc.wait()</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__name__</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">==</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&#39;__main__&#39;</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">   main(sys.argv[</span><span style="color: #79C0FF">1</span><span style="color: #C9D1D9">:])</span></span></code></pre>\n<h2 id="summary">Summary</h2>\n<p>We started this post out with a high level view of DeepSpeech, an open source speech recognition software. It was inspired by a 2014 paper from Baidu and is currently maintained by Mozilla.</p>\n<p>After a basic introduction, we stepped into a guide on how to use DeepSpeech to locally transcribe speech to text. While it may have been possible to create all of this code in one document, we opted for a modular approach with principles of software engineering in mind.</p>\n<p>We created three modules. One to handle WAV files, which are the audio data files that we can use DeepSpeech to transcribe. One to transcribe from WAV files, and one more file to create a command line interface to use DeepSpeech. Our CLI allows us to pass in options to pick if we want to do real time speech recognition or run speech recognition on an existing WAV audio file.</p>' };
const frontmatter = { "title": "A Guide to DeepSpeech Speech to Text", "description": "DeepSpeech is a Python library for doing ASR. In this post, we\u2019ll look at how to use DeepSpeech to do Speech to Text in Python", "date": "2022-08-01T00:00:00.000Z", "cover": "https://res.cloudinary.com/deepgram/image/upload/v1659364680/blog/2022/08/guide-deepspeech-speech-to-text/cover.jpg", "authors": ["yujian-tang"], "category": "tutorial", "tags": ["python", "deepspeech"], "seo": { "title": "A Guide to DeepSpeech Speech to Text", "description": "DeepSpeech is a Python library for doing ASR. In this post, we\u2019ll look at how to use DeepSpeech to do Speech to Text in Python" }, "shorturls": { "share": "https://dpgr.am/2b36a93", "twitter": "https://dpgr.am/3d69f41", "linkedin": "https://dpgr.am/d6f2658", "reddit": "https://dpgr.am/2dc3fb8", "facebook": "https://dpgr.am/7056571" }, "og": { "image": "https://res.cloudinary.com/deepgram/image/upload/v1661454117/blog/guide-deepspeech-speech-to-text/ograph.png" }, "astro": { "headings": [{ "depth": 2, "slug": "what-is-deepspeech", "text": "What is DeepSpeech?" }, { "depth": 2, "slug": "set-up-for-local-speech-to-text-with-deepspeech", "text": "Set Up for Local Speech to Text with DeepSpeech" }, { "depth": 2, "slug": "file-handler-for-deepspeech-speech-transcription", "text": "File Handler for DeepSpeech Speech Transcription" }, { "depth": 3, "slug": "reading-audio-data-from-a-wav-file", "text": "Reading Audio Data from a WAV file" }, { "depth": 3, "slug": "writing-audio-data-to-a-wav-file", "text": "Writing Audio Data to a WAV file" }, { "depth": 3, "slug": "creating-frames-of-audio-data-for-deepspeech-to-transcribe", "text": "Creating Frames of Audio Data for DeepSpeech to Transcribe" }, { "depth": 3, "slug": "collecting-voice-activated-frames-for-speech-to-text-with-deepspeech", "text": "Collecting Voice Activated Frames for Speech to Text with DeepSpeech" }, { "depth": 2, "slug": "transcribe-speech-to-text-for-wav-file-with-deepspeech", "text": "Transcribe Speech to Text for WAV file with DeepSpeech" }, { "depth": 3, "slug": "pick-which-deepspeech-model-to-use", "text": "Pick Which DeepSpeech Model to Use" }, { "depth": 3, "slug": "speech-to-text-on-an-audio-file-with-deepspeech", "text": "Speech to Text on an Audio File with DeepSpeech" }, { "depth": 3, "slug": "deepspeech-model-graph-creator-function", "text": "DeepSpeech Model Graph Creator Function" }, { "depth": 3, "slug": "voice-activation-detection-to-create-segments-for-speech-to-text", "text": "Voice Activation Detection to Create Segments for Speech to Text" }, { "depth": 2, "slug": "deepspeech-cli-for-real-time-and-asynchronous-speech-to-text", "text": "DeepSpeech CLI for Real Time and Asynchronous Speech to Text" }, { "depth": 3, "slug": "reading-arguments-for-deepspeech-speech-to-text", "text": "Reading Arguments for DeepSpeech Speech to Text" }, { "depth": 3, "slug": "using-deepspeech-for-real-time-or-asynchronous-speech-recognition", "text": "Using DeepSpeech for Real Time or Asynchronous Speech Recognition" }, { "depth": 2, "slug": "summary", "text": "Summary" }], "source": "\r\nNo, we\u2019re not talking about you Cthulhu. This is a different type of DeepSpeech. The DeepSpeech we\u2019re talking about today is a Python speech to text library. Speech to text is part of [Natural Language Processing (NLP)](https://pythonalgos.com/?p=1436). Automated speech recognition, or ASR, started out as an offshoot of NLP in the 1990s.\r\n\r\nToday, there are tons of audio libraries that can help you [manipulate audio data](https://blog.deepgram.com/best-python-audio-manipulation-tools/) such as DeepSpeech and [PyTorch](https://blog.deepgram.com/pytorch-intro-with-torchaudio/). In this post, we will be using DeepSpeech to do both asynchronous and real time speech transcription. We will cover:\r\n\r\n*   [What is DeepSpeech?](#what-is-deepspeech)\r\n*   [Set Up for Local Speech to Text with DeepSpeech](#set-up-for-local-speech-to-text-with-deepspeech)\r\n*   [File Handler for DeepSpeech Speech Transcription](#file-handler-for-deepspeech-speech-transcription)\r\n*   [Transcribe Speech to Text for WAV file with DeepSpeech](#transcribe-speech-to-text-for-wav-file-with-deepspeech)\r\n*   [DeepSpeech CLI for Real-Time and Asynchronous Speech to Text](#deepspeech-cli-for-real-time-and-asynchronous-speech-to-text)\r\n*   [Summary](#summary)\r\n\r\n## What is DeepSpeech?\r\n\r\nDeepSpeech is an open source Python library that enables us to build automatic speech recognition systems. It is based on Baidu\u2019s 2014 paper titled [Deep Speech: Scaling up end-to-end speech recognition](https://arxiv.org/abs/1412.5567).\r\n\r\nThe initial proposal for Deep Speech was simple - let\u2019s create a speech recognition system based entirely off of deep learning. The paper describes a solution using RNNs trained with multiple GPUs with no concept of phonemes. The authors, Hannun et al., show that their solution also outperformed the existing solutions at the time and was more robust to background noise without a need for filtering.\r\n\r\nSince then, Mozilla has been the one in charge of maintaining the open source Python package for DeepSpeech. Before moving on, it\u2019s important to note that DeepSpeech is not yet compatible with Python 3.10 nor some more recent versions of \\*nix kernels. I suggest using a virtual machine or Docker container to develop with DeepSpeech on unsupported OSes.\r\n\r\n## Set Up for Local Speech to Text with DeepSpeech\r\n\r\nTo use DeepSpeech, we have to install a few libraries. We need `deepspeech`, `numpy`, and `webrtcvad`. We can install all of these by running `pip install deepspeech numpy webrtcvad`. The `webrtcvad` library is the voice activity detection (VAD) library developed by Google for WebRTC (real time communication).\r\n\r\nFor the asynchronous transcription, we\u2019re going to need three files. One file to handle interaction with WAV data, one file to transcribe speech to text on a WAV file, and one to use these two in the command line. We will also be using a pretrained DeepSpeech model and scorer. You can download their model by running the following lines in your terminal:\r\n\r\n```bash\r\nwget https://github.com/mozilla/DeepSpeech/releases/download/v0.9.3/deepspeech-0.9.3-models.pbmm\r\nwget https://github.com/mozilla/DeepSpeech/releases/download/v0.9.3/deepspeech-0.9.3-models.scorer\r\n```\r\n\r\n## File Handler for DeepSpeech Speech Transcription\r\n\r\nThe first file we create is the WAV handling file. This file should be named something like `wav_handler.py`. We import three built-in libraries to do this, `wave`, `collections`, and `contextlib`. We create four functions and one class. We need one function each to read WAV files, write WAV files, create Frames, and detect voice activity. Our one class represents individual frames in the WAV file.\r\n\r\n### Reading Audio Data from a WAV file\r\n\r\nLet\u2019s start with creating a function to read WAV files. This function will take an input, this input is the path to a WAV file. The file will use the `contextlib` library to open the WAV file and read in the contents as bytes. Next, we run multiple asserts on the WAV file - it must have one channel, have a sample width of 2, have a sample rate of 8, 16, or 32 kHz.\r\n\r\nOnce we have asserted that the WAV file is in the right format for processing, we extract the frames. Next, we extract the pcm data from the frames and the duration from the metadata. Finally, we return the PCM data, the sample rate, and the duration.\r\n\r\n```py\r\nimport collections\r\nimport contextlib\r\nimport wave\r\n\r\n\"\"\"Reads a .wav file.\r\nInput: path to a .wav file\r\nOutput: tuple of pcm data, sample rate, and duration\r\n\"\"\"\r\ndef read_wave(path):\r\n   with contextlib.closing(wave.open(path, 'rb')) as wf:\r\n       num_channels = wf.getnchannels()\r\n       assert num_channels == 1\r\n       sample_width = wf.getsampwidth()\r\n       assert sample_width == 2\r\n       sample_rate = wf.getframerate()\r\n       assert sample_rate in (8000, 16000, 32000)\r\n       frames = wf.getnframes()\r\n       pcm_data = wf.readframes(frames)\r\n       duration = frames / sample_rate\r\n       return pcm_data, sample_rate, duration\r\n```\r\n\r\n### Writing Audio Data to a WAV file\r\n\r\nNow let\u2019s create the function to write audio data to a WAV file. This function requires three parameters, the path to a file to write to, the audio data, and the sample rate. This function writes a WAV file in the same way that the read function asserts its parameters. All we do is here is set the channels, sample width, and frame rate and then write the audio frames.\r\n\r\n```py\r\n\"\"\"Writes a .wav file.\r\nInput: path to new .wav file, PCM audio data, and sample rate.\r\nOutput: a .wav file\r\n\"\"\"\r\ndef write_wave(path, audio, sample_rate):\r\n   with contextlib.closing(wave.open(path, 'wb')) as wf:\r\n       wf.setnchannels(1)\r\n       wf.setsampwidth(2)\r\n       wf.setframerate(sample_rate)\r\n       wf.writeframes(audio)\r\n```\r\n\r\n### Creating Frames of Audio Data for DeepSpeech to Transcribe\r\n\r\nWe\u2019re going to create a class called `Frame` to hold some information to represent our audio data and make it easier to handle. This object requires three parameters to be created: the bytes, the timestamp in the audio file, and the duration of the `Frame`.\r\n\r\nWe also need to create a function to create frames. You can think of this function as a frame [generator](https://docs.google.com/document/d/1JwfuaBrao_pZjLBy6isPdqv_hndQnetCJ8MjYWlLtlI/edit?pli=1\\&disco=AAAAddu7kIo) or a frame factory that returns an iterator. This function requires three parameters: the frame duration in milliseconds, the audio data, and the sample rate.\r\n\r\nThis function starts by deriving an interval of frames from the passed in sample rate and frame duration in milliseconds. We start at an offset and timestamp of 0. We also create a duration constant equal to the number of frames in a second.\r\n\r\nWhile the current offset can be incremented by the interval constant and be within the number of frames of the audio, we generate a `Frame` for each interval and then increment the timestamp and offset appropriately.\r\n\r\n```py\r\n\"\"\"Represents a \"frame\" of audio data.\r\nRequires the number of byes, the timestamp of the frame, and the duration on init\"\"\"\r\nclass Frame(object):\r\n   def __init__(self, bytes, timestamp, duration):\r\n       self.bytes = bytes\r\n       self.timestamp = timestamp\r\n       self.duration = duration\r\n\r\n\"\"\"Generates audio frames from PCM audio data.\r\nInput: the desired frame duration in milliseconds, the PCM data, and\r\nthe sample rate.\r\nYields/Generates: Frames of the requested duration.\r\n\"\"\"\r\ndef frame_generator(frame_duration_ms, audio, sample_rate):\r\n   n = int(sample_rate * (frame_duration_ms / 1000.0) * 2)\r\n   offset = 0\r\n   timestamp = 0.0\r\n   duration = (float(n) / sample_rate) / 2.0\r\n   while offset + n < len(audio):\r\n       yield Frame(audio[offset:offset + n], timestamp, duration)\r\n       timestamp += duration\r\n       offset += n\r\n```\r\n\r\n### Collecting Voice Activated Frames for Speech to Text with DeepSpeech\r\n\r\nNext, let\u2019s create a function to collect all the frames that contain voice. This function requires a sample rate, the frame duration in milliseconds, the padding duration in milliseconds, a voice activation detector (VAD) from `webrtcvad`, and the audio data frames.\r\n\r\nThe VAD algorithm uses a padded ring buffer and checks to see what percentage of the frames in the window are voiced. When the window reaches a 90% voiced frame rate, the VAD triggers and begins yielding audio frames. While generating frames, if the percentage of voiced audio data in the frame drops below 10%, it will stop generating frames.\r\n\r\n```py\r\n\"\"\"Filters out non-voiced audio frames.\r\nGiven a webrtcvad.Vad and a source of audio frames, yields only\r\nthe voiced audio.\r\nArguments:\r\nsample_rate - The audio sample rate, in Hz.\r\nframe_duration_ms - The frame duration in milliseconds.\r\npadding_duration_ms - The amount to pad the window, in milliseconds.\r\nvad - An instance of webrtcvad.Vad.\r\nframes - a source of audio frames (sequence or generator).\r\nReturns: A generator that yields PCM audio data.\r\n\"\"\"\r\ndef vad_collector(sample_rate, frame_duration_ms,\r\n                 padding_duration_ms, vad, frames):\r\n   num_padding_frames = int(padding_duration_ms / frame_duration_ms)\r\n   # We use a deque for our sliding window/ring buffer.\r\n   ring_buffer = collections.deque(maxlen=num_padding_frames)\r\n   # We have two states: TRIGGERED and NOTTRIGGERED. We start in the\r\n   # NOTTRIGGERED state.\r\n   triggered = False\r\n\r\n   voiced_frames = []\r\n   for frame in frames:\r\n       is_speech = vad.is_speech(frame.bytes, sample_rate)\r\n\r\n       if not triggered:\r\n           ring_buffer.append((frame, is_speech))\r\n           num_voiced = len([f for f, speech in ring_buffer if speech])\r\n           # If we're NOTTRIGGERED and more than 90% of the frames in\r\n           # the ring buffer are voiced frames, then enter the\r\n           # TRIGGERED state.\r\n           if num_voiced > 0.9 * ring_buffer.maxlen:\r\n               triggered = True\r\n               # We want to yield all the audio we see from now until\r\n               # we are NOTTRIGGERED, but we have to start with the\r\n               # audio that's already in the ring buffer.\r\n               for f, s in ring_buffer:\r\n                   voiced_frames.append(f)\r\n               ring_buffer.clear()\r\n       else:\r\n           # We're in the TRIGGERED state, so collect the audio data\r\n           # and add it to the ring buffer.\r\n           voiced_frames.append(frame)\r\n           ring_buffer.append((frame, is_speech))\r\n           num_unvoiced = len([f for f, speech in ring_buffer if not speech])\r\n           # If more than 90% of the frames in the ring buffer are\r\n           # unvoiced, then enter NOTTRIGGERED and yield whatever\r\n           # audio we've collected.\r\n           if num_unvoiced > 0.9 * ring_buffer.maxlen:\r\n               triggered = False\r\n               yield b''.join([f.bytes for f in voiced_frames])\r\n               ring_buffer.clear()\r\n               voiced_frames = []\r\n   if triggered:\r\n       pass\r\n   # If we have any leftover voiced audio when we run out of input,\r\n   # yield it.\r\n   if voiced_frames:\r\n       yield b''.join([f.bytes for f in voiced_frames])\r\n```\r\n\r\n## Transcribe Speech to Text for WAV file with DeepSpeech\r\n\r\nWe\u2019re going to create a new file for this section. This file should be named something like `wav_transcriber.py`. This layer completely abstracts out WAV handling from the CLI (which we create below). We use these functions to call DeepSpeech on the audio data and transcribe it.\r\n\r\n### Pick Which DeepSpeech Model to Use\r\n\r\nThe first function we create in this file is the function to load up the model and scorer for DeepSpeech to run speech to text with. This function takes two parameters, the models graph, which we create a function to produce below, and the path to the scorer file. All it does is load the model from the graph and enable the use of the scorer. This function returns a DeepSpeech object.\r\n\r\n```py\r\nimport glob\r\nimport webrtcvad\r\nimport logging\r\nimport wav_handler\r\nfrom deepspeech import Model\r\nfrom timeit import default_timer as timer\r\n\r\n'''\r\nLoad the pre-trained model into the memory\r\n@param models: Output Graph Protocol Buffer file\r\n@param scorer: Scorer file\r\n@Retval\r\nReturns a DeepSpeech Object\r\n'''\r\ndef load_model(models, scorer):\r\n   ds = Model(models)\r\n   ds.enableExternalScorer(scorer)\r\n   return ds\r\n```\r\n\r\n### Speech to Text on an Audio File with DeepSpeech\r\n\r\nThis function is the one that does the actual speech recognition. It takes three inputs, a DeepSpeech model, the audio data, and the sample rate.\r\n\r\nWe begin by setting the time to 0 and calculating the length of the audio. All we really have to do is call the DeepSpeech model\u2019s `stt` function to do our own `stt` function. We pass the audio file to the `stt` function and return the output.\r\n\r\n```py\r\n'''\r\nRun Inference on input audio file\r\n@param ds: Deepspeech object\r\n@param audio: Input audio for running inference on\r\n@param fs: Sample rate of the input audio file\r\n@Retval:\r\nReturns a list [Inference, Inference Time, Audio Length]\r\n'''\r\ndef stt(ds, audio, fs):\r\n   inference_time = 0.0\r\n   audio_length = len(audio) * (1 / fs)\r\n\r\n   # Run Deepspeech\r\n   output = ds.stt(audio)\r\n\r\n   return output\r\n```\r\n\r\n### DeepSpeech Model Graph Creator Function\r\n\r\nThis is the function that creates the model graph for the `load_model` function we created a couple sections above. This function takes the path to a directory. From that directory, it looks for files with the DeepSpeech model extension, `pbmm` and the DeepSpeech scorer file extension, `.scorer`. Then, it returns both of those values.\r\n\r\n```py\r\n'''\r\nResolve directory path for the models and fetch each of them.\r\n@param dirName: Path to the directory containing pre-trained models\r\n@Retval:\r\nRetunns a tuple containing each of the model files (pb, scorer)\r\n'''\r\ndef resolve_models(dirName):\r\n   pb = glob.glob(dirName + \"/*.pbmm\")[0]\r\n   logging.debug(\"Found Model: %s\" % pb)\r\n\r\n   scorer = glob.glob(dirName + \"/*.scorer\")[0]\r\n   logging.debug(\"Found scorer: %s\" % scorer)\r\n\r\n   return pb, scorer\r\n```\r\n\r\n### Voice Activation Detection to Create Segments for Speech to Text\r\n\r\nThe last function in our WAV transcription file generates segments of text that contain voice. We use the WAV handler file we created earlier and `webrtcvad` to do the heavy lifting. This function requires two parameters: a WAV file and an integer value from 0 to 3 representing how aggressively we want to filter out non-voice activity.\r\n\r\nWe call the `read_wave` function from the `wav_handler.py` file we created earlier and imported above to get the audio data, sample rate, and audio length. We then assert that the sample rate is 16kHz before moving on to create a VAD object. Next, we call the frame generator from `wav_handler`.\r\n\r\nWe convert the generated iterator to a list which we pass to the `vad_collector` function from `wav_handler` along with the sample rate, frame duration (30 ms), padding duration (300 ms), and VAD object. Finally, we return the collected VAD segments along with the sample rate and audio length.\r\n\r\n```py\r\n'''\r\nGenerate VAD segments. Filters out non-voiced audio frames.\r\n@param waveFile: Input wav file to run VAD on.0\r\n@Retval:\r\nReturns tuple of\r\n   segments: a bytearray of multiple smaller audio frames\r\n             (The longer audio split into mutiple smaller one's)\r\n   sample_rate: Sample rate of the input audio file\r\n   audio_length: Duraton of the input audio file\r\n'''\r\ndef vad_segment_generator(wavFile, aggressiveness):\r\n   audio, sample_rate, audio_length = wav_handler.read_wave(wavFile)\r\n   assert sample_rate == 16000, \"Only 16000Hz input WAV files are supported for now!\"\r\n   vad = webrtcvad.Vad(int(aggressiveness))\r\n   frames = wav_handler.frame_generator(30, audio, sample_rate)\r\n   frames = list(frames)\r\n   segments = wav_handler.vad_collector(sample_rate, 30, 300, vad, frames)\r\n\r\n   return segments, sample_rate, audio_length\r\n```\r\n\r\n## DeepSpeech CLI for Real Time and Asynchronous Speech to Text\r\n\r\nEverything is set up to transcribe audio data with DeepSpeech via pretrained models. Now, let\u2019s look at how to turn the functionality we created above into a command line interface for real time and asynchronous speech to text. We start by importing a bunch of libraries for operating with the command line - `sys`, `os`, `logging`, `argparse`, `subprocess`, and `shlex`. We also need to import `numpy` and the `wav_transcriber` we made above to work with the audio data.\r\n\r\n### Reading Arguments for DeepSpeech Speech to Text\r\n\r\nWe create a main function that takes one parameter - `args`. These are the arguments passed in through the command line. We use the `argparse` libraries to parse the arguments sent in. We also create helpful tips on how to use each one.\r\n\r\nWe use `aggressive` to determine how aggressively we want to filter. `audio` directs us to the audio file path. `model` points us to the directory containing the model and scorer. Finally, `stream` dictates whether or not we are streaming audio. Neither `stream` nor `audio` is required, but one or the other must be present.\r\n\r\n```py\r\nimport sys\r\nimport os\r\nimport logging\r\nimport argparse\r\nimport subprocess\r\nimport shlex\r\nimport numpy as np\r\nimport wav_transcriber\r\n\r\n# Debug helpers\r\nlogging.basicConfig(stream=sys.stderr, level=logging.DEBUG)\r\n\r\ndef main(args):\r\n   parser = argparse.ArgumentParser(description='Transcribe long audio files using webRTC VAD or use the streaming interface')\r\n   parser.add_argument('--aggressive', type=int, choices=range(4), required=False,\r\n                       help='Determines how aggressive filtering out non-speech is. (Interger between 0-3)')\r\n   parser.add_argument('--audio', required=False,\r\n                       help='Path to the audio file to run (WAV format)')\r\n   parser.add_argument('--model', required=True,\r\n                       help='Path to directory that contains all model files (output_graph and scorer)')\r\n   parser.add_argument('--stream', required=False, action='store_true',\r\n                       help='To use deepspeech streaming interface')\r\n   args = parser.parse_args()\r\n   if args.stream is True:\r\n       print(\"Opening mic for streaming\")\r\n   elif args.audio is not None:\r\n       logging.debug(\"Transcribing audio file @ %s\" % args.audio)\r\n   else:\r\n       parser.print_help()\r\n       parser.exit()\r\n```\r\n\r\n### Using DeepSpeech for Real Time or Asynchronous Speech Recognition\r\n\r\nThis is still inside the main function we started above. Once we parse all the arguments, we load up DeepSpeech. First, we get the directory containing the models. Next, we call the `wav_transcriber` to resolve and load the models.\r\n\r\nIf we pass the path to an audio data file in the command line, then we will run asynchronous speech recognition. The first thing we do for that is call the VAD segment generator to generate the VAD segments and get the sample rate and audio length. Next, we open up a text file to transcribe to.\r\n\r\nFor each of the enumerated segments, we will process each chunk by using `numpy` to pull the segment from the buffer and the speech to text function from `wav_transcriber` to do the speech to text functionality. We write to the text file until we run out of audio segments.\r\n\r\nIf we pass stream instead of audio, then we open up the mic to stream audio data in. If you don\u2019t need real time automatic speech recognition, then you can ignore this part. First, we have to spin up a subprocess to open up a mic to stream in real time just like we did with PyTorch local speech recognition.\r\n\r\nWe use the `subprocess` and `shlex` libraries to open the mic to stream voice audio until we shut it down. The model will read 512 bytes of audio data at a time and transcribe it.\r\n\r\n```py\r\n# Point to a path containing the pre-trained models & resolve ~ if used\r\ndirName = os.path.expanduser(args.model)\r\n\r\n# Resolve all the paths of model files\r\noutput_graph, scorer = wav_transcriber.resolve_models(dirName)\r\n\r\n# Load output_graph, alpahbet and scorer\r\nmodel_retval = wav_transcriber.load_model(output_graph, scorer)\r\n\r\nif args.audio is not None:\r\n    # Run VAD on the input file\r\n    waveFile = args.audio\r\n    segments, sample_rate, audio_length = wav_transcriber.vad_segment_generator(waveFile, args.aggressive)\r\n    f = open(waveFile.rstrip(\".wav\") + \".txt\", 'w')\r\n    logging.debug(\"Saving Transcript @: %s\" % waveFile.rstrip(\".wav\") + \".txt\")\r\n\r\n    for i, segment in enumerate(segments):\r\n        # Run deepspeech on the chunk that just completed VAD\r\n        logging.debug(\"Processing chunk %002d\" % (i,))\r\n        audio = np.frombuffer(segment, dtype=np.int16)\r\n        output = wav_transcriber.stt(model_retval, audio, sample_rate)\r\n        logging.debug(\"Transcript: %s\" % output)\r\n\r\n        f.write(output + \" \")\r\n\r\n    # Summary of the files processed\r\n    f.close()\r\n\r\nelse:\r\n    sctx = model_retval.createStream()\r\n    subproc = subprocess.Popen(shlex.split('rec -q -V0 -e signed -L -c 1 -b 16 -r 16k -t raw - gain -2'),\r\n                                stdout=subprocess.PIPE,\r\n                                bufsize=0)\r\n    print('You can start speaking now. Press Control-C to stop recording.')\r\n\r\n    try:\r\n        while True:\r\n            data = subproc.stdout.read(512)\r\n            sctx.feedAudioContent(np.frombuffer(data, np.int16))\r\n    except KeyboardInterrupt:\r\n        print('Transcription: ', sctx.finishStream())\r\n        subproc.terminate()\r\n        subproc.wait()\r\n\r\nif __name__ == '__main__':\r\n   main(sys.argv[1:])\r\n```\r\n\r\n## Summary\r\n\r\nWe started this post out with a high level view of DeepSpeech, an open source speech recognition software. It was inspired by a 2014 paper from Baidu and is currently maintained by Mozilla.\r\n\r\nAfter a basic introduction, we stepped into a guide on how to use DeepSpeech to locally transcribe speech to text. While it may have been possible to create all of this code in one document, we opted for a modular approach with principles of software engineering in mind.\r\n\r\nWe created three modules. One to handle WAV files, which are the audio data files that we can use DeepSpeech to transcribe. One to transcribe from WAV files, and one more file to create a command line interface to use DeepSpeech. Our CLI allows us to pass in options to pick if we want to do real time speech recognition or run speech recognition on an existing WAV audio file.\r\n\r\n        ", "html": '<p>No, we\u2019re not talking about you Cthulhu. This is a different type of DeepSpeech. The DeepSpeech we\u2019re talking about today is a Python speech to text library. Speech to text is part of <a href="https://pythonalgos.com/?p=1436">Natural Language Processing (NLP)</a>. Automated speech recognition, or ASR, started out as an offshoot of NLP in the 1990s.</p>\n<p>Today, there are tons of audio libraries that can help you <a href="https://blog.deepgram.com/best-python-audio-manipulation-tools/">manipulate audio data</a> such as DeepSpeech and <a href="https://blog.deepgram.com/pytorch-intro-with-torchaudio/">PyTorch</a>. In this post, we will be using DeepSpeech to do both asynchronous and real time speech transcription. We will cover:</p>\n<ul>\n<li><a href="#what-is-deepspeech">What is DeepSpeech?</a></li>\n<li><a href="#set-up-for-local-speech-to-text-with-deepspeech">Set Up for Local Speech to Text with DeepSpeech</a></li>\n<li><a href="#file-handler-for-deepspeech-speech-transcription">File Handler for DeepSpeech Speech Transcription</a></li>\n<li><a href="#transcribe-speech-to-text-for-wav-file-with-deepspeech">Transcribe Speech to Text for WAV file with DeepSpeech</a></li>\n<li><a href="#deepspeech-cli-for-real-time-and-asynchronous-speech-to-text">DeepSpeech CLI for Real-Time and Asynchronous Speech to Text</a></li>\n<li><a href="#summary">Summary</a></li>\n</ul>\n<h2 id="what-is-deepspeech">What is DeepSpeech?</h2>\n<p>DeepSpeech is an open source Python library that enables us to build automatic speech recognition systems. It is based on Baidu\u2019s 2014 paper titled <a href="https://arxiv.org/abs/1412.5567">Deep Speech: Scaling up end-to-end speech recognition</a>.</p>\n<p>The initial proposal for Deep Speech was simple - let\u2019s create a speech recognition system based entirely off of deep learning. The paper describes a solution using RNNs trained with multiple GPUs with no concept of phonemes. The authors, Hannun et al., show that their solution also outperformed the existing solutions at the time and was more robust to background noise without a need for filtering.</p>\n<p>Since then, Mozilla has been the one in charge of maintaining the open source Python package for DeepSpeech. Before moving on, it\u2019s important to note that DeepSpeech is not yet compatible with Python 3.10 nor some more recent versions of *nix kernels. I suggest using a virtual machine or Docker container to develop with DeepSpeech on unsupported OSes.</p>\n<h2 id="set-up-for-local-speech-to-text-with-deepspeech">Set Up for Local Speech to Text with DeepSpeech</h2>\n<p>To use DeepSpeech, we have to install a few libraries. We need <code is:raw>deepspeech</code>, <code is:raw>numpy</code>, and <code is:raw>webrtcvad</code>. We can install all of these by running <code is:raw>pip install deepspeech numpy webrtcvad</code>. The <code is:raw>webrtcvad</code> library is the voice activity detection (VAD) library developed by Google for WebRTC (real time communication).</p>\n<p>For the asynchronous transcription, we\u2019re going to need three files. One file to handle interaction with WAV data, one file to transcribe speech to text on a WAV file, and one to use these two in the command line. We will also be using a pretrained DeepSpeech model and scorer. You can download their model by running the following lines in your terminal:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">wget https://github.com/mozilla/DeepSpeech/releases/download/v0.9.3/deepspeech-0.9.3-models.pbmm</span></span>\n<span class="line"><span style="color: #C9D1D9">wget https://github.com/mozilla/DeepSpeech/releases/download/v0.9.3/deepspeech-0.9.3-models.scorer</span></span></code></pre>\n<h2 id="file-handler-for-deepspeech-speech-transcription">File Handler for DeepSpeech Speech Transcription</h2>\n<p>The first file we create is the WAV handling file. This file should be named something like <code is:raw>wav_handler.py</code>. We import three built-in libraries to do this, <code is:raw>wave</code>, <code is:raw>collections</code>, and <code is:raw>contextlib</code>. We create four functions and one class. We need one function each to read WAV files, write WAV files, create Frames, and detect voice activity. Our one class represents individual frames in the WAV file.</p>\n<h3 id="reading-audio-data-from-a-wav-file">Reading Audio Data from a WAV file</h3>\n<p>Let\u2019s start with creating a function to read WAV files. This function will take an input, this input is the path to a WAV file. The file will use the <code is:raw>contextlib</code> library to open the WAV file and read in the contents as bytes. Next, we run multiple asserts on the WAV file - it must have one channel, have a sample width of 2, have a sample rate of 8, 16, or 32 kHz.</p>\n<p>Once we have asserted that the WAV file is in the right format for processing, we extract the frames. Next, we extract the pcm data from the frames and the duration from the metadata. Finally, we return the PCM data, the sample rate, and the duration.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> collections</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> contextlib</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> wave</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;Reads a .wav file.</span></span>\n<span class="line"><span style="color: #A5D6FF">Input: path to a .wav file</span></span>\n<span class="line"><span style="color: #A5D6FF">Output: tuple of pcm data, sample rate, and duration</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;</span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">read_wave</span><span style="color: #C9D1D9">(path):</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">with</span><span style="color: #C9D1D9"> contextlib.closing(wave.open(path, </span><span style="color: #A5D6FF">&#39;rb&#39;</span><span style="color: #C9D1D9">)) </span><span style="color: #FF7B72">as</span><span style="color: #C9D1D9"> wf:</span></span>\n<span class="line"><span style="color: #C9D1D9">       num_channels </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wf.getnchannels()</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">assert</span><span style="color: #C9D1D9"> num_channels </span><span style="color: #FF7B72">==</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">1</span></span>\n<span class="line"><span style="color: #C9D1D9">       sample_width </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wf.getsampwidth()</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">assert</span><span style="color: #C9D1D9"> sample_width </span><span style="color: #FF7B72">==</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">2</span></span>\n<span class="line"><span style="color: #C9D1D9">       sample_rate </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wf.getframerate()</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">assert</span><span style="color: #C9D1D9"> sample_rate </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> (</span><span style="color: #79C0FF">8000</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">16000</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">32000</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">       frames </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wf.getnframes()</span></span>\n<span class="line"><span style="color: #C9D1D9">       pcm_data </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wf.readframes(frames)</span></span>\n<span class="line"><span style="color: #C9D1D9">       duration </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> frames </span><span style="color: #FF7B72">/</span><span style="color: #C9D1D9"> sample_rate</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> pcm_data, sample_rate, duration</span></span></code></pre>\n<h3 id="writing-audio-data-to-a-wav-file">Writing Audio Data to a WAV file</h3>\n<p>Now let\u2019s create the function to write audio data to a WAV file. This function requires three parameters, the path to a file to write to, the audio data, and the sample rate. This function writes a WAV file in the same way that the read function asserts its parameters. All we do is here is set the channels, sample width, and frame rate and then write the audio frames.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;Writes a .wav file.</span></span>\n<span class="line"><span style="color: #A5D6FF">Input: path to new .wav file, PCM audio data, and sample rate.</span></span>\n<span class="line"><span style="color: #A5D6FF">Output: a .wav file</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;</span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">write_wave</span><span style="color: #C9D1D9">(path, audio, sample_rate):</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">with</span><span style="color: #C9D1D9"> contextlib.closing(wave.open(path, </span><span style="color: #A5D6FF">&#39;wb&#39;</span><span style="color: #C9D1D9">)) </span><span style="color: #FF7B72">as</span><span style="color: #C9D1D9"> wf:</span></span>\n<span class="line"><span style="color: #C9D1D9">       wf.setnchannels(</span><span style="color: #79C0FF">1</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">       wf.setsampwidth(</span><span style="color: #79C0FF">2</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">       wf.setframerate(sample_rate)</span></span>\n<span class="line"><span style="color: #C9D1D9">       wf.writeframes(audio)</span></span></code></pre>\n<h3 id="creating-frames-of-audio-data-for-deepspeech-to-transcribe">Creating Frames of Audio Data for DeepSpeech to Transcribe</h3>\n<p>We\u2019re going to create a class called <code is:raw>Frame</code> to hold some information to represent our audio data and make it easier to handle. This object requires three parameters to be created: the bytes, the timestamp in the audio file, and the duration of the <code is:raw>Frame</code>.</p>\n<p>We also need to create a function to create frames. You can think of this function as a frame <a href="https://docs.google.com/document/d/1JwfuaBrao_pZjLBy6isPdqv_hndQnetCJ8MjYWlLtlI/edit?pli=1&#x26;disco=AAAAddu7kIo">generator</a> or a frame factory that returns an iterator. This function requires three parameters: the frame duration in milliseconds, the audio data, and the sample rate.</p>\n<p>This function starts by deriving an interval of frames from the passed in sample rate and frame duration in milliseconds. We start at an offset and timestamp of 0. We also create a duration constant equal to the number of frames in a second.</p>\n<p>While the current offset can be incremented by the interval constant and be within the number of frames of the audio, we generate a <code is:raw>Frame</code> for each interval and then increment the timestamp and offset appropriately.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;Represents a &quot;frame&quot; of audio data.</span></span>\n<span class="line"><span style="color: #A5D6FF">Requires the number of byes, the timestamp of the frame, and the duration on init&quot;&quot;&quot;</span></span>\n<span class="line"><span style="color: #FF7B72">class</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">Frame</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">object</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__init__</span><span style="color: #C9D1D9">(self, bytes, timestamp, duration):</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.bytes </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">bytes</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.timestamp </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> timestamp</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.duration </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> duration</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;Generates audio frames from PCM audio data.</span></span>\n<span class="line"><span style="color: #A5D6FF">Input: the desired frame duration in milliseconds, the PCM data, and</span></span>\n<span class="line"><span style="color: #A5D6FF">the sample rate.</span></span>\n<span class="line"><span style="color: #A5D6FF">Yields/Generates: Frames of the requested duration.</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;</span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">frame_generator</span><span style="color: #C9D1D9">(frame_duration_ms, audio, sample_rate):</span></span>\n<span class="line"><span style="color: #C9D1D9">   n </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">(sample_rate </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> (frame_duration_ms </span><span style="color: #FF7B72">/</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">1000.0</span><span style="color: #C9D1D9">) </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">2</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   offset </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">0</span></span>\n<span class="line"><span style="color: #C9D1D9">   timestamp </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">0.0</span></span>\n<span class="line"><span style="color: #C9D1D9">   duration </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> (</span><span style="color: #79C0FF">float</span><span style="color: #C9D1D9">(n) </span><span style="color: #FF7B72">/</span><span style="color: #C9D1D9"> sample_rate) </span><span style="color: #FF7B72">/</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">2.0</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">while</span><span style="color: #C9D1D9"> offset </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> n </span><span style="color: #FF7B72">&lt;</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">len</span><span style="color: #C9D1D9">(audio):</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">yield</span><span style="color: #C9D1D9"> Frame(audio[offset:offset </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> n], timestamp, duration)</span></span>\n<span class="line"><span style="color: #C9D1D9">       timestamp </span><span style="color: #FF7B72">+=</span><span style="color: #C9D1D9"> duration</span></span>\n<span class="line"><span style="color: #C9D1D9">       offset </span><span style="color: #FF7B72">+=</span><span style="color: #C9D1D9"> n</span></span></code></pre>\n<h3 id="collecting-voice-activated-frames-for-speech-to-text-with-deepspeech">Collecting Voice Activated Frames for Speech to Text with DeepSpeech</h3>\n<p>Next, let\u2019s create a function to collect all the frames that contain voice. This function requires a sample rate, the frame duration in milliseconds, the padding duration in milliseconds, a voice activation detector (VAD) from <code is:raw>webrtcvad</code>, and the audio data frames.</p>\n<p>The VAD algorithm uses a padded ring buffer and checks to see what percentage of the frames in the window are voiced. When the window reaches a 90% voiced frame rate, the VAD triggers and begins yielding audio frames. While generating frames, if the percentage of voiced audio data in the frame drops below 10%, it will stop generating frames.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;Filters out non-voiced audio frames.</span></span>\n<span class="line"><span style="color: #A5D6FF">Given a webrtcvad.Vad and a source of audio frames, yields only</span></span>\n<span class="line"><span style="color: #A5D6FF">the voiced audio.</span></span>\n<span class="line"><span style="color: #A5D6FF">Arguments:</span></span>\n<span class="line"><span style="color: #A5D6FF">sample_rate - The audio sample rate, in Hz.</span></span>\n<span class="line"><span style="color: #A5D6FF">frame_duration_ms - The frame duration in milliseconds.</span></span>\n<span class="line"><span style="color: #A5D6FF">padding_duration_ms - The amount to pad the window, in milliseconds.</span></span>\n<span class="line"><span style="color: #A5D6FF">vad - An instance of webrtcvad.Vad.</span></span>\n<span class="line"><span style="color: #A5D6FF">frames - a source of audio frames (sequence or generator).</span></span>\n<span class="line"><span style="color: #A5D6FF">Returns: A generator that yields PCM audio data.</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;</span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">vad_collector</span><span style="color: #C9D1D9">(sample_rate, frame_duration_ms,</span></span>\n<span class="line"><span style="color: #C9D1D9">                 padding_duration_ms, vad, frames):</span></span>\n<span class="line"><span style="color: #C9D1D9">   num_padding_frames </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">(padding_duration_ms </span><span style="color: #FF7B72">/</span><span style="color: #C9D1D9"> frame_duration_ms)</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #8B949E"># We use a deque for our sliding window/ring buffer.</span></span>\n<span class="line"><span style="color: #C9D1D9">   ring_buffer </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> collections.deque(</span><span style="color: #FFA657">maxlen</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">num_padding_frames)</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #8B949E"># We have two states: TRIGGERED and NOTTRIGGERED. We start in the</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #8B949E"># NOTTRIGGERED state.</span></span>\n<span class="line"><span style="color: #C9D1D9">   triggered </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">False</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">   voiced_frames </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> []</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> frame </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> frames:</span></span>\n<span class="line"><span style="color: #C9D1D9">       is_speech </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> vad.is_speech(frame.bytes, sample_rate)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">not</span><span style="color: #C9D1D9"> triggered:</span></span>\n<span class="line"><span style="color: #C9D1D9">           ring_buffer.append((frame, is_speech))</span></span>\n<span class="line"><span style="color: #C9D1D9">           num_voiced </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">len</span><span style="color: #C9D1D9">([f </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> f, speech </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> ring_buffer </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> speech])</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #8B949E"># If we&#39;re NOTTRIGGERED and more than 90% of the frames in</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #8B949E"># the ring buffer are voiced frames, then enter the</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #8B949E"># TRIGGERED state.</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> num_voiced </span><span style="color: #FF7B72">&gt;</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">0.9</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> ring_buffer.maxlen:</span></span>\n<span class="line"><span style="color: #C9D1D9">               triggered </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">True</span></span>\n<span class="line"><span style="color: #C9D1D9">               </span><span style="color: #8B949E"># We want to yield all the audio we see from now until</span></span>\n<span class="line"><span style="color: #C9D1D9">               </span><span style="color: #8B949E"># we are NOTTRIGGERED, but we have to start with the</span></span>\n<span class="line"><span style="color: #C9D1D9">               </span><span style="color: #8B949E"># audio that&#39;s already in the ring buffer.</span></span>\n<span class="line"><span style="color: #C9D1D9">               </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> f, s </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> ring_buffer:</span></span>\n<span class="line"><span style="color: #C9D1D9">                   voiced_frames.append(f)</span></span>\n<span class="line"><span style="color: #C9D1D9">               ring_buffer.clear()</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">else</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #8B949E"># We&#39;re in the TRIGGERED state, so collect the audio data</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #8B949E"># and add it to the ring buffer.</span></span>\n<span class="line"><span style="color: #C9D1D9">           voiced_frames.append(frame)</span></span>\n<span class="line"><span style="color: #C9D1D9">           ring_buffer.append((frame, is_speech))</span></span>\n<span class="line"><span style="color: #C9D1D9">           num_unvoiced </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">len</span><span style="color: #C9D1D9">([f </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> f, speech </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> ring_buffer </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">not</span><span style="color: #C9D1D9"> speech])</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #8B949E"># If more than 90% of the frames in the ring buffer are</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #8B949E"># unvoiced, then enter NOTTRIGGERED and yield whatever</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #8B949E"># audio we&#39;ve collected.</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> num_unvoiced </span><span style="color: #FF7B72">&gt;</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">0.9</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> ring_buffer.maxlen:</span></span>\n<span class="line"><span style="color: #C9D1D9">               triggered </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">False</span></span>\n<span class="line"><span style="color: #C9D1D9">               </span><span style="color: #FF7B72">yield</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">b</span><span style="color: #A5D6FF">&#39;&#39;</span><span style="color: #C9D1D9">.join([f.bytes </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> f </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> voiced_frames])</span></span>\n<span class="line"><span style="color: #C9D1D9">               ring_buffer.clear()</span></span>\n<span class="line"><span style="color: #C9D1D9">               voiced_frames </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> []</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> triggered:</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">pass</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #8B949E"># If we have any leftover voiced audio when we run out of input,</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #8B949E"># yield it.</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> voiced_frames:</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">yield</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">b</span><span style="color: #A5D6FF">&#39;&#39;</span><span style="color: #C9D1D9">.join([f.bytes </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> f </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> voiced_frames])</span></span></code></pre>\n<h2 id="transcribe-speech-to-text-for-wav-file-with-deepspeech">Transcribe Speech to Text for WAV file with DeepSpeech</h2>\n<p>We\u2019re going to create a new file for this section. This file should be named something like <code is:raw>wav_transcriber.py</code>. This layer completely abstracts out WAV handling from the CLI (which we create below). We use these functions to call DeepSpeech on the audio data and transcribe it.</p>\n<h3 id="pick-which-deepspeech-model-to-use">Pick Which DeepSpeech Model to Use</h3>\n<p>The first function we create in this file is the function to load up the model and scorer for DeepSpeech to run speech to text with. This function takes two parameters, the models graph, which we create a function to produce below, and the path to the scorer file. All it does is load the model from the graph and enable the use of the scorer. This function returns a DeepSpeech object.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> glob</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> webrtcvad</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> logging</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> wav_handler</span></span>\n<span class="line"><span style="color: #FF7B72">from</span><span style="color: #C9D1D9"> deepspeech </span><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> Model</span></span>\n<span class="line"><span style="color: #FF7B72">from</span><span style="color: #C9D1D9"> timeit </span><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> default_timer </span><span style="color: #FF7B72">as</span><span style="color: #C9D1D9"> timer</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #A5D6FF">&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #A5D6FF">Load the pre-trained model into the memory</span></span>\n<span class="line"><span style="color: #A5D6FF">@param models: Output Graph Protocol Buffer file</span></span>\n<span class="line"><span style="color: #A5D6FF">@param scorer: Scorer file</span></span>\n<span class="line"><span style="color: #A5D6FF">@Retval</span></span>\n<span class="line"><span style="color: #A5D6FF">Returns a DeepSpeech Object</span></span>\n<span class="line"><span style="color: #A5D6FF">&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">load_model</span><span style="color: #C9D1D9">(models, scorer):</span></span>\n<span class="line"><span style="color: #C9D1D9">   ds </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> Model(models)</span></span>\n<span class="line"><span style="color: #C9D1D9">   ds.enableExternalScorer(scorer)</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> ds</span></span></code></pre>\n<h3 id="speech-to-text-on-an-audio-file-with-deepspeech">Speech to Text on an Audio File with DeepSpeech</h3>\n<p>This function is the one that does the actual speech recognition. It takes three inputs, a DeepSpeech model, the audio data, and the sample rate.</p>\n<p>We begin by setting the time to 0 and calculating the length of the audio. All we really have to do is call the DeepSpeech model\u2019s <code is:raw>stt</code> function to do our own <code is:raw>stt</code> function. We pass the audio file to the <code is:raw>stt</code> function and return the output.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #A5D6FF">&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #A5D6FF">Run Inference on input audio file</span></span>\n<span class="line"><span style="color: #A5D6FF">@param ds: Deepspeech object</span></span>\n<span class="line"><span style="color: #A5D6FF">@param audio: Input audio for running inference on</span></span>\n<span class="line"><span style="color: #A5D6FF">@param fs: Sample rate of the input audio file</span></span>\n<span class="line"><span style="color: #A5D6FF">@Retval:</span></span>\n<span class="line"><span style="color: #A5D6FF">Returns a list [Inference, Inference Time, Audio Length]</span></span>\n<span class="line"><span style="color: #A5D6FF">&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">stt</span><span style="color: #C9D1D9">(ds, audio, fs):</span></span>\n<span class="line"><span style="color: #C9D1D9">   inference_time </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">0.0</span></span>\n<span class="line"><span style="color: #C9D1D9">   audio_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">len</span><span style="color: #C9D1D9">(audio) </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> (</span><span style="color: #79C0FF">1</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">/</span><span style="color: #C9D1D9"> fs)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #8B949E"># Run Deepspeech</span></span>\n<span class="line"><span style="color: #C9D1D9">   output </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> ds.stt(audio)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> output</span></span></code></pre>\n<h3 id="deepspeech-model-graph-creator-function">DeepSpeech Model Graph Creator Function</h3>\n<p>This is the function that creates the model graph for the <code is:raw>load_model</code> function we created a couple sections above. This function takes the path to a directory. From that directory, it looks for files with the DeepSpeech model extension, <code is:raw>pbmm</code> and the DeepSpeech scorer file extension, <code is:raw>.scorer</code>. Then, it returns both of those values.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #A5D6FF">&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #A5D6FF">Resolve directory path for the models and fetch each of them.</span></span>\n<span class="line"><span style="color: #A5D6FF">@param dirName: Path to the directory containing pre-trained models</span></span>\n<span class="line"><span style="color: #A5D6FF">@Retval:</span></span>\n<span class="line"><span style="color: #A5D6FF">Retunns a tuple containing each of the model files (pb, scorer)</span></span>\n<span class="line"><span style="color: #A5D6FF">&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">resolve_models</span><span style="color: #C9D1D9">(dirName):</span></span>\n<span class="line"><span style="color: #C9D1D9">   pb </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> glob.glob(dirName </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;/*.pbmm&quot;</span><span style="color: #C9D1D9">)[</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">]</span></span>\n<span class="line"><span style="color: #C9D1D9">   logging.debug(</span><span style="color: #A5D6FF">&quot;Found Model: </span><span style="color: #79C0FF">%s</span><span style="color: #A5D6FF">&quot;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">%</span><span style="color: #C9D1D9"> pb)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">   scorer </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> glob.glob(dirName </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;/*.scorer&quot;</span><span style="color: #C9D1D9">)[</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">]</span></span>\n<span class="line"><span style="color: #C9D1D9">   logging.debug(</span><span style="color: #A5D6FF">&quot;Found scorer: </span><span style="color: #79C0FF">%s</span><span style="color: #A5D6FF">&quot;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">%</span><span style="color: #C9D1D9"> scorer)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> pb, scorer</span></span></code></pre>\n<h3 id="voice-activation-detection-to-create-segments-for-speech-to-text">Voice Activation Detection to Create Segments for Speech to Text</h3>\n<p>The last function in our WAV transcription file generates segments of text that contain voice. We use the WAV handler file we created earlier and <code is:raw>webrtcvad</code> to do the heavy lifting. This function requires two parameters: a WAV file and an integer value from 0 to 3 representing how aggressively we want to filter out non-voice activity.</p>\n<p>We call the <code is:raw>read_wave</code> function from the <code is:raw>wav_handler.py</code> file we created earlier and imported above to get the audio data, sample rate, and audio length. We then assert that the sample rate is 16kHz before moving on to create a VAD object. Next, we call the frame generator from <code is:raw>wav_handler</code>.</p>\n<p>We convert the generated iterator to a list which we pass to the <code is:raw>vad_collector</code> function from <code is:raw>wav_handler</code> along with the sample rate, frame duration (30 ms), padding duration (300 ms), and VAD object. Finally, we return the collected VAD segments along with the sample rate and audio length.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #A5D6FF">&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #A5D6FF">Generate VAD segments. Filters out non-voiced audio frames.</span></span>\n<span class="line"><span style="color: #A5D6FF">@param waveFile: Input wav file to run VAD on.0</span></span>\n<span class="line"><span style="color: #A5D6FF">@Retval:</span></span>\n<span class="line"><span style="color: #A5D6FF">Returns tuple of</span></span>\n<span class="line"><span style="color: #A5D6FF">   segments: a bytearray of multiple smaller audio frames</span></span>\n<span class="line"><span style="color: #A5D6FF">             (The longer audio split into mutiple smaller one&#39;s)</span></span>\n<span class="line"><span style="color: #A5D6FF">   sample_rate: Sample rate of the input audio file</span></span>\n<span class="line"><span style="color: #A5D6FF">   audio_length: Duraton of the input audio file</span></span>\n<span class="line"><span style="color: #A5D6FF">&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">vad_segment_generator</span><span style="color: #C9D1D9">(wavFile, aggressiveness):</span></span>\n<span class="line"><span style="color: #C9D1D9">   audio, sample_rate, audio_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wav_handler.read_wave(wavFile)</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">assert</span><span style="color: #C9D1D9"> sample_rate </span><span style="color: #FF7B72">==</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">16000</span><span style="color: #C9D1D9">, </span><span style="color: #A5D6FF">&quot;Only 16000Hz input WAV files are supported for now!&quot;</span></span>\n<span class="line"><span style="color: #C9D1D9">   vad </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> webrtcvad.Vad(</span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">(aggressiveness))</span></span>\n<span class="line"><span style="color: #C9D1D9">   frames </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wav_handler.frame_generator(</span><span style="color: #79C0FF">30</span><span style="color: #C9D1D9">, audio, sample_rate)</span></span>\n<span class="line"><span style="color: #C9D1D9">   frames </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">list</span><span style="color: #C9D1D9">(frames)</span></span>\n<span class="line"><span style="color: #C9D1D9">   segments </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wav_handler.vad_collector(sample_rate, </span><span style="color: #79C0FF">30</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">300</span><span style="color: #C9D1D9">, vad, frames)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> segments, sample_rate, audio_length</span></span></code></pre>\n<h2 id="deepspeech-cli-for-real-time-and-asynchronous-speech-to-text">DeepSpeech CLI for Real Time and Asynchronous Speech to Text</h2>\n<p>Everything is set up to transcribe audio data with DeepSpeech via pretrained models. Now, let\u2019s look at how to turn the functionality we created above into a command line interface for real time and asynchronous speech to text. We start by importing a bunch of libraries for operating with the command line - <code is:raw>sys</code>, <code is:raw>os</code>, <code is:raw>logging</code>, <code is:raw>argparse</code>, <code is:raw>subprocess</code>, and <code is:raw>shlex</code>. We also need to import <code is:raw>numpy</code> and the <code is:raw>wav_transcriber</code> we made above to work with the audio data.</p>\n<h3 id="reading-arguments-for-deepspeech-speech-to-text">Reading Arguments for DeepSpeech Speech to Text</h3>\n<p>We create a main function that takes one parameter - <code is:raw>args</code>. These are the arguments passed in through the command line. We use the <code is:raw>argparse</code> libraries to parse the arguments sent in. We also create helpful tips on how to use each one.</p>\n<p>We use <code is:raw>aggressive</code> to determine how aggressively we want to filter. <code is:raw>audio</code> directs us to the audio file path. <code is:raw>model</code> points us to the directory containing the model and scorer. Finally, <code is:raw>stream</code> dictates whether or not we are streaming audio. Neither <code is:raw>stream</code> nor <code is:raw>audio</code> is required, but one or the other must be present.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> sys</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> os</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> logging</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> argparse</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> subprocess</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> shlex</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> numpy </span><span style="color: #FF7B72">as</span><span style="color: #C9D1D9"> np</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> wav_transcriber</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #8B949E"># Debug helpers</span></span>\n<span class="line"><span style="color: #C9D1D9">logging.basicConfig(</span><span style="color: #FFA657">stream</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">sys.stderr, </span><span style="color: #FFA657">level</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">logging.</span><span style="color: #79C0FF">DEBUG</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">main</span><span style="color: #C9D1D9">(args):</span></span>\n<span class="line"><span style="color: #C9D1D9">   parser </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> argparse.ArgumentParser(</span><span style="color: #FFA657">description</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&#39;Transcribe long audio files using webRTC VAD or use the streaming interface&#39;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   parser.add_argument(</span><span style="color: #A5D6FF">&#39;--aggressive&#39;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">type</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">choices</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">range</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">4</span><span style="color: #C9D1D9">), </span><span style="color: #FFA657">required</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">False</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">                       </span><span style="color: #FFA657">help</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&#39;Determines how aggressive filtering out non-speech is. (Interger between 0-3)&#39;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   parser.add_argument(</span><span style="color: #A5D6FF">&#39;--audio&#39;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">required</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">False</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">                       </span><span style="color: #FFA657">help</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&#39;Path to the audio file to run (WAV format)&#39;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   parser.add_argument(</span><span style="color: #A5D6FF">&#39;--model&#39;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">required</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">True</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">                       </span><span style="color: #FFA657">help</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&#39;Path to directory that contains all model files (output_graph and scorer)&#39;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   parser.add_argument(</span><span style="color: #A5D6FF">&#39;--stream&#39;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">required</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">False</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">action</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&#39;store_true&#39;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">                       </span><span style="color: #FFA657">help</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&#39;To use deepspeech streaming interface&#39;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   args </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> parser.parse_args()</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> args.stream </span><span style="color: #FF7B72">is</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">True</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(</span><span style="color: #A5D6FF">&quot;Opening mic for streaming&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">elif</span><span style="color: #C9D1D9"> args.audio </span><span style="color: #FF7B72">is</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">not</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">None</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">       logging.debug(</span><span style="color: #A5D6FF">&quot;Transcribing audio file @ </span><span style="color: #79C0FF">%s</span><span style="color: #A5D6FF">&quot;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">%</span><span style="color: #C9D1D9"> args.audio)</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">else</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">       parser.print_help()</span></span>\n<span class="line"><span style="color: #C9D1D9">       parser.exit()</span></span></code></pre>\n<h3 id="using-deepspeech-for-real-time-or-asynchronous-speech-recognition">Using DeepSpeech for Real Time or Asynchronous Speech Recognition</h3>\n<p>This is still inside the main function we started above. Once we parse all the arguments, we load up DeepSpeech. First, we get the directory containing the models. Next, we call the <code is:raw>wav_transcriber</code> to resolve and load the models.</p>\n<p>If we pass the path to an audio data file in the command line, then we will run asynchronous speech recognition. The first thing we do for that is call the VAD segment generator to generate the VAD segments and get the sample rate and audio length. Next, we open up a text file to transcribe to.</p>\n<p>For each of the enumerated segments, we will process each chunk by using <code is:raw>numpy</code> to pull the segment from the buffer and the speech to text function from <code is:raw>wav_transcriber</code> to do the speech to text functionality. We write to the text file until we run out of audio segments.</p>\n<p>If we pass stream instead of audio, then we open up the mic to stream audio data in. If you don\u2019t need real time automatic speech recognition, then you can ignore this part. First, we have to spin up a subprocess to open up a mic to stream in real time just like we did with PyTorch local speech recognition.</p>\n<p>We use the <code is:raw>subprocess</code> and <code is:raw>shlex</code> libraries to open the mic to stream voice audio until we shut it down. The model will read 512 bytes of audio data at a time and transcribe it.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #8B949E"># Point to a path containing the pre-trained models &amp; resolve ~ if used</span></span>\n<span class="line"><span style="color: #C9D1D9">dirName </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> os.path.expanduser(args.model)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #8B949E"># Resolve all the paths of model files</span></span>\n<span class="line"><span style="color: #C9D1D9">output_graph, scorer </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wav_transcriber.resolve_models(dirName)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #8B949E"># Load output_graph, alpahbet and scorer</span></span>\n<span class="line"><span style="color: #C9D1D9">model_retval </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wav_transcriber.load_model(output_graph, scorer)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> args.audio </span><span style="color: #FF7B72">is</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">not</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">None</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">    </span><span style="color: #8B949E"># Run VAD on the input file</span></span>\n<span class="line"><span style="color: #C9D1D9">    waveFile </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> args.audio</span></span>\n<span class="line"><span style="color: #C9D1D9">    segments, sample_rate, audio_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wav_transcriber.vad_segment_generator(waveFile, args.aggressive)</span></span>\n<span class="line"><span style="color: #C9D1D9">    f </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">open</span><span style="color: #C9D1D9">(waveFile.rstrip(</span><span style="color: #A5D6FF">&quot;.wav&quot;</span><span style="color: #C9D1D9">) </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;.txt&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #A5D6FF">&#39;w&#39;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">    logging.debug(</span><span style="color: #A5D6FF">&quot;Saving Transcript @: </span><span style="color: #79C0FF">%s</span><span style="color: #A5D6FF">&quot;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">%</span><span style="color: #C9D1D9"> waveFile.rstrip(</span><span style="color: #A5D6FF">&quot;.wav&quot;</span><span style="color: #C9D1D9">) </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;.txt&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">    </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> i, segment </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">enumerate</span><span style="color: #C9D1D9">(segments):</span></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #8B949E"># Run deepspeech on the chunk that just completed VAD</span></span>\n<span class="line"><span style="color: #C9D1D9">        logging.debug(</span><span style="color: #A5D6FF">&quot;Processing chunk </span><span style="color: #79C0FF">%002d</span><span style="color: #A5D6FF">&quot;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">%</span><span style="color: #C9D1D9"> (i,))</span></span>\n<span class="line"><span style="color: #C9D1D9">        audio </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> np.frombuffer(segment, </span><span style="color: #FFA657">dtype</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">np.int16)</span></span>\n<span class="line"><span style="color: #C9D1D9">        output </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wav_transcriber.stt(model_retval, audio, sample_rate)</span></span>\n<span class="line"><span style="color: #C9D1D9">        logging.debug(</span><span style="color: #A5D6FF">&quot;Transcript: </span><span style="color: #79C0FF">%s</span><span style="color: #A5D6FF">&quot;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">%</span><span style="color: #C9D1D9"> output)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">        f.write(output </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot; &quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">    </span><span style="color: #8B949E"># Summary of the files processed</span></span>\n<span class="line"><span style="color: #C9D1D9">    f.close()</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #FF7B72">else</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">    sctx </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> model_retval.createStream()</span></span>\n<span class="line"><span style="color: #C9D1D9">    subproc </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> subprocess.Popen(shlex.split(</span><span style="color: #A5D6FF">&#39;rec -q -V0 -e signed -L -c 1 -b 16 -r 16k -t raw - gain -2&#39;</span><span style="color: #C9D1D9">),</span></span>\n<span class="line"><span style="color: #C9D1D9">                                </span><span style="color: #FFA657">stdout</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">subprocess.</span><span style="color: #79C0FF">PIPE</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">                                </span><span style="color: #FFA657">bufsize</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">    </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(</span><span style="color: #A5D6FF">&#39;You can start speaking now. Press Control-C to stop recording.&#39;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">    </span><span style="color: #FF7B72">try</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #FF7B72">while</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">True</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">            data </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> subproc.stdout.read(</span><span style="color: #79C0FF">512</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">            sctx.feedAudioContent(np.frombuffer(data, np.int16))</span></span>\n<span class="line"><span style="color: #C9D1D9">    </span><span style="color: #FF7B72">except</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">KeyboardInterrupt</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(</span><span style="color: #A5D6FF">&#39;Transcription: &#39;</span><span style="color: #C9D1D9">, sctx.finishStream())</span></span>\n<span class="line"><span style="color: #C9D1D9">        subproc.terminate()</span></span>\n<span class="line"><span style="color: #C9D1D9">        subproc.wait()</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__name__</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">==</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&#39;__main__&#39;</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">   main(sys.argv[</span><span style="color: #79C0FF">1</span><span style="color: #C9D1D9">:])</span></span></code></pre>\n<h2 id="summary">Summary</h2>\n<p>We started this post out with a high level view of DeepSpeech, an open source speech recognition software. It was inspired by a 2014 paper from Baidu and is currently maintained by Mozilla.</p>\n<p>After a basic introduction, we stepped into a guide on how to use DeepSpeech to locally transcribe speech to text. While it may have been possible to create all of this code in one document, we opted for a modular approach with principles of software engineering in mind.</p>\n<p>We created three modules. One to handle WAV files, which are the audio data files that we can use DeepSpeech to transcribe. One to transcribe from WAV files, and one more file to create a command line interface to use DeepSpeech. Our CLI allows us to pass in options to pick if we want to do real time speech recognition or run speech recognition on an existing WAV audio file.</p>' }, "file": "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/guide-deepspeech-speech-to-text/index.md" };
function rawContent() {
  return "\r\nNo, we\u2019re not talking about you Cthulhu. This is a different type of DeepSpeech. The DeepSpeech we\u2019re talking about today is a Python speech to text library. Speech to text is part of [Natural Language Processing (NLP)](https://pythonalgos.com/?p=1436). Automated speech recognition, or ASR, started out as an offshoot of NLP in the 1990s.\r\n\r\nToday, there are tons of audio libraries that can help you [manipulate audio data](https://blog.deepgram.com/best-python-audio-manipulation-tools/) such as DeepSpeech and [PyTorch](https://blog.deepgram.com/pytorch-intro-with-torchaudio/). In this post, we will be using DeepSpeech to do both asynchronous and real time speech transcription. We will cover:\r\n\r\n*   [What is DeepSpeech?](#what-is-deepspeech)\r\n*   [Set Up for Local Speech to Text with DeepSpeech](#set-up-for-local-speech-to-text-with-deepspeech)\r\n*   [File Handler for DeepSpeech Speech Transcription](#file-handler-for-deepspeech-speech-transcription)\r\n*   [Transcribe Speech to Text for WAV file with DeepSpeech](#transcribe-speech-to-text-for-wav-file-with-deepspeech)\r\n*   [DeepSpeech CLI for Real-Time and Asynchronous Speech to Text](#deepspeech-cli-for-real-time-and-asynchronous-speech-to-text)\r\n*   [Summary](#summary)\r\n\r\n## What is DeepSpeech?\r\n\r\nDeepSpeech is an open source Python library that enables us to build automatic speech recognition systems. It is based on Baidu\u2019s 2014 paper titled [Deep Speech: Scaling up end-to-end speech recognition](https://arxiv.org/abs/1412.5567).\r\n\r\nThe initial proposal for Deep Speech was simple - let\u2019s create a speech recognition system based entirely off of deep learning. The paper describes a solution using RNNs trained with multiple GPUs with no concept of phonemes. The authors, Hannun et al., show that their solution also outperformed the existing solutions at the time and was more robust to background noise without a need for filtering.\r\n\r\nSince then, Mozilla has been the one in charge of maintaining the open source Python package for DeepSpeech. Before moving on, it\u2019s important to note that DeepSpeech is not yet compatible with Python 3.10 nor some more recent versions of \\*nix kernels. I suggest using a virtual machine or Docker container to develop with DeepSpeech on unsupported OSes.\r\n\r\n## Set Up for Local Speech to Text with DeepSpeech\r\n\r\nTo use DeepSpeech, we have to install a few libraries. We need `deepspeech`, `numpy`, and `webrtcvad`. We can install all of these by running `pip install deepspeech numpy webrtcvad`. The `webrtcvad` library is the voice activity detection (VAD) library developed by Google for WebRTC (real time communication).\r\n\r\nFor the asynchronous transcription, we\u2019re going to need three files. One file to handle interaction with WAV data, one file to transcribe speech to text on a WAV file, and one to use these two in the command line. We will also be using a pretrained DeepSpeech model and scorer. You can download their model by running the following lines in your terminal:\r\n\r\n```bash\r\nwget https://github.com/mozilla/DeepSpeech/releases/download/v0.9.3/deepspeech-0.9.3-models.pbmm\r\nwget https://github.com/mozilla/DeepSpeech/releases/download/v0.9.3/deepspeech-0.9.3-models.scorer\r\n```\r\n\r\n## File Handler for DeepSpeech Speech Transcription\r\n\r\nThe first file we create is the WAV handling file. This file should be named something like `wav_handler.py`. We import three built-in libraries to do this, `wave`, `collections`, and `contextlib`. We create four functions and one class. We need one function each to read WAV files, write WAV files, create Frames, and detect voice activity. Our one class represents individual frames in the WAV file.\r\n\r\n### Reading Audio Data from a WAV file\r\n\r\nLet\u2019s start with creating a function to read WAV files. This function will take an input, this input is the path to a WAV file. The file will use the `contextlib` library to open the WAV file and read in the contents as bytes. Next, we run multiple asserts on the WAV file - it must have one channel, have a sample width of 2, have a sample rate of 8, 16, or 32 kHz.\r\n\r\nOnce we have asserted that the WAV file is in the right format for processing, we extract the frames. Next, we extract the pcm data from the frames and the duration from the metadata. Finally, we return the PCM data, the sample rate, and the duration.\r\n\r\n```py\r\nimport collections\r\nimport contextlib\r\nimport wave\r\n\r\n\"\"\"Reads a .wav file.\r\nInput: path to a .wav file\r\nOutput: tuple of pcm data, sample rate, and duration\r\n\"\"\"\r\ndef read_wave(path):\r\n   with contextlib.closing(wave.open(path, 'rb')) as wf:\r\n       num_channels = wf.getnchannels()\r\n       assert num_channels == 1\r\n       sample_width = wf.getsampwidth()\r\n       assert sample_width == 2\r\n       sample_rate = wf.getframerate()\r\n       assert sample_rate in (8000, 16000, 32000)\r\n       frames = wf.getnframes()\r\n       pcm_data = wf.readframes(frames)\r\n       duration = frames / sample_rate\r\n       return pcm_data, sample_rate, duration\r\n```\r\n\r\n### Writing Audio Data to a WAV file\r\n\r\nNow let\u2019s create the function to write audio data to a WAV file. This function requires three parameters, the path to a file to write to, the audio data, and the sample rate. This function writes a WAV file in the same way that the read function asserts its parameters. All we do is here is set the channels, sample width, and frame rate and then write the audio frames.\r\n\r\n```py\r\n\"\"\"Writes a .wav file.\r\nInput: path to new .wav file, PCM audio data, and sample rate.\r\nOutput: a .wav file\r\n\"\"\"\r\ndef write_wave(path, audio, sample_rate):\r\n   with contextlib.closing(wave.open(path, 'wb')) as wf:\r\n       wf.setnchannels(1)\r\n       wf.setsampwidth(2)\r\n       wf.setframerate(sample_rate)\r\n       wf.writeframes(audio)\r\n```\r\n\r\n### Creating Frames of Audio Data for DeepSpeech to Transcribe\r\n\r\nWe\u2019re going to create a class called `Frame` to hold some information to represent our audio data and make it easier to handle. This object requires three parameters to be created: the bytes, the timestamp in the audio file, and the duration of the `Frame`.\r\n\r\nWe also need to create a function to create frames. You can think of this function as a frame [generator](https://docs.google.com/document/d/1JwfuaBrao_pZjLBy6isPdqv_hndQnetCJ8MjYWlLtlI/edit?pli=1\\&disco=AAAAddu7kIo) or a frame factory that returns an iterator. This function requires three parameters: the frame duration in milliseconds, the audio data, and the sample rate.\r\n\r\nThis function starts by deriving an interval of frames from the passed in sample rate and frame duration in milliseconds. We start at an offset and timestamp of 0. We also create a duration constant equal to the number of frames in a second.\r\n\r\nWhile the current offset can be incremented by the interval constant and be within the number of frames of the audio, we generate a `Frame` for each interval and then increment the timestamp and offset appropriately.\r\n\r\n```py\r\n\"\"\"Represents a \"frame\" of audio data.\r\nRequires the number of byes, the timestamp of the frame, and the duration on init\"\"\"\r\nclass Frame(object):\r\n   def __init__(self, bytes, timestamp, duration):\r\n       self.bytes = bytes\r\n       self.timestamp = timestamp\r\n       self.duration = duration\r\n\r\n\"\"\"Generates audio frames from PCM audio data.\r\nInput: the desired frame duration in milliseconds, the PCM data, and\r\nthe sample rate.\r\nYields/Generates: Frames of the requested duration.\r\n\"\"\"\r\ndef frame_generator(frame_duration_ms, audio, sample_rate):\r\n   n = int(sample_rate * (frame_duration_ms / 1000.0) * 2)\r\n   offset = 0\r\n   timestamp = 0.0\r\n   duration = (float(n) / sample_rate) / 2.0\r\n   while offset + n < len(audio):\r\n       yield Frame(audio[offset:offset + n], timestamp, duration)\r\n       timestamp += duration\r\n       offset += n\r\n```\r\n\r\n### Collecting Voice Activated Frames for Speech to Text with DeepSpeech\r\n\r\nNext, let\u2019s create a function to collect all the frames that contain voice. This function requires a sample rate, the frame duration in milliseconds, the padding duration in milliseconds, a voice activation detector (VAD) from `webrtcvad`, and the audio data frames.\r\n\r\nThe VAD algorithm uses a padded ring buffer and checks to see what percentage of the frames in the window are voiced. When the window reaches a 90% voiced frame rate, the VAD triggers and begins yielding audio frames. While generating frames, if the percentage of voiced audio data in the frame drops below 10%, it will stop generating frames.\r\n\r\n```py\r\n\"\"\"Filters out non-voiced audio frames.\r\nGiven a webrtcvad.Vad and a source of audio frames, yields only\r\nthe voiced audio.\r\nArguments:\r\nsample_rate - The audio sample rate, in Hz.\r\nframe_duration_ms - The frame duration in milliseconds.\r\npadding_duration_ms - The amount to pad the window, in milliseconds.\r\nvad - An instance of webrtcvad.Vad.\r\nframes - a source of audio frames (sequence or generator).\r\nReturns: A generator that yields PCM audio data.\r\n\"\"\"\r\ndef vad_collector(sample_rate, frame_duration_ms,\r\n                 padding_duration_ms, vad, frames):\r\n   num_padding_frames = int(padding_duration_ms / frame_duration_ms)\r\n   # We use a deque for our sliding window/ring buffer.\r\n   ring_buffer = collections.deque(maxlen=num_padding_frames)\r\n   # We have two states: TRIGGERED and NOTTRIGGERED. We start in the\r\n   # NOTTRIGGERED state.\r\n   triggered = False\r\n\r\n   voiced_frames = []\r\n   for frame in frames:\r\n       is_speech = vad.is_speech(frame.bytes, sample_rate)\r\n\r\n       if not triggered:\r\n           ring_buffer.append((frame, is_speech))\r\n           num_voiced = len([f for f, speech in ring_buffer if speech])\r\n           # If we're NOTTRIGGERED and more than 90% of the frames in\r\n           # the ring buffer are voiced frames, then enter the\r\n           # TRIGGERED state.\r\n           if num_voiced > 0.9 * ring_buffer.maxlen:\r\n               triggered = True\r\n               # We want to yield all the audio we see from now until\r\n               # we are NOTTRIGGERED, but we have to start with the\r\n               # audio that's already in the ring buffer.\r\n               for f, s in ring_buffer:\r\n                   voiced_frames.append(f)\r\n               ring_buffer.clear()\r\n       else:\r\n           # We're in the TRIGGERED state, so collect the audio data\r\n           # and add it to the ring buffer.\r\n           voiced_frames.append(frame)\r\n           ring_buffer.append((frame, is_speech))\r\n           num_unvoiced = len([f for f, speech in ring_buffer if not speech])\r\n           # If more than 90% of the frames in the ring buffer are\r\n           # unvoiced, then enter NOTTRIGGERED and yield whatever\r\n           # audio we've collected.\r\n           if num_unvoiced > 0.9 * ring_buffer.maxlen:\r\n               triggered = False\r\n               yield b''.join([f.bytes for f in voiced_frames])\r\n               ring_buffer.clear()\r\n               voiced_frames = []\r\n   if triggered:\r\n       pass\r\n   # If we have any leftover voiced audio when we run out of input,\r\n   # yield it.\r\n   if voiced_frames:\r\n       yield b''.join([f.bytes for f in voiced_frames])\r\n```\r\n\r\n## Transcribe Speech to Text for WAV file with DeepSpeech\r\n\r\nWe\u2019re going to create a new file for this section. This file should be named something like `wav_transcriber.py`. This layer completely abstracts out WAV handling from the CLI (which we create below). We use these functions to call DeepSpeech on the audio data and transcribe it.\r\n\r\n### Pick Which DeepSpeech Model to Use\r\n\r\nThe first function we create in this file is the function to load up the model and scorer for DeepSpeech to run speech to text with. This function takes two parameters, the models graph, which we create a function to produce below, and the path to the scorer file. All it does is load the model from the graph and enable the use of the scorer. This function returns a DeepSpeech object.\r\n\r\n```py\r\nimport glob\r\nimport webrtcvad\r\nimport logging\r\nimport wav_handler\r\nfrom deepspeech import Model\r\nfrom timeit import default_timer as timer\r\n\r\n'''\r\nLoad the pre-trained model into the memory\r\n@param models: Output Graph Protocol Buffer file\r\n@param scorer: Scorer file\r\n@Retval\r\nReturns a DeepSpeech Object\r\n'''\r\ndef load_model(models, scorer):\r\n   ds = Model(models)\r\n   ds.enableExternalScorer(scorer)\r\n   return ds\r\n```\r\n\r\n### Speech to Text on an Audio File with DeepSpeech\r\n\r\nThis function is the one that does the actual speech recognition. It takes three inputs, a DeepSpeech model, the audio data, and the sample rate.\r\n\r\nWe begin by setting the time to 0 and calculating the length of the audio. All we really have to do is call the DeepSpeech model\u2019s `stt` function to do our own `stt` function. We pass the audio file to the `stt` function and return the output.\r\n\r\n```py\r\n'''\r\nRun Inference on input audio file\r\n@param ds: Deepspeech object\r\n@param audio: Input audio for running inference on\r\n@param fs: Sample rate of the input audio file\r\n@Retval:\r\nReturns a list [Inference, Inference Time, Audio Length]\r\n'''\r\ndef stt(ds, audio, fs):\r\n   inference_time = 0.0\r\n   audio_length = len(audio) * (1 / fs)\r\n\r\n   # Run Deepspeech\r\n   output = ds.stt(audio)\r\n\r\n   return output\r\n```\r\n\r\n### DeepSpeech Model Graph Creator Function\r\n\r\nThis is the function that creates the model graph for the `load_model` function we created a couple sections above. This function takes the path to a directory. From that directory, it looks for files with the DeepSpeech model extension, `pbmm` and the DeepSpeech scorer file extension, `.scorer`. Then, it returns both of those values.\r\n\r\n```py\r\n'''\r\nResolve directory path for the models and fetch each of them.\r\n@param dirName: Path to the directory containing pre-trained models\r\n@Retval:\r\nRetunns a tuple containing each of the model files (pb, scorer)\r\n'''\r\ndef resolve_models(dirName):\r\n   pb = glob.glob(dirName + \"/*.pbmm\")[0]\r\n   logging.debug(\"Found Model: %s\" % pb)\r\n\r\n   scorer = glob.glob(dirName + \"/*.scorer\")[0]\r\n   logging.debug(\"Found scorer: %s\" % scorer)\r\n\r\n   return pb, scorer\r\n```\r\n\r\n### Voice Activation Detection to Create Segments for Speech to Text\r\n\r\nThe last function in our WAV transcription file generates segments of text that contain voice. We use the WAV handler file we created earlier and `webrtcvad` to do the heavy lifting. This function requires two parameters: a WAV file and an integer value from 0 to 3 representing how aggressively we want to filter out non-voice activity.\r\n\r\nWe call the `read_wave` function from the `wav_handler.py` file we created earlier and imported above to get the audio data, sample rate, and audio length. We then assert that the sample rate is 16kHz before moving on to create a VAD object. Next, we call the frame generator from `wav_handler`.\r\n\r\nWe convert the generated iterator to a list which we pass to the `vad_collector` function from `wav_handler` along with the sample rate, frame duration (30 ms), padding duration (300 ms), and VAD object. Finally, we return the collected VAD segments along with the sample rate and audio length.\r\n\r\n```py\r\n'''\r\nGenerate VAD segments. Filters out non-voiced audio frames.\r\n@param waveFile: Input wav file to run VAD on.0\r\n@Retval:\r\nReturns tuple of\r\n   segments: a bytearray of multiple smaller audio frames\r\n             (The longer audio split into mutiple smaller one's)\r\n   sample_rate: Sample rate of the input audio file\r\n   audio_length: Duraton of the input audio file\r\n'''\r\ndef vad_segment_generator(wavFile, aggressiveness):\r\n   audio, sample_rate, audio_length = wav_handler.read_wave(wavFile)\r\n   assert sample_rate == 16000, \"Only 16000Hz input WAV files are supported for now!\"\r\n   vad = webrtcvad.Vad(int(aggressiveness))\r\n   frames = wav_handler.frame_generator(30, audio, sample_rate)\r\n   frames = list(frames)\r\n   segments = wav_handler.vad_collector(sample_rate, 30, 300, vad, frames)\r\n\r\n   return segments, sample_rate, audio_length\r\n```\r\n\r\n## DeepSpeech CLI for Real Time and Asynchronous Speech to Text\r\n\r\nEverything is set up to transcribe audio data with DeepSpeech via pretrained models. Now, let\u2019s look at how to turn the functionality we created above into a command line interface for real time and asynchronous speech to text. We start by importing a bunch of libraries for operating with the command line - `sys`, `os`, `logging`, `argparse`, `subprocess`, and `shlex`. We also need to import `numpy` and the `wav_transcriber` we made above to work with the audio data.\r\n\r\n### Reading Arguments for DeepSpeech Speech to Text\r\n\r\nWe create a main function that takes one parameter - `args`. These are the arguments passed in through the command line. We use the `argparse` libraries to parse the arguments sent in. We also create helpful tips on how to use each one.\r\n\r\nWe use `aggressive` to determine how aggressively we want to filter. `audio` directs us to the audio file path. `model` points us to the directory containing the model and scorer. Finally, `stream` dictates whether or not we are streaming audio. Neither `stream` nor `audio` is required, but one or the other must be present.\r\n\r\n```py\r\nimport sys\r\nimport os\r\nimport logging\r\nimport argparse\r\nimport subprocess\r\nimport shlex\r\nimport numpy as np\r\nimport wav_transcriber\r\n\r\n# Debug helpers\r\nlogging.basicConfig(stream=sys.stderr, level=logging.DEBUG)\r\n\r\ndef main(args):\r\n   parser = argparse.ArgumentParser(description='Transcribe long audio files using webRTC VAD or use the streaming interface')\r\n   parser.add_argument('--aggressive', type=int, choices=range(4), required=False,\r\n                       help='Determines how aggressive filtering out non-speech is. (Interger between 0-3)')\r\n   parser.add_argument('--audio', required=False,\r\n                       help='Path to the audio file to run (WAV format)')\r\n   parser.add_argument('--model', required=True,\r\n                       help='Path to directory that contains all model files (output_graph and scorer)')\r\n   parser.add_argument('--stream', required=False, action='store_true',\r\n                       help='To use deepspeech streaming interface')\r\n   args = parser.parse_args()\r\n   if args.stream is True:\r\n       print(\"Opening mic for streaming\")\r\n   elif args.audio is not None:\r\n       logging.debug(\"Transcribing audio file @ %s\" % args.audio)\r\n   else:\r\n       parser.print_help()\r\n       parser.exit()\r\n```\r\n\r\n### Using DeepSpeech for Real Time or Asynchronous Speech Recognition\r\n\r\nThis is still inside the main function we started above. Once we parse all the arguments, we load up DeepSpeech. First, we get the directory containing the models. Next, we call the `wav_transcriber` to resolve and load the models.\r\n\r\nIf we pass the path to an audio data file in the command line, then we will run asynchronous speech recognition. The first thing we do for that is call the VAD segment generator to generate the VAD segments and get the sample rate and audio length. Next, we open up a text file to transcribe to.\r\n\r\nFor each of the enumerated segments, we will process each chunk by using `numpy` to pull the segment from the buffer and the speech to text function from `wav_transcriber` to do the speech to text functionality. We write to the text file until we run out of audio segments.\r\n\r\nIf we pass stream instead of audio, then we open up the mic to stream audio data in. If you don\u2019t need real time automatic speech recognition, then you can ignore this part. First, we have to spin up a subprocess to open up a mic to stream in real time just like we did with PyTorch local speech recognition.\r\n\r\nWe use the `subprocess` and `shlex` libraries to open the mic to stream voice audio until we shut it down. The model will read 512 bytes of audio data at a time and transcribe it.\r\n\r\n```py\r\n# Point to a path containing the pre-trained models & resolve ~ if used\r\ndirName = os.path.expanduser(args.model)\r\n\r\n# Resolve all the paths of model files\r\noutput_graph, scorer = wav_transcriber.resolve_models(dirName)\r\n\r\n# Load output_graph, alpahbet and scorer\r\nmodel_retval = wav_transcriber.load_model(output_graph, scorer)\r\n\r\nif args.audio is not None:\r\n    # Run VAD on the input file\r\n    waveFile = args.audio\r\n    segments, sample_rate, audio_length = wav_transcriber.vad_segment_generator(waveFile, args.aggressive)\r\n    f = open(waveFile.rstrip(\".wav\") + \".txt\", 'w')\r\n    logging.debug(\"Saving Transcript @: %s\" % waveFile.rstrip(\".wav\") + \".txt\")\r\n\r\n    for i, segment in enumerate(segments):\r\n        # Run deepspeech on the chunk that just completed VAD\r\n        logging.debug(\"Processing chunk %002d\" % (i,))\r\n        audio = np.frombuffer(segment, dtype=np.int16)\r\n        output = wav_transcriber.stt(model_retval, audio, sample_rate)\r\n        logging.debug(\"Transcript: %s\" % output)\r\n\r\n        f.write(output + \" \")\r\n\r\n    # Summary of the files processed\r\n    f.close()\r\n\r\nelse:\r\n    sctx = model_retval.createStream()\r\n    subproc = subprocess.Popen(shlex.split('rec -q -V0 -e signed -L -c 1 -b 16 -r 16k -t raw - gain -2'),\r\n                                stdout=subprocess.PIPE,\r\n                                bufsize=0)\r\n    print('You can start speaking now. Press Control-C to stop recording.')\r\n\r\n    try:\r\n        while True:\r\n            data = subproc.stdout.read(512)\r\n            sctx.feedAudioContent(np.frombuffer(data, np.int16))\r\n    except KeyboardInterrupt:\r\n        print('Transcription: ', sctx.finishStream())\r\n        subproc.terminate()\r\n        subproc.wait()\r\n\r\nif __name__ == '__main__':\r\n   main(sys.argv[1:])\r\n```\r\n\r\n## Summary\r\n\r\nWe started this post out with a high level view of DeepSpeech, an open source speech recognition software. It was inspired by a 2014 paper from Baidu and is currently maintained by Mozilla.\r\n\r\nAfter a basic introduction, we stepped into a guide on how to use DeepSpeech to locally transcribe speech to text. While it may have been possible to create all of this code in one document, we opted for a modular approach with principles of software engineering in mind.\r\n\r\nWe created three modules. One to handle WAV files, which are the audio data files that we can use DeepSpeech to transcribe. One to transcribe from WAV files, and one more file to create a command line interface to use DeepSpeech. Our CLI allows us to pass in options to pick if we want to do real time speech recognition or run speech recognition on an existing WAV audio file.\r\n\r\n        ";
}
function compiledContent() {
  return '<p>No, we\u2019re not talking about you Cthulhu. This is a different type of DeepSpeech. The DeepSpeech we\u2019re talking about today is a Python speech to text library. Speech to text is part of <a href="https://pythonalgos.com/?p=1436">Natural Language Processing (NLP)</a>. Automated speech recognition, or ASR, started out as an offshoot of NLP in the 1990s.</p>\n<p>Today, there are tons of audio libraries that can help you <a href="https://blog.deepgram.com/best-python-audio-manipulation-tools/">manipulate audio data</a> such as DeepSpeech and <a href="https://blog.deepgram.com/pytorch-intro-with-torchaudio/">PyTorch</a>. In this post, we will be using DeepSpeech to do both asynchronous and real time speech transcription. We will cover:</p>\n<ul>\n<li><a href="#what-is-deepspeech">What is DeepSpeech?</a></li>\n<li><a href="#set-up-for-local-speech-to-text-with-deepspeech">Set Up for Local Speech to Text with DeepSpeech</a></li>\n<li><a href="#file-handler-for-deepspeech-speech-transcription">File Handler for DeepSpeech Speech Transcription</a></li>\n<li><a href="#transcribe-speech-to-text-for-wav-file-with-deepspeech">Transcribe Speech to Text for WAV file with DeepSpeech</a></li>\n<li><a href="#deepspeech-cli-for-real-time-and-asynchronous-speech-to-text">DeepSpeech CLI for Real-Time and Asynchronous Speech to Text</a></li>\n<li><a href="#summary">Summary</a></li>\n</ul>\n<h2 id="what-is-deepspeech">What is DeepSpeech?</h2>\n<p>DeepSpeech is an open source Python library that enables us to build automatic speech recognition systems. It is based on Baidu\u2019s 2014 paper titled <a href="https://arxiv.org/abs/1412.5567">Deep Speech: Scaling up end-to-end speech recognition</a>.</p>\n<p>The initial proposal for Deep Speech was simple - let\u2019s create a speech recognition system based entirely off of deep learning. The paper describes a solution using RNNs trained with multiple GPUs with no concept of phonemes. The authors, Hannun et al., show that their solution also outperformed the existing solutions at the time and was more robust to background noise without a need for filtering.</p>\n<p>Since then, Mozilla has been the one in charge of maintaining the open source Python package for DeepSpeech. Before moving on, it\u2019s important to note that DeepSpeech is not yet compatible with Python 3.10 nor some more recent versions of *nix kernels. I suggest using a virtual machine or Docker container to develop with DeepSpeech on unsupported OSes.</p>\n<h2 id="set-up-for-local-speech-to-text-with-deepspeech">Set Up for Local Speech to Text with DeepSpeech</h2>\n<p>To use DeepSpeech, we have to install a few libraries. We need <code is:raw>deepspeech</code>, <code is:raw>numpy</code>, and <code is:raw>webrtcvad</code>. We can install all of these by running <code is:raw>pip install deepspeech numpy webrtcvad</code>. The <code is:raw>webrtcvad</code> library is the voice activity detection (VAD) library developed by Google for WebRTC (real time communication).</p>\n<p>For the asynchronous transcription, we\u2019re going to need three files. One file to handle interaction with WAV data, one file to transcribe speech to text on a WAV file, and one to use these two in the command line. We will also be using a pretrained DeepSpeech model and scorer. You can download their model by running the following lines in your terminal:</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">wget https://github.com/mozilla/DeepSpeech/releases/download/v0.9.3/deepspeech-0.9.3-models.pbmm</span></span>\n<span class="line"><span style="color: #C9D1D9">wget https://github.com/mozilla/DeepSpeech/releases/download/v0.9.3/deepspeech-0.9.3-models.scorer</span></span></code></pre>\n<h2 id="file-handler-for-deepspeech-speech-transcription">File Handler for DeepSpeech Speech Transcription</h2>\n<p>The first file we create is the WAV handling file. This file should be named something like <code is:raw>wav_handler.py</code>. We import three built-in libraries to do this, <code is:raw>wave</code>, <code is:raw>collections</code>, and <code is:raw>contextlib</code>. We create four functions and one class. We need one function each to read WAV files, write WAV files, create Frames, and detect voice activity. Our one class represents individual frames in the WAV file.</p>\n<h3 id="reading-audio-data-from-a-wav-file">Reading Audio Data from a WAV file</h3>\n<p>Let\u2019s start with creating a function to read WAV files. This function will take an input, this input is the path to a WAV file. The file will use the <code is:raw>contextlib</code> library to open the WAV file and read in the contents as bytes. Next, we run multiple asserts on the WAV file - it must have one channel, have a sample width of 2, have a sample rate of 8, 16, or 32 kHz.</p>\n<p>Once we have asserted that the WAV file is in the right format for processing, we extract the frames. Next, we extract the pcm data from the frames and the duration from the metadata. Finally, we return the PCM data, the sample rate, and the duration.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> collections</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> contextlib</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> wave</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;Reads a .wav file.</span></span>\n<span class="line"><span style="color: #A5D6FF">Input: path to a .wav file</span></span>\n<span class="line"><span style="color: #A5D6FF">Output: tuple of pcm data, sample rate, and duration</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;</span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">read_wave</span><span style="color: #C9D1D9">(path):</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">with</span><span style="color: #C9D1D9"> contextlib.closing(wave.open(path, </span><span style="color: #A5D6FF">&#39;rb&#39;</span><span style="color: #C9D1D9">)) </span><span style="color: #FF7B72">as</span><span style="color: #C9D1D9"> wf:</span></span>\n<span class="line"><span style="color: #C9D1D9">       num_channels </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wf.getnchannels()</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">assert</span><span style="color: #C9D1D9"> num_channels </span><span style="color: #FF7B72">==</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">1</span></span>\n<span class="line"><span style="color: #C9D1D9">       sample_width </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wf.getsampwidth()</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">assert</span><span style="color: #C9D1D9"> sample_width </span><span style="color: #FF7B72">==</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">2</span></span>\n<span class="line"><span style="color: #C9D1D9">       sample_rate </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wf.getframerate()</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">assert</span><span style="color: #C9D1D9"> sample_rate </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> (</span><span style="color: #79C0FF">8000</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">16000</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">32000</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">       frames </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wf.getnframes()</span></span>\n<span class="line"><span style="color: #C9D1D9">       pcm_data </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wf.readframes(frames)</span></span>\n<span class="line"><span style="color: #C9D1D9">       duration </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> frames </span><span style="color: #FF7B72">/</span><span style="color: #C9D1D9"> sample_rate</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> pcm_data, sample_rate, duration</span></span></code></pre>\n<h3 id="writing-audio-data-to-a-wav-file">Writing Audio Data to a WAV file</h3>\n<p>Now let\u2019s create the function to write audio data to a WAV file. This function requires three parameters, the path to a file to write to, the audio data, and the sample rate. This function writes a WAV file in the same way that the read function asserts its parameters. All we do is here is set the channels, sample width, and frame rate and then write the audio frames.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;Writes a .wav file.</span></span>\n<span class="line"><span style="color: #A5D6FF">Input: path to new .wav file, PCM audio data, and sample rate.</span></span>\n<span class="line"><span style="color: #A5D6FF">Output: a .wav file</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;</span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">write_wave</span><span style="color: #C9D1D9">(path, audio, sample_rate):</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">with</span><span style="color: #C9D1D9"> contextlib.closing(wave.open(path, </span><span style="color: #A5D6FF">&#39;wb&#39;</span><span style="color: #C9D1D9">)) </span><span style="color: #FF7B72">as</span><span style="color: #C9D1D9"> wf:</span></span>\n<span class="line"><span style="color: #C9D1D9">       wf.setnchannels(</span><span style="color: #79C0FF">1</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">       wf.setsampwidth(</span><span style="color: #79C0FF">2</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">       wf.setframerate(sample_rate)</span></span>\n<span class="line"><span style="color: #C9D1D9">       wf.writeframes(audio)</span></span></code></pre>\n<h3 id="creating-frames-of-audio-data-for-deepspeech-to-transcribe">Creating Frames of Audio Data for DeepSpeech to Transcribe</h3>\n<p>We\u2019re going to create a class called <code is:raw>Frame</code> to hold some information to represent our audio data and make it easier to handle. This object requires three parameters to be created: the bytes, the timestamp in the audio file, and the duration of the <code is:raw>Frame</code>.</p>\n<p>We also need to create a function to create frames. You can think of this function as a frame <a href="https://docs.google.com/document/d/1JwfuaBrao_pZjLBy6isPdqv_hndQnetCJ8MjYWlLtlI/edit?pli=1&#x26;disco=AAAAddu7kIo">generator</a> or a frame factory that returns an iterator. This function requires three parameters: the frame duration in milliseconds, the audio data, and the sample rate.</p>\n<p>This function starts by deriving an interval of frames from the passed in sample rate and frame duration in milliseconds. We start at an offset and timestamp of 0. We also create a duration constant equal to the number of frames in a second.</p>\n<p>While the current offset can be incremented by the interval constant and be within the number of frames of the audio, we generate a <code is:raw>Frame</code> for each interval and then increment the timestamp and offset appropriately.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;Represents a &quot;frame&quot; of audio data.</span></span>\n<span class="line"><span style="color: #A5D6FF">Requires the number of byes, the timestamp of the frame, and the duration on init&quot;&quot;&quot;</span></span>\n<span class="line"><span style="color: #FF7B72">class</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">Frame</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">object</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__init__</span><span style="color: #C9D1D9">(self, bytes, timestamp, duration):</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.bytes </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">bytes</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.timestamp </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> timestamp</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.duration </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> duration</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;Generates audio frames from PCM audio data.</span></span>\n<span class="line"><span style="color: #A5D6FF">Input: the desired frame duration in milliseconds, the PCM data, and</span></span>\n<span class="line"><span style="color: #A5D6FF">the sample rate.</span></span>\n<span class="line"><span style="color: #A5D6FF">Yields/Generates: Frames of the requested duration.</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;</span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">frame_generator</span><span style="color: #C9D1D9">(frame_duration_ms, audio, sample_rate):</span></span>\n<span class="line"><span style="color: #C9D1D9">   n </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">(sample_rate </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> (frame_duration_ms </span><span style="color: #FF7B72">/</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">1000.0</span><span style="color: #C9D1D9">) </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">2</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   offset </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">0</span></span>\n<span class="line"><span style="color: #C9D1D9">   timestamp </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">0.0</span></span>\n<span class="line"><span style="color: #C9D1D9">   duration </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> (</span><span style="color: #79C0FF">float</span><span style="color: #C9D1D9">(n) </span><span style="color: #FF7B72">/</span><span style="color: #C9D1D9"> sample_rate) </span><span style="color: #FF7B72">/</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">2.0</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">while</span><span style="color: #C9D1D9"> offset </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> n </span><span style="color: #FF7B72">&lt;</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">len</span><span style="color: #C9D1D9">(audio):</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">yield</span><span style="color: #C9D1D9"> Frame(audio[offset:offset </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> n], timestamp, duration)</span></span>\n<span class="line"><span style="color: #C9D1D9">       timestamp </span><span style="color: #FF7B72">+=</span><span style="color: #C9D1D9"> duration</span></span>\n<span class="line"><span style="color: #C9D1D9">       offset </span><span style="color: #FF7B72">+=</span><span style="color: #C9D1D9"> n</span></span></code></pre>\n<h3 id="collecting-voice-activated-frames-for-speech-to-text-with-deepspeech">Collecting Voice Activated Frames for Speech to Text with DeepSpeech</h3>\n<p>Next, let\u2019s create a function to collect all the frames that contain voice. This function requires a sample rate, the frame duration in milliseconds, the padding duration in milliseconds, a voice activation detector (VAD) from <code is:raw>webrtcvad</code>, and the audio data frames.</p>\n<p>The VAD algorithm uses a padded ring buffer and checks to see what percentage of the frames in the window are voiced. When the window reaches a 90% voiced frame rate, the VAD triggers and begins yielding audio frames. While generating frames, if the percentage of voiced audio data in the frame drops below 10%, it will stop generating frames.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;Filters out non-voiced audio frames.</span></span>\n<span class="line"><span style="color: #A5D6FF">Given a webrtcvad.Vad and a source of audio frames, yields only</span></span>\n<span class="line"><span style="color: #A5D6FF">the voiced audio.</span></span>\n<span class="line"><span style="color: #A5D6FF">Arguments:</span></span>\n<span class="line"><span style="color: #A5D6FF">sample_rate - The audio sample rate, in Hz.</span></span>\n<span class="line"><span style="color: #A5D6FF">frame_duration_ms - The frame duration in milliseconds.</span></span>\n<span class="line"><span style="color: #A5D6FF">padding_duration_ms - The amount to pad the window, in milliseconds.</span></span>\n<span class="line"><span style="color: #A5D6FF">vad - An instance of webrtcvad.Vad.</span></span>\n<span class="line"><span style="color: #A5D6FF">frames - a source of audio frames (sequence or generator).</span></span>\n<span class="line"><span style="color: #A5D6FF">Returns: A generator that yields PCM audio data.</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;</span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">vad_collector</span><span style="color: #C9D1D9">(sample_rate, frame_duration_ms,</span></span>\n<span class="line"><span style="color: #C9D1D9">                 padding_duration_ms, vad, frames):</span></span>\n<span class="line"><span style="color: #C9D1D9">   num_padding_frames </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">(padding_duration_ms </span><span style="color: #FF7B72">/</span><span style="color: #C9D1D9"> frame_duration_ms)</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #8B949E"># We use a deque for our sliding window/ring buffer.</span></span>\n<span class="line"><span style="color: #C9D1D9">   ring_buffer </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> collections.deque(</span><span style="color: #FFA657">maxlen</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">num_padding_frames)</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #8B949E"># We have two states: TRIGGERED and NOTTRIGGERED. We start in the</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #8B949E"># NOTTRIGGERED state.</span></span>\n<span class="line"><span style="color: #C9D1D9">   triggered </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">False</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">   voiced_frames </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> []</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> frame </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> frames:</span></span>\n<span class="line"><span style="color: #C9D1D9">       is_speech </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> vad.is_speech(frame.bytes, sample_rate)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">not</span><span style="color: #C9D1D9"> triggered:</span></span>\n<span class="line"><span style="color: #C9D1D9">           ring_buffer.append((frame, is_speech))</span></span>\n<span class="line"><span style="color: #C9D1D9">           num_voiced </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">len</span><span style="color: #C9D1D9">([f </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> f, speech </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> ring_buffer </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> speech])</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #8B949E"># If we&#39;re NOTTRIGGERED and more than 90% of the frames in</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #8B949E"># the ring buffer are voiced frames, then enter the</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #8B949E"># TRIGGERED state.</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> num_voiced </span><span style="color: #FF7B72">&gt;</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">0.9</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> ring_buffer.maxlen:</span></span>\n<span class="line"><span style="color: #C9D1D9">               triggered </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">True</span></span>\n<span class="line"><span style="color: #C9D1D9">               </span><span style="color: #8B949E"># We want to yield all the audio we see from now until</span></span>\n<span class="line"><span style="color: #C9D1D9">               </span><span style="color: #8B949E"># we are NOTTRIGGERED, but we have to start with the</span></span>\n<span class="line"><span style="color: #C9D1D9">               </span><span style="color: #8B949E"># audio that&#39;s already in the ring buffer.</span></span>\n<span class="line"><span style="color: #C9D1D9">               </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> f, s </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> ring_buffer:</span></span>\n<span class="line"><span style="color: #C9D1D9">                   voiced_frames.append(f)</span></span>\n<span class="line"><span style="color: #C9D1D9">               ring_buffer.clear()</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">else</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #8B949E"># We&#39;re in the TRIGGERED state, so collect the audio data</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #8B949E"># and add it to the ring buffer.</span></span>\n<span class="line"><span style="color: #C9D1D9">           voiced_frames.append(frame)</span></span>\n<span class="line"><span style="color: #C9D1D9">           ring_buffer.append((frame, is_speech))</span></span>\n<span class="line"><span style="color: #C9D1D9">           num_unvoiced </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">len</span><span style="color: #C9D1D9">([f </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> f, speech </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> ring_buffer </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">not</span><span style="color: #C9D1D9"> speech])</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #8B949E"># If more than 90% of the frames in the ring buffer are</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #8B949E"># unvoiced, then enter NOTTRIGGERED and yield whatever</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #8B949E"># audio we&#39;ve collected.</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> num_unvoiced </span><span style="color: #FF7B72">&gt;</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">0.9</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> ring_buffer.maxlen:</span></span>\n<span class="line"><span style="color: #C9D1D9">               triggered </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">False</span></span>\n<span class="line"><span style="color: #C9D1D9">               </span><span style="color: #FF7B72">yield</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">b</span><span style="color: #A5D6FF">&#39;&#39;</span><span style="color: #C9D1D9">.join([f.bytes </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> f </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> voiced_frames])</span></span>\n<span class="line"><span style="color: #C9D1D9">               ring_buffer.clear()</span></span>\n<span class="line"><span style="color: #C9D1D9">               voiced_frames </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> []</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> triggered:</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">pass</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #8B949E"># If we have any leftover voiced audio when we run out of input,</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #8B949E"># yield it.</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> voiced_frames:</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">yield</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">b</span><span style="color: #A5D6FF">&#39;&#39;</span><span style="color: #C9D1D9">.join([f.bytes </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> f </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> voiced_frames])</span></span></code></pre>\n<h2 id="transcribe-speech-to-text-for-wav-file-with-deepspeech">Transcribe Speech to Text for WAV file with DeepSpeech</h2>\n<p>We\u2019re going to create a new file for this section. This file should be named something like <code is:raw>wav_transcriber.py</code>. This layer completely abstracts out WAV handling from the CLI (which we create below). We use these functions to call DeepSpeech on the audio data and transcribe it.</p>\n<h3 id="pick-which-deepspeech-model-to-use">Pick Which DeepSpeech Model to Use</h3>\n<p>The first function we create in this file is the function to load up the model and scorer for DeepSpeech to run speech to text with. This function takes two parameters, the models graph, which we create a function to produce below, and the path to the scorer file. All it does is load the model from the graph and enable the use of the scorer. This function returns a DeepSpeech object.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> glob</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> webrtcvad</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> logging</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> wav_handler</span></span>\n<span class="line"><span style="color: #FF7B72">from</span><span style="color: #C9D1D9"> deepspeech </span><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> Model</span></span>\n<span class="line"><span style="color: #FF7B72">from</span><span style="color: #C9D1D9"> timeit </span><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> default_timer </span><span style="color: #FF7B72">as</span><span style="color: #C9D1D9"> timer</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #A5D6FF">&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #A5D6FF">Load the pre-trained model into the memory</span></span>\n<span class="line"><span style="color: #A5D6FF">@param models: Output Graph Protocol Buffer file</span></span>\n<span class="line"><span style="color: #A5D6FF">@param scorer: Scorer file</span></span>\n<span class="line"><span style="color: #A5D6FF">@Retval</span></span>\n<span class="line"><span style="color: #A5D6FF">Returns a DeepSpeech Object</span></span>\n<span class="line"><span style="color: #A5D6FF">&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">load_model</span><span style="color: #C9D1D9">(models, scorer):</span></span>\n<span class="line"><span style="color: #C9D1D9">   ds </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> Model(models)</span></span>\n<span class="line"><span style="color: #C9D1D9">   ds.enableExternalScorer(scorer)</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> ds</span></span></code></pre>\n<h3 id="speech-to-text-on-an-audio-file-with-deepspeech">Speech to Text on an Audio File with DeepSpeech</h3>\n<p>This function is the one that does the actual speech recognition. It takes three inputs, a DeepSpeech model, the audio data, and the sample rate.</p>\n<p>We begin by setting the time to 0 and calculating the length of the audio. All we really have to do is call the DeepSpeech model\u2019s <code is:raw>stt</code> function to do our own <code is:raw>stt</code> function. We pass the audio file to the <code is:raw>stt</code> function and return the output.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #A5D6FF">&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #A5D6FF">Run Inference on input audio file</span></span>\n<span class="line"><span style="color: #A5D6FF">@param ds: Deepspeech object</span></span>\n<span class="line"><span style="color: #A5D6FF">@param audio: Input audio for running inference on</span></span>\n<span class="line"><span style="color: #A5D6FF">@param fs: Sample rate of the input audio file</span></span>\n<span class="line"><span style="color: #A5D6FF">@Retval:</span></span>\n<span class="line"><span style="color: #A5D6FF">Returns a list [Inference, Inference Time, Audio Length]</span></span>\n<span class="line"><span style="color: #A5D6FF">&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">stt</span><span style="color: #C9D1D9">(ds, audio, fs):</span></span>\n<span class="line"><span style="color: #C9D1D9">   inference_time </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">0.0</span></span>\n<span class="line"><span style="color: #C9D1D9">   audio_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">len</span><span style="color: #C9D1D9">(audio) </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> (</span><span style="color: #79C0FF">1</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">/</span><span style="color: #C9D1D9"> fs)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #8B949E"># Run Deepspeech</span></span>\n<span class="line"><span style="color: #C9D1D9">   output </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> ds.stt(audio)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> output</span></span></code></pre>\n<h3 id="deepspeech-model-graph-creator-function">DeepSpeech Model Graph Creator Function</h3>\n<p>This is the function that creates the model graph for the <code is:raw>load_model</code> function we created a couple sections above. This function takes the path to a directory. From that directory, it looks for files with the DeepSpeech model extension, <code is:raw>pbmm</code> and the DeepSpeech scorer file extension, <code is:raw>.scorer</code>. Then, it returns both of those values.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #A5D6FF">&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #A5D6FF">Resolve directory path for the models and fetch each of them.</span></span>\n<span class="line"><span style="color: #A5D6FF">@param dirName: Path to the directory containing pre-trained models</span></span>\n<span class="line"><span style="color: #A5D6FF">@Retval:</span></span>\n<span class="line"><span style="color: #A5D6FF">Retunns a tuple containing each of the model files (pb, scorer)</span></span>\n<span class="line"><span style="color: #A5D6FF">&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">resolve_models</span><span style="color: #C9D1D9">(dirName):</span></span>\n<span class="line"><span style="color: #C9D1D9">   pb </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> glob.glob(dirName </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;/*.pbmm&quot;</span><span style="color: #C9D1D9">)[</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">]</span></span>\n<span class="line"><span style="color: #C9D1D9">   logging.debug(</span><span style="color: #A5D6FF">&quot;Found Model: </span><span style="color: #79C0FF">%s</span><span style="color: #A5D6FF">&quot;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">%</span><span style="color: #C9D1D9"> pb)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">   scorer </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> glob.glob(dirName </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;/*.scorer&quot;</span><span style="color: #C9D1D9">)[</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">]</span></span>\n<span class="line"><span style="color: #C9D1D9">   logging.debug(</span><span style="color: #A5D6FF">&quot;Found scorer: </span><span style="color: #79C0FF">%s</span><span style="color: #A5D6FF">&quot;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">%</span><span style="color: #C9D1D9"> scorer)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> pb, scorer</span></span></code></pre>\n<h3 id="voice-activation-detection-to-create-segments-for-speech-to-text">Voice Activation Detection to Create Segments for Speech to Text</h3>\n<p>The last function in our WAV transcription file generates segments of text that contain voice. We use the WAV handler file we created earlier and <code is:raw>webrtcvad</code> to do the heavy lifting. This function requires two parameters: a WAV file and an integer value from 0 to 3 representing how aggressively we want to filter out non-voice activity.</p>\n<p>We call the <code is:raw>read_wave</code> function from the <code is:raw>wav_handler.py</code> file we created earlier and imported above to get the audio data, sample rate, and audio length. We then assert that the sample rate is 16kHz before moving on to create a VAD object. Next, we call the frame generator from <code is:raw>wav_handler</code>.</p>\n<p>We convert the generated iterator to a list which we pass to the <code is:raw>vad_collector</code> function from <code is:raw>wav_handler</code> along with the sample rate, frame duration (30 ms), padding duration (300 ms), and VAD object. Finally, we return the collected VAD segments along with the sample rate and audio length.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #A5D6FF">&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #A5D6FF">Generate VAD segments. Filters out non-voiced audio frames.</span></span>\n<span class="line"><span style="color: #A5D6FF">@param waveFile: Input wav file to run VAD on.0</span></span>\n<span class="line"><span style="color: #A5D6FF">@Retval:</span></span>\n<span class="line"><span style="color: #A5D6FF">Returns tuple of</span></span>\n<span class="line"><span style="color: #A5D6FF">   segments: a bytearray of multiple smaller audio frames</span></span>\n<span class="line"><span style="color: #A5D6FF">             (The longer audio split into mutiple smaller one&#39;s)</span></span>\n<span class="line"><span style="color: #A5D6FF">   sample_rate: Sample rate of the input audio file</span></span>\n<span class="line"><span style="color: #A5D6FF">   audio_length: Duraton of the input audio file</span></span>\n<span class="line"><span style="color: #A5D6FF">&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">vad_segment_generator</span><span style="color: #C9D1D9">(wavFile, aggressiveness):</span></span>\n<span class="line"><span style="color: #C9D1D9">   audio, sample_rate, audio_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wav_handler.read_wave(wavFile)</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">assert</span><span style="color: #C9D1D9"> sample_rate </span><span style="color: #FF7B72">==</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">16000</span><span style="color: #C9D1D9">, </span><span style="color: #A5D6FF">&quot;Only 16000Hz input WAV files are supported for now!&quot;</span></span>\n<span class="line"><span style="color: #C9D1D9">   vad </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> webrtcvad.Vad(</span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">(aggressiveness))</span></span>\n<span class="line"><span style="color: #C9D1D9">   frames </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wav_handler.frame_generator(</span><span style="color: #79C0FF">30</span><span style="color: #C9D1D9">, audio, sample_rate)</span></span>\n<span class="line"><span style="color: #C9D1D9">   frames </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">list</span><span style="color: #C9D1D9">(frames)</span></span>\n<span class="line"><span style="color: #C9D1D9">   segments </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wav_handler.vad_collector(sample_rate, </span><span style="color: #79C0FF">30</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">300</span><span style="color: #C9D1D9">, vad, frames)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> segments, sample_rate, audio_length</span></span></code></pre>\n<h2 id="deepspeech-cli-for-real-time-and-asynchronous-speech-to-text">DeepSpeech CLI for Real Time and Asynchronous Speech to Text</h2>\n<p>Everything is set up to transcribe audio data with DeepSpeech via pretrained models. Now, let\u2019s look at how to turn the functionality we created above into a command line interface for real time and asynchronous speech to text. We start by importing a bunch of libraries for operating with the command line - <code is:raw>sys</code>, <code is:raw>os</code>, <code is:raw>logging</code>, <code is:raw>argparse</code>, <code is:raw>subprocess</code>, and <code is:raw>shlex</code>. We also need to import <code is:raw>numpy</code> and the <code is:raw>wav_transcriber</code> we made above to work with the audio data.</p>\n<h3 id="reading-arguments-for-deepspeech-speech-to-text">Reading Arguments for DeepSpeech Speech to Text</h3>\n<p>We create a main function that takes one parameter - <code is:raw>args</code>. These are the arguments passed in through the command line. We use the <code is:raw>argparse</code> libraries to parse the arguments sent in. We also create helpful tips on how to use each one.</p>\n<p>We use <code is:raw>aggressive</code> to determine how aggressively we want to filter. <code is:raw>audio</code> directs us to the audio file path. <code is:raw>model</code> points us to the directory containing the model and scorer. Finally, <code is:raw>stream</code> dictates whether or not we are streaming audio. Neither <code is:raw>stream</code> nor <code is:raw>audio</code> is required, but one or the other must be present.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> sys</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> os</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> logging</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> argparse</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> subprocess</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> shlex</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> numpy </span><span style="color: #FF7B72">as</span><span style="color: #C9D1D9"> np</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> wav_transcriber</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #8B949E"># Debug helpers</span></span>\n<span class="line"><span style="color: #C9D1D9">logging.basicConfig(</span><span style="color: #FFA657">stream</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">sys.stderr, </span><span style="color: #FFA657">level</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">logging.</span><span style="color: #79C0FF">DEBUG</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">main</span><span style="color: #C9D1D9">(args):</span></span>\n<span class="line"><span style="color: #C9D1D9">   parser </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> argparse.ArgumentParser(</span><span style="color: #FFA657">description</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&#39;Transcribe long audio files using webRTC VAD or use the streaming interface&#39;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   parser.add_argument(</span><span style="color: #A5D6FF">&#39;--aggressive&#39;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">type</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">choices</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">range</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">4</span><span style="color: #C9D1D9">), </span><span style="color: #FFA657">required</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">False</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">                       </span><span style="color: #FFA657">help</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&#39;Determines how aggressive filtering out non-speech is. (Interger between 0-3)&#39;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   parser.add_argument(</span><span style="color: #A5D6FF">&#39;--audio&#39;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">required</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">False</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">                       </span><span style="color: #FFA657">help</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&#39;Path to the audio file to run (WAV format)&#39;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   parser.add_argument(</span><span style="color: #A5D6FF">&#39;--model&#39;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">required</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">True</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">                       </span><span style="color: #FFA657">help</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&#39;Path to directory that contains all model files (output_graph and scorer)&#39;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   parser.add_argument(</span><span style="color: #A5D6FF">&#39;--stream&#39;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">required</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">False</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">action</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&#39;store_true&#39;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">                       </span><span style="color: #FFA657">help</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&#39;To use deepspeech streaming interface&#39;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   args </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> parser.parse_args()</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> args.stream </span><span style="color: #FF7B72">is</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">True</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(</span><span style="color: #A5D6FF">&quot;Opening mic for streaming&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">elif</span><span style="color: #C9D1D9"> args.audio </span><span style="color: #FF7B72">is</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">not</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">None</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">       logging.debug(</span><span style="color: #A5D6FF">&quot;Transcribing audio file @ </span><span style="color: #79C0FF">%s</span><span style="color: #A5D6FF">&quot;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">%</span><span style="color: #C9D1D9"> args.audio)</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">else</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">       parser.print_help()</span></span>\n<span class="line"><span style="color: #C9D1D9">       parser.exit()</span></span></code></pre>\n<h3 id="using-deepspeech-for-real-time-or-asynchronous-speech-recognition">Using DeepSpeech for Real Time or Asynchronous Speech Recognition</h3>\n<p>This is still inside the main function we started above. Once we parse all the arguments, we load up DeepSpeech. First, we get the directory containing the models. Next, we call the <code is:raw>wav_transcriber</code> to resolve and load the models.</p>\n<p>If we pass the path to an audio data file in the command line, then we will run asynchronous speech recognition. The first thing we do for that is call the VAD segment generator to generate the VAD segments and get the sample rate and audio length. Next, we open up a text file to transcribe to.</p>\n<p>For each of the enumerated segments, we will process each chunk by using <code is:raw>numpy</code> to pull the segment from the buffer and the speech to text function from <code is:raw>wav_transcriber</code> to do the speech to text functionality. We write to the text file until we run out of audio segments.</p>\n<p>If we pass stream instead of audio, then we open up the mic to stream audio data in. If you don\u2019t need real time automatic speech recognition, then you can ignore this part. First, we have to spin up a subprocess to open up a mic to stream in real time just like we did with PyTorch local speech recognition.</p>\n<p>We use the <code is:raw>subprocess</code> and <code is:raw>shlex</code> libraries to open the mic to stream voice audio until we shut it down. The model will read 512 bytes of audio data at a time and transcribe it.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #8B949E"># Point to a path containing the pre-trained models &amp; resolve ~ if used</span></span>\n<span class="line"><span style="color: #C9D1D9">dirName </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> os.path.expanduser(args.model)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #8B949E"># Resolve all the paths of model files</span></span>\n<span class="line"><span style="color: #C9D1D9">output_graph, scorer </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wav_transcriber.resolve_models(dirName)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #8B949E"># Load output_graph, alpahbet and scorer</span></span>\n<span class="line"><span style="color: #C9D1D9">model_retval </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wav_transcriber.load_model(output_graph, scorer)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> args.audio </span><span style="color: #FF7B72">is</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">not</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">None</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">    </span><span style="color: #8B949E"># Run VAD on the input file</span></span>\n<span class="line"><span style="color: #C9D1D9">    waveFile </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> args.audio</span></span>\n<span class="line"><span style="color: #C9D1D9">    segments, sample_rate, audio_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wav_transcriber.vad_segment_generator(waveFile, args.aggressive)</span></span>\n<span class="line"><span style="color: #C9D1D9">    f </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">open</span><span style="color: #C9D1D9">(waveFile.rstrip(</span><span style="color: #A5D6FF">&quot;.wav&quot;</span><span style="color: #C9D1D9">) </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;.txt&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #A5D6FF">&#39;w&#39;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">    logging.debug(</span><span style="color: #A5D6FF">&quot;Saving Transcript @: </span><span style="color: #79C0FF">%s</span><span style="color: #A5D6FF">&quot;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">%</span><span style="color: #C9D1D9"> waveFile.rstrip(</span><span style="color: #A5D6FF">&quot;.wav&quot;</span><span style="color: #C9D1D9">) </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;.txt&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">    </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> i, segment </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">enumerate</span><span style="color: #C9D1D9">(segments):</span></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #8B949E"># Run deepspeech on the chunk that just completed VAD</span></span>\n<span class="line"><span style="color: #C9D1D9">        logging.debug(</span><span style="color: #A5D6FF">&quot;Processing chunk </span><span style="color: #79C0FF">%002d</span><span style="color: #A5D6FF">&quot;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">%</span><span style="color: #C9D1D9"> (i,))</span></span>\n<span class="line"><span style="color: #C9D1D9">        audio </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> np.frombuffer(segment, </span><span style="color: #FFA657">dtype</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">np.int16)</span></span>\n<span class="line"><span style="color: #C9D1D9">        output </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wav_transcriber.stt(model_retval, audio, sample_rate)</span></span>\n<span class="line"><span style="color: #C9D1D9">        logging.debug(</span><span style="color: #A5D6FF">&quot;Transcript: </span><span style="color: #79C0FF">%s</span><span style="color: #A5D6FF">&quot;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">%</span><span style="color: #C9D1D9"> output)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">        f.write(output </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot; &quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">    </span><span style="color: #8B949E"># Summary of the files processed</span></span>\n<span class="line"><span style="color: #C9D1D9">    f.close()</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #FF7B72">else</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">    sctx </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> model_retval.createStream()</span></span>\n<span class="line"><span style="color: #C9D1D9">    subproc </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> subprocess.Popen(shlex.split(</span><span style="color: #A5D6FF">&#39;rec -q -V0 -e signed -L -c 1 -b 16 -r 16k -t raw - gain -2&#39;</span><span style="color: #C9D1D9">),</span></span>\n<span class="line"><span style="color: #C9D1D9">                                </span><span style="color: #FFA657">stdout</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">subprocess.</span><span style="color: #79C0FF">PIPE</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">                                </span><span style="color: #FFA657">bufsize</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">    </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(</span><span style="color: #A5D6FF">&#39;You can start speaking now. Press Control-C to stop recording.&#39;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">    </span><span style="color: #FF7B72">try</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #FF7B72">while</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">True</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">            data </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> subproc.stdout.read(</span><span style="color: #79C0FF">512</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">            sctx.feedAudioContent(np.frombuffer(data, np.int16))</span></span>\n<span class="line"><span style="color: #C9D1D9">    </span><span style="color: #FF7B72">except</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">KeyboardInterrupt</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(</span><span style="color: #A5D6FF">&#39;Transcription: &#39;</span><span style="color: #C9D1D9">, sctx.finishStream())</span></span>\n<span class="line"><span style="color: #C9D1D9">        subproc.terminate()</span></span>\n<span class="line"><span style="color: #C9D1D9">        subproc.wait()</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__name__</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">==</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&#39;__main__&#39;</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">   main(sys.argv[</span><span style="color: #79C0FF">1</span><span style="color: #C9D1D9">:])</span></span></code></pre>\n<h2 id="summary">Summary</h2>\n<p>We started this post out with a high level view of DeepSpeech, an open source speech recognition software. It was inspired by a 2014 paper from Baidu and is currently maintained by Mozilla.</p>\n<p>After a basic introduction, we stepped into a guide on how to use DeepSpeech to locally transcribe speech to text. While it may have been possible to create all of this code in one document, we opted for a modular approach with principles of software engineering in mind.</p>\n<p>We created three modules. One to handle WAV files, which are the audio data files that we can use DeepSpeech to transcribe. One to transcribe from WAV files, and one more file to create a command line interface to use DeepSpeech. Our CLI allows us to pass in options to pick if we want to do real time speech recognition or run speech recognition on an existing WAV audio file.</p>';
}
const $$Astro = createAstro("/Users/sandrarodgers/web-next/blog/src/content/blog/posts/guide-deepspeech-speech-to-text/index.md", "https://blog.deepgram.com/", "file:///Users/sandrarodgers/web-next/blog/");
const $$Index = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro, $$props, $$slots);
  Astro2.self = $$Index;
  new Slugger();
  return renderTemplate`<head>${renderHead($$result)}</head><p>No, we’re not talking about you Cthulhu. This is a different type of DeepSpeech. The DeepSpeech we’re talking about today is a Python speech to text library. Speech to text is part of <a href="https://pythonalgos.com/?p=1436">Natural Language Processing (NLP)</a>. Automated speech recognition, or ASR, started out as an offshoot of NLP in the 1990s.</p>
<p>Today, there are tons of audio libraries that can help you <a href="https://blog.deepgram.com/best-python-audio-manipulation-tools/">manipulate audio data</a> such as DeepSpeech and <a href="https://blog.deepgram.com/pytorch-intro-with-torchaudio/">PyTorch</a>. In this post, we will be using DeepSpeech to do both asynchronous and real time speech transcription. We will cover:</p>
<ul>
<li><a href="#what-is-deepspeech">What is DeepSpeech?</a></li>
<li><a href="#set-up-for-local-speech-to-text-with-deepspeech">Set Up for Local Speech to Text with DeepSpeech</a></li>
<li><a href="#file-handler-for-deepspeech-speech-transcription">File Handler for DeepSpeech Speech Transcription</a></li>
<li><a href="#transcribe-speech-to-text-for-wav-file-with-deepspeech">Transcribe Speech to Text for WAV file with DeepSpeech</a></li>
<li><a href="#deepspeech-cli-for-real-time-and-asynchronous-speech-to-text">DeepSpeech CLI for Real-Time and Asynchronous Speech to Text</a></li>
<li><a href="#summary">Summary</a></li>
</ul>
<h2 id="what-is-deepspeech">What is DeepSpeech?</h2>
<p>DeepSpeech is an open source Python library that enables us to build automatic speech recognition systems. It is based on Baidu’s 2014 paper titled <a href="https://arxiv.org/abs/1412.5567">Deep Speech: Scaling up end-to-end speech recognition</a>.</p>
<p>The initial proposal for Deep Speech was simple - let’s create a speech recognition system based entirely off of deep learning. The paper describes a solution using RNNs trained with multiple GPUs with no concept of phonemes. The authors, Hannun et al., show that their solution also outperformed the existing solutions at the time and was more robust to background noise without a need for filtering.</p>
<p>Since then, Mozilla has been the one in charge of maintaining the open source Python package for DeepSpeech. Before moving on, it’s important to note that DeepSpeech is not yet compatible with Python 3.10 nor some more recent versions of *nix kernels. I suggest using a virtual machine or Docker container to develop with DeepSpeech on unsupported OSes.</p>
<h2 id="set-up-for-local-speech-to-text-with-deepspeech">Set Up for Local Speech to Text with DeepSpeech</h2>
<p>To use DeepSpeech, we have to install a few libraries. We need <code>deepspeech</code>, <code>numpy</code>, and <code>webrtcvad</code>. We can install all of these by running <code>pip install deepspeech numpy webrtcvad</code>. The <code>webrtcvad</code> library is the voice activity detection (VAD) library developed by Google for WebRTC (real time communication).</p>
<p>For the asynchronous transcription, we’re going to need three files. One file to handle interaction with WAV data, one file to transcribe speech to text on a WAV file, and one to use these two in the command line. We will also be using a pretrained DeepSpeech model and scorer. You can download their model by running the following lines in your terminal:</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #C9D1D9">wget https://github.com/mozilla/DeepSpeech/releases/download/v0.9.3/deepspeech-0.9.3-models.pbmm</span></span>
<span class="line"><span style="color: #C9D1D9">wget https://github.com/mozilla/DeepSpeech/releases/download/v0.9.3/deepspeech-0.9.3-models.scorer</span></span></code></pre>
<h2 id="file-handler-for-deepspeech-speech-transcription">File Handler for DeepSpeech Speech Transcription</h2>
<p>The first file we create is the WAV handling file. This file should be named something like <code>wav_handler.py</code>. We import three built-in libraries to do this, <code>wave</code>, <code>collections</code>, and <code>contextlib</code>. We create four functions and one class. We need one function each to read WAV files, write WAV files, create Frames, and detect voice activity. Our one class represents individual frames in the WAV file.</p>
<h3 id="reading-audio-data-from-a-wav-file">Reading Audio Data from a WAV file</h3>
<p>Let’s start with creating a function to read WAV files. This function will take an input, this input is the path to a WAV file. The file will use the <code>contextlib</code> library to open the WAV file and read in the contents as bytes. Next, we run multiple asserts on the WAV file - it must have one channel, have a sample width of 2, have a sample rate of 8, 16, or 32 kHz.</p>
<p>Once we have asserted that the WAV file is in the right format for processing, we extract the frames. Next, we extract the pcm data from the frames and the duration from the metadata. Finally, we return the PCM data, the sample rate, and the duration.</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> collections</span></span>
<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> contextlib</span></span>
<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> wave</span></span>
<span class="line"></span>
<span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;Reads a .wav file.</span></span>
<span class="line"><span style="color: #A5D6FF">Input: path to a .wav file</span></span>
<span class="line"><span style="color: #A5D6FF">Output: tuple of pcm data, sample rate, and duration</span></span>
<span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">read_wave</span><span style="color: #C9D1D9">(path):</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">with</span><span style="color: #C9D1D9"> contextlib.closing(wave.open(path, </span><span style="color: #A5D6FF">&#39;rb&#39;</span><span style="color: #C9D1D9">)) </span><span style="color: #FF7B72">as</span><span style="color: #C9D1D9"> wf:</span></span>
<span class="line"><span style="color: #C9D1D9">       num_channels </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wf.getnchannels()</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">assert</span><span style="color: #C9D1D9"> num_channels </span><span style="color: #FF7B72">==</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">1</span></span>
<span class="line"><span style="color: #C9D1D9">       sample_width </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wf.getsampwidth()</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">assert</span><span style="color: #C9D1D9"> sample_width </span><span style="color: #FF7B72">==</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">2</span></span>
<span class="line"><span style="color: #C9D1D9">       sample_rate </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wf.getframerate()</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">assert</span><span style="color: #C9D1D9"> sample_rate </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> (</span><span style="color: #79C0FF">8000</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">16000</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">32000</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">       frames </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wf.getnframes()</span></span>
<span class="line"><span style="color: #C9D1D9">       pcm_data </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wf.readframes(frames)</span></span>
<span class="line"><span style="color: #C9D1D9">       duration </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> frames </span><span style="color: #FF7B72">/</span><span style="color: #C9D1D9"> sample_rate</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> pcm_data, sample_rate, duration</span></span></code></pre>
<h3 id="writing-audio-data-to-a-wav-file">Writing Audio Data to a WAV file</h3>
<p>Now let’s create the function to write audio data to a WAV file. This function requires three parameters, the path to a file to write to, the audio data, and the sample rate. This function writes a WAV file in the same way that the read function asserts its parameters. All we do is here is set the channels, sample width, and frame rate and then write the audio frames.</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;Writes a .wav file.</span></span>
<span class="line"><span style="color: #A5D6FF">Input: path to new .wav file, PCM audio data, and sample rate.</span></span>
<span class="line"><span style="color: #A5D6FF">Output: a .wav file</span></span>
<span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">write_wave</span><span style="color: #C9D1D9">(path, audio, sample_rate):</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">with</span><span style="color: #C9D1D9"> contextlib.closing(wave.open(path, </span><span style="color: #A5D6FF">&#39;wb&#39;</span><span style="color: #C9D1D9">)) </span><span style="color: #FF7B72">as</span><span style="color: #C9D1D9"> wf:</span></span>
<span class="line"><span style="color: #C9D1D9">       wf.setnchannels(</span><span style="color: #79C0FF">1</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">       wf.setsampwidth(</span><span style="color: #79C0FF">2</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">       wf.setframerate(sample_rate)</span></span>
<span class="line"><span style="color: #C9D1D9">       wf.writeframes(audio)</span></span></code></pre>
<h3 id="creating-frames-of-audio-data-for-deepspeech-to-transcribe">Creating Frames of Audio Data for DeepSpeech to Transcribe</h3>
<p>We’re going to create a class called <code>Frame</code> to hold some information to represent our audio data and make it easier to handle. This object requires three parameters to be created: the bytes, the timestamp in the audio file, and the duration of the <code>Frame</code>.</p>
<p>We also need to create a function to create frames. You can think of this function as a frame <a href="https://docs.google.com/document/d/1JwfuaBrao_pZjLBy6isPdqv_hndQnetCJ8MjYWlLtlI/edit?pli=1&disco=AAAAddu7kIo">generator</a> or a frame factory that returns an iterator. This function requires three parameters: the frame duration in milliseconds, the audio data, and the sample rate.</p>
<p>This function starts by deriving an interval of frames from the passed in sample rate and frame duration in milliseconds. We start at an offset and timestamp of 0. We also create a duration constant equal to the number of frames in a second.</p>
<p>While the current offset can be incremented by the interval constant and be within the number of frames of the audio, we generate a <code>Frame</code> for each interval and then increment the timestamp and offset appropriately.</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;Represents a &quot;frame&quot; of audio data.</span></span>
<span class="line"><span style="color: #A5D6FF">Requires the number of byes, the timestamp of the frame, and the duration on init&quot;&quot;&quot;</span></span>
<span class="line"><span style="color: #FF7B72">class</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">Frame</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">object</span><span style="color: #C9D1D9">):</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__init__</span><span style="color: #C9D1D9">(self, bytes, timestamp, duration):</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.bytes </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">bytes</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.timestamp </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> timestamp</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.duration </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> duration</span></span>
<span class="line"></span>
<span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;Generates audio frames from PCM audio data.</span></span>
<span class="line"><span style="color: #A5D6FF">Input: the desired frame duration in milliseconds, the PCM data, and</span></span>
<span class="line"><span style="color: #A5D6FF">the sample rate.</span></span>
<span class="line"><span style="color: #A5D6FF">Yields/Generates: Frames of the requested duration.</span></span>
<span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">frame_generator</span><span style="color: #C9D1D9">(frame_duration_ms, audio, sample_rate):</span></span>
<span class="line"><span style="color: #C9D1D9">   n </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">(sample_rate </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> (frame_duration_ms </span><span style="color: #FF7B72">/</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">1000.0</span><span style="color: #C9D1D9">) </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">2</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">   offset </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">0</span></span>
<span class="line"><span style="color: #C9D1D9">   timestamp </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">0.0</span></span>
<span class="line"><span style="color: #C9D1D9">   duration </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> (</span><span style="color: #79C0FF">float</span><span style="color: #C9D1D9">(n) </span><span style="color: #FF7B72">/</span><span style="color: #C9D1D9"> sample_rate) </span><span style="color: #FF7B72">/</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">2.0</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">while</span><span style="color: #C9D1D9"> offset </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> n </span><span style="color: #FF7B72">&lt;</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">len</span><span style="color: #C9D1D9">(audio):</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">yield</span><span style="color: #C9D1D9"> Frame(audio[offset:offset </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> n], timestamp, duration)</span></span>
<span class="line"><span style="color: #C9D1D9">       timestamp </span><span style="color: #FF7B72">+=</span><span style="color: #C9D1D9"> duration</span></span>
<span class="line"><span style="color: #C9D1D9">       offset </span><span style="color: #FF7B72">+=</span><span style="color: #C9D1D9"> n</span></span></code></pre>
<h3 id="collecting-voice-activated-frames-for-speech-to-text-with-deepspeech">Collecting Voice Activated Frames for Speech to Text with DeepSpeech</h3>
<p>Next, let’s create a function to collect all the frames that contain voice. This function requires a sample rate, the frame duration in milliseconds, the padding duration in milliseconds, a voice activation detector (VAD) from <code>webrtcvad</code>, and the audio data frames.</p>
<p>The VAD algorithm uses a padded ring buffer and checks to see what percentage of the frames in the window are voiced. When the window reaches a 90% voiced frame rate, the VAD triggers and begins yielding audio frames. While generating frames, if the percentage of voiced audio data in the frame drops below 10%, it will stop generating frames.</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;Filters out non-voiced audio frames.</span></span>
<span class="line"><span style="color: #A5D6FF">Given a webrtcvad.Vad and a source of audio frames, yields only</span></span>
<span class="line"><span style="color: #A5D6FF">the voiced audio.</span></span>
<span class="line"><span style="color: #A5D6FF">Arguments:</span></span>
<span class="line"><span style="color: #A5D6FF">sample_rate - The audio sample rate, in Hz.</span></span>
<span class="line"><span style="color: #A5D6FF">frame_duration_ms - The frame duration in milliseconds.</span></span>
<span class="line"><span style="color: #A5D6FF">padding_duration_ms - The amount to pad the window, in milliseconds.</span></span>
<span class="line"><span style="color: #A5D6FF">vad - An instance of webrtcvad.Vad.</span></span>
<span class="line"><span style="color: #A5D6FF">frames - a source of audio frames (sequence or generator).</span></span>
<span class="line"><span style="color: #A5D6FF">Returns: A generator that yields PCM audio data.</span></span>
<span class="line"><span style="color: #A5D6FF">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">vad_collector</span><span style="color: #C9D1D9">(sample_rate, frame_duration_ms,</span></span>
<span class="line"><span style="color: #C9D1D9">                 padding_duration_ms, vad, frames):</span></span>
<span class="line"><span style="color: #C9D1D9">   num_padding_frames </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">(padding_duration_ms </span><span style="color: #FF7B72">/</span><span style="color: #C9D1D9"> frame_duration_ms)</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #8B949E"># We use a deque for our sliding window/ring buffer.</span></span>
<span class="line"><span style="color: #C9D1D9">   ring_buffer </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> collections.deque(</span><span style="color: #FFA657">maxlen</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">num_padding_frames)</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #8B949E"># We have two states: TRIGGERED and NOTTRIGGERED. We start in the</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #8B949E"># NOTTRIGGERED state.</span></span>
<span class="line"><span style="color: #C9D1D9">   triggered </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">False</span></span>
<span class="line"></span>
<span class="line"><span style="color: #C9D1D9">   voiced_frames </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> []</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> frame </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> frames:</span></span>
<span class="line"><span style="color: #C9D1D9">       is_speech </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> vad.is_speech(frame.bytes, sample_rate)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">not</span><span style="color: #C9D1D9"> triggered:</span></span>
<span class="line"><span style="color: #C9D1D9">           ring_buffer.append((frame, is_speech))</span></span>
<span class="line"><span style="color: #C9D1D9">           num_voiced </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">len</span><span style="color: #C9D1D9">([f </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> f, speech </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> ring_buffer </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> speech])</span></span>
<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #8B949E"># If we&#39;re NOTTRIGGERED and more than 90% of the frames in</span></span>
<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #8B949E"># the ring buffer are voiced frames, then enter the</span></span>
<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #8B949E"># TRIGGERED state.</span></span>
<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> num_voiced </span><span style="color: #FF7B72">&gt;</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">0.9</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> ring_buffer.maxlen:</span></span>
<span class="line"><span style="color: #C9D1D9">               triggered </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">True</span></span>
<span class="line"><span style="color: #C9D1D9">               </span><span style="color: #8B949E"># We want to yield all the audio we see from now until</span></span>
<span class="line"><span style="color: #C9D1D9">               </span><span style="color: #8B949E"># we are NOTTRIGGERED, but we have to start with the</span></span>
<span class="line"><span style="color: #C9D1D9">               </span><span style="color: #8B949E"># audio that&#39;s already in the ring buffer.</span></span>
<span class="line"><span style="color: #C9D1D9">               </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> f, s </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> ring_buffer:</span></span>
<span class="line"><span style="color: #C9D1D9">                   voiced_frames.append(f)</span></span>
<span class="line"><span style="color: #C9D1D9">               ring_buffer.clear()</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">else</span><span style="color: #C9D1D9">:</span></span>
<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #8B949E"># We&#39;re in the TRIGGERED state, so collect the audio data</span></span>
<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #8B949E"># and add it to the ring buffer.</span></span>
<span class="line"><span style="color: #C9D1D9">           voiced_frames.append(frame)</span></span>
<span class="line"><span style="color: #C9D1D9">           ring_buffer.append((frame, is_speech))</span></span>
<span class="line"><span style="color: #C9D1D9">           num_unvoiced </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">len</span><span style="color: #C9D1D9">([f </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> f, speech </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> ring_buffer </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">not</span><span style="color: #C9D1D9"> speech])</span></span>
<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #8B949E"># If more than 90% of the frames in the ring buffer are</span></span>
<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #8B949E"># unvoiced, then enter NOTTRIGGERED and yield whatever</span></span>
<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #8B949E"># audio we&#39;ve collected.</span></span>
<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> num_unvoiced </span><span style="color: #FF7B72">&gt;</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">0.9</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> ring_buffer.maxlen:</span></span>
<span class="line"><span style="color: #C9D1D9">               triggered </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">False</span></span>
<span class="line"><span style="color: #C9D1D9">               </span><span style="color: #FF7B72">yield</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">b</span><span style="color: #A5D6FF">&#39;&#39;</span><span style="color: #C9D1D9">.join([f.bytes </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> f </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> voiced_frames])</span></span>
<span class="line"><span style="color: #C9D1D9">               ring_buffer.clear()</span></span>
<span class="line"><span style="color: #C9D1D9">               voiced_frames </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> []</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> triggered:</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">pass</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #8B949E"># If we have any leftover voiced audio when we run out of input,</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #8B949E"># yield it.</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> voiced_frames:</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">yield</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">b</span><span style="color: #A5D6FF">&#39;&#39;</span><span style="color: #C9D1D9">.join([f.bytes </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> f </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> voiced_frames])</span></span></code></pre>
<h2 id="transcribe-speech-to-text-for-wav-file-with-deepspeech">Transcribe Speech to Text for WAV file with DeepSpeech</h2>
<p>We’re going to create a new file for this section. This file should be named something like <code>wav_transcriber.py</code>. This layer completely abstracts out WAV handling from the CLI (which we create below). We use these functions to call DeepSpeech on the audio data and transcribe it.</p>
<h3 id="pick-which-deepspeech-model-to-use">Pick Which DeepSpeech Model to Use</h3>
<p>The first function we create in this file is the function to load up the model and scorer for DeepSpeech to run speech to text with. This function takes two parameters, the models graph, which we create a function to produce below, and the path to the scorer file. All it does is load the model from the graph and enable the use of the scorer. This function returns a DeepSpeech object.</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> glob</span></span>
<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> webrtcvad</span></span>
<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> logging</span></span>
<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> wav_handler</span></span>
<span class="line"><span style="color: #FF7B72">from</span><span style="color: #C9D1D9"> deepspeech </span><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> Model</span></span>
<span class="line"><span style="color: #FF7B72">from</span><span style="color: #C9D1D9"> timeit </span><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> default_timer </span><span style="color: #FF7B72">as</span><span style="color: #C9D1D9"> timer</span></span>
<span class="line"></span>
<span class="line"><span style="color: #A5D6FF">&#39;&#39;&#39;</span></span>
<span class="line"><span style="color: #A5D6FF">Load the pre-trained model into the memory</span></span>
<span class="line"><span style="color: #A5D6FF">@param models: Output Graph Protocol Buffer file</span></span>
<span class="line"><span style="color: #A5D6FF">@param scorer: Scorer file</span></span>
<span class="line"><span style="color: #A5D6FF">@Retval</span></span>
<span class="line"><span style="color: #A5D6FF">Returns a DeepSpeech Object</span></span>
<span class="line"><span style="color: #A5D6FF">&#39;&#39;&#39;</span></span>
<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">load_model</span><span style="color: #C9D1D9">(models, scorer):</span></span>
<span class="line"><span style="color: #C9D1D9">   ds </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> Model(models)</span></span>
<span class="line"><span style="color: #C9D1D9">   ds.enableExternalScorer(scorer)</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> ds</span></span></code></pre>
<h3 id="speech-to-text-on-an-audio-file-with-deepspeech">Speech to Text on an Audio File with DeepSpeech</h3>
<p>This function is the one that does the actual speech recognition. It takes three inputs, a DeepSpeech model, the audio data, and the sample rate.</p>
<p>We begin by setting the time to 0 and calculating the length of the audio. All we really have to do is call the DeepSpeech model’s <code>stt</code> function to do our own <code>stt</code> function. We pass the audio file to the <code>stt</code> function and return the output.</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #A5D6FF">&#39;&#39;&#39;</span></span>
<span class="line"><span style="color: #A5D6FF">Run Inference on input audio file</span></span>
<span class="line"><span style="color: #A5D6FF">@param ds: Deepspeech object</span></span>
<span class="line"><span style="color: #A5D6FF">@param audio: Input audio for running inference on</span></span>
<span class="line"><span style="color: #A5D6FF">@param fs: Sample rate of the input audio file</span></span>
<span class="line"><span style="color: #A5D6FF">@Retval:</span></span>
<span class="line"><span style="color: #A5D6FF">Returns a list [Inference, Inference Time, Audio Length]</span></span>
<span class="line"><span style="color: #A5D6FF">&#39;&#39;&#39;</span></span>
<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">stt</span><span style="color: #C9D1D9">(ds, audio, fs):</span></span>
<span class="line"><span style="color: #C9D1D9">   inference_time </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">0.0</span></span>
<span class="line"><span style="color: #C9D1D9">   audio_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">len</span><span style="color: #C9D1D9">(audio) </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> (</span><span style="color: #79C0FF">1</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">/</span><span style="color: #C9D1D9"> fs)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #8B949E"># Run Deepspeech</span></span>
<span class="line"><span style="color: #C9D1D9">   output </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> ds.stt(audio)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> output</span></span></code></pre>
<h3 id="deepspeech-model-graph-creator-function">DeepSpeech Model Graph Creator Function</h3>
<p>This is the function that creates the model graph for the <code>load_model</code> function we created a couple sections above. This function takes the path to a directory. From that directory, it looks for files with the DeepSpeech model extension, <code>pbmm</code> and the DeepSpeech scorer file extension, <code>.scorer</code>. Then, it returns both of those values.</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #A5D6FF">&#39;&#39;&#39;</span></span>
<span class="line"><span style="color: #A5D6FF">Resolve directory path for the models and fetch each of them.</span></span>
<span class="line"><span style="color: #A5D6FF">@param dirName: Path to the directory containing pre-trained models</span></span>
<span class="line"><span style="color: #A5D6FF">@Retval:</span></span>
<span class="line"><span style="color: #A5D6FF">Retunns a tuple containing each of the model files (pb, scorer)</span></span>
<span class="line"><span style="color: #A5D6FF">&#39;&#39;&#39;</span></span>
<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">resolve_models</span><span style="color: #C9D1D9">(dirName):</span></span>
<span class="line"><span style="color: #C9D1D9">   pb </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> glob.glob(dirName </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;/*.pbmm&quot;</span><span style="color: #C9D1D9">)[</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">]</span></span>
<span class="line"><span style="color: #C9D1D9">   logging.debug(</span><span style="color: #A5D6FF">&quot;Found Model: </span><span style="color: #79C0FF">%s</span><span style="color: #A5D6FF">&quot;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">%</span><span style="color: #C9D1D9"> pb)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #C9D1D9">   scorer </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> glob.glob(dirName </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;/*.scorer&quot;</span><span style="color: #C9D1D9">)[</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">]</span></span>
<span class="line"><span style="color: #C9D1D9">   logging.debug(</span><span style="color: #A5D6FF">&quot;Found scorer: </span><span style="color: #79C0FF">%s</span><span style="color: #A5D6FF">&quot;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">%</span><span style="color: #C9D1D9"> scorer)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> pb, scorer</span></span></code></pre>
<h3 id="voice-activation-detection-to-create-segments-for-speech-to-text">Voice Activation Detection to Create Segments for Speech to Text</h3>
<p>The last function in our WAV transcription file generates segments of text that contain voice. We use the WAV handler file we created earlier and <code>webrtcvad</code> to do the heavy lifting. This function requires two parameters: a WAV file and an integer value from 0 to 3 representing how aggressively we want to filter out non-voice activity.</p>
<p>We call the <code>read_wave</code> function from the <code>wav_handler.py</code> file we created earlier and imported above to get the audio data, sample rate, and audio length. We then assert that the sample rate is 16kHz before moving on to create a VAD object. Next, we call the frame generator from <code>wav_handler</code>.</p>
<p>We convert the generated iterator to a list which we pass to the <code>vad_collector</code> function from <code>wav_handler</code> along with the sample rate, frame duration (30 ms), padding duration (300 ms), and VAD object. Finally, we return the collected VAD segments along with the sample rate and audio length.</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #A5D6FF">&#39;&#39;&#39;</span></span>
<span class="line"><span style="color: #A5D6FF">Generate VAD segments. Filters out non-voiced audio frames.</span></span>
<span class="line"><span style="color: #A5D6FF">@param waveFile: Input wav file to run VAD on.0</span></span>
<span class="line"><span style="color: #A5D6FF">@Retval:</span></span>
<span class="line"><span style="color: #A5D6FF">Returns tuple of</span></span>
<span class="line"><span style="color: #A5D6FF">   segments: a bytearray of multiple smaller audio frames</span></span>
<span class="line"><span style="color: #A5D6FF">             (The longer audio split into mutiple smaller one&#39;s)</span></span>
<span class="line"><span style="color: #A5D6FF">   sample_rate: Sample rate of the input audio file</span></span>
<span class="line"><span style="color: #A5D6FF">   audio_length: Duraton of the input audio file</span></span>
<span class="line"><span style="color: #A5D6FF">&#39;&#39;&#39;</span></span>
<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">vad_segment_generator</span><span style="color: #C9D1D9">(wavFile, aggressiveness):</span></span>
<span class="line"><span style="color: #C9D1D9">   audio, sample_rate, audio_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wav_handler.read_wave(wavFile)</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">assert</span><span style="color: #C9D1D9"> sample_rate </span><span style="color: #FF7B72">==</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">16000</span><span style="color: #C9D1D9">, </span><span style="color: #A5D6FF">&quot;Only 16000Hz input WAV files are supported for now!&quot;</span></span>
<span class="line"><span style="color: #C9D1D9">   vad </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> webrtcvad.Vad(</span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">(aggressiveness))</span></span>
<span class="line"><span style="color: #C9D1D9">   frames </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wav_handler.frame_generator(</span><span style="color: #79C0FF">30</span><span style="color: #C9D1D9">, audio, sample_rate)</span></span>
<span class="line"><span style="color: #C9D1D9">   frames </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">list</span><span style="color: #C9D1D9">(frames)</span></span>
<span class="line"><span style="color: #C9D1D9">   segments </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wav_handler.vad_collector(sample_rate, </span><span style="color: #79C0FF">30</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">300</span><span style="color: #C9D1D9">, vad, frames)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> segments, sample_rate, audio_length</span></span></code></pre>
<h2 id="deepspeech-cli-for-real-time-and-asynchronous-speech-to-text">DeepSpeech CLI for Real Time and Asynchronous Speech to Text</h2>
<p>Everything is set up to transcribe audio data with DeepSpeech via pretrained models. Now, let’s look at how to turn the functionality we created above into a command line interface for real time and asynchronous speech to text. We start by importing a bunch of libraries for operating with the command line - <code>sys</code>, <code>os</code>, <code>logging</code>, <code>argparse</code>, <code>subprocess</code>, and <code>shlex</code>. We also need to import <code>numpy</code> and the <code>wav_transcriber</code> we made above to work with the audio data.</p>
<h3 id="reading-arguments-for-deepspeech-speech-to-text">Reading Arguments for DeepSpeech Speech to Text</h3>
<p>We create a main function that takes one parameter - <code>args</code>. These are the arguments passed in through the command line. We use the <code>argparse</code> libraries to parse the arguments sent in. We also create helpful tips on how to use each one.</p>
<p>We use <code>aggressive</code> to determine how aggressively we want to filter. <code>audio</code> directs us to the audio file path. <code>model</code> points us to the directory containing the model and scorer. Finally, <code>stream</code> dictates whether or not we are streaming audio. Neither <code>stream</code> nor <code>audio</code> is required, but one or the other must be present.</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> sys</span></span>
<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> os</span></span>
<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> logging</span></span>
<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> argparse</span></span>
<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> subprocess</span></span>
<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> shlex</span></span>
<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> numpy </span><span style="color: #FF7B72">as</span><span style="color: #C9D1D9"> np</span></span>
<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> wav_transcriber</span></span>
<span class="line"></span>
<span class="line"><span style="color: #8B949E"># Debug helpers</span></span>
<span class="line"><span style="color: #C9D1D9">logging.basicConfig(</span><span style="color: #FFA657">stream</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">sys.stderr, </span><span style="color: #FFA657">level</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">logging.</span><span style="color: #79C0FF">DEBUG</span><span style="color: #C9D1D9">)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">main</span><span style="color: #C9D1D9">(args):</span></span>
<span class="line"><span style="color: #C9D1D9">   parser </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> argparse.ArgumentParser(</span><span style="color: #FFA657">description</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&#39;Transcribe long audio files using webRTC VAD or use the streaming interface&#39;</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">   parser.add_argument(</span><span style="color: #A5D6FF">&#39;--aggressive&#39;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">type</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">choices</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">range</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">4</span><span style="color: #C9D1D9">), </span><span style="color: #FFA657">required</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">False</span><span style="color: #C9D1D9">,</span></span>
<span class="line"><span style="color: #C9D1D9">                       </span><span style="color: #FFA657">help</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&#39;Determines how aggressive filtering out non-speech is. (Interger between 0-3)&#39;</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">   parser.add_argument(</span><span style="color: #A5D6FF">&#39;--audio&#39;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">required</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">False</span><span style="color: #C9D1D9">,</span></span>
<span class="line"><span style="color: #C9D1D9">                       </span><span style="color: #FFA657">help</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&#39;Path to the audio file to run (WAV format)&#39;</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">   parser.add_argument(</span><span style="color: #A5D6FF">&#39;--model&#39;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">required</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">True</span><span style="color: #C9D1D9">,</span></span>
<span class="line"><span style="color: #C9D1D9">                       </span><span style="color: #FFA657">help</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&#39;Path to directory that contains all model files (output_graph and scorer)&#39;</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">   parser.add_argument(</span><span style="color: #A5D6FF">&#39;--stream&#39;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">required</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">False</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">action</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&#39;store_true&#39;</span><span style="color: #C9D1D9">,</span></span>
<span class="line"><span style="color: #C9D1D9">                       </span><span style="color: #FFA657">help</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&#39;To use deepspeech streaming interface&#39;</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">   args </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> parser.parse_args()</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> args.stream </span><span style="color: #FF7B72">is</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">True</span><span style="color: #C9D1D9">:</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(</span><span style="color: #A5D6FF">&quot;Opening mic for streaming&quot;</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">elif</span><span style="color: #C9D1D9"> args.audio </span><span style="color: #FF7B72">is</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">not</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">None</span><span style="color: #C9D1D9">:</span></span>
<span class="line"><span style="color: #C9D1D9">       logging.debug(</span><span style="color: #A5D6FF">&quot;Transcribing audio file @ </span><span style="color: #79C0FF">%s</span><span style="color: #A5D6FF">&quot;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">%</span><span style="color: #C9D1D9"> args.audio)</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">else</span><span style="color: #C9D1D9">:</span></span>
<span class="line"><span style="color: #C9D1D9">       parser.print_help()</span></span>
<span class="line"><span style="color: #C9D1D9">       parser.exit()</span></span></code></pre>
<h3 id="using-deepspeech-for-real-time-or-asynchronous-speech-recognition">Using DeepSpeech for Real Time or Asynchronous Speech Recognition</h3>
<p>This is still inside the main function we started above. Once we parse all the arguments, we load up DeepSpeech. First, we get the directory containing the models. Next, we call the <code>wav_transcriber</code> to resolve and load the models.</p>
<p>If we pass the path to an audio data file in the command line, then we will run asynchronous speech recognition. The first thing we do for that is call the VAD segment generator to generate the VAD segments and get the sample rate and audio length. Next, we open up a text file to transcribe to.</p>
<p>For each of the enumerated segments, we will process each chunk by using <code>numpy</code> to pull the segment from the buffer and the speech to text function from <code>wav_transcriber</code> to do the speech to text functionality. We write to the text file until we run out of audio segments.</p>
<p>If we pass stream instead of audio, then we open up the mic to stream audio data in. If you don’t need real time automatic speech recognition, then you can ignore this part. First, we have to spin up a subprocess to open up a mic to stream in real time just like we did with PyTorch local speech recognition.</p>
<p>We use the <code>subprocess</code> and <code>shlex</code> libraries to open the mic to stream voice audio until we shut it down. The model will read 512 bytes of audio data at a time and transcribe it.</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #8B949E"># Point to a path containing the pre-trained models &amp; resolve ~ if used</span></span>
<span class="line"><span style="color: #C9D1D9">dirName </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> os.path.expanduser(args.model)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #8B949E"># Resolve all the paths of model files</span></span>
<span class="line"><span style="color: #C9D1D9">output_graph, scorer </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wav_transcriber.resolve_models(dirName)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #8B949E"># Load output_graph, alpahbet and scorer</span></span>
<span class="line"><span style="color: #C9D1D9">model_retval </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wav_transcriber.load_model(output_graph, scorer)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> args.audio </span><span style="color: #FF7B72">is</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">not</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">None</span><span style="color: #C9D1D9">:</span></span>
<span class="line"><span style="color: #C9D1D9">    </span><span style="color: #8B949E"># Run VAD on the input file</span></span>
<span class="line"><span style="color: #C9D1D9">    waveFile </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> args.audio</span></span>
<span class="line"><span style="color: #C9D1D9">    segments, sample_rate, audio_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wav_transcriber.vad_segment_generator(waveFile, args.aggressive)</span></span>
<span class="line"><span style="color: #C9D1D9">    f </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">open</span><span style="color: #C9D1D9">(waveFile.rstrip(</span><span style="color: #A5D6FF">&quot;.wav&quot;</span><span style="color: #C9D1D9">) </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;.txt&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #A5D6FF">&#39;w&#39;</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">    logging.debug(</span><span style="color: #A5D6FF">&quot;Saving Transcript @: </span><span style="color: #79C0FF">%s</span><span style="color: #A5D6FF">&quot;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">%</span><span style="color: #C9D1D9"> waveFile.rstrip(</span><span style="color: #A5D6FF">&quot;.wav&quot;</span><span style="color: #C9D1D9">) </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;.txt&quot;</span><span style="color: #C9D1D9">)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #C9D1D9">    </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> i, segment </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">enumerate</span><span style="color: #C9D1D9">(segments):</span></span>
<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #8B949E"># Run deepspeech on the chunk that just completed VAD</span></span>
<span class="line"><span style="color: #C9D1D9">        logging.debug(</span><span style="color: #A5D6FF">&quot;Processing chunk </span><span style="color: #79C0FF">%002d</span><span style="color: #A5D6FF">&quot;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">%</span><span style="color: #C9D1D9"> (i,))</span></span>
<span class="line"><span style="color: #C9D1D9">        audio </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> np.frombuffer(segment, </span><span style="color: #FFA657">dtype</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">np.int16)</span></span>
<span class="line"><span style="color: #C9D1D9">        output </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> wav_transcriber.stt(model_retval, audio, sample_rate)</span></span>
<span class="line"><span style="color: #C9D1D9">        logging.debug(</span><span style="color: #A5D6FF">&quot;Transcript: </span><span style="color: #79C0FF">%s</span><span style="color: #A5D6FF">&quot;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">%</span><span style="color: #C9D1D9"> output)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #C9D1D9">        f.write(output </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot; &quot;</span><span style="color: #C9D1D9">)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #C9D1D9">    </span><span style="color: #8B949E"># Summary of the files processed</span></span>
<span class="line"><span style="color: #C9D1D9">    f.close()</span></span>
<span class="line"></span>
<span class="line"><span style="color: #FF7B72">else</span><span style="color: #C9D1D9">:</span></span>
<span class="line"><span style="color: #C9D1D9">    sctx </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> model_retval.createStream()</span></span>
<span class="line"><span style="color: #C9D1D9">    subproc </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> subprocess.Popen(shlex.split(</span><span style="color: #A5D6FF">&#39;rec -q -V0 -e signed -L -c 1 -b 16 -r 16k -t raw - gain -2&#39;</span><span style="color: #C9D1D9">),</span></span>
<span class="line"><span style="color: #C9D1D9">                                </span><span style="color: #FFA657">stdout</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">subprocess.</span><span style="color: #79C0FF">PIPE</span><span style="color: #C9D1D9">,</span></span>
<span class="line"><span style="color: #C9D1D9">                                </span><span style="color: #FFA657">bufsize</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">    </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(</span><span style="color: #A5D6FF">&#39;You can start speaking now. Press Control-C to stop recording.&#39;</span><span style="color: #C9D1D9">)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #C9D1D9">    </span><span style="color: #FF7B72">try</span><span style="color: #C9D1D9">:</span></span>
<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #FF7B72">while</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">True</span><span style="color: #C9D1D9">:</span></span>
<span class="line"><span style="color: #C9D1D9">            data </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> subproc.stdout.read(</span><span style="color: #79C0FF">512</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">            sctx.feedAudioContent(np.frombuffer(data, np.int16))</span></span>
<span class="line"><span style="color: #C9D1D9">    </span><span style="color: #FF7B72">except</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">KeyboardInterrupt</span><span style="color: #C9D1D9">:</span></span>
<span class="line"><span style="color: #C9D1D9">        </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(</span><span style="color: #A5D6FF">&#39;Transcription: &#39;</span><span style="color: #C9D1D9">, sctx.finishStream())</span></span>
<span class="line"><span style="color: #C9D1D9">        subproc.terminate()</span></span>
<span class="line"><span style="color: #C9D1D9">        subproc.wait()</span></span>
<span class="line"></span>
<span class="line"><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__name__</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">==</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&#39;__main__&#39;</span><span style="color: #C9D1D9">:</span></span>
<span class="line"><span style="color: #C9D1D9">   main(sys.argv[</span><span style="color: #79C0FF">1</span><span style="color: #C9D1D9">:])</span></span></code></pre>
<h2 id="summary">Summary</h2>
<p>We started this post out with a high level view of DeepSpeech, an open source speech recognition software. It was inspired by a 2014 paper from Baidu and is currently maintained by Mozilla.</p>
<p>After a basic introduction, we stepped into a guide on how to use DeepSpeech to locally transcribe speech to text. While it may have been possible to create all of this code in one document, we opted for a modular approach with principles of software engineering in mind.</p>
<p>We created three modules. One to handle WAV files, which are the audio data files that we can use DeepSpeech to transcribe. One to transcribe from WAV files, and one more file to create a command line interface to use DeepSpeech. Our CLI allows us to pass in options to pick if we want to do real time speech recognition or run speech recognition on an existing WAV audio file.</p>`;
}, "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/guide-deepspeech-speech-to-text/index.md");

export { compiledContent, $$Index as default, frontmatter, metadata, rawContent };
