import { c as createAstro, a as createComponent, r as renderTemplate, b as renderHead, d as renderComponent } from '../entry.mjs';
import Slugger from 'github-slugger';
import '@astrojs/netlify/netlify-functions.js';
import 'preact';
import 'preact-render-to-string';
import 'vue';
import 'vue/server-renderer';
import 'html-escaper';
import 'node-html-parser';
import 'axios';
/* empty css                           *//* empty css                           *//* empty css                           */import '@storyblok/js';
/* empty css                          *//* empty css                              */import 'clone-deep';
import 'slugify';
import 'shiki';
/* empty css                           */import 'camelcase';
/* empty css                              */import '@astrojs/rss';
/* empty css                           */import 'mime';
import 'cookie';
import 'kleur/colors';
import 'string-width';
import 'path-browserify';
import 'path-to-regexp';

const metadata = { "headings": [{ "depth": 2, "slug": "two-kinds-of-bias", "text": "Two Kinds of Bias" }, { "depth": 2, "slug": "where-does-bias-come-from", "text": "Where Does Bias Come From?" }, { "depth": 2, "slug": "how-to-detect-bias-in-asr-systems", "text": "How to Detect Bias in ASR Systems" }, { "depth": 2, "slug": "how-to-reduce-bias-in-asr-systems", "text": "How to Reduce Bias in ASR Systems" }, { "depth": 3, "slug": "option-1-add-data-to-the-model", "text": "Option 1: Add Data to the Model" }, { "depth": 3, "slug": "option-2-create-distinct-models", "text": "Option 2: Create Distinct Models" }, { "depth": 2, "slug": "wrapping-up", "text": "Wrapping Up" }], "source": `Bias in machine learning has been a hot topic in the news lately, and bias in ASR is no exception. In the recent [State of Voice Technology 2022 report](https://deepgram.com/state-of-voice-technology-2022/), we saw that 92% of companies believe voice technology bias has an important impact on their customers, including in the domains of gender, race, age, and accent (both native and non-native speaker). So how do you detect bias in your automated speech recognition (ASR) systems? And what do you do if you find it? We've got answers below. But before we dive in, let's start with a definition of what "bias" is to make sure we're all on the same page.

## Two Kinds of Bias

It's important to understand that when people talk about bias and machine learning, they might be talking about two different things. On the one hand, they might mean **real-world bias** related to things like race, gender, age, etc. But the term bias is also used in **machine learning** when a model is overpredicting or underpredicting the probability for a given category. That's true even if that category is something that we might not apply the term "bias" to in the real world. For example, a model developed by a brewery to track when its vats are at risk of overheating might sound the alarm frequently, even when there is no risk of overheating-a problem of machine learning bias, but not real-world bias.

In many cases, though, when people talk about "machine learning bias" generally-especially in the media-they're referring to the intersection of real world and machine learning bias: a case where a model is overpredicting or underpredicting in terms of something like gender or race. With these definitions out of the way, let's turn to examining where bias comes from before moving on to how you might figure out if you've got bias in your ASR system, as well as what your next steps are if you've found it.

## Where Does Bias Come From?

Machine learning bias often comes from data-biased data leads to a biased model. If you've taken a statistics course, you're familiar with **sampling bias,** which is when data that is supposed to represent the true range of possibilities is instead skewed in some way. If you use data like this to train a model, the model is going to be skewed in the same way as the data. This sampling bias can show up in your model as machine learning or real-world bias because many organizations who want to create models rely on their own past decisions. If your company has a history of making biased decisions, any model trained on that data will likewise be biased. 

For example, if a company that trains a model to help make promotion decisions has a poor track record of promoting women, it's likely that their model will make the same kinds of biased decisions if it's trained on the company's past behavior. In some cases, however, a model might be biased towards a certain group of people for less nefarious reasons. For example, an initial ASR model might simply be biased because it was trained on speech that was easy for the creator to collect-from themself, their family, and their friends, who likely all speak similarly. Here, the issue isn't biased past decisions, per se, but simply a lack of data to cover different possibilities.

## How to Detect Bias in ASR Systems

So how do you know if you've got bias on your ASR system? Detecting it is relatively easy on its face. For example, you might ask if your transcripts for some speakers have significantly higher [word error rates (WER)](https://blog.deepgram.com/what-is-word-error-rate/) than for other speakers. But even the answers to a simple question like this can sometimes be difficult to understand on their face. Language is infinitely varied, and even though we speak of, for example, "American English", this actually represents a wide range of linguistic variation, from Massachusetts to North Dakota to Alabama. And in other places around the world, the variation between dialects is sometimes even greater than what we see in the US.

When you're thinking about how well your ASR system is performing, you're always going to encounter edge cases that it might have trouble understanding. For example, your model might encounter a non-native accent that it's never heard before-say, a native speaker of Polish who moved to South Africa at 16 and learned English there. To figure out if you've found bias in your ASR system, you need to be very nuanced in how you're looking at the data. For example, if you're finding a few sparse cases of poor transcripts in your ASR system, it could be that you're simply finding people with accents, like the Polish person mentioned above. In most cases, trying to chase down these edge cases and figure out how to fix them isn't going to be worth the time or effort, and isn't evidence that there's a systematic problem with your model.

But what if the bias that you've detected seems to be for an entire group of people? Maybe you notice that your call center in the Southern US has transcripts that are consistently of poorer quality than call centers in other parts of the country. Because this looks like a systematic problem, rather than a one-off problem, you'll probably want to take some kind of action to address this bias.

<WhitepaperPromo whitepaper="deepgram-whitepaper-how-deepgram-works"></WhitepaperPromo>

## How to Reduce Bias in ASR Systems

Because of the wide-ranging variation in human speech, it's unrealistic to think that you're going to completely remove bias (in the machine learning sense) from ASR systems. Human beings, after all, sometimes have trouble understanding speakers of other dialects, or with non-native accents, and it's not realistic to expect that AI and ML is going to do better than this-at least, not yet. So the question becomes-when you find that your model is not doing a good job for a specific class of people, what do you do? **You can use that information to make your model better.** And you have two options for how to go about doing this.

### Option 1: Add Data to the Model

The first option is to add more data to your model. In this case, you retrain with a wider range of sample data to also include the varieties of speech that you've discovered your model doesn't perform well on. This can work to improve the model for everyone, especially if the varieties are relatively similar.  However, in some cases, this can be a mistake. If we take the example above, if you're a company in the US, it's unlikely that adding data for Polish-accented South African English is a good choice, even if you're dealing with a community of speakers. That's because doing so might end up making your model perform *worse* for the majority of speakers you want it to handle. So what do you do if this is a population you want to cover, but you think adding data might reduce the model's performance overall? That's where the second option comes in.

### Option 2: Create Distinct Models

The second option is to create a new model specifically for the newly identified area where the main model is struggling. For example, a bank that has a lot of customers in Guam might discover that Guamanian English is challenging for the general model, and that if you add Guamanian speakers to your training data, the model's performance suffers for speakers in North America. The solution in these cases is to have two distinct models. If you've got a call center in Guam, for example, you could just use a Guam-specific model in that call center only, while you let your more general model run elsewhere. The prevents data from different varieties degrading the quality of a model, while also making sure speech of all relevant groups is transcribed correctly.

## Wrapping Up

As you can see, bias is far from straightforward, and there isn't a simple, one-size-fits all means of identifying bias in ASR systems and addressing it. It all depends on what you're seeing in your data, what populations you expect the models to work well on, and how similar the speech of those populations is to one another. But hopefully this post will serve as a guide for you as you're thinking through these issues. If you'd like to learn more about bias in speech recognition, and how it might even be able to help your ASR efforts, watch our on-demand webinar [When is Speech Recognition Bias a Good Thing?](https://offers.deepgram.com/when-is-speech-recognition-bias-a-good-thing-webinar-on-demand)`, "html": '<p>Bias in machine learning has been a hot topic in the news lately, and bias in ASR is no exception. In the recent <a href="https://deepgram.com/state-of-voice-technology-2022/">State of Voice Technology 2022 report</a>, we saw that 92% of companies believe voice technology bias has an important impact on their customers, including in the domains of gender, race, age, and accent (both native and non-native speaker). So how do you detect bias in your automated speech recognition (ASR) systems? And what do you do if you find it? We\u2019ve got answers below. But before we dive in, let\u2019s start with a definition of what \u201Cbias\u201D is to make sure we\u2019re all on the same page.</p>\n<h2 id="two-kinds-of-bias">Two Kinds of Bias</h2>\n<p>It\u2019s important to understand that when people talk about bias and machine learning, they might be talking about two different things. On the one hand, they might mean <strong>real-world bias</strong> related to things like race, gender, age, etc. But the term bias is also used in <strong>machine learning</strong> when a model is overpredicting or underpredicting the probability for a given category. That\u2019s true even if that category is something that we might not apply the term \u201Cbias\u201D to in the real world. For example, a model developed by a brewery to track when its vats are at risk of overheating might sound the alarm frequently, even when there is no risk of overheating-a problem of machine learning bias, but not real-world bias.</p>\n<p>In many cases, though, when people talk about \u201Cmachine learning bias\u201D generally-especially in the media-they\u2019re referring to the intersection of real world and machine learning bias: a case where a model is overpredicting or underpredicting in terms of something like gender or race. With these definitions out of the way, let\u2019s turn to examining where bias comes from before moving on to how you might figure out if you\u2019ve got bias in your ASR system, as well as what your next steps are if you\u2019ve found it.</p>\n<h2 id="where-does-bias-come-from">Where Does Bias Come From?</h2>\n<p>Machine learning bias often comes from data-biased data leads to a biased model. If you\u2019ve taken a statistics course, you\u2019re familiar with <strong>sampling bias,</strong> which is when data that is supposed to represent the true range of possibilities is instead skewed in some way. If you use data like this to train a model, the model is going to be skewed in the same way as the data. This sampling bias can show up in your model as machine learning or real-world bias because many organizations who want to create models rely on their own past decisions. If your company has a history of making biased decisions, any model trained on that data will likewise be biased.</p>\n<p>For example, if a company that trains a model to help make promotion decisions has a poor track record of promoting women, it\u2019s likely that their model will make the same kinds of biased decisions if it\u2019s trained on the company\u2019s past behavior. In some cases, however, a model might be biased towards a certain group of people for less nefarious reasons. For example, an initial ASR model might simply be biased because it was trained on speech that was easy for the creator to collect-from themself, their family, and their friends, who likely all speak similarly. Here, the issue isn\u2019t biased past decisions, per se, but simply a lack of data to cover different possibilities.</p>\n<h2 id="how-to-detect-bias-in-asr-systems">How to Detect Bias in ASR Systems</h2>\n<p>So how do you know if you\u2019ve got bias on your ASR system? Detecting it is relatively easy on its face. For example, you might ask if your transcripts for some speakers have significantly higher <a href="https://blog.deepgram.com/what-is-word-error-rate/">word error rates (WER)</a> than for other speakers. But even the answers to a simple question like this can sometimes be difficult to understand on their face. Language is infinitely varied, and even though we speak of, for example, \u201CAmerican English\u201D, this actually represents a wide range of linguistic variation, from Massachusetts to North Dakota to Alabama. And in other places around the world, the variation between dialects is sometimes even greater than what we see in the US.</p>\n<p>When you\u2019re thinking about how well your ASR system is performing, you\u2019re always going to encounter edge cases that it might have trouble understanding. For example, your model might encounter a non-native accent that it\u2019s never heard before-say, a native speaker of Polish who moved to South Africa at 16 and learned English there. To figure out if you\u2019ve found bias in your ASR system, you need to be very nuanced in how you\u2019re looking at the data. For example, if you\u2019re finding a few sparse cases of poor transcripts in your ASR system, it could be that you\u2019re simply finding people with accents, like the Polish person mentioned above. In most cases, trying to chase down these edge cases and figure out how to fix them isn\u2019t going to be worth the time or effort, and isn\u2019t evidence that there\u2019s a systematic problem with your model.</p>\n<p>But what if the bias that you\u2019ve detected seems to be for an entire group of people? Maybe you notice that your call center in the Southern US has transcripts that are consistently of poorer quality than call centers in other parts of the country. Because this looks like a systematic problem, rather than a one-off problem, you\u2019ll probably want to take some kind of action to address this bias.</p>\n<WhitepaperPromo whitepaper="deepgram-whitepaper-how-deepgram-works" />\n<h2 id="how-to-reduce-bias-in-asr-systems">How to Reduce Bias in ASR Systems</h2>\n<p>Because of the wide-ranging variation in human speech, it\u2019s unrealistic to think that you\u2019re going to completely remove bias (in the machine learning sense) from ASR systems. Human beings, after all, sometimes have trouble understanding speakers of other dialects, or with non-native accents, and it\u2019s not realistic to expect that AI and ML is going to do better than this-at least, not yet. So the question becomes-when you find that your model is not doing a good job for a specific class of people, what do you do? <strong>You can use that information to make your model better.</strong> And you have two options for how to go about doing this.</p>\n<h3 id="option-1-add-data-to-the-model">Option 1: Add Data to the Model</h3>\n<p>The first option is to add more data to your model. In this case, you retrain with a wider range of sample data to also include the varieties of speech that you\u2019ve discovered your model doesn\u2019t perform well on. This can work to improve the model for everyone, especially if the varieties are relatively similar.  However, in some cases, this can be a mistake. If we take the example above, if you\u2019re a company in the US, it\u2019s unlikely that adding data for Polish-accented South African English is a good choice, even if you\u2019re dealing with a community of speakers. That\u2019s because doing so might end up making your model perform <em>worse</em> for the majority of speakers you want it to handle. So what do you do if this is a population you want to cover, but you think adding data might reduce the model\u2019s performance overall? That\u2019s where the second option comes in.</p>\n<h3 id="option-2-create-distinct-models">Option 2: Create Distinct Models</h3>\n<p>The second option is to create a new model specifically for the newly identified area where the main model is struggling. For example, a bank that has a lot of customers in Guam might discover that Guamanian English is challenging for the general model, and that if you add Guamanian speakers to your training data, the model\u2019s performance suffers for speakers in North America. The solution in these cases is to have two distinct models. If you\u2019ve got a call center in Guam, for example, you could just use a Guam-specific model in that call center only, while you let your more general model run elsewhere. The prevents data from different varieties degrading the quality of a model, while also making sure speech of all relevant groups is transcribed correctly.</p>\n<h2 id="wrapping-up">Wrapping Up</h2>\n<p>As you can see, bias is far from straightforward, and there isn\u2019t a simple, one-size-fits all means of identifying bias in ASR systems and addressing it. It all depends on what you\u2019re seeing in your data, what populations you expect the models to work well on, and how similar the speech of those populations is to one another. But hopefully this post will serve as a guide for you as you\u2019re thinking through these issues. If you\u2019d like to learn more about bias in speech recognition, and how it might even be able to help your ASR efforts, watch our on-demand webinar <a href="https://offers.deepgram.com/when-is-speech-recognition-bias-a-good-thing-webinar-on-demand">When is Speech Recognition Bias a Good Thing?</a></p>' };
const frontmatter = { "title": "Detecting and Reducing Bias in Speech Recognition", "description": "Machine learning bias is top of mind for many people. This blog post will teach you how to identify ASR bias and what to do about it.", "date": "2022-03-09T00:00:00.000Z", "cover": "https://res.cloudinary.com/deepgram/image/upload/v1661981409/blog/detecting-and-reducing-bias-in-speech-recognition/Detecting-Reducing-Bias-in-Speech-thumb-554x220%402x.png", "authors": ["chris-doty"], "category": "ai-and-engineering", "tags": ["bias"], "seo": { "title": "Detecting and Reducing Bias in Speech Recognition", "description": "Machine learning bias is top of mind for many people. This blog post will teach you how to identify ASR bias and what to do about it." }, "og": { "image": "https://res.cloudinary.com/deepgram/image/upload/v1661981409/blog/detecting-and-reducing-bias-in-speech-recognition/Detecting-Reducing-Bias-in-Speech-thumb-554x220%402x.png" }, "shorturls": { "share": "https://dpgr.am/4911de4", "twitter": "https://dpgr.am/d10214e", "linkedin": "https://dpgr.am/a82d026", "reddit": "https://dpgr.am/5852ee4", "facebook": "https://dpgr.am/3da44b9" }, "astro": { "headings": [{ "depth": 2, "slug": "two-kinds-of-bias", "text": "Two Kinds of Bias" }, { "depth": 2, "slug": "where-does-bias-come-from", "text": "Where Does Bias Come From?" }, { "depth": 2, "slug": "how-to-detect-bias-in-asr-systems", "text": "How to Detect Bias in ASR Systems" }, { "depth": 2, "slug": "how-to-reduce-bias-in-asr-systems", "text": "How to Reduce Bias in ASR Systems" }, { "depth": 3, "slug": "option-1-add-data-to-the-model", "text": "Option 1: Add Data to the Model" }, { "depth": 3, "slug": "option-2-create-distinct-models", "text": "Option 2: Create Distinct Models" }, { "depth": 2, "slug": "wrapping-up", "text": "Wrapping Up" }], "source": `Bias in machine learning has been a hot topic in the news lately, and bias in ASR is no exception. In the recent [State of Voice Technology 2022 report](https://deepgram.com/state-of-voice-technology-2022/), we saw that 92% of companies believe voice technology bias has an important impact on their customers, including in the domains of gender, race, age, and accent (both native and non-native speaker). So how do you detect bias in your automated speech recognition (ASR) systems? And what do you do if you find it? We've got answers below. But before we dive in, let's start with a definition of what "bias" is to make sure we're all on the same page.

## Two Kinds of Bias

It's important to understand that when people talk about bias and machine learning, they might be talking about two different things. On the one hand, they might mean **real-world bias** related to things like race, gender, age, etc. But the term bias is also used in **machine learning** when a model is overpredicting or underpredicting the probability for a given category. That's true even if that category is something that we might not apply the term "bias" to in the real world. For example, a model developed by a brewery to track when its vats are at risk of overheating might sound the alarm frequently, even when there is no risk of overheating-a problem of machine learning bias, but not real-world bias.

In many cases, though, when people talk about "machine learning bias" generally-especially in the media-they're referring to the intersection of real world and machine learning bias: a case where a model is overpredicting or underpredicting in terms of something like gender or race. With these definitions out of the way, let's turn to examining where bias comes from before moving on to how you might figure out if you've got bias in your ASR system, as well as what your next steps are if you've found it.

## Where Does Bias Come From?

Machine learning bias often comes from data-biased data leads to a biased model. If you've taken a statistics course, you're familiar with **sampling bias,** which is when data that is supposed to represent the true range of possibilities is instead skewed in some way. If you use data like this to train a model, the model is going to be skewed in the same way as the data. This sampling bias can show up in your model as machine learning or real-world bias because many organizations who want to create models rely on their own past decisions. If your company has a history of making biased decisions, any model trained on that data will likewise be biased. 

For example, if a company that trains a model to help make promotion decisions has a poor track record of promoting women, it's likely that their model will make the same kinds of biased decisions if it's trained on the company's past behavior. In some cases, however, a model might be biased towards a certain group of people for less nefarious reasons. For example, an initial ASR model might simply be biased because it was trained on speech that was easy for the creator to collect-from themself, their family, and their friends, who likely all speak similarly. Here, the issue isn't biased past decisions, per se, but simply a lack of data to cover different possibilities.

## How to Detect Bias in ASR Systems

So how do you know if you've got bias on your ASR system? Detecting it is relatively easy on its face. For example, you might ask if your transcripts for some speakers have significantly higher [word error rates (WER)](https://blog.deepgram.com/what-is-word-error-rate/) than for other speakers. But even the answers to a simple question like this can sometimes be difficult to understand on their face. Language is infinitely varied, and even though we speak of, for example, "American English", this actually represents a wide range of linguistic variation, from Massachusetts to North Dakota to Alabama. And in other places around the world, the variation between dialects is sometimes even greater than what we see in the US.

When you're thinking about how well your ASR system is performing, you're always going to encounter edge cases that it might have trouble understanding. For example, your model might encounter a non-native accent that it's never heard before-say, a native speaker of Polish who moved to South Africa at 16 and learned English there. To figure out if you've found bias in your ASR system, you need to be very nuanced in how you're looking at the data. For example, if you're finding a few sparse cases of poor transcripts in your ASR system, it could be that you're simply finding people with accents, like the Polish person mentioned above. In most cases, trying to chase down these edge cases and figure out how to fix them isn't going to be worth the time or effort, and isn't evidence that there's a systematic problem with your model.

But what if the bias that you've detected seems to be for an entire group of people? Maybe you notice that your call center in the Southern US has transcripts that are consistently of poorer quality than call centers in other parts of the country. Because this looks like a systematic problem, rather than a one-off problem, you'll probably want to take some kind of action to address this bias.

<WhitepaperPromo whitepaper="deepgram-whitepaper-how-deepgram-works"></WhitepaperPromo>

## How to Reduce Bias in ASR Systems

Because of the wide-ranging variation in human speech, it's unrealistic to think that you're going to completely remove bias (in the machine learning sense) from ASR systems. Human beings, after all, sometimes have trouble understanding speakers of other dialects, or with non-native accents, and it's not realistic to expect that AI and ML is going to do better than this-at least, not yet. So the question becomes-when you find that your model is not doing a good job for a specific class of people, what do you do? **You can use that information to make your model better.** And you have two options for how to go about doing this.

### Option 1: Add Data to the Model

The first option is to add more data to your model. In this case, you retrain with a wider range of sample data to also include the varieties of speech that you've discovered your model doesn't perform well on. This can work to improve the model for everyone, especially if the varieties are relatively similar.  However, in some cases, this can be a mistake. If we take the example above, if you're a company in the US, it's unlikely that adding data for Polish-accented South African English is a good choice, even if you're dealing with a community of speakers. That's because doing so might end up making your model perform *worse* for the majority of speakers you want it to handle. So what do you do if this is a population you want to cover, but you think adding data might reduce the model's performance overall? That's where the second option comes in.

### Option 2: Create Distinct Models

The second option is to create a new model specifically for the newly identified area where the main model is struggling. For example, a bank that has a lot of customers in Guam might discover that Guamanian English is challenging for the general model, and that if you add Guamanian speakers to your training data, the model's performance suffers for speakers in North America. The solution in these cases is to have two distinct models. If you've got a call center in Guam, for example, you could just use a Guam-specific model in that call center only, while you let your more general model run elsewhere. The prevents data from different varieties degrading the quality of a model, while also making sure speech of all relevant groups is transcribed correctly.

## Wrapping Up

As you can see, bias is far from straightforward, and there isn't a simple, one-size-fits all means of identifying bias in ASR systems and addressing it. It all depends on what you're seeing in your data, what populations you expect the models to work well on, and how similar the speech of those populations is to one another. But hopefully this post will serve as a guide for you as you're thinking through these issues. If you'd like to learn more about bias in speech recognition, and how it might even be able to help your ASR efforts, watch our on-demand webinar [When is Speech Recognition Bias a Good Thing?](https://offers.deepgram.com/when-is-speech-recognition-bias-a-good-thing-webinar-on-demand)`, "html": '<p>Bias in machine learning has been a hot topic in the news lately, and bias in ASR is no exception. In the recent <a href="https://deepgram.com/state-of-voice-technology-2022/">State of Voice Technology 2022 report</a>, we saw that 92% of companies believe voice technology bias has an important impact on their customers, including in the domains of gender, race, age, and accent (both native and non-native speaker). So how do you detect bias in your automated speech recognition (ASR) systems? And what do you do if you find it? We\u2019ve got answers below. But before we dive in, let\u2019s start with a definition of what \u201Cbias\u201D is to make sure we\u2019re all on the same page.</p>\n<h2 id="two-kinds-of-bias">Two Kinds of Bias</h2>\n<p>It\u2019s important to understand that when people talk about bias and machine learning, they might be talking about two different things. On the one hand, they might mean <strong>real-world bias</strong> related to things like race, gender, age, etc. But the term bias is also used in <strong>machine learning</strong> when a model is overpredicting or underpredicting the probability for a given category. That\u2019s true even if that category is something that we might not apply the term \u201Cbias\u201D to in the real world. For example, a model developed by a brewery to track when its vats are at risk of overheating might sound the alarm frequently, even when there is no risk of overheating-a problem of machine learning bias, but not real-world bias.</p>\n<p>In many cases, though, when people talk about \u201Cmachine learning bias\u201D generally-especially in the media-they\u2019re referring to the intersection of real world and machine learning bias: a case where a model is overpredicting or underpredicting in terms of something like gender or race. With these definitions out of the way, let\u2019s turn to examining where bias comes from before moving on to how you might figure out if you\u2019ve got bias in your ASR system, as well as what your next steps are if you\u2019ve found it.</p>\n<h2 id="where-does-bias-come-from">Where Does Bias Come From?</h2>\n<p>Machine learning bias often comes from data-biased data leads to a biased model. If you\u2019ve taken a statistics course, you\u2019re familiar with <strong>sampling bias,</strong> which is when data that is supposed to represent the true range of possibilities is instead skewed in some way. If you use data like this to train a model, the model is going to be skewed in the same way as the data. This sampling bias can show up in your model as machine learning or real-world bias because many organizations who want to create models rely on their own past decisions. If your company has a history of making biased decisions, any model trained on that data will likewise be biased.</p>\n<p>For example, if a company that trains a model to help make promotion decisions has a poor track record of promoting women, it\u2019s likely that their model will make the same kinds of biased decisions if it\u2019s trained on the company\u2019s past behavior. In some cases, however, a model might be biased towards a certain group of people for less nefarious reasons. For example, an initial ASR model might simply be biased because it was trained on speech that was easy for the creator to collect-from themself, their family, and their friends, who likely all speak similarly. Here, the issue isn\u2019t biased past decisions, per se, but simply a lack of data to cover different possibilities.</p>\n<h2 id="how-to-detect-bias-in-asr-systems">How to Detect Bias in ASR Systems</h2>\n<p>So how do you know if you\u2019ve got bias on your ASR system? Detecting it is relatively easy on its face. For example, you might ask if your transcripts for some speakers have significantly higher <a href="https://blog.deepgram.com/what-is-word-error-rate/">word error rates (WER)</a> than for other speakers. But even the answers to a simple question like this can sometimes be difficult to understand on their face. Language is infinitely varied, and even though we speak of, for example, \u201CAmerican English\u201D, this actually represents a wide range of linguistic variation, from Massachusetts to North Dakota to Alabama. And in other places around the world, the variation between dialects is sometimes even greater than what we see in the US.</p>\n<p>When you\u2019re thinking about how well your ASR system is performing, you\u2019re always going to encounter edge cases that it might have trouble understanding. For example, your model might encounter a non-native accent that it\u2019s never heard before-say, a native speaker of Polish who moved to South Africa at 16 and learned English there. To figure out if you\u2019ve found bias in your ASR system, you need to be very nuanced in how you\u2019re looking at the data. For example, if you\u2019re finding a few sparse cases of poor transcripts in your ASR system, it could be that you\u2019re simply finding people with accents, like the Polish person mentioned above. In most cases, trying to chase down these edge cases and figure out how to fix them isn\u2019t going to be worth the time or effort, and isn\u2019t evidence that there\u2019s a systematic problem with your model.</p>\n<p>But what if the bias that you\u2019ve detected seems to be for an entire group of people? Maybe you notice that your call center in the Southern US has transcripts that are consistently of poorer quality than call centers in other parts of the country. Because this looks like a systematic problem, rather than a one-off problem, you\u2019ll probably want to take some kind of action to address this bias.</p>\n<WhitepaperPromo whitepaper="deepgram-whitepaper-how-deepgram-works" />\n<h2 id="how-to-reduce-bias-in-asr-systems">How to Reduce Bias in ASR Systems</h2>\n<p>Because of the wide-ranging variation in human speech, it\u2019s unrealistic to think that you\u2019re going to completely remove bias (in the machine learning sense) from ASR systems. Human beings, after all, sometimes have trouble understanding speakers of other dialects, or with non-native accents, and it\u2019s not realistic to expect that AI and ML is going to do better than this-at least, not yet. So the question becomes-when you find that your model is not doing a good job for a specific class of people, what do you do? <strong>You can use that information to make your model better.</strong> And you have two options for how to go about doing this.</p>\n<h3 id="option-1-add-data-to-the-model">Option 1: Add Data to the Model</h3>\n<p>The first option is to add more data to your model. In this case, you retrain with a wider range of sample data to also include the varieties of speech that you\u2019ve discovered your model doesn\u2019t perform well on. This can work to improve the model for everyone, especially if the varieties are relatively similar.  However, in some cases, this can be a mistake. If we take the example above, if you\u2019re a company in the US, it\u2019s unlikely that adding data for Polish-accented South African English is a good choice, even if you\u2019re dealing with a community of speakers. That\u2019s because doing so might end up making your model perform <em>worse</em> for the majority of speakers you want it to handle. So what do you do if this is a population you want to cover, but you think adding data might reduce the model\u2019s performance overall? That\u2019s where the second option comes in.</p>\n<h3 id="option-2-create-distinct-models">Option 2: Create Distinct Models</h3>\n<p>The second option is to create a new model specifically for the newly identified area where the main model is struggling. For example, a bank that has a lot of customers in Guam might discover that Guamanian English is challenging for the general model, and that if you add Guamanian speakers to your training data, the model\u2019s performance suffers for speakers in North America. The solution in these cases is to have two distinct models. If you\u2019ve got a call center in Guam, for example, you could just use a Guam-specific model in that call center only, while you let your more general model run elsewhere. The prevents data from different varieties degrading the quality of a model, while also making sure speech of all relevant groups is transcribed correctly.</p>\n<h2 id="wrapping-up">Wrapping Up</h2>\n<p>As you can see, bias is far from straightforward, and there isn\u2019t a simple, one-size-fits all means of identifying bias in ASR systems and addressing it. It all depends on what you\u2019re seeing in your data, what populations you expect the models to work well on, and how similar the speech of those populations is to one another. But hopefully this post will serve as a guide for you as you\u2019re thinking through these issues. If you\u2019d like to learn more about bias in speech recognition, and how it might even be able to help your ASR efforts, watch our on-demand webinar <a href="https://offers.deepgram.com/when-is-speech-recognition-bias-a-good-thing-webinar-on-demand">When is Speech Recognition Bias a Good Thing?</a></p>' }, "file": "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/detecting-and-reducing-bias-in-speech-recognition/index.md" };
function rawContent() {
  return `Bias in machine learning has been a hot topic in the news lately, and bias in ASR is no exception. In the recent [State of Voice Technology 2022 report](https://deepgram.com/state-of-voice-technology-2022/), we saw that 92% of companies believe voice technology bias has an important impact on their customers, including in the domains of gender, race, age, and accent (both native and non-native speaker). So how do you detect bias in your automated speech recognition (ASR) systems? And what do you do if you find it? We've got answers below. But before we dive in, let's start with a definition of what "bias" is to make sure we're all on the same page.

## Two Kinds of Bias

It's important to understand that when people talk about bias and machine learning, they might be talking about two different things. On the one hand, they might mean **real-world bias** related to things like race, gender, age, etc. But the term bias is also used in **machine learning** when a model is overpredicting or underpredicting the probability for a given category. That's true even if that category is something that we might not apply the term "bias" to in the real world. For example, a model developed by a brewery to track when its vats are at risk of overheating might sound the alarm frequently, even when there is no risk of overheating-a problem of machine learning bias, but not real-world bias.

In many cases, though, when people talk about "machine learning bias" generally-especially in the media-they're referring to the intersection of real world and machine learning bias: a case where a model is overpredicting or underpredicting in terms of something like gender or race. With these definitions out of the way, let's turn to examining where bias comes from before moving on to how you might figure out if you've got bias in your ASR system, as well as what your next steps are if you've found it.

## Where Does Bias Come From?

Machine learning bias often comes from data-biased data leads to a biased model. If you've taken a statistics course, you're familiar with **sampling bias,** which is when data that is supposed to represent the true range of possibilities is instead skewed in some way. If you use data like this to train a model, the model is going to be skewed in the same way as the data. This sampling bias can show up in your model as machine learning or real-world bias because many organizations who want to create models rely on their own past decisions. If your company has a history of making biased decisions, any model trained on that data will likewise be biased. 

For example, if a company that trains a model to help make promotion decisions has a poor track record of promoting women, it's likely that their model will make the same kinds of biased decisions if it's trained on the company's past behavior. In some cases, however, a model might be biased towards a certain group of people for less nefarious reasons. For example, an initial ASR model might simply be biased because it was trained on speech that was easy for the creator to collect-from themself, their family, and their friends, who likely all speak similarly. Here, the issue isn't biased past decisions, per se, but simply a lack of data to cover different possibilities.

## How to Detect Bias in ASR Systems

So how do you know if you've got bias on your ASR system? Detecting it is relatively easy on its face. For example, you might ask if your transcripts for some speakers have significantly higher [word error rates (WER)](https://blog.deepgram.com/what-is-word-error-rate/) than for other speakers. But even the answers to a simple question like this can sometimes be difficult to understand on their face. Language is infinitely varied, and even though we speak of, for example, "American English", this actually represents a wide range of linguistic variation, from Massachusetts to North Dakota to Alabama. And in other places around the world, the variation between dialects is sometimes even greater than what we see in the US.

When you're thinking about how well your ASR system is performing, you're always going to encounter edge cases that it might have trouble understanding. For example, your model might encounter a non-native accent that it's never heard before-say, a native speaker of Polish who moved to South Africa at 16 and learned English there. To figure out if you've found bias in your ASR system, you need to be very nuanced in how you're looking at the data. For example, if you're finding a few sparse cases of poor transcripts in your ASR system, it could be that you're simply finding people with accents, like the Polish person mentioned above. In most cases, trying to chase down these edge cases and figure out how to fix them isn't going to be worth the time or effort, and isn't evidence that there's a systematic problem with your model.

But what if the bias that you've detected seems to be for an entire group of people? Maybe you notice that your call center in the Southern US has transcripts that are consistently of poorer quality than call centers in other parts of the country. Because this looks like a systematic problem, rather than a one-off problem, you'll probably want to take some kind of action to address this bias.

<WhitepaperPromo whitepaper="deepgram-whitepaper-how-deepgram-works"></WhitepaperPromo>

## How to Reduce Bias in ASR Systems

Because of the wide-ranging variation in human speech, it's unrealistic to think that you're going to completely remove bias (in the machine learning sense) from ASR systems. Human beings, after all, sometimes have trouble understanding speakers of other dialects, or with non-native accents, and it's not realistic to expect that AI and ML is going to do better than this-at least, not yet. So the question becomes-when you find that your model is not doing a good job for a specific class of people, what do you do? **You can use that information to make your model better.** And you have two options for how to go about doing this.

### Option 1: Add Data to the Model

The first option is to add more data to your model. In this case, you retrain with a wider range of sample data to also include the varieties of speech that you've discovered your model doesn't perform well on. This can work to improve the model for everyone, especially if the varieties are relatively similar.  However, in some cases, this can be a mistake. If we take the example above, if you're a company in the US, it's unlikely that adding data for Polish-accented South African English is a good choice, even if you're dealing with a community of speakers. That's because doing so might end up making your model perform *worse* for the majority of speakers you want it to handle. So what do you do if this is a population you want to cover, but you think adding data might reduce the model's performance overall? That's where the second option comes in.

### Option 2: Create Distinct Models

The second option is to create a new model specifically for the newly identified area where the main model is struggling. For example, a bank that has a lot of customers in Guam might discover that Guamanian English is challenging for the general model, and that if you add Guamanian speakers to your training data, the model's performance suffers for speakers in North America. The solution in these cases is to have two distinct models. If you've got a call center in Guam, for example, you could just use a Guam-specific model in that call center only, while you let your more general model run elsewhere. The prevents data from different varieties degrading the quality of a model, while also making sure speech of all relevant groups is transcribed correctly.

## Wrapping Up

As you can see, bias is far from straightforward, and there isn't a simple, one-size-fits all means of identifying bias in ASR systems and addressing it. It all depends on what you're seeing in your data, what populations you expect the models to work well on, and how similar the speech of those populations is to one another. But hopefully this post will serve as a guide for you as you're thinking through these issues. If you'd like to learn more about bias in speech recognition, and how it might even be able to help your ASR efforts, watch our on-demand webinar [When is Speech Recognition Bias a Good Thing?](https://offers.deepgram.com/when-is-speech-recognition-bias-a-good-thing-webinar-on-demand)`;
}
function compiledContent() {
  return '<p>Bias in machine learning has been a hot topic in the news lately, and bias in ASR is no exception. In the recent <a href="https://deepgram.com/state-of-voice-technology-2022/">State of Voice Technology 2022 report</a>, we saw that 92% of companies believe voice technology bias has an important impact on their customers, including in the domains of gender, race, age, and accent (both native and non-native speaker). So how do you detect bias in your automated speech recognition (ASR) systems? And what do you do if you find it? We\u2019ve got answers below. But before we dive in, let\u2019s start with a definition of what \u201Cbias\u201D is to make sure we\u2019re all on the same page.</p>\n<h2 id="two-kinds-of-bias">Two Kinds of Bias</h2>\n<p>It\u2019s important to understand that when people talk about bias and machine learning, they might be talking about two different things. On the one hand, they might mean <strong>real-world bias</strong> related to things like race, gender, age, etc. But the term bias is also used in <strong>machine learning</strong> when a model is overpredicting or underpredicting the probability for a given category. That\u2019s true even if that category is something that we might not apply the term \u201Cbias\u201D to in the real world. For example, a model developed by a brewery to track when its vats are at risk of overheating might sound the alarm frequently, even when there is no risk of overheating-a problem of machine learning bias, but not real-world bias.</p>\n<p>In many cases, though, when people talk about \u201Cmachine learning bias\u201D generally-especially in the media-they\u2019re referring to the intersection of real world and machine learning bias: a case where a model is overpredicting or underpredicting in terms of something like gender or race. With these definitions out of the way, let\u2019s turn to examining where bias comes from before moving on to how you might figure out if you\u2019ve got bias in your ASR system, as well as what your next steps are if you\u2019ve found it.</p>\n<h2 id="where-does-bias-come-from">Where Does Bias Come From?</h2>\n<p>Machine learning bias often comes from data-biased data leads to a biased model. If you\u2019ve taken a statistics course, you\u2019re familiar with <strong>sampling bias,</strong> which is when data that is supposed to represent the true range of possibilities is instead skewed in some way. If you use data like this to train a model, the model is going to be skewed in the same way as the data. This sampling bias can show up in your model as machine learning or real-world bias because many organizations who want to create models rely on their own past decisions. If your company has a history of making biased decisions, any model trained on that data will likewise be biased.</p>\n<p>For example, if a company that trains a model to help make promotion decisions has a poor track record of promoting women, it\u2019s likely that their model will make the same kinds of biased decisions if it\u2019s trained on the company\u2019s past behavior. In some cases, however, a model might be biased towards a certain group of people for less nefarious reasons. For example, an initial ASR model might simply be biased because it was trained on speech that was easy for the creator to collect-from themself, their family, and their friends, who likely all speak similarly. Here, the issue isn\u2019t biased past decisions, per se, but simply a lack of data to cover different possibilities.</p>\n<h2 id="how-to-detect-bias-in-asr-systems">How to Detect Bias in ASR Systems</h2>\n<p>So how do you know if you\u2019ve got bias on your ASR system? Detecting it is relatively easy on its face. For example, you might ask if your transcripts for some speakers have significantly higher <a href="https://blog.deepgram.com/what-is-word-error-rate/">word error rates (WER)</a> than for other speakers. But even the answers to a simple question like this can sometimes be difficult to understand on their face. Language is infinitely varied, and even though we speak of, for example, \u201CAmerican English\u201D, this actually represents a wide range of linguistic variation, from Massachusetts to North Dakota to Alabama. And in other places around the world, the variation between dialects is sometimes even greater than what we see in the US.</p>\n<p>When you\u2019re thinking about how well your ASR system is performing, you\u2019re always going to encounter edge cases that it might have trouble understanding. For example, your model might encounter a non-native accent that it\u2019s never heard before-say, a native speaker of Polish who moved to South Africa at 16 and learned English there. To figure out if you\u2019ve found bias in your ASR system, you need to be very nuanced in how you\u2019re looking at the data. For example, if you\u2019re finding a few sparse cases of poor transcripts in your ASR system, it could be that you\u2019re simply finding people with accents, like the Polish person mentioned above. In most cases, trying to chase down these edge cases and figure out how to fix them isn\u2019t going to be worth the time or effort, and isn\u2019t evidence that there\u2019s a systematic problem with your model.</p>\n<p>But what if the bias that you\u2019ve detected seems to be for an entire group of people? Maybe you notice that your call center in the Southern US has transcripts that are consistently of poorer quality than call centers in other parts of the country. Because this looks like a systematic problem, rather than a one-off problem, you\u2019ll probably want to take some kind of action to address this bias.</p>\n<WhitepaperPromo whitepaper="deepgram-whitepaper-how-deepgram-works" />\n<h2 id="how-to-reduce-bias-in-asr-systems">How to Reduce Bias in ASR Systems</h2>\n<p>Because of the wide-ranging variation in human speech, it\u2019s unrealistic to think that you\u2019re going to completely remove bias (in the machine learning sense) from ASR systems. Human beings, after all, sometimes have trouble understanding speakers of other dialects, or with non-native accents, and it\u2019s not realistic to expect that AI and ML is going to do better than this-at least, not yet. So the question becomes-when you find that your model is not doing a good job for a specific class of people, what do you do? <strong>You can use that information to make your model better.</strong> And you have two options for how to go about doing this.</p>\n<h3 id="option-1-add-data-to-the-model">Option 1: Add Data to the Model</h3>\n<p>The first option is to add more data to your model. In this case, you retrain with a wider range of sample data to also include the varieties of speech that you\u2019ve discovered your model doesn\u2019t perform well on. This can work to improve the model for everyone, especially if the varieties are relatively similar.  However, in some cases, this can be a mistake. If we take the example above, if you\u2019re a company in the US, it\u2019s unlikely that adding data for Polish-accented South African English is a good choice, even if you\u2019re dealing with a community of speakers. That\u2019s because doing so might end up making your model perform <em>worse</em> for the majority of speakers you want it to handle. So what do you do if this is a population you want to cover, but you think adding data might reduce the model\u2019s performance overall? That\u2019s where the second option comes in.</p>\n<h3 id="option-2-create-distinct-models">Option 2: Create Distinct Models</h3>\n<p>The second option is to create a new model specifically for the newly identified area where the main model is struggling. For example, a bank that has a lot of customers in Guam might discover that Guamanian English is challenging for the general model, and that if you add Guamanian speakers to your training data, the model\u2019s performance suffers for speakers in North America. The solution in these cases is to have two distinct models. If you\u2019ve got a call center in Guam, for example, you could just use a Guam-specific model in that call center only, while you let your more general model run elsewhere. The prevents data from different varieties degrading the quality of a model, while also making sure speech of all relevant groups is transcribed correctly.</p>\n<h2 id="wrapping-up">Wrapping Up</h2>\n<p>As you can see, bias is far from straightforward, and there isn\u2019t a simple, one-size-fits all means of identifying bias in ASR systems and addressing it. It all depends on what you\u2019re seeing in your data, what populations you expect the models to work well on, and how similar the speech of those populations is to one another. But hopefully this post will serve as a guide for you as you\u2019re thinking through these issues. If you\u2019d like to learn more about bias in speech recognition, and how it might even be able to help your ASR efforts, watch our on-demand webinar <a href="https://offers.deepgram.com/when-is-speech-recognition-bias-a-good-thing-webinar-on-demand">When is Speech Recognition Bias a Good Thing?</a></p>';
}
const $$Astro = createAstro("/Users/sandrarodgers/web-next/blog/src/content/blog/posts/detecting-and-reducing-bias-in-speech-recognition/index.md", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Index = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro, $$props, $$slots);
  Astro2.self = $$Index;
  new Slugger();
  return renderTemplate`<head>${renderHead($$result)}</head><p>Bias in machine learning has been a hot topic in the news lately, and bias in ASR is no exception. In the recent <a href="https://deepgram.com/state-of-voice-technology-2022/">State of Voice Technology 2022 report</a>, we saw that 92% of companies believe voice technology bias has an important impact on their customers, including in the domains of gender, race, age, and accent (both native and non-native speaker). So how do you detect bias in your automated speech recognition (ASR) systems? And what do you do if you find it? We’ve got answers below. But before we dive in, let’s start with a definition of what “bias” is to make sure we’re all on the same page.</p>
<h2 id="two-kinds-of-bias">Two Kinds of Bias</h2>
<p>It’s important to understand that when people talk about bias and machine learning, they might be talking about two different things. On the one hand, they might mean <strong>real-world bias</strong> related to things like race, gender, age, etc. But the term bias is also used in <strong>machine learning</strong> when a model is overpredicting or underpredicting the probability for a given category. That’s true even if that category is something that we might not apply the term “bias” to in the real world. For example, a model developed by a brewery to track when its vats are at risk of overheating might sound the alarm frequently, even when there is no risk of overheating-a problem of machine learning bias, but not real-world bias.</p>
<p>In many cases, though, when people talk about “machine learning bias” generally-especially in the media-they’re referring to the intersection of real world and machine learning bias: a case where a model is overpredicting or underpredicting in terms of something like gender or race. With these definitions out of the way, let’s turn to examining where bias comes from before moving on to how you might figure out if you’ve got bias in your ASR system, as well as what your next steps are if you’ve found it.</p>
<h2 id="where-does-bias-come-from">Where Does Bias Come From?</h2>
<p>Machine learning bias often comes from data-biased data leads to a biased model. If you’ve taken a statistics course, you’re familiar with <strong>sampling bias,</strong> which is when data that is supposed to represent the true range of possibilities is instead skewed in some way. If you use data like this to train a model, the model is going to be skewed in the same way as the data. This sampling bias can show up in your model as machine learning or real-world bias because many organizations who want to create models rely on their own past decisions. If your company has a history of making biased decisions, any model trained on that data will likewise be biased.</p>
<p>For example, if a company that trains a model to help make promotion decisions has a poor track record of promoting women, it’s likely that their model will make the same kinds of biased decisions if it’s trained on the company’s past behavior. In some cases, however, a model might be biased towards a certain group of people for less nefarious reasons. For example, an initial ASR model might simply be biased because it was trained on speech that was easy for the creator to collect-from themself, their family, and their friends, who likely all speak similarly. Here, the issue isn’t biased past decisions, per se, but simply a lack of data to cover different possibilities.</p>
<h2 id="how-to-detect-bias-in-asr-systems">How to Detect Bias in ASR Systems</h2>
<p>So how do you know if you’ve got bias on your ASR system? Detecting it is relatively easy on its face. For example, you might ask if your transcripts for some speakers have significantly higher <a href="https://blog.deepgram.com/what-is-word-error-rate/">word error rates (WER)</a> than for other speakers. But even the answers to a simple question like this can sometimes be difficult to understand on their face. Language is infinitely varied, and even though we speak of, for example, “American English”, this actually represents a wide range of linguistic variation, from Massachusetts to North Dakota to Alabama. And in other places around the world, the variation between dialects is sometimes even greater than what we see in the US.</p>
<p>When you’re thinking about how well your ASR system is performing, you’re always going to encounter edge cases that it might have trouble understanding. For example, your model might encounter a non-native accent that it’s never heard before-say, a native speaker of Polish who moved to South Africa at 16 and learned English there. To figure out if you’ve found bias in your ASR system, you need to be very nuanced in how you’re looking at the data. For example, if you’re finding a few sparse cases of poor transcripts in your ASR system, it could be that you’re simply finding people with accents, like the Polish person mentioned above. In most cases, trying to chase down these edge cases and figure out how to fix them isn’t going to be worth the time or effort, and isn’t evidence that there’s a systematic problem with your model.</p>
<p>But what if the bias that you’ve detected seems to be for an entire group of people? Maybe you notice that your call center in the Southern US has transcripts that are consistently of poorer quality than call centers in other parts of the country. Because this looks like a systematic problem, rather than a one-off problem, you’ll probably want to take some kind of action to address this bias.</p>
${renderComponent($$result, "WhitepaperPromo", WhitepaperPromo, { "whitepaper": "deepgram-whitepaper-how-deepgram-works" })}
<h2 id="how-to-reduce-bias-in-asr-systems">How to Reduce Bias in ASR Systems</h2>
<p>Because of the wide-ranging variation in human speech, it’s unrealistic to think that you’re going to completely remove bias (in the machine learning sense) from ASR systems. Human beings, after all, sometimes have trouble understanding speakers of other dialects, or with non-native accents, and it’s not realistic to expect that AI and ML is going to do better than this-at least, not yet. So the question becomes-when you find that your model is not doing a good job for a specific class of people, what do you do? <strong>You can use that information to make your model better.</strong> And you have two options for how to go about doing this.</p>
<h3 id="option-1-add-data-to-the-model">Option 1: Add Data to the Model</h3>
<p>The first option is to add more data to your model. In this case, you retrain with a wider range of sample data to also include the varieties of speech that you’ve discovered your model doesn’t perform well on. This can work to improve the model for everyone, especially if the varieties are relatively similar.  However, in some cases, this can be a mistake. If we take the example above, if you’re a company in the US, it’s unlikely that adding data for Polish-accented South African English is a good choice, even if you’re dealing with a community of speakers. That’s because doing so might end up making your model perform <em>worse</em> for the majority of speakers you want it to handle. So what do you do if this is a population you want to cover, but you think adding data might reduce the model’s performance overall? That’s where the second option comes in.</p>
<h3 id="option-2-create-distinct-models">Option 2: Create Distinct Models</h3>
<p>The second option is to create a new model specifically for the newly identified area where the main model is struggling. For example, a bank that has a lot of customers in Guam might discover that Guamanian English is challenging for the general model, and that if you add Guamanian speakers to your training data, the model’s performance suffers for speakers in North America. The solution in these cases is to have two distinct models. If you’ve got a call center in Guam, for example, you could just use a Guam-specific model in that call center only, while you let your more general model run elsewhere. The prevents data from different varieties degrading the quality of a model, while also making sure speech of all relevant groups is transcribed correctly.</p>
<h2 id="wrapping-up">Wrapping Up</h2>
<p>As you can see, bias is far from straightforward, and there isn’t a simple, one-size-fits all means of identifying bias in ASR systems and addressing it. It all depends on what you’re seeing in your data, what populations you expect the models to work well on, and how similar the speech of those populations is to one another. But hopefully this post will serve as a guide for you as you’re thinking through these issues. If you’d like to learn more about bias in speech recognition, and how it might even be able to help your ASR efforts, watch our on-demand webinar <a href="https://offers.deepgram.com/when-is-speech-recognition-bias-a-good-thing-webinar-on-demand">When is Speech Recognition Bias a Good Thing?</a></p>`;
}, "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/detecting-and-reducing-bias-in-speech-recognition/index.md");

export { compiledContent, $$Index as default, frontmatter, metadata, rawContent };
