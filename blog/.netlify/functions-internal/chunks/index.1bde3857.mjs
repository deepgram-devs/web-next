import { c as createAstro, a as createComponent, r as renderTemplate, b as renderHead } from '../entry.mjs';
import Slugger from 'github-slugger';
import '@astrojs/netlify/netlify-functions.js';
import 'preact';
import 'preact-render-to-string';
import 'vue';
import 'vue/server-renderer';
import 'html-escaper';
import 'node-html-parser';
import 'axios';
/* empty css                           *//* empty css                           *//* empty css                           */import '@storyblok/js';
/* empty css                           *//* empty css                          */import 'clone-deep';
import 'slugify';
import 'shiki';
/* empty css                           */import 'camelcase';
import '@astrojs/rss';
/* empty css                           */import 'mime';
import 'cookie';
import 'kleur/colors';
import 'string-width';
import 'path-browserify';
import 'path-to-regexp';

const metadata = { "headings": [{ "depth": 2, "slug": "understanding-pytorch-torchaudio", "text": "Understanding PyTorch TorchAudio" }, { "depth": 2, "slug": "setting-up-torchaudio-for-speech-recognition", "text": "Setting Up TorchAudio for Speech Recognition" }, { "depth": 2, "slug": "building-a-python-audio-data-streaming-function-for-speech-recognition", "text": "Building a Python Audio Data Streaming Function for Speech Recognition" }, { "depth": 2, "slug": "setting-up-python-speech-recognition-inference-pipeline", "text": "Setting up Python Speech Recognition Inference Pipeline" }, { "depth": 2, "slug": "creating-a-context-cache-to-store-audio-data-for-speech-recognition", "text": "Creating a Context Cache to Store Audio Data for Speech Recognition" }, { "depth": 2, "slug": "using-torchaudios-emformer-model-for-local-speech-recognition-in-python", "text": "Using TorchAudio\u2019s Emformer Model for Local Speech Recognition in Python" }, { "depth": 2, "slug": "full-code-for-building-a-local-streaming-audio-transcription-tool-with-pytorch-torchaudio", "text": "Full Code for Building a Local Streaming Audio Transcription Tool with PyTorch TorchAudio" }, { "depth": 2, "slug": "summary-of-python-speech-recognition-locally-with-pytorch", "text": "Summary of Python Speech Recognition Locally with PyTorch" }], "source": "\"Your call may be recorded for quality assurance purposes.\"\r\n\r\nWe\u2019ve all heard this when calling customer service. What are they doing with that call? How are they transcribing it? Back in the early 2000s and late 1990s, companies were using people to transcribe these. Now, [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing) has come far along enough that we can use Python and machine learning to do automatic speech recognition.\r\n\r\nIn this post, we\u2019ll focus on how to do speech recognition locally on your device using TorchAudio\u2019s pre-built Emformer RNN-T model. We will cover:\r\n\r\n* [Understanding PyTorch TorchAudio](#understanding-pytorch-torchaudio)\r\n* [Setting Up TorchAudio for Speech Recognition](#setting-up-torchaudio-for-speech-recognition)\r\n* [Building a Python Audio Data Streaming Function for Speech Recognition](#building-a-python-audio-data-streaming-function-for-speech-recognition)\r\n* [Setting up Python Speech Recognition Inference Pipeline](#setting-up-python-speech-recognition-inference-pipeline)\r\n* [Creating a Context Cache to Store Audio Data for Speech Recognition](#creating-a-context-cache-to-store-audio-data-for-speech-recognition)\r\n* [Using TorchAudio\u2019s Emformer Model for Local Speech Recognition in Python](#using-torchaudios-emformer-model-for-local-speech-recognition-in-python)\r\n* [In Summary](#in-summary)\r\n\r\n## Understanding PyTorch TorchAudio\r\n\r\nPyTorch is a versatile and powerful Python library for quickly developing machine learning models. Is it the open-source Python version of the Torch library (built on Lua) and primarily developed by Meta/Facebook. TorchAudio is an additional library to PyTorch that handles dealing with audio data for machine learning models.\r\n\r\nTorchAudio isn\u2019t just for creating machine learning models, you can also use it to do some audio data manipulation. We previously covered how to [use TorchAudio to manipulate audio data in Python](https://blog.deepgram.com/pytorch-intro-with-torchaudio/). In this piece, we\u2019re going to use it to build an inference pipeline to do speech recognition in real time.\r\n\r\n## Setting Up TorchAudio for Speech Recognition\r\n\r\nBefore we can dive into the program for doing speech recognition with TorchAudio, we need to set up our system. We need to install the `pytorch`, `torchaudio`, `sentencepiece`, and `ffmpeg-python` libraries. We can install all of these with the command `pip install pytorch torchaudio sentencepiece ffmpeg-python`. If you encounter errors, you may need to upgrade `pip`, which you can do using `pip install -U pip`.\r\n\r\nIf you already have PyTorch and TorchAudio installed and you encounter an error importing `torchaudio` as I did. To get around this, force update both libraries with `pip install -U torch torchaudio --no-cache-dir`. Next, we\u2019re going to build our script. It\u2019s going to consist of an audio data function, the machine learning model pipeline, a context cache, and a main function to run.\r\n\r\n## Building a Python Audio Data Streaming Function for Speech Recognition\r\n\r\nNow that our system is set up to work with PyTorch and Torchaudio for speech recognition, let\u2019s build our script. The first thing we need is an audio data streaming function, we\u2019ll call ours `stream`. We also declare a constant number of iterations for the streaming function to stream.\r\n\r\nOur `stream` function needs five parameters. It needs a queue, a format or device (different for Mac vs Windows), a source, a segment length, and a sample rate. There are some print statements in our function that I\u2019ve commented out. You can uncomment these to show more information about the program as it runs.\r\n\r\nThe first thing we\u2019re going to do in our `stream` function is create a `StreamReader` instance from `torchaudio.io`. Next, we\u2019ll populate that stream reader with an audio stream of the passed in segment length and sample rate. I would leave the `\"Streaming\"` and blank `print` statements uncommented so you know when you can start talking into the mic.\r\n\r\nNext, we\u2019ll create an iterator to iterate across our stream. Finally, we\u2019ll loop through the number of predefined iterations, in our case 100, and get the next chunk of audio data from the stream iterator and put it in the queue.\r\n\r\n```py\r\nimport torch\r\nimport torch.multiprocessing as mp\r\nimport torchaudio\r\nfrom torchaudio.io import StreamReader\r\n \r\nITERATIONS = 100\r\ndef stream(queue: mp.Queue(),\r\n   format: str,\r\n   src: str,\r\n   frames_per_chunk: int,\r\n   sample_rate: int):\r\n   '''Streams audio data\r\n  \r\n   Parameters:\r\n       queue: Queue of data chunks\r\n       format: Format\r\n       src: Source\r\n       frames_per_chunk: How many frames are in each data chunk\r\n       sample_rate: Sample rate\r\n \r\n   Returns:\r\n       None'''\r\n   print(\"Initializing Audio Stream\")\r\n   streamer = StreamReader(src, format=format)\r\n   streamer.add_basic_audio_stream(frames_per_chunk=frames_per_chunk,\r\n       sample_rate=sample_rate)\r\n   print(\"Streaming\\n\")\r\n   stream_iterator = streamer.stream(timeout=-1, backoff=1.0)\r\n   for _ in range(ITERATIONS):\r\n       (chunk,) = next(stream_iterator)\r\n       queue.put(chunk)\r\n```\r\n\r\n## Setting up Python Speech Recognition Inference Pipeline\r\n\r\nNow that we have a way to smoothly stream the audio data from a microphone, we can run predictions on it. Let\u2019s build a pipeline to do speech recognition with. Unlike the part above, this time we will be creating a `Class` and not a function.\r\n\r\nWe require our class to be initialized with one variable, an RNN-T model from TorchAudio\u2019s Pipelines library. We also allow a second, optional variable to define beam width, which is set at 10 by default. Our `__init__` function sets the instance\u2019s `bundle` value to the model we passed in. Then, it uses that model to get the feature extractor, decoder, and token processor. Next, it sets the beam width, the state of the pipeline, and the predictions.\r\n\r\nThe other function that our `Pipeline` object has is an inference function. This function takes a PyTorch `Tensor` object and returns a string, the predicted text. This function uses the feature extractor to get the features and the length of the input tensor. Next, call the inference function from the decoder to get the hypothesis and set the state of the Pipeline. We set the instance\u2019s `hypothesis` attribute to the value in the first index of the returned hypothesis. Finally, we use the token processor on the first entry in the instance\u2019s hypothesis attribute and return that value.\r\n\r\n```py\r\nclass InferencePipeline:\r\n   '''Creates an inference pipeline for streaming audio data'''\r\n   def __init__(self,\r\n       pipeline: torchaudio.pipelines.RNNTBundle,\r\n       beam_width: int=10):\r\n       '''Initializes TorchAudio RNNT Pipeline\r\n      \r\n       Parameters:\r\n           pipeline: TorchAudio Pipeline to use\r\n           beam_width: Beam width\r\n \r\n       Returns:\r\n           None'''\r\n \r\n       self.pipeline = pipeline\r\n       self.feature_extractor = pipeline.get_streaming_feature_extractor()\r\n       self.decoder = pipeline.get_decoder()\r\n       self.token_processor = pipeline.get_token_processor()\r\n       self.beam_width = beam_width\r\n       self.state = None\r\n       self.hypothesis = None\r\n  \r\n   def infer(self, segment: torch.Tensor) -> str:\r\n       '''Runs inference using the initialized pipeline\r\n      \r\n       Parameters:\r\n           segment: Torch tensor with features to extract\r\n      \r\n       Returns:\r\n           Transcript as string type'''\r\n \r\n       features, length = self.feature_extractor(segment)\r\n       predictions, self.state = self.decoder.infer(\r\n           features, length, self.beam_width, state=self.state,\r\n           hypothesis=self.hypothesis\r\n       )\r\n       self.hypothesis = predictions[0]\r\n       transcript = self.token_processor(self.hypothesis[0], lstrip=False)\r\n       return transcript\r\n```\r\n\r\n## Creating a Context Cache to Store Audio Data for Speech Recognition\r\n\r\nWe need a context cache to batch store the audio data frames as we read them. The context cache requires two parameters to initialize. First, a segment length, then a context length. The initialization function sets the instance\u2019s attributes to the passed in values and initializes a tensor of zeros of the context length as the instance\u2019s current context.\r\n\r\nThe second function we\u2019ll define is the `call` function. This is another automatic function that we\u2019re overwriting. Whenever you see a class function preceded and followed by two underscores, that\u2019s a default function. In this case, we require the call function to intake a PyTorch tensor object.\r\n\r\nIf the size of the first object in the tensor is less than the set segment length for the cache, we\u2019ll pad that chunk with 0s. Next, we use the concatenate function from `torch` to add that chunk to the current context. Then, we set the instance\u2019s context attribute to the last entries in the chunk equivalent to the context length. Finally, we return the concatenate context.\r\n\r\n```py\r\nclass ContextCacher:\r\n   def __init__(self, segment_length: int, context_length: int):\r\n       '''Creates initial context cache\r\n      \r\n       Parameters:\r\n           segment_length: length of one audio segment\r\n           context_length: length of the context\r\n \r\n       Returns:\r\n           None'''\r\n       self.segment_length = segment_length\r\n       self.context_length = context_length\r\n       self.context = torch.zeros([context_length])\r\n  \r\n   def __call__(self, chunk: torch.Tensor):\r\n       '''Adds chunk to context and returns it\r\n      \r\n       Parameters:\r\n           chunk: chunk of audio data to process\r\n      \r\n       Returns:\r\n           Tensor'''\r\n       if chunk.size(0) < self.segment_length:\r\n           chunk = torch.nn.functional.pad(chunk,\r\n               (0, self.segment_length - chunk.size(0)))\r\n       chunk_with_context = torch.cat((self.context, chunk))\r\n       self.context = chunk[-self.context_length :]\r\n       return chunk_with_context\r\n```\r\n\r\n## Using TorchAudio's Emformer Model for Local Speech Recognition in Python\r\n\r\nFinally, all our helper functions have been made. It is now time to create the main function to orchestrate the audio data streamer, the machine learning pipeline, and the cache. Our main function will take three inputs: the input device, the source, and the RNN-T model. As before, I\u2019ve left some print statements in here commented out, uncomment them if you\u2019d like.\r\n\r\nFirst, we create an instance of our Pipeline. This is what turns the audio data into text. Next, we use the RNN-T model to set the sample rate, the segment length, and the context length. Then, we instantiate the context with the newly gotten segment and context lengths.\r\n\r\nNow things get funky. We use an annotation on the `infer` function we\u2019re creating that signals to `torch` to turn on inference mode while this function runs. The infer function does not need any parameters. It loops through the preset number of iterations. For each iteration, it gets a chunk of data from the queue, calls the cache to return the concatenated first column entry of that chunk of data and the context, calls the pipeline to do speech recognition on that cached segment of data, and finally prints the transcript and flushes the I/O buffer.\r\n\r\nWe won\u2019t actually call that `infer` function just yet. We\u2019re just defining it. Next, we import PyTorch\u2019s multiprocessing capability to spawn subprocesses. We need to spawn the audio data capture as a subprocess for correct functionality. Otherwise, it will block I/O.\r\n\r\nThe first thing we do with the multiprocessing function is to signal we are spawning a new process. Second, we create a queue object in the context. Next, we create a process that runs the `stream` function from above in the context with the created queue, the passed in device, source, and model, the segment length, and the sample rate. Once we start that subprocess, we run our infer function until we end the subprocess.\r\n\r\nWhen the script actually runs (in the `if __name__ == \"__main__\"` section), we call the main function. For Mac\u2019s we pass the \"avfoundation\" for the device and the second entry for the source. The first entry in a Mac\u2019s setup is the video.\r\n\r\nFor Windows, see [the original notebook](https://pytorch.org/audio/main/tutorials/device_asr.html).\r\n\r\n```py\r\ndef main(device: str, src: str, bundle: torchaudio.pipelines):\r\n   '''Transcribed audio data from the mic\r\n  \r\n   Parameters:\r\n       device: Input device name\r\n       src: Source from input\r\n       bundle: TorchAudio pipeline\r\n  \r\n   Returns:\r\n       None'''\r\n   pipeline = InferencePipeline(bundle)\r\n  \r\n   sample_rate = bundle.sample_rate\r\n   segment_length = bundle.segment_length * bundle.hop_length\r\n   context_length = bundle.right_context_length * bundle.hop_length\r\n  \r\n   cacher = ContextCacher(segment_length, context_length)\r\n  \r\n   @torch.inference_mode()\r\n   def infer():\r\n       for _ in range(ITERATIONS):\r\n           chunk = q.get()\r\n           segment = cacher(chunk[:,0])\r\n           transcript = pipeline.infer(segment)\r\n           print(transcript, end=\"\", flush=True)\r\n  \r\n   ctx = mp.get_context(\"spawn\")\r\n   q = ctx.Queue()\r\n   p = ctx.Process(target=stream, args=(q, device, src, segment_length, sample_rate))\r\n   p.start()\r\n   infer()\r\n   p.join()\r\nif __name__ == \"__main__\":\r\n   main(\r\n       device=\"avfoundation\",\r\n       src=\":1\",\r\n       bundle=torchaudio.pipelines.EMFORMER_RNNT_BASE_LIBRISPEECH\r\n   )\r\n```\r\n\r\n## Full Code for Building a Local Streaming Audio Transcription Tool with PyTorch TorchAudio\r\n\r\nThat was a lot of code. Let\u2019s see how it looks all together in one file.\r\n\r\n```py\r\nimport torch\r\nimport torch.multiprocessing as mp\r\nimport torchaudio\r\nfrom torchaudio.io import StreamReader\r\n \r\nITERATIONS = 100\r\ndef stream(queue: mp.Queue(),\r\n   format: str,\r\n   src: str,\r\n   frames_per_chunk: int,\r\n   sample_rate: int):\r\n   '''Streams audio data\r\n  \r\n   Parameters:\r\n       queue: Queue of data chunks\r\n       format: Format\r\n       src: Source\r\n       frames_per_chunk: How many frames are in each data chunk\r\n       sample_rate: Sample rate\r\n \r\n   Returns:\r\n       None'''\r\n   print(\"Initializing Audio Stream\")\r\n   streamer = StreamReader(src, format=format)\r\n   streamer.add_basic_audio_stream(frames_per_chunk=frames_per_chunk,\r\n       sample_rate=sample_rate)\r\n   print(\"Streaming\\n\")\r\n   stream_iterator = streamer.stream(timeout=-1, backoff=1.0)\r\n   for _ in range(ITERATIONS):\r\n       (chunk,) = next(stream_iterator)\r\n       queue.put(chunk)\r\n \r\nclass InferencePipeline:\r\n   '''Creates an inference pipeline for streaming audio data'''\r\n   def __init__(self,\r\n       pipeline: torchaudio.pipelines.RNNTBundle,\r\n       beam_width: int=10):\r\n       '''Initializes TorchAudio RNNT Pipeline\r\n      \r\n       Parameters:\r\n           pipeline: TorchAudio Pipeline to use\r\n           beam_width: Beam width\r\n \r\n       Returns:\r\n           None'''\r\n \r\n       self.pipeline = pipeline\r\n       self.feature_extractor = pipeline.get_streaming_feature_extractor()\r\n       self.decoder = pipeline.get_decoder()\r\n       self.token_processor = pipeline.get_token_processor()\r\n       self.beam_width = beam_width\r\n       self.state = None\r\n       self.hypothesis = None\r\n  \r\n   def infer(self, segment: torch.Tensor) -> str:\r\n       '''Runs inference using the initialized pipeline\r\n      \r\n       Parameters:\r\n           segment: Torch tensor with features to extract\r\n      \r\n       Returns:\r\n           Transcript as string type'''\r\n \r\n       features, length = self.feature_extractor(segment)\r\n       predictions, self.state = self.decoder.infer(\r\n           features, length, self.beam_width, state=self.state,\r\n           hypothesis=self.hypothesis\r\n       )\r\n       self.hypothesis = predictions[0]\r\n       transcript = self.token_processor(self.hypothesis[0], lstrip=False)\r\n       return transcript\r\n \r\nclass ContextCacher:\r\n   def __init__(self, segment_length: int, context_length: int):\r\n       '''Creates initial context cache\r\n      \r\n       Parameters:\r\n           segment_length: length of one audio segment\r\n           context_length: length of the context\r\n \r\n       Returns:\r\n           None'''\r\n       self.segment_length = segment_length\r\n       self.context_length = context_length\r\n       self.context = torch.zeros([context_length])\r\n  \r\n   def __call__(self, chunk: torch.Tensor):\r\n       '''Adds chunk to context and returns it\r\n      \r\n       Parameters:\r\n           chunk: chunk of audio data to process\r\n      \r\n       Returns:\r\n           Tensor'''\r\n       if chunk.size(0) < self.segment_length:\r\n           chunk = torch.nn.functional.pad(chunk,\r\n               (0, self.segment_length - chunk.size(0)))\r\n       chunk_with_context = torch.cat((self.context, chunk))\r\n       self.context = chunk[-self.context_length :]\r\n       return chunk_with_context\r\n \r\ndef main(device: str, src: str, bundle: torchaudio.pipelines):\r\n   '''Transcribed audio data from the mic\r\n  \r\n   Parameters:\r\n       device: Input device name\r\n       src: Source from input\r\n       bundle: TorchAudio pipeline\r\n  \r\n   Returns:\r\n       None'''\r\n   pipeline = InferencePipeline(bundle)\r\n  \r\n   sample_rate = bundle.sample_rate\r\n   segment_length = bundle.segment_length * bundle.hop_length\r\n   context_length = bundle.right_context_length * bundle.hop_length\r\n  \r\n   cacher = ContextCacher(segment_length, context_length)\r\n  \r\n   @torch.inference_mode()\r\n   def infer():\r\n       for _ in range(ITERATIONS):\r\n           chunk = q.get()\r\n           segment = cacher(chunk[:,0])\r\n           transcript = pipeline.infer(segment)\r\n           print(transcript, end=\"\", flush=True)\r\n  \r\n   ctx = mp.get_context(\"spawn\")\r\n   q = ctx.Queue()\r\n   p = ctx.Process(target=stream, args=(q, device, src, segment_length, sample_rate))\r\n   p.start()\r\n   infer()\r\n   p.join()\r\nif __name__ == \"__main__\":\r\n   main(\r\n       device=\"avfoundation\",\r\n       src=\":1\",\r\n       bundle=torchaudio.pipelines.EMFORMER_RNNT_BASE_LIBRISPEECH\r\n   )\r\n```\r\n\r\n## Summary of Python Speech Recognition Locally with PyTorch\r\n\r\nSpeech recognition has come a long way from its first inception. We can now use Python to do speech recognition in many ways, including with the TorchAudio library from PyTorch. The TorchAudio library comes with many pre-built models we can use for automatic speech recognition as well as many audio data manipulation tools.\r\n\r\nIn this post, we covered how to run speech recognition locally with their Emformer RNN-T. First, we created an audio data streaming function. Next, we defined a pipeline object which can run speech recognition on tensors and turn them into text. Then, we created a cache to cache the audio data as it came in. We also created a main function to handle running the audio data stream as a subprocess and run inference on it while it ran. Finally, we ran that main function with varying parameters to let it know we are streaming data in from the mic and using an Emformer RNN-T model.\r\n\r\nWant to do speech recognition without all that code? [Sign up for Deepgram](https://console.deepgram.com) today and be up and running in just a few minutes.", "html": '<p>\u201CYour call may be recorded for quality assurance purposes.\u201D</p>\n<p>We\u2019ve all heard this when calling customer service. What are they doing with that call? How are they transcribing it? Back in the early 2000s and late 1990s, companies were using people to transcribe these. Now, <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> has come far along enough that we can use Python and machine learning to do automatic speech recognition.</p>\n<p>In this post, we\u2019ll focus on how to do speech recognition locally on your device using TorchAudio\u2019s pre-built Emformer RNN-T model. We will cover:</p>\n<ul>\n<li><a href="#understanding-pytorch-torchaudio">Understanding PyTorch TorchAudio</a></li>\n<li><a href="#setting-up-torchaudio-for-speech-recognition">Setting Up TorchAudio for Speech Recognition</a></li>\n<li><a href="#building-a-python-audio-data-streaming-function-for-speech-recognition">Building a Python Audio Data Streaming Function for Speech Recognition</a></li>\n<li><a href="#setting-up-python-speech-recognition-inference-pipeline">Setting up Python Speech Recognition Inference Pipeline</a></li>\n<li><a href="#creating-a-context-cache-to-store-audio-data-for-speech-recognition">Creating a Context Cache to Store Audio Data for Speech Recognition</a></li>\n<li><a href="#using-torchaudios-emformer-model-for-local-speech-recognition-in-python">Using TorchAudio\u2019s Emformer Model for Local Speech Recognition in Python</a></li>\n<li><a href="#in-summary">In Summary</a></li>\n</ul>\n<h2 id="understanding-pytorch-torchaudio">Understanding PyTorch TorchAudio</h2>\n<p>PyTorch is a versatile and powerful Python library for quickly developing machine learning models. Is it the open-source Python version of the Torch library (built on Lua) and primarily developed by Meta/Facebook. TorchAudio is an additional library to PyTorch that handles dealing with audio data for machine learning models.</p>\n<p>TorchAudio isn\u2019t just for creating machine learning models, you can also use it to do some audio data manipulation. We previously covered how to <a href="https://blog.deepgram.com/pytorch-intro-with-torchaudio/">use TorchAudio to manipulate audio data in Python</a>. In this piece, we\u2019re going to use it to build an inference pipeline to do speech recognition in real time.</p>\n<h2 id="setting-up-torchaudio-for-speech-recognition">Setting Up TorchAudio for Speech Recognition</h2>\n<p>Before we can dive into the program for doing speech recognition with TorchAudio, we need to set up our system. We need to install the <code is:raw>pytorch</code>, <code is:raw>torchaudio</code>, <code is:raw>sentencepiece</code>, and <code is:raw>ffmpeg-python</code> libraries. We can install all of these with the command <code is:raw>pip install pytorch torchaudio sentencepiece ffmpeg-python</code>. If you encounter errors, you may need to upgrade <code is:raw>pip</code>, which you can do using <code is:raw>pip install -U pip</code>.</p>\n<p>If you already have PyTorch and TorchAudio installed and you encounter an error importing <code is:raw>torchaudio</code> as I did. To get around this, force update both libraries with <code is:raw>pip install -U torch torchaudio --no-cache-dir</code>. Next, we\u2019re going to build our script. It\u2019s going to consist of an audio data function, the machine learning model pipeline, a context cache, and a main function to run.</p>\n<h2 id="building-a-python-audio-data-streaming-function-for-speech-recognition">Building a Python Audio Data Streaming Function for Speech Recognition</h2>\n<p>Now that our system is set up to work with PyTorch and Torchaudio for speech recognition, let\u2019s build our script. The first thing we need is an audio data streaming function, we\u2019ll call ours <code is:raw>stream</code>. We also declare a constant number of iterations for the streaming function to stream.</p>\n<p>Our <code is:raw>stream</code> function needs five parameters. It needs a queue, a format or device (different for Mac vs Windows), a source, a segment length, and a sample rate. There are some print statements in our function that I\u2019ve commented out. You can uncomment these to show more information about the program as it runs.</p>\n<p>The first thing we\u2019re going to do in our <code is:raw>stream</code> function is create a <code is:raw>StreamReader</code> instance from <code is:raw>torchaudio.io</code>. Next, we\u2019ll populate that stream reader with an audio stream of the passed in segment length and sample rate. I would leave the <code is:raw>"Streaming"</code> and blank <code is:raw>print</code> statements uncommented so you know when you can start talking into the mic.</p>\n<p>Next, we\u2019ll create an iterator to iterate across our stream. Finally, we\u2019ll loop through the number of predefined iterations, in our case 100, and get the next chunk of audio data from the stream iterator and put it in the queue.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> torch</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> torch.multiprocessing </span><span style="color: #FF7B72">as</span><span style="color: #C9D1D9"> mp</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> torchaudio</span></span>\n<span class="line"><span style="color: #FF7B72">from</span><span style="color: #C9D1D9"> torchaudio.io </span><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> StreamReader</span></span>\n<span class="line"><span style="color: #C9D1D9"> </span></span>\n<span class="line"><span style="color: #79C0FF">ITERATIONS</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">100</span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">stream</span><span style="color: #C9D1D9">(queue: mp.Queue(),</span></span>\n<span class="line"><span style="color: #C9D1D9">   format: </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">   src: </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">   frames_per_chunk: </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">   sample_rate: </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #A5D6FF">&#39;&#39;&#39;Streams audio data</span></span>\n<span class="line"><span style="color: #A5D6FF">  </span></span>\n<span class="line"><span style="color: #A5D6FF">   Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">       queue: Queue of data chunks</span></span>\n<span class="line"><span style="color: #A5D6FF">       format: Format</span></span>\n<span class="line"><span style="color: #A5D6FF">       src: Source</span></span>\n<span class="line"><span style="color: #A5D6FF">       frames_per_chunk: How many frames are in each data chunk</span></span>\n<span class="line"><span style="color: #A5D6FF">       sample_rate: Sample rate</span></span>\n<span class="line"><span style="color: #A5D6FF"> </span></span>\n<span class="line"><span style="color: #A5D6FF">   Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">       None&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(</span><span style="color: #A5D6FF">&quot;Initializing Audio Stream&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   streamer </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> StreamReader(src, </span><span style="color: #FFA657">format</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">format</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   streamer.add_basic_audio_stream(</span><span style="color: #FFA657">frames_per_chunk</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">frames_per_chunk,</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FFA657">sample_rate</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">sample_rate)</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(</span><span style="color: #A5D6FF">&quot;Streaming</span><span style="color: #79C0FF">\\n</span><span style="color: #A5D6FF">&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   stream_iterator </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> streamer.stream(</span><span style="color: #FFA657">timeout</span><span style="color: #FF7B72">=-</span><span style="color: #79C0FF">1</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">backoff</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">1.0</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> _ </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">range</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">ITERATIONS</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">       (chunk,) </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">next</span><span style="color: #C9D1D9">(stream_iterator)</span></span>\n<span class="line"><span style="color: #C9D1D9">       queue.put(chunk)</span></span></code></pre>\n<h2 id="setting-up-python-speech-recognition-inference-pipeline">Setting up Python Speech Recognition Inference Pipeline</h2>\n<p>Now that we have a way to smoothly stream the audio data from a microphone, we can run predictions on it. Let\u2019s build a pipeline to do speech recognition with. Unlike the part above, this time we will be creating a <code is:raw>Class</code> and not a function.</p>\n<p>We require our class to be initialized with one variable, an RNN-T model from TorchAudio\u2019s Pipelines library. We also allow a second, optional variable to define beam width, which is set at 10 by default. Our <code is:raw>__init__</code> function sets the instance\u2019s <code is:raw>bundle</code> value to the model we passed in. Then, it uses that model to get the feature extractor, decoder, and token processor. Next, it sets the beam width, the state of the pipeline, and the predictions.</p>\n<p>The other function that our <code is:raw>Pipeline</code> object has is an inference function. This function takes a PyTorch <code is:raw>Tensor</code> object and returns a string, the predicted text. This function uses the feature extractor to get the features and the length of the input tensor. Next, call the inference function from the decoder to get the hypothesis and set the state of the Pipeline. We set the instance\u2019s <code is:raw>hypothesis</code> attribute to the value in the first index of the returned hypothesis. Finally, we use the token processor on the first entry in the instance\u2019s hypothesis attribute and return that value.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">class</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">InferencePipeline</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #A5D6FF">&#39;&#39;&#39;Creates an inference pipeline for streaming audio data&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__init__</span><span style="color: #C9D1D9">(self,</span></span>\n<span class="line"><span style="color: #C9D1D9">       pipeline: torchaudio.pipelines.RNNTBundle,</span></span>\n<span class="line"><span style="color: #C9D1D9">       beam_width: </span><span style="color: #79C0FF">int</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">10</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #A5D6FF">&#39;&#39;&#39;Initializes TorchAudio RNNT Pipeline</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">           pipeline: TorchAudio Pipeline to use</span></span>\n<span class="line"><span style="color: #A5D6FF">           beam_width: Beam width</span></span>\n<span class="line"><span style="color: #A5D6FF"> </span></span>\n<span class="line"><span style="color: #A5D6FF">       Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">           None&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9"> </span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.pipeline </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.feature_extractor </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline.get_streaming_feature_extractor()</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.decoder </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline.get_decoder()</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.token_processor </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline.get_token_processor()</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.beam_width </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> beam_width</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.state </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">None</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.hypothesis </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">None</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">infer</span><span style="color: #C9D1D9">(self, segment: torch.Tensor) -&gt; </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #A5D6FF">&#39;&#39;&#39;Runs inference using the initialized pipeline</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">           segment: Torch tensor with features to extract</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">           Transcript as string type&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9"> </span></span>\n<span class="line"><span style="color: #C9D1D9">       features, length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.feature_extractor(segment)</span></span>\n<span class="line"><span style="color: #C9D1D9">       predictions, </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.state </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.decoder.infer(</span></span>\n<span class="line"><span style="color: #C9D1D9">           features, length, </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.beam_width, </span><span style="color: #FFA657">state</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.state,</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #FFA657">hypothesis</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.hypothesis</span></span>\n<span class="line"><span style="color: #C9D1D9">       )</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.hypothesis </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> predictions[</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">]</span></span>\n<span class="line"><span style="color: #C9D1D9">       transcript </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.token_processor(</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.hypothesis[</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">], </span><span style="color: #FFA657">lstrip</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">False</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> transcript</span></span></code></pre>\n<h2 id="creating-a-context-cache-to-store-audio-data-for-speech-recognition">Creating a Context Cache to Store Audio Data for Speech Recognition</h2>\n<p>We need a context cache to batch store the audio data frames as we read them. The context cache requires two parameters to initialize. First, a segment length, then a context length. The initialization function sets the instance\u2019s attributes to the passed in values and initializes a tensor of zeros of the context length as the instance\u2019s current context.</p>\n<p>The second function we\u2019ll define is the <code is:raw>call</code> function. This is another automatic function that we\u2019re overwriting. Whenever you see a class function preceded and followed by two underscores, that\u2019s a default function. In this case, we require the call function to intake a PyTorch tensor object.</p>\n<p>If the size of the first object in the tensor is less than the set segment length for the cache, we\u2019ll pad that chunk with 0s. Next, we use the concatenate function from <code is:raw>torch</code> to add that chunk to the current context. Then, we set the instance\u2019s context attribute to the last entries in the chunk equivalent to the context length. Finally, we return the concatenate context.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">class</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">ContextCacher</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__init__</span><span style="color: #C9D1D9">(self, segment_length: </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">, context_length: </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #A5D6FF">&#39;&#39;&#39;Creates initial context cache</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">           segment_length: length of one audio segment</span></span>\n<span class="line"><span style="color: #A5D6FF">           context_length: length of the context</span></span>\n<span class="line"><span style="color: #A5D6FF"> </span></span>\n<span class="line"><span style="color: #A5D6FF">       Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">           None&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.segment_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> segment_length</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> context_length</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> torch.zeros([context_length])</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__call__</span><span style="color: #C9D1D9">(self, chunk: torch.Tensor):</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #A5D6FF">&#39;&#39;&#39;Adds chunk to context and returns it</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">           chunk: chunk of audio data to process</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">           Tensor&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> chunk.size(</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">) </span><span style="color: #FF7B72">&lt;</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.segment_length:</span></span>\n<span class="line"><span style="color: #C9D1D9">           chunk </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> torch.nn.functional.pad(chunk,</span></span>\n<span class="line"><span style="color: #C9D1D9">               (</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.segment_length </span><span style="color: #FF7B72">-</span><span style="color: #C9D1D9"> chunk.size(</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">)))</span></span>\n<span class="line"><span style="color: #C9D1D9">       chunk_with_context </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> torch.cat((</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context, chunk))</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> chunk[</span><span style="color: #FF7B72">-</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context_length :]</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> chunk_with_context</span></span></code></pre>\n<h2 id="using-torchaudios-emformer-model-for-local-speech-recognition-in-python">Using TorchAudio\u2019s Emformer Model for Local Speech Recognition in Python</h2>\n<p>Finally, all our helper functions have been made. It is now time to create the main function to orchestrate the audio data streamer, the machine learning pipeline, and the cache. Our main function will take three inputs: the input device, the source, and the RNN-T model. As before, I\u2019ve left some print statements in here commented out, uncomment them if you\u2019d like.</p>\n<p>First, we create an instance of our Pipeline. This is what turns the audio data into text. Next, we use the RNN-T model to set the sample rate, the segment length, and the context length. Then, we instantiate the context with the newly gotten segment and context lengths.</p>\n<p>Now things get funky. We use an annotation on the <code is:raw>infer</code> function we\u2019re creating that signals to <code is:raw>torch</code> to turn on inference mode while this function runs. The infer function does not need any parameters. It loops through the preset number of iterations. For each iteration, it gets a chunk of data from the queue, calls the cache to return the concatenated first column entry of that chunk of data and the context, calls the pipeline to do speech recognition on that cached segment of data, and finally prints the transcript and flushes the I/O buffer.</p>\n<p>We won\u2019t actually call that <code is:raw>infer</code> function just yet. We\u2019re just defining it. Next, we import PyTorch\u2019s multiprocessing capability to spawn subprocesses. We need to spawn the audio data capture as a subprocess for correct functionality. Otherwise, it will block I/O.</p>\n<p>The first thing we do with the multiprocessing function is to signal we are spawning a new process. Second, we create a queue object in the context. Next, we create a process that runs the <code is:raw>stream</code> function from above in the context with the created queue, the passed in device, source, and model, the segment length, and the sample rate. Once we start that subprocess, we run our infer function until we end the subprocess.</p>\n<p>When the script actually runs (in the <code is:raw>if __name__ == "__main__"</code> section), we call the main function. For Mac\u2019s we pass the \u201Cavfoundation\u201D for the device and the second entry for the source. The first entry in a Mac\u2019s setup is the video.</p>\n<p>For Windows, see <a href="https://pytorch.org/audio/main/tutorials/device_asr.html">the original notebook</a>.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">main</span><span style="color: #C9D1D9">(device: </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">, src: </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">, bundle: torchaudio.pipelines):</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #A5D6FF">&#39;&#39;&#39;Transcribed audio data from the mic</span></span>\n<span class="line"><span style="color: #A5D6FF">  </span></span>\n<span class="line"><span style="color: #A5D6FF">   Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">       device: Input device name</span></span>\n<span class="line"><span style="color: #A5D6FF">       src: Source from input</span></span>\n<span class="line"><span style="color: #A5D6FF">       bundle: TorchAudio pipeline</span></span>\n<span class="line"><span style="color: #A5D6FF">  </span></span>\n<span class="line"><span style="color: #A5D6FF">   Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">       None&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9">   pipeline </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> InferencePipeline(bundle)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   sample_rate </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> bundle.sample_rate</span></span>\n<span class="line"><span style="color: #C9D1D9">   segment_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> bundle.segment_length </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> bundle.hop_length</span></span>\n<span class="line"><span style="color: #C9D1D9">   context_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> bundle.right_context_length </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> bundle.hop_length</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   cacher </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> ContextCacher(segment_length, context_length)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #D2A8FF">@torch.inference_mode</span><span style="color: #C9D1D9">()</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">infer</span><span style="color: #C9D1D9">():</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> _ </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">range</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">ITERATIONS</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">           chunk </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> q.get()</span></span>\n<span class="line"><span style="color: #C9D1D9">           segment </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> cacher(chunk[:,</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">])</span></span>\n<span class="line"><span style="color: #C9D1D9">           transcript </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline.infer(segment)</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcript, </span><span style="color: #FFA657">end</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">flush</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">True</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   ctx </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> mp.get_context(</span><span style="color: #A5D6FF">&quot;spawn&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   q </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> ctx.Queue()</span></span>\n<span class="line"><span style="color: #C9D1D9">   p </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> ctx.Process(</span><span style="color: #FFA657">target</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">stream, </span><span style="color: #FFA657">args</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">(q, device, src, segment_length, sample_rate))</span></span>\n<span class="line"><span style="color: #C9D1D9">   p.start()</span></span>\n<span class="line"><span style="color: #C9D1D9">   infer()</span></span>\n<span class="line"><span style="color: #C9D1D9">   p.join()</span></span>\n<span class="line"><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__name__</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">==</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;__main__&quot;</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">   main(</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FFA657">device</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;avfoundation&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FFA657">src</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;:1&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FFA657">bundle</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">torchaudio.pipelines.</span><span style="color: #79C0FF">EMFORMER_RNNT_BASE_LIBRISPEECH</span></span>\n<span class="line"><span style="color: #C9D1D9">   )</span></span></code></pre>\n<h2 id="full-code-for-building-a-local-streaming-audio-transcription-tool-with-pytorch-torchaudio">Full Code for Building a Local Streaming Audio Transcription Tool with PyTorch TorchAudio</h2>\n<p>That was a lot of code. Let\u2019s see how it looks all together in one file.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> torch</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> torch.multiprocessing </span><span style="color: #FF7B72">as</span><span style="color: #C9D1D9"> mp</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> torchaudio</span></span>\n<span class="line"><span style="color: #FF7B72">from</span><span style="color: #C9D1D9"> torchaudio.io </span><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> StreamReader</span></span>\n<span class="line"><span style="color: #C9D1D9"> </span></span>\n<span class="line"><span style="color: #79C0FF">ITERATIONS</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">100</span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">stream</span><span style="color: #C9D1D9">(queue: mp.Queue(),</span></span>\n<span class="line"><span style="color: #C9D1D9">   format: </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">   src: </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">   frames_per_chunk: </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">   sample_rate: </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #A5D6FF">&#39;&#39;&#39;Streams audio data</span></span>\n<span class="line"><span style="color: #A5D6FF">  </span></span>\n<span class="line"><span style="color: #A5D6FF">   Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">       queue: Queue of data chunks</span></span>\n<span class="line"><span style="color: #A5D6FF">       format: Format</span></span>\n<span class="line"><span style="color: #A5D6FF">       src: Source</span></span>\n<span class="line"><span style="color: #A5D6FF">       frames_per_chunk: How many frames are in each data chunk</span></span>\n<span class="line"><span style="color: #A5D6FF">       sample_rate: Sample rate</span></span>\n<span class="line"><span style="color: #A5D6FF"> </span></span>\n<span class="line"><span style="color: #A5D6FF">   Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">       None&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(</span><span style="color: #A5D6FF">&quot;Initializing Audio Stream&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   streamer </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> StreamReader(src, </span><span style="color: #FFA657">format</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">format</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   streamer.add_basic_audio_stream(</span><span style="color: #FFA657">frames_per_chunk</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">frames_per_chunk,</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FFA657">sample_rate</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">sample_rate)</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(</span><span style="color: #A5D6FF">&quot;Streaming</span><span style="color: #79C0FF">\\n</span><span style="color: #A5D6FF">&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   stream_iterator </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> streamer.stream(</span><span style="color: #FFA657">timeout</span><span style="color: #FF7B72">=-</span><span style="color: #79C0FF">1</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">backoff</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">1.0</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> _ </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">range</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">ITERATIONS</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">       (chunk,) </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">next</span><span style="color: #C9D1D9">(stream_iterator)</span></span>\n<span class="line"><span style="color: #C9D1D9">       queue.put(chunk)</span></span>\n<span class="line"><span style="color: #C9D1D9"> </span></span>\n<span class="line"><span style="color: #FF7B72">class</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">InferencePipeline</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #A5D6FF">&#39;&#39;&#39;Creates an inference pipeline for streaming audio data&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__init__</span><span style="color: #C9D1D9">(self,</span></span>\n<span class="line"><span style="color: #C9D1D9">       pipeline: torchaudio.pipelines.RNNTBundle,</span></span>\n<span class="line"><span style="color: #C9D1D9">       beam_width: </span><span style="color: #79C0FF">int</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">10</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #A5D6FF">&#39;&#39;&#39;Initializes TorchAudio RNNT Pipeline</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">           pipeline: TorchAudio Pipeline to use</span></span>\n<span class="line"><span style="color: #A5D6FF">           beam_width: Beam width</span></span>\n<span class="line"><span style="color: #A5D6FF"> </span></span>\n<span class="line"><span style="color: #A5D6FF">       Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">           None&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9"> </span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.pipeline </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.feature_extractor </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline.get_streaming_feature_extractor()</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.decoder </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline.get_decoder()</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.token_processor </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline.get_token_processor()</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.beam_width </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> beam_width</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.state </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">None</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.hypothesis </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">None</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">infer</span><span style="color: #C9D1D9">(self, segment: torch.Tensor) -&gt; </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #A5D6FF">&#39;&#39;&#39;Runs inference using the initialized pipeline</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">           segment: Torch tensor with features to extract</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">           Transcript as string type&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9"> </span></span>\n<span class="line"><span style="color: #C9D1D9">       features, length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.feature_extractor(segment)</span></span>\n<span class="line"><span style="color: #C9D1D9">       predictions, </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.state </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.decoder.infer(</span></span>\n<span class="line"><span style="color: #C9D1D9">           features, length, </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.beam_width, </span><span style="color: #FFA657">state</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.state,</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #FFA657">hypothesis</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.hypothesis</span></span>\n<span class="line"><span style="color: #C9D1D9">       )</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.hypothesis </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> predictions[</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">]</span></span>\n<span class="line"><span style="color: #C9D1D9">       transcript </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.token_processor(</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.hypothesis[</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">], </span><span style="color: #FFA657">lstrip</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">False</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> transcript</span></span>\n<span class="line"><span style="color: #C9D1D9"> </span></span>\n<span class="line"><span style="color: #FF7B72">class</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">ContextCacher</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__init__</span><span style="color: #C9D1D9">(self, segment_length: </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">, context_length: </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #A5D6FF">&#39;&#39;&#39;Creates initial context cache</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">           segment_length: length of one audio segment</span></span>\n<span class="line"><span style="color: #A5D6FF">           context_length: length of the context</span></span>\n<span class="line"><span style="color: #A5D6FF"> </span></span>\n<span class="line"><span style="color: #A5D6FF">       Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">           None&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.segment_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> segment_length</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> context_length</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> torch.zeros([context_length])</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__call__</span><span style="color: #C9D1D9">(self, chunk: torch.Tensor):</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #A5D6FF">&#39;&#39;&#39;Adds chunk to context and returns it</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">           chunk: chunk of audio data to process</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">           Tensor&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> chunk.size(</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">) </span><span style="color: #FF7B72">&lt;</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.segment_length:</span></span>\n<span class="line"><span style="color: #C9D1D9">           chunk </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> torch.nn.functional.pad(chunk,</span></span>\n<span class="line"><span style="color: #C9D1D9">               (</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.segment_length </span><span style="color: #FF7B72">-</span><span style="color: #C9D1D9"> chunk.size(</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">)))</span></span>\n<span class="line"><span style="color: #C9D1D9">       chunk_with_context </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> torch.cat((</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context, chunk))</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> chunk[</span><span style="color: #FF7B72">-</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context_length :]</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> chunk_with_context</span></span>\n<span class="line"><span style="color: #C9D1D9"> </span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">main</span><span style="color: #C9D1D9">(device: </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">, src: </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">, bundle: torchaudio.pipelines):</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #A5D6FF">&#39;&#39;&#39;Transcribed audio data from the mic</span></span>\n<span class="line"><span style="color: #A5D6FF">  </span></span>\n<span class="line"><span style="color: #A5D6FF">   Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">       device: Input device name</span></span>\n<span class="line"><span style="color: #A5D6FF">       src: Source from input</span></span>\n<span class="line"><span style="color: #A5D6FF">       bundle: TorchAudio pipeline</span></span>\n<span class="line"><span style="color: #A5D6FF">  </span></span>\n<span class="line"><span style="color: #A5D6FF">   Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">       None&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9">   pipeline </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> InferencePipeline(bundle)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   sample_rate </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> bundle.sample_rate</span></span>\n<span class="line"><span style="color: #C9D1D9">   segment_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> bundle.segment_length </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> bundle.hop_length</span></span>\n<span class="line"><span style="color: #C9D1D9">   context_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> bundle.right_context_length </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> bundle.hop_length</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   cacher </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> ContextCacher(segment_length, context_length)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #D2A8FF">@torch.inference_mode</span><span style="color: #C9D1D9">()</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">infer</span><span style="color: #C9D1D9">():</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> _ </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">range</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">ITERATIONS</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">           chunk </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> q.get()</span></span>\n<span class="line"><span style="color: #C9D1D9">           segment </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> cacher(chunk[:,</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">])</span></span>\n<span class="line"><span style="color: #C9D1D9">           transcript </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline.infer(segment)</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcript, </span><span style="color: #FFA657">end</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">flush</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">True</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   ctx </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> mp.get_context(</span><span style="color: #A5D6FF">&quot;spawn&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   q </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> ctx.Queue()</span></span>\n<span class="line"><span style="color: #C9D1D9">   p </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> ctx.Process(</span><span style="color: #FFA657">target</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">stream, </span><span style="color: #FFA657">args</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">(q, device, src, segment_length, sample_rate))</span></span>\n<span class="line"><span style="color: #C9D1D9">   p.start()</span></span>\n<span class="line"><span style="color: #C9D1D9">   infer()</span></span>\n<span class="line"><span style="color: #C9D1D9">   p.join()</span></span>\n<span class="line"><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__name__</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">==</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;__main__&quot;</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">   main(</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FFA657">device</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;avfoundation&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FFA657">src</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;:1&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FFA657">bundle</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">torchaudio.pipelines.</span><span style="color: #79C0FF">EMFORMER_RNNT_BASE_LIBRISPEECH</span></span>\n<span class="line"><span style="color: #C9D1D9">   )</span></span></code></pre>\n<h2 id="summary-of-python-speech-recognition-locally-with-pytorch">Summary of Python Speech Recognition Locally with PyTorch</h2>\n<p>Speech recognition has come a long way from its first inception. We can now use Python to do speech recognition in many ways, including with the TorchAudio library from PyTorch. The TorchAudio library comes with many pre-built models we can use for automatic speech recognition as well as many audio data manipulation tools.</p>\n<p>In this post, we covered how to run speech recognition locally with their Emformer RNN-T. First, we created an audio data streaming function. Next, we defined a pipeline object which can run speech recognition on tensors and turn them into text. Then, we created a cache to cache the audio data as it came in. We also created a main function to handle running the audio data stream as a subprocess and run inference on it while it ran. Finally, we ran that main function with varying parameters to let it know we are streaming data in from the mic and using an Emformer RNN-T model.</p>\n<p>Want to do speech recognition without all that code? <a href="https://console.deepgram.com">Sign up for Deepgram</a> today and be up and running in just a few minutes.</p>' };
const frontmatter = { "title": "Python Speech Recognition Locally with TorchAudio", "description": "Learn how to use the Python TorchAudio library and its Emformer Model for local speech recognition", "date": "2022-07-14T00:00:00.000Z", "cover": "https://res.cloudinary.com/deepgram/image/upload/v1657708926/blog/2022/07/python-speech-recognition-locally-torchaudio/cov.jpg", "authors": ["yujian-tang"], "category": "tutorial", "tags": ["python"], "seo": { "title": "Python Speech Recognition Locally with TorchAudio", "description": "Learn how to use the Python TorchAudio library and its Emformer Model for local speech recognition" }, "og": { "image": "https://res.cloudinary.com/deepgram/image/upload/v1661454110/blog/python-speech-recognition-locally-torchaudio/ograph.png" }, "shorturls": { "share": "https://dpgr.am/6f6eb4e", "twitter": "https://dpgr.am/7fdfd20", "linkedin": "https://dpgr.am/d2e2258", "reddit": "https://dpgr.am/1385e37", "facebook": "https://dpgr.am/b02940e" }, "astro": { "headings": [{ "depth": 2, "slug": "understanding-pytorch-torchaudio", "text": "Understanding PyTorch TorchAudio" }, { "depth": 2, "slug": "setting-up-torchaudio-for-speech-recognition", "text": "Setting Up TorchAudio for Speech Recognition" }, { "depth": 2, "slug": "building-a-python-audio-data-streaming-function-for-speech-recognition", "text": "Building a Python Audio Data Streaming Function for Speech Recognition" }, { "depth": 2, "slug": "setting-up-python-speech-recognition-inference-pipeline", "text": "Setting up Python Speech Recognition Inference Pipeline" }, { "depth": 2, "slug": "creating-a-context-cache-to-store-audio-data-for-speech-recognition", "text": "Creating a Context Cache to Store Audio Data for Speech Recognition" }, { "depth": 2, "slug": "using-torchaudios-emformer-model-for-local-speech-recognition-in-python", "text": "Using TorchAudio\u2019s Emformer Model for Local Speech Recognition in Python" }, { "depth": 2, "slug": "full-code-for-building-a-local-streaming-audio-transcription-tool-with-pytorch-torchaudio", "text": "Full Code for Building a Local Streaming Audio Transcription Tool with PyTorch TorchAudio" }, { "depth": 2, "slug": "summary-of-python-speech-recognition-locally-with-pytorch", "text": "Summary of Python Speech Recognition Locally with PyTorch" }], "source": "\"Your call may be recorded for quality assurance purposes.\"\r\n\r\nWe\u2019ve all heard this when calling customer service. What are they doing with that call? How are they transcribing it? Back in the early 2000s and late 1990s, companies were using people to transcribe these. Now, [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing) has come far along enough that we can use Python and machine learning to do automatic speech recognition.\r\n\r\nIn this post, we\u2019ll focus on how to do speech recognition locally on your device using TorchAudio\u2019s pre-built Emformer RNN-T model. We will cover:\r\n\r\n* [Understanding PyTorch TorchAudio](#understanding-pytorch-torchaudio)\r\n* [Setting Up TorchAudio for Speech Recognition](#setting-up-torchaudio-for-speech-recognition)\r\n* [Building a Python Audio Data Streaming Function for Speech Recognition](#building-a-python-audio-data-streaming-function-for-speech-recognition)\r\n* [Setting up Python Speech Recognition Inference Pipeline](#setting-up-python-speech-recognition-inference-pipeline)\r\n* [Creating a Context Cache to Store Audio Data for Speech Recognition](#creating-a-context-cache-to-store-audio-data-for-speech-recognition)\r\n* [Using TorchAudio\u2019s Emformer Model for Local Speech Recognition in Python](#using-torchaudios-emformer-model-for-local-speech-recognition-in-python)\r\n* [In Summary](#in-summary)\r\n\r\n## Understanding PyTorch TorchAudio\r\n\r\nPyTorch is a versatile and powerful Python library for quickly developing machine learning models. Is it the open-source Python version of the Torch library (built on Lua) and primarily developed by Meta/Facebook. TorchAudio is an additional library to PyTorch that handles dealing with audio data for machine learning models.\r\n\r\nTorchAudio isn\u2019t just for creating machine learning models, you can also use it to do some audio data manipulation. We previously covered how to [use TorchAudio to manipulate audio data in Python](https://blog.deepgram.com/pytorch-intro-with-torchaudio/). In this piece, we\u2019re going to use it to build an inference pipeline to do speech recognition in real time.\r\n\r\n## Setting Up TorchAudio for Speech Recognition\r\n\r\nBefore we can dive into the program for doing speech recognition with TorchAudio, we need to set up our system. We need to install the `pytorch`, `torchaudio`, `sentencepiece`, and `ffmpeg-python` libraries. We can install all of these with the command `pip install pytorch torchaudio sentencepiece ffmpeg-python`. If you encounter errors, you may need to upgrade `pip`, which you can do using `pip install -U pip`.\r\n\r\nIf you already have PyTorch and TorchAudio installed and you encounter an error importing `torchaudio` as I did. To get around this, force update both libraries with `pip install -U torch torchaudio --no-cache-dir`. Next, we\u2019re going to build our script. It\u2019s going to consist of an audio data function, the machine learning model pipeline, a context cache, and a main function to run.\r\n\r\n## Building a Python Audio Data Streaming Function for Speech Recognition\r\n\r\nNow that our system is set up to work with PyTorch and Torchaudio for speech recognition, let\u2019s build our script. The first thing we need is an audio data streaming function, we\u2019ll call ours `stream`. We also declare a constant number of iterations for the streaming function to stream.\r\n\r\nOur `stream` function needs five parameters. It needs a queue, a format or device (different for Mac vs Windows), a source, a segment length, and a sample rate. There are some print statements in our function that I\u2019ve commented out. You can uncomment these to show more information about the program as it runs.\r\n\r\nThe first thing we\u2019re going to do in our `stream` function is create a `StreamReader` instance from `torchaudio.io`. Next, we\u2019ll populate that stream reader with an audio stream of the passed in segment length and sample rate. I would leave the `\"Streaming\"` and blank `print` statements uncommented so you know when you can start talking into the mic.\r\n\r\nNext, we\u2019ll create an iterator to iterate across our stream. Finally, we\u2019ll loop through the number of predefined iterations, in our case 100, and get the next chunk of audio data from the stream iterator and put it in the queue.\r\n\r\n```py\r\nimport torch\r\nimport torch.multiprocessing as mp\r\nimport torchaudio\r\nfrom torchaudio.io import StreamReader\r\n \r\nITERATIONS = 100\r\ndef stream(queue: mp.Queue(),\r\n   format: str,\r\n   src: str,\r\n   frames_per_chunk: int,\r\n   sample_rate: int):\r\n   '''Streams audio data\r\n  \r\n   Parameters:\r\n       queue: Queue of data chunks\r\n       format: Format\r\n       src: Source\r\n       frames_per_chunk: How many frames are in each data chunk\r\n       sample_rate: Sample rate\r\n \r\n   Returns:\r\n       None'''\r\n   print(\"Initializing Audio Stream\")\r\n   streamer = StreamReader(src, format=format)\r\n   streamer.add_basic_audio_stream(frames_per_chunk=frames_per_chunk,\r\n       sample_rate=sample_rate)\r\n   print(\"Streaming\\n\")\r\n   stream_iterator = streamer.stream(timeout=-1, backoff=1.0)\r\n   for _ in range(ITERATIONS):\r\n       (chunk,) = next(stream_iterator)\r\n       queue.put(chunk)\r\n```\r\n\r\n## Setting up Python Speech Recognition Inference Pipeline\r\n\r\nNow that we have a way to smoothly stream the audio data from a microphone, we can run predictions on it. Let\u2019s build a pipeline to do speech recognition with. Unlike the part above, this time we will be creating a `Class` and not a function.\r\n\r\nWe require our class to be initialized with one variable, an RNN-T model from TorchAudio\u2019s Pipelines library. We also allow a second, optional variable to define beam width, which is set at 10 by default. Our `__init__` function sets the instance\u2019s `bundle` value to the model we passed in. Then, it uses that model to get the feature extractor, decoder, and token processor. Next, it sets the beam width, the state of the pipeline, and the predictions.\r\n\r\nThe other function that our `Pipeline` object has is an inference function. This function takes a PyTorch `Tensor` object and returns a string, the predicted text. This function uses the feature extractor to get the features and the length of the input tensor. Next, call the inference function from the decoder to get the hypothesis and set the state of the Pipeline. We set the instance\u2019s `hypothesis` attribute to the value in the first index of the returned hypothesis. Finally, we use the token processor on the first entry in the instance\u2019s hypothesis attribute and return that value.\r\n\r\n```py\r\nclass InferencePipeline:\r\n   '''Creates an inference pipeline for streaming audio data'''\r\n   def __init__(self,\r\n       pipeline: torchaudio.pipelines.RNNTBundle,\r\n       beam_width: int=10):\r\n       '''Initializes TorchAudio RNNT Pipeline\r\n      \r\n       Parameters:\r\n           pipeline: TorchAudio Pipeline to use\r\n           beam_width: Beam width\r\n \r\n       Returns:\r\n           None'''\r\n \r\n       self.pipeline = pipeline\r\n       self.feature_extractor = pipeline.get_streaming_feature_extractor()\r\n       self.decoder = pipeline.get_decoder()\r\n       self.token_processor = pipeline.get_token_processor()\r\n       self.beam_width = beam_width\r\n       self.state = None\r\n       self.hypothesis = None\r\n  \r\n   def infer(self, segment: torch.Tensor) -> str:\r\n       '''Runs inference using the initialized pipeline\r\n      \r\n       Parameters:\r\n           segment: Torch tensor with features to extract\r\n      \r\n       Returns:\r\n           Transcript as string type'''\r\n \r\n       features, length = self.feature_extractor(segment)\r\n       predictions, self.state = self.decoder.infer(\r\n           features, length, self.beam_width, state=self.state,\r\n           hypothesis=self.hypothesis\r\n       )\r\n       self.hypothesis = predictions[0]\r\n       transcript = self.token_processor(self.hypothesis[0], lstrip=False)\r\n       return transcript\r\n```\r\n\r\n## Creating a Context Cache to Store Audio Data for Speech Recognition\r\n\r\nWe need a context cache to batch store the audio data frames as we read them. The context cache requires two parameters to initialize. First, a segment length, then a context length. The initialization function sets the instance\u2019s attributes to the passed in values and initializes a tensor of zeros of the context length as the instance\u2019s current context.\r\n\r\nThe second function we\u2019ll define is the `call` function. This is another automatic function that we\u2019re overwriting. Whenever you see a class function preceded and followed by two underscores, that\u2019s a default function. In this case, we require the call function to intake a PyTorch tensor object.\r\n\r\nIf the size of the first object in the tensor is less than the set segment length for the cache, we\u2019ll pad that chunk with 0s. Next, we use the concatenate function from `torch` to add that chunk to the current context. Then, we set the instance\u2019s context attribute to the last entries in the chunk equivalent to the context length. Finally, we return the concatenate context.\r\n\r\n```py\r\nclass ContextCacher:\r\n   def __init__(self, segment_length: int, context_length: int):\r\n       '''Creates initial context cache\r\n      \r\n       Parameters:\r\n           segment_length: length of one audio segment\r\n           context_length: length of the context\r\n \r\n       Returns:\r\n           None'''\r\n       self.segment_length = segment_length\r\n       self.context_length = context_length\r\n       self.context = torch.zeros([context_length])\r\n  \r\n   def __call__(self, chunk: torch.Tensor):\r\n       '''Adds chunk to context and returns it\r\n      \r\n       Parameters:\r\n           chunk: chunk of audio data to process\r\n      \r\n       Returns:\r\n           Tensor'''\r\n       if chunk.size(0) < self.segment_length:\r\n           chunk = torch.nn.functional.pad(chunk,\r\n               (0, self.segment_length - chunk.size(0)))\r\n       chunk_with_context = torch.cat((self.context, chunk))\r\n       self.context = chunk[-self.context_length :]\r\n       return chunk_with_context\r\n```\r\n\r\n## Using TorchAudio's Emformer Model for Local Speech Recognition in Python\r\n\r\nFinally, all our helper functions have been made. It is now time to create the main function to orchestrate the audio data streamer, the machine learning pipeline, and the cache. Our main function will take three inputs: the input device, the source, and the RNN-T model. As before, I\u2019ve left some print statements in here commented out, uncomment them if you\u2019d like.\r\n\r\nFirst, we create an instance of our Pipeline. This is what turns the audio data into text. Next, we use the RNN-T model to set the sample rate, the segment length, and the context length. Then, we instantiate the context with the newly gotten segment and context lengths.\r\n\r\nNow things get funky. We use an annotation on the `infer` function we\u2019re creating that signals to `torch` to turn on inference mode while this function runs. The infer function does not need any parameters. It loops through the preset number of iterations. For each iteration, it gets a chunk of data from the queue, calls the cache to return the concatenated first column entry of that chunk of data and the context, calls the pipeline to do speech recognition on that cached segment of data, and finally prints the transcript and flushes the I/O buffer.\r\n\r\nWe won\u2019t actually call that `infer` function just yet. We\u2019re just defining it. Next, we import PyTorch\u2019s multiprocessing capability to spawn subprocesses. We need to spawn the audio data capture as a subprocess for correct functionality. Otherwise, it will block I/O.\r\n\r\nThe first thing we do with the multiprocessing function is to signal we are spawning a new process. Second, we create a queue object in the context. Next, we create a process that runs the `stream` function from above in the context with the created queue, the passed in device, source, and model, the segment length, and the sample rate. Once we start that subprocess, we run our infer function until we end the subprocess.\r\n\r\nWhen the script actually runs (in the `if __name__ == \"__main__\"` section), we call the main function. For Mac\u2019s we pass the \"avfoundation\" for the device and the second entry for the source. The first entry in a Mac\u2019s setup is the video.\r\n\r\nFor Windows, see [the original notebook](https://pytorch.org/audio/main/tutorials/device_asr.html).\r\n\r\n```py\r\ndef main(device: str, src: str, bundle: torchaudio.pipelines):\r\n   '''Transcribed audio data from the mic\r\n  \r\n   Parameters:\r\n       device: Input device name\r\n       src: Source from input\r\n       bundle: TorchAudio pipeline\r\n  \r\n   Returns:\r\n       None'''\r\n   pipeline = InferencePipeline(bundle)\r\n  \r\n   sample_rate = bundle.sample_rate\r\n   segment_length = bundle.segment_length * bundle.hop_length\r\n   context_length = bundle.right_context_length * bundle.hop_length\r\n  \r\n   cacher = ContextCacher(segment_length, context_length)\r\n  \r\n   @torch.inference_mode()\r\n   def infer():\r\n       for _ in range(ITERATIONS):\r\n           chunk = q.get()\r\n           segment = cacher(chunk[:,0])\r\n           transcript = pipeline.infer(segment)\r\n           print(transcript, end=\"\", flush=True)\r\n  \r\n   ctx = mp.get_context(\"spawn\")\r\n   q = ctx.Queue()\r\n   p = ctx.Process(target=stream, args=(q, device, src, segment_length, sample_rate))\r\n   p.start()\r\n   infer()\r\n   p.join()\r\nif __name__ == \"__main__\":\r\n   main(\r\n       device=\"avfoundation\",\r\n       src=\":1\",\r\n       bundle=torchaudio.pipelines.EMFORMER_RNNT_BASE_LIBRISPEECH\r\n   )\r\n```\r\n\r\n## Full Code for Building a Local Streaming Audio Transcription Tool with PyTorch TorchAudio\r\n\r\nThat was a lot of code. Let\u2019s see how it looks all together in one file.\r\n\r\n```py\r\nimport torch\r\nimport torch.multiprocessing as mp\r\nimport torchaudio\r\nfrom torchaudio.io import StreamReader\r\n \r\nITERATIONS = 100\r\ndef stream(queue: mp.Queue(),\r\n   format: str,\r\n   src: str,\r\n   frames_per_chunk: int,\r\n   sample_rate: int):\r\n   '''Streams audio data\r\n  \r\n   Parameters:\r\n       queue: Queue of data chunks\r\n       format: Format\r\n       src: Source\r\n       frames_per_chunk: How many frames are in each data chunk\r\n       sample_rate: Sample rate\r\n \r\n   Returns:\r\n       None'''\r\n   print(\"Initializing Audio Stream\")\r\n   streamer = StreamReader(src, format=format)\r\n   streamer.add_basic_audio_stream(frames_per_chunk=frames_per_chunk,\r\n       sample_rate=sample_rate)\r\n   print(\"Streaming\\n\")\r\n   stream_iterator = streamer.stream(timeout=-1, backoff=1.0)\r\n   for _ in range(ITERATIONS):\r\n       (chunk,) = next(stream_iterator)\r\n       queue.put(chunk)\r\n \r\nclass InferencePipeline:\r\n   '''Creates an inference pipeline for streaming audio data'''\r\n   def __init__(self,\r\n       pipeline: torchaudio.pipelines.RNNTBundle,\r\n       beam_width: int=10):\r\n       '''Initializes TorchAudio RNNT Pipeline\r\n      \r\n       Parameters:\r\n           pipeline: TorchAudio Pipeline to use\r\n           beam_width: Beam width\r\n \r\n       Returns:\r\n           None'''\r\n \r\n       self.pipeline = pipeline\r\n       self.feature_extractor = pipeline.get_streaming_feature_extractor()\r\n       self.decoder = pipeline.get_decoder()\r\n       self.token_processor = pipeline.get_token_processor()\r\n       self.beam_width = beam_width\r\n       self.state = None\r\n       self.hypothesis = None\r\n  \r\n   def infer(self, segment: torch.Tensor) -> str:\r\n       '''Runs inference using the initialized pipeline\r\n      \r\n       Parameters:\r\n           segment: Torch tensor with features to extract\r\n      \r\n       Returns:\r\n           Transcript as string type'''\r\n \r\n       features, length = self.feature_extractor(segment)\r\n       predictions, self.state = self.decoder.infer(\r\n           features, length, self.beam_width, state=self.state,\r\n           hypothesis=self.hypothesis\r\n       )\r\n       self.hypothesis = predictions[0]\r\n       transcript = self.token_processor(self.hypothesis[0], lstrip=False)\r\n       return transcript\r\n \r\nclass ContextCacher:\r\n   def __init__(self, segment_length: int, context_length: int):\r\n       '''Creates initial context cache\r\n      \r\n       Parameters:\r\n           segment_length: length of one audio segment\r\n           context_length: length of the context\r\n \r\n       Returns:\r\n           None'''\r\n       self.segment_length = segment_length\r\n       self.context_length = context_length\r\n       self.context = torch.zeros([context_length])\r\n  \r\n   def __call__(self, chunk: torch.Tensor):\r\n       '''Adds chunk to context and returns it\r\n      \r\n       Parameters:\r\n           chunk: chunk of audio data to process\r\n      \r\n       Returns:\r\n           Tensor'''\r\n       if chunk.size(0) < self.segment_length:\r\n           chunk = torch.nn.functional.pad(chunk,\r\n               (0, self.segment_length - chunk.size(0)))\r\n       chunk_with_context = torch.cat((self.context, chunk))\r\n       self.context = chunk[-self.context_length :]\r\n       return chunk_with_context\r\n \r\ndef main(device: str, src: str, bundle: torchaudio.pipelines):\r\n   '''Transcribed audio data from the mic\r\n  \r\n   Parameters:\r\n       device: Input device name\r\n       src: Source from input\r\n       bundle: TorchAudio pipeline\r\n  \r\n   Returns:\r\n       None'''\r\n   pipeline = InferencePipeline(bundle)\r\n  \r\n   sample_rate = bundle.sample_rate\r\n   segment_length = bundle.segment_length * bundle.hop_length\r\n   context_length = bundle.right_context_length * bundle.hop_length\r\n  \r\n   cacher = ContextCacher(segment_length, context_length)\r\n  \r\n   @torch.inference_mode()\r\n   def infer():\r\n       for _ in range(ITERATIONS):\r\n           chunk = q.get()\r\n           segment = cacher(chunk[:,0])\r\n           transcript = pipeline.infer(segment)\r\n           print(transcript, end=\"\", flush=True)\r\n  \r\n   ctx = mp.get_context(\"spawn\")\r\n   q = ctx.Queue()\r\n   p = ctx.Process(target=stream, args=(q, device, src, segment_length, sample_rate))\r\n   p.start()\r\n   infer()\r\n   p.join()\r\nif __name__ == \"__main__\":\r\n   main(\r\n       device=\"avfoundation\",\r\n       src=\":1\",\r\n       bundle=torchaudio.pipelines.EMFORMER_RNNT_BASE_LIBRISPEECH\r\n   )\r\n```\r\n\r\n## Summary of Python Speech Recognition Locally with PyTorch\r\n\r\nSpeech recognition has come a long way from its first inception. We can now use Python to do speech recognition in many ways, including with the TorchAudio library from PyTorch. The TorchAudio library comes with many pre-built models we can use for automatic speech recognition as well as many audio data manipulation tools.\r\n\r\nIn this post, we covered how to run speech recognition locally with their Emformer RNN-T. First, we created an audio data streaming function. Next, we defined a pipeline object which can run speech recognition on tensors and turn them into text. Then, we created a cache to cache the audio data as it came in. We also created a main function to handle running the audio data stream as a subprocess and run inference on it while it ran. Finally, we ran that main function with varying parameters to let it know we are streaming data in from the mic and using an Emformer RNN-T model.\r\n\r\nWant to do speech recognition without all that code? [Sign up for Deepgram](https://console.deepgram.com) today and be up and running in just a few minutes.", "html": '<p>\u201CYour call may be recorded for quality assurance purposes.\u201D</p>\n<p>We\u2019ve all heard this when calling customer service. What are they doing with that call? How are they transcribing it? Back in the early 2000s and late 1990s, companies were using people to transcribe these. Now, <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> has come far along enough that we can use Python and machine learning to do automatic speech recognition.</p>\n<p>In this post, we\u2019ll focus on how to do speech recognition locally on your device using TorchAudio\u2019s pre-built Emformer RNN-T model. We will cover:</p>\n<ul>\n<li><a href="#understanding-pytorch-torchaudio">Understanding PyTorch TorchAudio</a></li>\n<li><a href="#setting-up-torchaudio-for-speech-recognition">Setting Up TorchAudio for Speech Recognition</a></li>\n<li><a href="#building-a-python-audio-data-streaming-function-for-speech-recognition">Building a Python Audio Data Streaming Function for Speech Recognition</a></li>\n<li><a href="#setting-up-python-speech-recognition-inference-pipeline">Setting up Python Speech Recognition Inference Pipeline</a></li>\n<li><a href="#creating-a-context-cache-to-store-audio-data-for-speech-recognition">Creating a Context Cache to Store Audio Data for Speech Recognition</a></li>\n<li><a href="#using-torchaudios-emformer-model-for-local-speech-recognition-in-python">Using TorchAudio\u2019s Emformer Model for Local Speech Recognition in Python</a></li>\n<li><a href="#in-summary">In Summary</a></li>\n</ul>\n<h2 id="understanding-pytorch-torchaudio">Understanding PyTorch TorchAudio</h2>\n<p>PyTorch is a versatile and powerful Python library for quickly developing machine learning models. Is it the open-source Python version of the Torch library (built on Lua) and primarily developed by Meta/Facebook. TorchAudio is an additional library to PyTorch that handles dealing with audio data for machine learning models.</p>\n<p>TorchAudio isn\u2019t just for creating machine learning models, you can also use it to do some audio data manipulation. We previously covered how to <a href="https://blog.deepgram.com/pytorch-intro-with-torchaudio/">use TorchAudio to manipulate audio data in Python</a>. In this piece, we\u2019re going to use it to build an inference pipeline to do speech recognition in real time.</p>\n<h2 id="setting-up-torchaudio-for-speech-recognition">Setting Up TorchAudio for Speech Recognition</h2>\n<p>Before we can dive into the program for doing speech recognition with TorchAudio, we need to set up our system. We need to install the <code is:raw>pytorch</code>, <code is:raw>torchaudio</code>, <code is:raw>sentencepiece</code>, and <code is:raw>ffmpeg-python</code> libraries. We can install all of these with the command <code is:raw>pip install pytorch torchaudio sentencepiece ffmpeg-python</code>. If you encounter errors, you may need to upgrade <code is:raw>pip</code>, which you can do using <code is:raw>pip install -U pip</code>.</p>\n<p>If you already have PyTorch and TorchAudio installed and you encounter an error importing <code is:raw>torchaudio</code> as I did. To get around this, force update both libraries with <code is:raw>pip install -U torch torchaudio --no-cache-dir</code>. Next, we\u2019re going to build our script. It\u2019s going to consist of an audio data function, the machine learning model pipeline, a context cache, and a main function to run.</p>\n<h2 id="building-a-python-audio-data-streaming-function-for-speech-recognition">Building a Python Audio Data Streaming Function for Speech Recognition</h2>\n<p>Now that our system is set up to work with PyTorch and Torchaudio for speech recognition, let\u2019s build our script. The first thing we need is an audio data streaming function, we\u2019ll call ours <code is:raw>stream</code>. We also declare a constant number of iterations for the streaming function to stream.</p>\n<p>Our <code is:raw>stream</code> function needs five parameters. It needs a queue, a format or device (different for Mac vs Windows), a source, a segment length, and a sample rate. There are some print statements in our function that I\u2019ve commented out. You can uncomment these to show more information about the program as it runs.</p>\n<p>The first thing we\u2019re going to do in our <code is:raw>stream</code> function is create a <code is:raw>StreamReader</code> instance from <code is:raw>torchaudio.io</code>. Next, we\u2019ll populate that stream reader with an audio stream of the passed in segment length and sample rate. I would leave the <code is:raw>"Streaming"</code> and blank <code is:raw>print</code> statements uncommented so you know when you can start talking into the mic.</p>\n<p>Next, we\u2019ll create an iterator to iterate across our stream. Finally, we\u2019ll loop through the number of predefined iterations, in our case 100, and get the next chunk of audio data from the stream iterator and put it in the queue.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> torch</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> torch.multiprocessing </span><span style="color: #FF7B72">as</span><span style="color: #C9D1D9"> mp</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> torchaudio</span></span>\n<span class="line"><span style="color: #FF7B72">from</span><span style="color: #C9D1D9"> torchaudio.io </span><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> StreamReader</span></span>\n<span class="line"><span style="color: #C9D1D9"> </span></span>\n<span class="line"><span style="color: #79C0FF">ITERATIONS</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">100</span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">stream</span><span style="color: #C9D1D9">(queue: mp.Queue(),</span></span>\n<span class="line"><span style="color: #C9D1D9">   format: </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">   src: </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">   frames_per_chunk: </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">   sample_rate: </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #A5D6FF">&#39;&#39;&#39;Streams audio data</span></span>\n<span class="line"><span style="color: #A5D6FF">  </span></span>\n<span class="line"><span style="color: #A5D6FF">   Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">       queue: Queue of data chunks</span></span>\n<span class="line"><span style="color: #A5D6FF">       format: Format</span></span>\n<span class="line"><span style="color: #A5D6FF">       src: Source</span></span>\n<span class="line"><span style="color: #A5D6FF">       frames_per_chunk: How many frames are in each data chunk</span></span>\n<span class="line"><span style="color: #A5D6FF">       sample_rate: Sample rate</span></span>\n<span class="line"><span style="color: #A5D6FF"> </span></span>\n<span class="line"><span style="color: #A5D6FF">   Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">       None&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(</span><span style="color: #A5D6FF">&quot;Initializing Audio Stream&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   streamer </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> StreamReader(src, </span><span style="color: #FFA657">format</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">format</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   streamer.add_basic_audio_stream(</span><span style="color: #FFA657">frames_per_chunk</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">frames_per_chunk,</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FFA657">sample_rate</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">sample_rate)</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(</span><span style="color: #A5D6FF">&quot;Streaming</span><span style="color: #79C0FF">\\n</span><span style="color: #A5D6FF">&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   stream_iterator </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> streamer.stream(</span><span style="color: #FFA657">timeout</span><span style="color: #FF7B72">=-</span><span style="color: #79C0FF">1</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">backoff</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">1.0</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> _ </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">range</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">ITERATIONS</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">       (chunk,) </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">next</span><span style="color: #C9D1D9">(stream_iterator)</span></span>\n<span class="line"><span style="color: #C9D1D9">       queue.put(chunk)</span></span></code></pre>\n<h2 id="setting-up-python-speech-recognition-inference-pipeline">Setting up Python Speech Recognition Inference Pipeline</h2>\n<p>Now that we have a way to smoothly stream the audio data from a microphone, we can run predictions on it. Let\u2019s build a pipeline to do speech recognition with. Unlike the part above, this time we will be creating a <code is:raw>Class</code> and not a function.</p>\n<p>We require our class to be initialized with one variable, an RNN-T model from TorchAudio\u2019s Pipelines library. We also allow a second, optional variable to define beam width, which is set at 10 by default. Our <code is:raw>__init__</code> function sets the instance\u2019s <code is:raw>bundle</code> value to the model we passed in. Then, it uses that model to get the feature extractor, decoder, and token processor. Next, it sets the beam width, the state of the pipeline, and the predictions.</p>\n<p>The other function that our <code is:raw>Pipeline</code> object has is an inference function. This function takes a PyTorch <code is:raw>Tensor</code> object and returns a string, the predicted text. This function uses the feature extractor to get the features and the length of the input tensor. Next, call the inference function from the decoder to get the hypothesis and set the state of the Pipeline. We set the instance\u2019s <code is:raw>hypothesis</code> attribute to the value in the first index of the returned hypothesis. Finally, we use the token processor on the first entry in the instance\u2019s hypothesis attribute and return that value.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">class</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">InferencePipeline</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #A5D6FF">&#39;&#39;&#39;Creates an inference pipeline for streaming audio data&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__init__</span><span style="color: #C9D1D9">(self,</span></span>\n<span class="line"><span style="color: #C9D1D9">       pipeline: torchaudio.pipelines.RNNTBundle,</span></span>\n<span class="line"><span style="color: #C9D1D9">       beam_width: </span><span style="color: #79C0FF">int</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">10</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #A5D6FF">&#39;&#39;&#39;Initializes TorchAudio RNNT Pipeline</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">           pipeline: TorchAudio Pipeline to use</span></span>\n<span class="line"><span style="color: #A5D6FF">           beam_width: Beam width</span></span>\n<span class="line"><span style="color: #A5D6FF"> </span></span>\n<span class="line"><span style="color: #A5D6FF">       Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">           None&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9"> </span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.pipeline </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.feature_extractor </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline.get_streaming_feature_extractor()</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.decoder </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline.get_decoder()</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.token_processor </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline.get_token_processor()</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.beam_width </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> beam_width</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.state </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">None</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.hypothesis </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">None</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">infer</span><span style="color: #C9D1D9">(self, segment: torch.Tensor) -&gt; </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #A5D6FF">&#39;&#39;&#39;Runs inference using the initialized pipeline</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">           segment: Torch tensor with features to extract</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">           Transcript as string type&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9"> </span></span>\n<span class="line"><span style="color: #C9D1D9">       features, length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.feature_extractor(segment)</span></span>\n<span class="line"><span style="color: #C9D1D9">       predictions, </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.state </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.decoder.infer(</span></span>\n<span class="line"><span style="color: #C9D1D9">           features, length, </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.beam_width, </span><span style="color: #FFA657">state</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.state,</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #FFA657">hypothesis</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.hypothesis</span></span>\n<span class="line"><span style="color: #C9D1D9">       )</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.hypothesis </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> predictions[</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">]</span></span>\n<span class="line"><span style="color: #C9D1D9">       transcript </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.token_processor(</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.hypothesis[</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">], </span><span style="color: #FFA657">lstrip</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">False</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> transcript</span></span></code></pre>\n<h2 id="creating-a-context-cache-to-store-audio-data-for-speech-recognition">Creating a Context Cache to Store Audio Data for Speech Recognition</h2>\n<p>We need a context cache to batch store the audio data frames as we read them. The context cache requires two parameters to initialize. First, a segment length, then a context length. The initialization function sets the instance\u2019s attributes to the passed in values and initializes a tensor of zeros of the context length as the instance\u2019s current context.</p>\n<p>The second function we\u2019ll define is the <code is:raw>call</code> function. This is another automatic function that we\u2019re overwriting. Whenever you see a class function preceded and followed by two underscores, that\u2019s a default function. In this case, we require the call function to intake a PyTorch tensor object.</p>\n<p>If the size of the first object in the tensor is less than the set segment length for the cache, we\u2019ll pad that chunk with 0s. Next, we use the concatenate function from <code is:raw>torch</code> to add that chunk to the current context. Then, we set the instance\u2019s context attribute to the last entries in the chunk equivalent to the context length. Finally, we return the concatenate context.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">class</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">ContextCacher</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__init__</span><span style="color: #C9D1D9">(self, segment_length: </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">, context_length: </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #A5D6FF">&#39;&#39;&#39;Creates initial context cache</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">           segment_length: length of one audio segment</span></span>\n<span class="line"><span style="color: #A5D6FF">           context_length: length of the context</span></span>\n<span class="line"><span style="color: #A5D6FF"> </span></span>\n<span class="line"><span style="color: #A5D6FF">       Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">           None&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.segment_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> segment_length</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> context_length</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> torch.zeros([context_length])</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__call__</span><span style="color: #C9D1D9">(self, chunk: torch.Tensor):</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #A5D6FF">&#39;&#39;&#39;Adds chunk to context and returns it</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">           chunk: chunk of audio data to process</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">           Tensor&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> chunk.size(</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">) </span><span style="color: #FF7B72">&lt;</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.segment_length:</span></span>\n<span class="line"><span style="color: #C9D1D9">           chunk </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> torch.nn.functional.pad(chunk,</span></span>\n<span class="line"><span style="color: #C9D1D9">               (</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.segment_length </span><span style="color: #FF7B72">-</span><span style="color: #C9D1D9"> chunk.size(</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">)))</span></span>\n<span class="line"><span style="color: #C9D1D9">       chunk_with_context </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> torch.cat((</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context, chunk))</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> chunk[</span><span style="color: #FF7B72">-</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context_length :]</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> chunk_with_context</span></span></code></pre>\n<h2 id="using-torchaudios-emformer-model-for-local-speech-recognition-in-python">Using TorchAudio\u2019s Emformer Model for Local Speech Recognition in Python</h2>\n<p>Finally, all our helper functions have been made. It is now time to create the main function to orchestrate the audio data streamer, the machine learning pipeline, and the cache. Our main function will take three inputs: the input device, the source, and the RNN-T model. As before, I\u2019ve left some print statements in here commented out, uncomment them if you\u2019d like.</p>\n<p>First, we create an instance of our Pipeline. This is what turns the audio data into text. Next, we use the RNN-T model to set the sample rate, the segment length, and the context length. Then, we instantiate the context with the newly gotten segment and context lengths.</p>\n<p>Now things get funky. We use an annotation on the <code is:raw>infer</code> function we\u2019re creating that signals to <code is:raw>torch</code> to turn on inference mode while this function runs. The infer function does not need any parameters. It loops through the preset number of iterations. For each iteration, it gets a chunk of data from the queue, calls the cache to return the concatenated first column entry of that chunk of data and the context, calls the pipeline to do speech recognition on that cached segment of data, and finally prints the transcript and flushes the I/O buffer.</p>\n<p>We won\u2019t actually call that <code is:raw>infer</code> function just yet. We\u2019re just defining it. Next, we import PyTorch\u2019s multiprocessing capability to spawn subprocesses. We need to spawn the audio data capture as a subprocess for correct functionality. Otherwise, it will block I/O.</p>\n<p>The first thing we do with the multiprocessing function is to signal we are spawning a new process. Second, we create a queue object in the context. Next, we create a process that runs the <code is:raw>stream</code> function from above in the context with the created queue, the passed in device, source, and model, the segment length, and the sample rate. Once we start that subprocess, we run our infer function until we end the subprocess.</p>\n<p>When the script actually runs (in the <code is:raw>if __name__ == "__main__"</code> section), we call the main function. For Mac\u2019s we pass the \u201Cavfoundation\u201D for the device and the second entry for the source. The first entry in a Mac\u2019s setup is the video.</p>\n<p>For Windows, see <a href="https://pytorch.org/audio/main/tutorials/device_asr.html">the original notebook</a>.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">main</span><span style="color: #C9D1D9">(device: </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">, src: </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">, bundle: torchaudio.pipelines):</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #A5D6FF">&#39;&#39;&#39;Transcribed audio data from the mic</span></span>\n<span class="line"><span style="color: #A5D6FF">  </span></span>\n<span class="line"><span style="color: #A5D6FF">   Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">       device: Input device name</span></span>\n<span class="line"><span style="color: #A5D6FF">       src: Source from input</span></span>\n<span class="line"><span style="color: #A5D6FF">       bundle: TorchAudio pipeline</span></span>\n<span class="line"><span style="color: #A5D6FF">  </span></span>\n<span class="line"><span style="color: #A5D6FF">   Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">       None&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9">   pipeline </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> InferencePipeline(bundle)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   sample_rate </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> bundle.sample_rate</span></span>\n<span class="line"><span style="color: #C9D1D9">   segment_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> bundle.segment_length </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> bundle.hop_length</span></span>\n<span class="line"><span style="color: #C9D1D9">   context_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> bundle.right_context_length </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> bundle.hop_length</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   cacher </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> ContextCacher(segment_length, context_length)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #D2A8FF">@torch.inference_mode</span><span style="color: #C9D1D9">()</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">infer</span><span style="color: #C9D1D9">():</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> _ </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">range</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">ITERATIONS</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">           chunk </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> q.get()</span></span>\n<span class="line"><span style="color: #C9D1D9">           segment </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> cacher(chunk[:,</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">])</span></span>\n<span class="line"><span style="color: #C9D1D9">           transcript </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline.infer(segment)</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcript, </span><span style="color: #FFA657">end</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">flush</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">True</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   ctx </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> mp.get_context(</span><span style="color: #A5D6FF">&quot;spawn&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   q </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> ctx.Queue()</span></span>\n<span class="line"><span style="color: #C9D1D9">   p </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> ctx.Process(</span><span style="color: #FFA657">target</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">stream, </span><span style="color: #FFA657">args</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">(q, device, src, segment_length, sample_rate))</span></span>\n<span class="line"><span style="color: #C9D1D9">   p.start()</span></span>\n<span class="line"><span style="color: #C9D1D9">   infer()</span></span>\n<span class="line"><span style="color: #C9D1D9">   p.join()</span></span>\n<span class="line"><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__name__</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">==</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;__main__&quot;</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">   main(</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FFA657">device</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;avfoundation&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FFA657">src</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;:1&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FFA657">bundle</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">torchaudio.pipelines.</span><span style="color: #79C0FF">EMFORMER_RNNT_BASE_LIBRISPEECH</span></span>\n<span class="line"><span style="color: #C9D1D9">   )</span></span></code></pre>\n<h2 id="full-code-for-building-a-local-streaming-audio-transcription-tool-with-pytorch-torchaudio">Full Code for Building a Local Streaming Audio Transcription Tool with PyTorch TorchAudio</h2>\n<p>That was a lot of code. Let\u2019s see how it looks all together in one file.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> torch</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> torch.multiprocessing </span><span style="color: #FF7B72">as</span><span style="color: #C9D1D9"> mp</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> torchaudio</span></span>\n<span class="line"><span style="color: #FF7B72">from</span><span style="color: #C9D1D9"> torchaudio.io </span><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> StreamReader</span></span>\n<span class="line"><span style="color: #C9D1D9"> </span></span>\n<span class="line"><span style="color: #79C0FF">ITERATIONS</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">100</span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">stream</span><span style="color: #C9D1D9">(queue: mp.Queue(),</span></span>\n<span class="line"><span style="color: #C9D1D9">   format: </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">   src: </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">   frames_per_chunk: </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">   sample_rate: </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #A5D6FF">&#39;&#39;&#39;Streams audio data</span></span>\n<span class="line"><span style="color: #A5D6FF">  </span></span>\n<span class="line"><span style="color: #A5D6FF">   Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">       queue: Queue of data chunks</span></span>\n<span class="line"><span style="color: #A5D6FF">       format: Format</span></span>\n<span class="line"><span style="color: #A5D6FF">       src: Source</span></span>\n<span class="line"><span style="color: #A5D6FF">       frames_per_chunk: How many frames are in each data chunk</span></span>\n<span class="line"><span style="color: #A5D6FF">       sample_rate: Sample rate</span></span>\n<span class="line"><span style="color: #A5D6FF"> </span></span>\n<span class="line"><span style="color: #A5D6FF">   Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">       None&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(</span><span style="color: #A5D6FF">&quot;Initializing Audio Stream&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   streamer </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> StreamReader(src, </span><span style="color: #FFA657">format</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">format</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   streamer.add_basic_audio_stream(</span><span style="color: #FFA657">frames_per_chunk</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">frames_per_chunk,</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FFA657">sample_rate</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">sample_rate)</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(</span><span style="color: #A5D6FF">&quot;Streaming</span><span style="color: #79C0FF">\\n</span><span style="color: #A5D6FF">&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   stream_iterator </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> streamer.stream(</span><span style="color: #FFA657">timeout</span><span style="color: #FF7B72">=-</span><span style="color: #79C0FF">1</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">backoff</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">1.0</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> _ </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">range</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">ITERATIONS</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">       (chunk,) </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">next</span><span style="color: #C9D1D9">(stream_iterator)</span></span>\n<span class="line"><span style="color: #C9D1D9">       queue.put(chunk)</span></span>\n<span class="line"><span style="color: #C9D1D9"> </span></span>\n<span class="line"><span style="color: #FF7B72">class</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">InferencePipeline</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #A5D6FF">&#39;&#39;&#39;Creates an inference pipeline for streaming audio data&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__init__</span><span style="color: #C9D1D9">(self,</span></span>\n<span class="line"><span style="color: #C9D1D9">       pipeline: torchaudio.pipelines.RNNTBundle,</span></span>\n<span class="line"><span style="color: #C9D1D9">       beam_width: </span><span style="color: #79C0FF">int</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">10</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #A5D6FF">&#39;&#39;&#39;Initializes TorchAudio RNNT Pipeline</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">           pipeline: TorchAudio Pipeline to use</span></span>\n<span class="line"><span style="color: #A5D6FF">           beam_width: Beam width</span></span>\n<span class="line"><span style="color: #A5D6FF"> </span></span>\n<span class="line"><span style="color: #A5D6FF">       Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">           None&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9"> </span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.pipeline </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.feature_extractor </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline.get_streaming_feature_extractor()</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.decoder </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline.get_decoder()</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.token_processor </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline.get_token_processor()</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.beam_width </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> beam_width</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.state </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">None</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.hypothesis </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">None</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">infer</span><span style="color: #C9D1D9">(self, segment: torch.Tensor) -&gt; </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #A5D6FF">&#39;&#39;&#39;Runs inference using the initialized pipeline</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">           segment: Torch tensor with features to extract</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">           Transcript as string type&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9"> </span></span>\n<span class="line"><span style="color: #C9D1D9">       features, length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.feature_extractor(segment)</span></span>\n<span class="line"><span style="color: #C9D1D9">       predictions, </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.state </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.decoder.infer(</span></span>\n<span class="line"><span style="color: #C9D1D9">           features, length, </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.beam_width, </span><span style="color: #FFA657">state</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.state,</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #FFA657">hypothesis</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.hypothesis</span></span>\n<span class="line"><span style="color: #C9D1D9">       )</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.hypothesis </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> predictions[</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">]</span></span>\n<span class="line"><span style="color: #C9D1D9">       transcript </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.token_processor(</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.hypothesis[</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">], </span><span style="color: #FFA657">lstrip</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">False</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> transcript</span></span>\n<span class="line"><span style="color: #C9D1D9"> </span></span>\n<span class="line"><span style="color: #FF7B72">class</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">ContextCacher</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__init__</span><span style="color: #C9D1D9">(self, segment_length: </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">, context_length: </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #A5D6FF">&#39;&#39;&#39;Creates initial context cache</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">           segment_length: length of one audio segment</span></span>\n<span class="line"><span style="color: #A5D6FF">           context_length: length of the context</span></span>\n<span class="line"><span style="color: #A5D6FF"> </span></span>\n<span class="line"><span style="color: #A5D6FF">       Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">           None&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.segment_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> segment_length</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> context_length</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> torch.zeros([context_length])</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__call__</span><span style="color: #C9D1D9">(self, chunk: torch.Tensor):</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #A5D6FF">&#39;&#39;&#39;Adds chunk to context and returns it</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">           chunk: chunk of audio data to process</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">           Tensor&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> chunk.size(</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">) </span><span style="color: #FF7B72">&lt;</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.segment_length:</span></span>\n<span class="line"><span style="color: #C9D1D9">           chunk </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> torch.nn.functional.pad(chunk,</span></span>\n<span class="line"><span style="color: #C9D1D9">               (</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.segment_length </span><span style="color: #FF7B72">-</span><span style="color: #C9D1D9"> chunk.size(</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">)))</span></span>\n<span class="line"><span style="color: #C9D1D9">       chunk_with_context </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> torch.cat((</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context, chunk))</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> chunk[</span><span style="color: #FF7B72">-</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context_length :]</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> chunk_with_context</span></span>\n<span class="line"><span style="color: #C9D1D9"> </span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">main</span><span style="color: #C9D1D9">(device: </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">, src: </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">, bundle: torchaudio.pipelines):</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #A5D6FF">&#39;&#39;&#39;Transcribed audio data from the mic</span></span>\n<span class="line"><span style="color: #A5D6FF">  </span></span>\n<span class="line"><span style="color: #A5D6FF">   Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">       device: Input device name</span></span>\n<span class="line"><span style="color: #A5D6FF">       src: Source from input</span></span>\n<span class="line"><span style="color: #A5D6FF">       bundle: TorchAudio pipeline</span></span>\n<span class="line"><span style="color: #A5D6FF">  </span></span>\n<span class="line"><span style="color: #A5D6FF">   Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">       None&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9">   pipeline </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> InferencePipeline(bundle)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   sample_rate </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> bundle.sample_rate</span></span>\n<span class="line"><span style="color: #C9D1D9">   segment_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> bundle.segment_length </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> bundle.hop_length</span></span>\n<span class="line"><span style="color: #C9D1D9">   context_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> bundle.right_context_length </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> bundle.hop_length</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   cacher </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> ContextCacher(segment_length, context_length)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #D2A8FF">@torch.inference_mode</span><span style="color: #C9D1D9">()</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">infer</span><span style="color: #C9D1D9">():</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> _ </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">range</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">ITERATIONS</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">           chunk </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> q.get()</span></span>\n<span class="line"><span style="color: #C9D1D9">           segment </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> cacher(chunk[:,</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">])</span></span>\n<span class="line"><span style="color: #C9D1D9">           transcript </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline.infer(segment)</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcript, </span><span style="color: #FFA657">end</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">flush</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">True</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   ctx </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> mp.get_context(</span><span style="color: #A5D6FF">&quot;spawn&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   q </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> ctx.Queue()</span></span>\n<span class="line"><span style="color: #C9D1D9">   p </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> ctx.Process(</span><span style="color: #FFA657">target</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">stream, </span><span style="color: #FFA657">args</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">(q, device, src, segment_length, sample_rate))</span></span>\n<span class="line"><span style="color: #C9D1D9">   p.start()</span></span>\n<span class="line"><span style="color: #C9D1D9">   infer()</span></span>\n<span class="line"><span style="color: #C9D1D9">   p.join()</span></span>\n<span class="line"><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__name__</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">==</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;__main__&quot;</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">   main(</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FFA657">device</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;avfoundation&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FFA657">src</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;:1&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FFA657">bundle</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">torchaudio.pipelines.</span><span style="color: #79C0FF">EMFORMER_RNNT_BASE_LIBRISPEECH</span></span>\n<span class="line"><span style="color: #C9D1D9">   )</span></span></code></pre>\n<h2 id="summary-of-python-speech-recognition-locally-with-pytorch">Summary of Python Speech Recognition Locally with PyTorch</h2>\n<p>Speech recognition has come a long way from its first inception. We can now use Python to do speech recognition in many ways, including with the TorchAudio library from PyTorch. The TorchAudio library comes with many pre-built models we can use for automatic speech recognition as well as many audio data manipulation tools.</p>\n<p>In this post, we covered how to run speech recognition locally with their Emformer RNN-T. First, we created an audio data streaming function. Next, we defined a pipeline object which can run speech recognition on tensors and turn them into text. Then, we created a cache to cache the audio data as it came in. We also created a main function to handle running the audio data stream as a subprocess and run inference on it while it ran. Finally, we ran that main function with varying parameters to let it know we are streaming data in from the mic and using an Emformer RNN-T model.</p>\n<p>Want to do speech recognition without all that code? <a href="https://console.deepgram.com">Sign up for Deepgram</a> today and be up and running in just a few minutes.</p>' }, "file": "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/python-speech-recognition-locally-torchaudio/index.md" };
function rawContent() {
  return "\"Your call may be recorded for quality assurance purposes.\"\r\n\r\nWe\u2019ve all heard this when calling customer service. What are they doing with that call? How are they transcribing it? Back in the early 2000s and late 1990s, companies were using people to transcribe these. Now, [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing) has come far along enough that we can use Python and machine learning to do automatic speech recognition.\r\n\r\nIn this post, we\u2019ll focus on how to do speech recognition locally on your device using TorchAudio\u2019s pre-built Emformer RNN-T model. We will cover:\r\n\r\n* [Understanding PyTorch TorchAudio](#understanding-pytorch-torchaudio)\r\n* [Setting Up TorchAudio for Speech Recognition](#setting-up-torchaudio-for-speech-recognition)\r\n* [Building a Python Audio Data Streaming Function for Speech Recognition](#building-a-python-audio-data-streaming-function-for-speech-recognition)\r\n* [Setting up Python Speech Recognition Inference Pipeline](#setting-up-python-speech-recognition-inference-pipeline)\r\n* [Creating a Context Cache to Store Audio Data for Speech Recognition](#creating-a-context-cache-to-store-audio-data-for-speech-recognition)\r\n* [Using TorchAudio\u2019s Emformer Model for Local Speech Recognition in Python](#using-torchaudios-emformer-model-for-local-speech-recognition-in-python)\r\n* [In Summary](#in-summary)\r\n\r\n## Understanding PyTorch TorchAudio\r\n\r\nPyTorch is a versatile and powerful Python library for quickly developing machine learning models. Is it the open-source Python version of the Torch library (built on Lua) and primarily developed by Meta/Facebook. TorchAudio is an additional library to PyTorch that handles dealing with audio data for machine learning models.\r\n\r\nTorchAudio isn\u2019t just for creating machine learning models, you can also use it to do some audio data manipulation. We previously covered how to [use TorchAudio to manipulate audio data in Python](https://blog.deepgram.com/pytorch-intro-with-torchaudio/). In this piece, we\u2019re going to use it to build an inference pipeline to do speech recognition in real time.\r\n\r\n## Setting Up TorchAudio for Speech Recognition\r\n\r\nBefore we can dive into the program for doing speech recognition with TorchAudio, we need to set up our system. We need to install the `pytorch`, `torchaudio`, `sentencepiece`, and `ffmpeg-python` libraries. We can install all of these with the command `pip install pytorch torchaudio sentencepiece ffmpeg-python`. If you encounter errors, you may need to upgrade `pip`, which you can do using `pip install -U pip`.\r\n\r\nIf you already have PyTorch and TorchAudio installed and you encounter an error importing `torchaudio` as I did. To get around this, force update both libraries with `pip install -U torch torchaudio --no-cache-dir`. Next, we\u2019re going to build our script. It\u2019s going to consist of an audio data function, the machine learning model pipeline, a context cache, and a main function to run.\r\n\r\n## Building a Python Audio Data Streaming Function for Speech Recognition\r\n\r\nNow that our system is set up to work with PyTorch and Torchaudio for speech recognition, let\u2019s build our script. The first thing we need is an audio data streaming function, we\u2019ll call ours `stream`. We also declare a constant number of iterations for the streaming function to stream.\r\n\r\nOur `stream` function needs five parameters. It needs a queue, a format or device (different for Mac vs Windows), a source, a segment length, and a sample rate. There are some print statements in our function that I\u2019ve commented out. You can uncomment these to show more information about the program as it runs.\r\n\r\nThe first thing we\u2019re going to do in our `stream` function is create a `StreamReader` instance from `torchaudio.io`. Next, we\u2019ll populate that stream reader with an audio stream of the passed in segment length and sample rate. I would leave the `\"Streaming\"` and blank `print` statements uncommented so you know when you can start talking into the mic.\r\n\r\nNext, we\u2019ll create an iterator to iterate across our stream. Finally, we\u2019ll loop through the number of predefined iterations, in our case 100, and get the next chunk of audio data from the stream iterator and put it in the queue.\r\n\r\n```py\r\nimport torch\r\nimport torch.multiprocessing as mp\r\nimport torchaudio\r\nfrom torchaudio.io import StreamReader\r\n \r\nITERATIONS = 100\r\ndef stream(queue: mp.Queue(),\r\n   format: str,\r\n   src: str,\r\n   frames_per_chunk: int,\r\n   sample_rate: int):\r\n   '''Streams audio data\r\n  \r\n   Parameters:\r\n       queue: Queue of data chunks\r\n       format: Format\r\n       src: Source\r\n       frames_per_chunk: How many frames are in each data chunk\r\n       sample_rate: Sample rate\r\n \r\n   Returns:\r\n       None'''\r\n   print(\"Initializing Audio Stream\")\r\n   streamer = StreamReader(src, format=format)\r\n   streamer.add_basic_audio_stream(frames_per_chunk=frames_per_chunk,\r\n       sample_rate=sample_rate)\r\n   print(\"Streaming\\n\")\r\n   stream_iterator = streamer.stream(timeout=-1, backoff=1.0)\r\n   for _ in range(ITERATIONS):\r\n       (chunk,) = next(stream_iterator)\r\n       queue.put(chunk)\r\n```\r\n\r\n## Setting up Python Speech Recognition Inference Pipeline\r\n\r\nNow that we have a way to smoothly stream the audio data from a microphone, we can run predictions on it. Let\u2019s build a pipeline to do speech recognition with. Unlike the part above, this time we will be creating a `Class` and not a function.\r\n\r\nWe require our class to be initialized with one variable, an RNN-T model from TorchAudio\u2019s Pipelines library. We also allow a second, optional variable to define beam width, which is set at 10 by default. Our `__init__` function sets the instance\u2019s `bundle` value to the model we passed in. Then, it uses that model to get the feature extractor, decoder, and token processor. Next, it sets the beam width, the state of the pipeline, and the predictions.\r\n\r\nThe other function that our `Pipeline` object has is an inference function. This function takes a PyTorch `Tensor` object and returns a string, the predicted text. This function uses the feature extractor to get the features and the length of the input tensor. Next, call the inference function from the decoder to get the hypothesis and set the state of the Pipeline. We set the instance\u2019s `hypothesis` attribute to the value in the first index of the returned hypothesis. Finally, we use the token processor on the first entry in the instance\u2019s hypothesis attribute and return that value.\r\n\r\n```py\r\nclass InferencePipeline:\r\n   '''Creates an inference pipeline for streaming audio data'''\r\n   def __init__(self,\r\n       pipeline: torchaudio.pipelines.RNNTBundle,\r\n       beam_width: int=10):\r\n       '''Initializes TorchAudio RNNT Pipeline\r\n      \r\n       Parameters:\r\n           pipeline: TorchAudio Pipeline to use\r\n           beam_width: Beam width\r\n \r\n       Returns:\r\n           None'''\r\n \r\n       self.pipeline = pipeline\r\n       self.feature_extractor = pipeline.get_streaming_feature_extractor()\r\n       self.decoder = pipeline.get_decoder()\r\n       self.token_processor = pipeline.get_token_processor()\r\n       self.beam_width = beam_width\r\n       self.state = None\r\n       self.hypothesis = None\r\n  \r\n   def infer(self, segment: torch.Tensor) -> str:\r\n       '''Runs inference using the initialized pipeline\r\n      \r\n       Parameters:\r\n           segment: Torch tensor with features to extract\r\n      \r\n       Returns:\r\n           Transcript as string type'''\r\n \r\n       features, length = self.feature_extractor(segment)\r\n       predictions, self.state = self.decoder.infer(\r\n           features, length, self.beam_width, state=self.state,\r\n           hypothesis=self.hypothesis\r\n       )\r\n       self.hypothesis = predictions[0]\r\n       transcript = self.token_processor(self.hypothesis[0], lstrip=False)\r\n       return transcript\r\n```\r\n\r\n## Creating a Context Cache to Store Audio Data for Speech Recognition\r\n\r\nWe need a context cache to batch store the audio data frames as we read them. The context cache requires two parameters to initialize. First, a segment length, then a context length. The initialization function sets the instance\u2019s attributes to the passed in values and initializes a tensor of zeros of the context length as the instance\u2019s current context.\r\n\r\nThe second function we\u2019ll define is the `call` function. This is another automatic function that we\u2019re overwriting. Whenever you see a class function preceded and followed by two underscores, that\u2019s a default function. In this case, we require the call function to intake a PyTorch tensor object.\r\n\r\nIf the size of the first object in the tensor is less than the set segment length for the cache, we\u2019ll pad that chunk with 0s. Next, we use the concatenate function from `torch` to add that chunk to the current context. Then, we set the instance\u2019s context attribute to the last entries in the chunk equivalent to the context length. Finally, we return the concatenate context.\r\n\r\n```py\r\nclass ContextCacher:\r\n   def __init__(self, segment_length: int, context_length: int):\r\n       '''Creates initial context cache\r\n      \r\n       Parameters:\r\n           segment_length: length of one audio segment\r\n           context_length: length of the context\r\n \r\n       Returns:\r\n           None'''\r\n       self.segment_length = segment_length\r\n       self.context_length = context_length\r\n       self.context = torch.zeros([context_length])\r\n  \r\n   def __call__(self, chunk: torch.Tensor):\r\n       '''Adds chunk to context and returns it\r\n      \r\n       Parameters:\r\n           chunk: chunk of audio data to process\r\n      \r\n       Returns:\r\n           Tensor'''\r\n       if chunk.size(0) < self.segment_length:\r\n           chunk = torch.nn.functional.pad(chunk,\r\n               (0, self.segment_length - chunk.size(0)))\r\n       chunk_with_context = torch.cat((self.context, chunk))\r\n       self.context = chunk[-self.context_length :]\r\n       return chunk_with_context\r\n```\r\n\r\n## Using TorchAudio's Emformer Model for Local Speech Recognition in Python\r\n\r\nFinally, all our helper functions have been made. It is now time to create the main function to orchestrate the audio data streamer, the machine learning pipeline, and the cache. Our main function will take three inputs: the input device, the source, and the RNN-T model. As before, I\u2019ve left some print statements in here commented out, uncomment them if you\u2019d like.\r\n\r\nFirst, we create an instance of our Pipeline. This is what turns the audio data into text. Next, we use the RNN-T model to set the sample rate, the segment length, and the context length. Then, we instantiate the context with the newly gotten segment and context lengths.\r\n\r\nNow things get funky. We use an annotation on the `infer` function we\u2019re creating that signals to `torch` to turn on inference mode while this function runs. The infer function does not need any parameters. It loops through the preset number of iterations. For each iteration, it gets a chunk of data from the queue, calls the cache to return the concatenated first column entry of that chunk of data and the context, calls the pipeline to do speech recognition on that cached segment of data, and finally prints the transcript and flushes the I/O buffer.\r\n\r\nWe won\u2019t actually call that `infer` function just yet. We\u2019re just defining it. Next, we import PyTorch\u2019s multiprocessing capability to spawn subprocesses. We need to spawn the audio data capture as a subprocess for correct functionality. Otherwise, it will block I/O.\r\n\r\nThe first thing we do with the multiprocessing function is to signal we are spawning a new process. Second, we create a queue object in the context. Next, we create a process that runs the `stream` function from above in the context with the created queue, the passed in device, source, and model, the segment length, and the sample rate. Once we start that subprocess, we run our infer function until we end the subprocess.\r\n\r\nWhen the script actually runs (in the `if __name__ == \"__main__\"` section), we call the main function. For Mac\u2019s we pass the \"avfoundation\" for the device and the second entry for the source. The first entry in a Mac\u2019s setup is the video.\r\n\r\nFor Windows, see [the original notebook](https://pytorch.org/audio/main/tutorials/device_asr.html).\r\n\r\n```py\r\ndef main(device: str, src: str, bundle: torchaudio.pipelines):\r\n   '''Transcribed audio data from the mic\r\n  \r\n   Parameters:\r\n       device: Input device name\r\n       src: Source from input\r\n       bundle: TorchAudio pipeline\r\n  \r\n   Returns:\r\n       None'''\r\n   pipeline = InferencePipeline(bundle)\r\n  \r\n   sample_rate = bundle.sample_rate\r\n   segment_length = bundle.segment_length * bundle.hop_length\r\n   context_length = bundle.right_context_length * bundle.hop_length\r\n  \r\n   cacher = ContextCacher(segment_length, context_length)\r\n  \r\n   @torch.inference_mode()\r\n   def infer():\r\n       for _ in range(ITERATIONS):\r\n           chunk = q.get()\r\n           segment = cacher(chunk[:,0])\r\n           transcript = pipeline.infer(segment)\r\n           print(transcript, end=\"\", flush=True)\r\n  \r\n   ctx = mp.get_context(\"spawn\")\r\n   q = ctx.Queue()\r\n   p = ctx.Process(target=stream, args=(q, device, src, segment_length, sample_rate))\r\n   p.start()\r\n   infer()\r\n   p.join()\r\nif __name__ == \"__main__\":\r\n   main(\r\n       device=\"avfoundation\",\r\n       src=\":1\",\r\n       bundle=torchaudio.pipelines.EMFORMER_RNNT_BASE_LIBRISPEECH\r\n   )\r\n```\r\n\r\n## Full Code for Building a Local Streaming Audio Transcription Tool with PyTorch TorchAudio\r\n\r\nThat was a lot of code. Let\u2019s see how it looks all together in one file.\r\n\r\n```py\r\nimport torch\r\nimport torch.multiprocessing as mp\r\nimport torchaudio\r\nfrom torchaudio.io import StreamReader\r\n \r\nITERATIONS = 100\r\ndef stream(queue: mp.Queue(),\r\n   format: str,\r\n   src: str,\r\n   frames_per_chunk: int,\r\n   sample_rate: int):\r\n   '''Streams audio data\r\n  \r\n   Parameters:\r\n       queue: Queue of data chunks\r\n       format: Format\r\n       src: Source\r\n       frames_per_chunk: How many frames are in each data chunk\r\n       sample_rate: Sample rate\r\n \r\n   Returns:\r\n       None'''\r\n   print(\"Initializing Audio Stream\")\r\n   streamer = StreamReader(src, format=format)\r\n   streamer.add_basic_audio_stream(frames_per_chunk=frames_per_chunk,\r\n       sample_rate=sample_rate)\r\n   print(\"Streaming\\n\")\r\n   stream_iterator = streamer.stream(timeout=-1, backoff=1.0)\r\n   for _ in range(ITERATIONS):\r\n       (chunk,) = next(stream_iterator)\r\n       queue.put(chunk)\r\n \r\nclass InferencePipeline:\r\n   '''Creates an inference pipeline for streaming audio data'''\r\n   def __init__(self,\r\n       pipeline: torchaudio.pipelines.RNNTBundle,\r\n       beam_width: int=10):\r\n       '''Initializes TorchAudio RNNT Pipeline\r\n      \r\n       Parameters:\r\n           pipeline: TorchAudio Pipeline to use\r\n           beam_width: Beam width\r\n \r\n       Returns:\r\n           None'''\r\n \r\n       self.pipeline = pipeline\r\n       self.feature_extractor = pipeline.get_streaming_feature_extractor()\r\n       self.decoder = pipeline.get_decoder()\r\n       self.token_processor = pipeline.get_token_processor()\r\n       self.beam_width = beam_width\r\n       self.state = None\r\n       self.hypothesis = None\r\n  \r\n   def infer(self, segment: torch.Tensor) -> str:\r\n       '''Runs inference using the initialized pipeline\r\n      \r\n       Parameters:\r\n           segment: Torch tensor with features to extract\r\n      \r\n       Returns:\r\n           Transcript as string type'''\r\n \r\n       features, length = self.feature_extractor(segment)\r\n       predictions, self.state = self.decoder.infer(\r\n           features, length, self.beam_width, state=self.state,\r\n           hypothesis=self.hypothesis\r\n       )\r\n       self.hypothesis = predictions[0]\r\n       transcript = self.token_processor(self.hypothesis[0], lstrip=False)\r\n       return transcript\r\n \r\nclass ContextCacher:\r\n   def __init__(self, segment_length: int, context_length: int):\r\n       '''Creates initial context cache\r\n      \r\n       Parameters:\r\n           segment_length: length of one audio segment\r\n           context_length: length of the context\r\n \r\n       Returns:\r\n           None'''\r\n       self.segment_length = segment_length\r\n       self.context_length = context_length\r\n       self.context = torch.zeros([context_length])\r\n  \r\n   def __call__(self, chunk: torch.Tensor):\r\n       '''Adds chunk to context and returns it\r\n      \r\n       Parameters:\r\n           chunk: chunk of audio data to process\r\n      \r\n       Returns:\r\n           Tensor'''\r\n       if chunk.size(0) < self.segment_length:\r\n           chunk = torch.nn.functional.pad(chunk,\r\n               (0, self.segment_length - chunk.size(0)))\r\n       chunk_with_context = torch.cat((self.context, chunk))\r\n       self.context = chunk[-self.context_length :]\r\n       return chunk_with_context\r\n \r\ndef main(device: str, src: str, bundle: torchaudio.pipelines):\r\n   '''Transcribed audio data from the mic\r\n  \r\n   Parameters:\r\n       device: Input device name\r\n       src: Source from input\r\n       bundle: TorchAudio pipeline\r\n  \r\n   Returns:\r\n       None'''\r\n   pipeline = InferencePipeline(bundle)\r\n  \r\n   sample_rate = bundle.sample_rate\r\n   segment_length = bundle.segment_length * bundle.hop_length\r\n   context_length = bundle.right_context_length * bundle.hop_length\r\n  \r\n   cacher = ContextCacher(segment_length, context_length)\r\n  \r\n   @torch.inference_mode()\r\n   def infer():\r\n       for _ in range(ITERATIONS):\r\n           chunk = q.get()\r\n           segment = cacher(chunk[:,0])\r\n           transcript = pipeline.infer(segment)\r\n           print(transcript, end=\"\", flush=True)\r\n  \r\n   ctx = mp.get_context(\"spawn\")\r\n   q = ctx.Queue()\r\n   p = ctx.Process(target=stream, args=(q, device, src, segment_length, sample_rate))\r\n   p.start()\r\n   infer()\r\n   p.join()\r\nif __name__ == \"__main__\":\r\n   main(\r\n       device=\"avfoundation\",\r\n       src=\":1\",\r\n       bundle=torchaudio.pipelines.EMFORMER_RNNT_BASE_LIBRISPEECH\r\n   )\r\n```\r\n\r\n## Summary of Python Speech Recognition Locally with PyTorch\r\n\r\nSpeech recognition has come a long way from its first inception. We can now use Python to do speech recognition in many ways, including with the TorchAudio library from PyTorch. The TorchAudio library comes with many pre-built models we can use for automatic speech recognition as well as many audio data manipulation tools.\r\n\r\nIn this post, we covered how to run speech recognition locally with their Emformer RNN-T. First, we created an audio data streaming function. Next, we defined a pipeline object which can run speech recognition on tensors and turn them into text. Then, we created a cache to cache the audio data as it came in. We also created a main function to handle running the audio data stream as a subprocess and run inference on it while it ran. Finally, we ran that main function with varying parameters to let it know we are streaming data in from the mic and using an Emformer RNN-T model.\r\n\r\nWant to do speech recognition without all that code? [Sign up for Deepgram](https://console.deepgram.com) today and be up and running in just a few minutes.";
}
function compiledContent() {
  return '<p>\u201CYour call may be recorded for quality assurance purposes.\u201D</p>\n<p>We\u2019ve all heard this when calling customer service. What are they doing with that call? How are they transcribing it? Back in the early 2000s and late 1990s, companies were using people to transcribe these. Now, <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> has come far along enough that we can use Python and machine learning to do automatic speech recognition.</p>\n<p>In this post, we\u2019ll focus on how to do speech recognition locally on your device using TorchAudio\u2019s pre-built Emformer RNN-T model. We will cover:</p>\n<ul>\n<li><a href="#understanding-pytorch-torchaudio">Understanding PyTorch TorchAudio</a></li>\n<li><a href="#setting-up-torchaudio-for-speech-recognition">Setting Up TorchAudio for Speech Recognition</a></li>\n<li><a href="#building-a-python-audio-data-streaming-function-for-speech-recognition">Building a Python Audio Data Streaming Function for Speech Recognition</a></li>\n<li><a href="#setting-up-python-speech-recognition-inference-pipeline">Setting up Python Speech Recognition Inference Pipeline</a></li>\n<li><a href="#creating-a-context-cache-to-store-audio-data-for-speech-recognition">Creating a Context Cache to Store Audio Data for Speech Recognition</a></li>\n<li><a href="#using-torchaudios-emformer-model-for-local-speech-recognition-in-python">Using TorchAudio\u2019s Emformer Model for Local Speech Recognition in Python</a></li>\n<li><a href="#in-summary">In Summary</a></li>\n</ul>\n<h2 id="understanding-pytorch-torchaudio">Understanding PyTorch TorchAudio</h2>\n<p>PyTorch is a versatile and powerful Python library for quickly developing machine learning models. Is it the open-source Python version of the Torch library (built on Lua) and primarily developed by Meta/Facebook. TorchAudio is an additional library to PyTorch that handles dealing with audio data for machine learning models.</p>\n<p>TorchAudio isn\u2019t just for creating machine learning models, you can also use it to do some audio data manipulation. We previously covered how to <a href="https://blog.deepgram.com/pytorch-intro-with-torchaudio/">use TorchAudio to manipulate audio data in Python</a>. In this piece, we\u2019re going to use it to build an inference pipeline to do speech recognition in real time.</p>\n<h2 id="setting-up-torchaudio-for-speech-recognition">Setting Up TorchAudio for Speech Recognition</h2>\n<p>Before we can dive into the program for doing speech recognition with TorchAudio, we need to set up our system. We need to install the <code is:raw>pytorch</code>, <code is:raw>torchaudio</code>, <code is:raw>sentencepiece</code>, and <code is:raw>ffmpeg-python</code> libraries. We can install all of these with the command <code is:raw>pip install pytorch torchaudio sentencepiece ffmpeg-python</code>. If you encounter errors, you may need to upgrade <code is:raw>pip</code>, which you can do using <code is:raw>pip install -U pip</code>.</p>\n<p>If you already have PyTorch and TorchAudio installed and you encounter an error importing <code is:raw>torchaudio</code> as I did. To get around this, force update both libraries with <code is:raw>pip install -U torch torchaudio --no-cache-dir</code>. Next, we\u2019re going to build our script. It\u2019s going to consist of an audio data function, the machine learning model pipeline, a context cache, and a main function to run.</p>\n<h2 id="building-a-python-audio-data-streaming-function-for-speech-recognition">Building a Python Audio Data Streaming Function for Speech Recognition</h2>\n<p>Now that our system is set up to work with PyTorch and Torchaudio for speech recognition, let\u2019s build our script. The first thing we need is an audio data streaming function, we\u2019ll call ours <code is:raw>stream</code>. We also declare a constant number of iterations for the streaming function to stream.</p>\n<p>Our <code is:raw>stream</code> function needs five parameters. It needs a queue, a format or device (different for Mac vs Windows), a source, a segment length, and a sample rate. There are some print statements in our function that I\u2019ve commented out. You can uncomment these to show more information about the program as it runs.</p>\n<p>The first thing we\u2019re going to do in our <code is:raw>stream</code> function is create a <code is:raw>StreamReader</code> instance from <code is:raw>torchaudio.io</code>. Next, we\u2019ll populate that stream reader with an audio stream of the passed in segment length and sample rate. I would leave the <code is:raw>"Streaming"</code> and blank <code is:raw>print</code> statements uncommented so you know when you can start talking into the mic.</p>\n<p>Next, we\u2019ll create an iterator to iterate across our stream. Finally, we\u2019ll loop through the number of predefined iterations, in our case 100, and get the next chunk of audio data from the stream iterator and put it in the queue.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> torch</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> torch.multiprocessing </span><span style="color: #FF7B72">as</span><span style="color: #C9D1D9"> mp</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> torchaudio</span></span>\n<span class="line"><span style="color: #FF7B72">from</span><span style="color: #C9D1D9"> torchaudio.io </span><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> StreamReader</span></span>\n<span class="line"><span style="color: #C9D1D9"> </span></span>\n<span class="line"><span style="color: #79C0FF">ITERATIONS</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">100</span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">stream</span><span style="color: #C9D1D9">(queue: mp.Queue(),</span></span>\n<span class="line"><span style="color: #C9D1D9">   format: </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">   src: </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">   frames_per_chunk: </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">   sample_rate: </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #A5D6FF">&#39;&#39;&#39;Streams audio data</span></span>\n<span class="line"><span style="color: #A5D6FF">  </span></span>\n<span class="line"><span style="color: #A5D6FF">   Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">       queue: Queue of data chunks</span></span>\n<span class="line"><span style="color: #A5D6FF">       format: Format</span></span>\n<span class="line"><span style="color: #A5D6FF">       src: Source</span></span>\n<span class="line"><span style="color: #A5D6FF">       frames_per_chunk: How many frames are in each data chunk</span></span>\n<span class="line"><span style="color: #A5D6FF">       sample_rate: Sample rate</span></span>\n<span class="line"><span style="color: #A5D6FF"> </span></span>\n<span class="line"><span style="color: #A5D6FF">   Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">       None&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(</span><span style="color: #A5D6FF">&quot;Initializing Audio Stream&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   streamer </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> StreamReader(src, </span><span style="color: #FFA657">format</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">format</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   streamer.add_basic_audio_stream(</span><span style="color: #FFA657">frames_per_chunk</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">frames_per_chunk,</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FFA657">sample_rate</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">sample_rate)</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(</span><span style="color: #A5D6FF">&quot;Streaming</span><span style="color: #79C0FF">\\n</span><span style="color: #A5D6FF">&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   stream_iterator </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> streamer.stream(</span><span style="color: #FFA657">timeout</span><span style="color: #FF7B72">=-</span><span style="color: #79C0FF">1</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">backoff</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">1.0</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> _ </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">range</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">ITERATIONS</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">       (chunk,) </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">next</span><span style="color: #C9D1D9">(stream_iterator)</span></span>\n<span class="line"><span style="color: #C9D1D9">       queue.put(chunk)</span></span></code></pre>\n<h2 id="setting-up-python-speech-recognition-inference-pipeline">Setting up Python Speech Recognition Inference Pipeline</h2>\n<p>Now that we have a way to smoothly stream the audio data from a microphone, we can run predictions on it. Let\u2019s build a pipeline to do speech recognition with. Unlike the part above, this time we will be creating a <code is:raw>Class</code> and not a function.</p>\n<p>We require our class to be initialized with one variable, an RNN-T model from TorchAudio\u2019s Pipelines library. We also allow a second, optional variable to define beam width, which is set at 10 by default. Our <code is:raw>__init__</code> function sets the instance\u2019s <code is:raw>bundle</code> value to the model we passed in. Then, it uses that model to get the feature extractor, decoder, and token processor. Next, it sets the beam width, the state of the pipeline, and the predictions.</p>\n<p>The other function that our <code is:raw>Pipeline</code> object has is an inference function. This function takes a PyTorch <code is:raw>Tensor</code> object and returns a string, the predicted text. This function uses the feature extractor to get the features and the length of the input tensor. Next, call the inference function from the decoder to get the hypothesis and set the state of the Pipeline. We set the instance\u2019s <code is:raw>hypothesis</code> attribute to the value in the first index of the returned hypothesis. Finally, we use the token processor on the first entry in the instance\u2019s hypothesis attribute and return that value.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">class</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">InferencePipeline</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #A5D6FF">&#39;&#39;&#39;Creates an inference pipeline for streaming audio data&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__init__</span><span style="color: #C9D1D9">(self,</span></span>\n<span class="line"><span style="color: #C9D1D9">       pipeline: torchaudio.pipelines.RNNTBundle,</span></span>\n<span class="line"><span style="color: #C9D1D9">       beam_width: </span><span style="color: #79C0FF">int</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">10</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #A5D6FF">&#39;&#39;&#39;Initializes TorchAudio RNNT Pipeline</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">           pipeline: TorchAudio Pipeline to use</span></span>\n<span class="line"><span style="color: #A5D6FF">           beam_width: Beam width</span></span>\n<span class="line"><span style="color: #A5D6FF"> </span></span>\n<span class="line"><span style="color: #A5D6FF">       Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">           None&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9"> </span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.pipeline </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.feature_extractor </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline.get_streaming_feature_extractor()</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.decoder </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline.get_decoder()</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.token_processor </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline.get_token_processor()</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.beam_width </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> beam_width</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.state </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">None</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.hypothesis </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">None</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">infer</span><span style="color: #C9D1D9">(self, segment: torch.Tensor) -&gt; </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #A5D6FF">&#39;&#39;&#39;Runs inference using the initialized pipeline</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">           segment: Torch tensor with features to extract</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">           Transcript as string type&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9"> </span></span>\n<span class="line"><span style="color: #C9D1D9">       features, length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.feature_extractor(segment)</span></span>\n<span class="line"><span style="color: #C9D1D9">       predictions, </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.state </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.decoder.infer(</span></span>\n<span class="line"><span style="color: #C9D1D9">           features, length, </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.beam_width, </span><span style="color: #FFA657">state</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.state,</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #FFA657">hypothesis</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.hypothesis</span></span>\n<span class="line"><span style="color: #C9D1D9">       )</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.hypothesis </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> predictions[</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">]</span></span>\n<span class="line"><span style="color: #C9D1D9">       transcript </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.token_processor(</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.hypothesis[</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">], </span><span style="color: #FFA657">lstrip</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">False</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> transcript</span></span></code></pre>\n<h2 id="creating-a-context-cache-to-store-audio-data-for-speech-recognition">Creating a Context Cache to Store Audio Data for Speech Recognition</h2>\n<p>We need a context cache to batch store the audio data frames as we read them. The context cache requires two parameters to initialize. First, a segment length, then a context length. The initialization function sets the instance\u2019s attributes to the passed in values and initializes a tensor of zeros of the context length as the instance\u2019s current context.</p>\n<p>The second function we\u2019ll define is the <code is:raw>call</code> function. This is another automatic function that we\u2019re overwriting. Whenever you see a class function preceded and followed by two underscores, that\u2019s a default function. In this case, we require the call function to intake a PyTorch tensor object.</p>\n<p>If the size of the first object in the tensor is less than the set segment length for the cache, we\u2019ll pad that chunk with 0s. Next, we use the concatenate function from <code is:raw>torch</code> to add that chunk to the current context. Then, we set the instance\u2019s context attribute to the last entries in the chunk equivalent to the context length. Finally, we return the concatenate context.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">class</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">ContextCacher</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__init__</span><span style="color: #C9D1D9">(self, segment_length: </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">, context_length: </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #A5D6FF">&#39;&#39;&#39;Creates initial context cache</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">           segment_length: length of one audio segment</span></span>\n<span class="line"><span style="color: #A5D6FF">           context_length: length of the context</span></span>\n<span class="line"><span style="color: #A5D6FF"> </span></span>\n<span class="line"><span style="color: #A5D6FF">       Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">           None&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.segment_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> segment_length</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> context_length</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> torch.zeros([context_length])</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__call__</span><span style="color: #C9D1D9">(self, chunk: torch.Tensor):</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #A5D6FF">&#39;&#39;&#39;Adds chunk to context and returns it</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">           chunk: chunk of audio data to process</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">           Tensor&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> chunk.size(</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">) </span><span style="color: #FF7B72">&lt;</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.segment_length:</span></span>\n<span class="line"><span style="color: #C9D1D9">           chunk </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> torch.nn.functional.pad(chunk,</span></span>\n<span class="line"><span style="color: #C9D1D9">               (</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.segment_length </span><span style="color: #FF7B72">-</span><span style="color: #C9D1D9"> chunk.size(</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">)))</span></span>\n<span class="line"><span style="color: #C9D1D9">       chunk_with_context </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> torch.cat((</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context, chunk))</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> chunk[</span><span style="color: #FF7B72">-</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context_length :]</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> chunk_with_context</span></span></code></pre>\n<h2 id="using-torchaudios-emformer-model-for-local-speech-recognition-in-python">Using TorchAudio\u2019s Emformer Model for Local Speech Recognition in Python</h2>\n<p>Finally, all our helper functions have been made. It is now time to create the main function to orchestrate the audio data streamer, the machine learning pipeline, and the cache. Our main function will take three inputs: the input device, the source, and the RNN-T model. As before, I\u2019ve left some print statements in here commented out, uncomment them if you\u2019d like.</p>\n<p>First, we create an instance of our Pipeline. This is what turns the audio data into text. Next, we use the RNN-T model to set the sample rate, the segment length, and the context length. Then, we instantiate the context with the newly gotten segment and context lengths.</p>\n<p>Now things get funky. We use an annotation on the <code is:raw>infer</code> function we\u2019re creating that signals to <code is:raw>torch</code> to turn on inference mode while this function runs. The infer function does not need any parameters. It loops through the preset number of iterations. For each iteration, it gets a chunk of data from the queue, calls the cache to return the concatenated first column entry of that chunk of data and the context, calls the pipeline to do speech recognition on that cached segment of data, and finally prints the transcript and flushes the I/O buffer.</p>\n<p>We won\u2019t actually call that <code is:raw>infer</code> function just yet. We\u2019re just defining it. Next, we import PyTorch\u2019s multiprocessing capability to spawn subprocesses. We need to spawn the audio data capture as a subprocess for correct functionality. Otherwise, it will block I/O.</p>\n<p>The first thing we do with the multiprocessing function is to signal we are spawning a new process. Second, we create a queue object in the context. Next, we create a process that runs the <code is:raw>stream</code> function from above in the context with the created queue, the passed in device, source, and model, the segment length, and the sample rate. Once we start that subprocess, we run our infer function until we end the subprocess.</p>\n<p>When the script actually runs (in the <code is:raw>if __name__ == "__main__"</code> section), we call the main function. For Mac\u2019s we pass the \u201Cavfoundation\u201D for the device and the second entry for the source. The first entry in a Mac\u2019s setup is the video.</p>\n<p>For Windows, see <a href="https://pytorch.org/audio/main/tutorials/device_asr.html">the original notebook</a>.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">main</span><span style="color: #C9D1D9">(device: </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">, src: </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">, bundle: torchaudio.pipelines):</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #A5D6FF">&#39;&#39;&#39;Transcribed audio data from the mic</span></span>\n<span class="line"><span style="color: #A5D6FF">  </span></span>\n<span class="line"><span style="color: #A5D6FF">   Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">       device: Input device name</span></span>\n<span class="line"><span style="color: #A5D6FF">       src: Source from input</span></span>\n<span class="line"><span style="color: #A5D6FF">       bundle: TorchAudio pipeline</span></span>\n<span class="line"><span style="color: #A5D6FF">  </span></span>\n<span class="line"><span style="color: #A5D6FF">   Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">       None&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9">   pipeline </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> InferencePipeline(bundle)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   sample_rate </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> bundle.sample_rate</span></span>\n<span class="line"><span style="color: #C9D1D9">   segment_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> bundle.segment_length </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> bundle.hop_length</span></span>\n<span class="line"><span style="color: #C9D1D9">   context_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> bundle.right_context_length </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> bundle.hop_length</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   cacher </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> ContextCacher(segment_length, context_length)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #D2A8FF">@torch.inference_mode</span><span style="color: #C9D1D9">()</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">infer</span><span style="color: #C9D1D9">():</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> _ </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">range</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">ITERATIONS</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">           chunk </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> q.get()</span></span>\n<span class="line"><span style="color: #C9D1D9">           segment </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> cacher(chunk[:,</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">])</span></span>\n<span class="line"><span style="color: #C9D1D9">           transcript </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline.infer(segment)</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcript, </span><span style="color: #FFA657">end</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">flush</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">True</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   ctx </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> mp.get_context(</span><span style="color: #A5D6FF">&quot;spawn&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   q </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> ctx.Queue()</span></span>\n<span class="line"><span style="color: #C9D1D9">   p </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> ctx.Process(</span><span style="color: #FFA657">target</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">stream, </span><span style="color: #FFA657">args</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">(q, device, src, segment_length, sample_rate))</span></span>\n<span class="line"><span style="color: #C9D1D9">   p.start()</span></span>\n<span class="line"><span style="color: #C9D1D9">   infer()</span></span>\n<span class="line"><span style="color: #C9D1D9">   p.join()</span></span>\n<span class="line"><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__name__</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">==</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;__main__&quot;</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">   main(</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FFA657">device</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;avfoundation&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FFA657">src</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;:1&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FFA657">bundle</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">torchaudio.pipelines.</span><span style="color: #79C0FF">EMFORMER_RNNT_BASE_LIBRISPEECH</span></span>\n<span class="line"><span style="color: #C9D1D9">   )</span></span></code></pre>\n<h2 id="full-code-for-building-a-local-streaming-audio-transcription-tool-with-pytorch-torchaudio">Full Code for Building a Local Streaming Audio Transcription Tool with PyTorch TorchAudio</h2>\n<p>That was a lot of code. Let\u2019s see how it looks all together in one file.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> torch</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> torch.multiprocessing </span><span style="color: #FF7B72">as</span><span style="color: #C9D1D9"> mp</span></span>\n<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> torchaudio</span></span>\n<span class="line"><span style="color: #FF7B72">from</span><span style="color: #C9D1D9"> torchaudio.io </span><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> StreamReader</span></span>\n<span class="line"><span style="color: #C9D1D9"> </span></span>\n<span class="line"><span style="color: #79C0FF">ITERATIONS</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">100</span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">stream</span><span style="color: #C9D1D9">(queue: mp.Queue(),</span></span>\n<span class="line"><span style="color: #C9D1D9">   format: </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">   src: </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">   frames_per_chunk: </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">   sample_rate: </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #A5D6FF">&#39;&#39;&#39;Streams audio data</span></span>\n<span class="line"><span style="color: #A5D6FF">  </span></span>\n<span class="line"><span style="color: #A5D6FF">   Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">       queue: Queue of data chunks</span></span>\n<span class="line"><span style="color: #A5D6FF">       format: Format</span></span>\n<span class="line"><span style="color: #A5D6FF">       src: Source</span></span>\n<span class="line"><span style="color: #A5D6FF">       frames_per_chunk: How many frames are in each data chunk</span></span>\n<span class="line"><span style="color: #A5D6FF">       sample_rate: Sample rate</span></span>\n<span class="line"><span style="color: #A5D6FF"> </span></span>\n<span class="line"><span style="color: #A5D6FF">   Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">       None&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(</span><span style="color: #A5D6FF">&quot;Initializing Audio Stream&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   streamer </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> StreamReader(src, </span><span style="color: #FFA657">format</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">format</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   streamer.add_basic_audio_stream(</span><span style="color: #FFA657">frames_per_chunk</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">frames_per_chunk,</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FFA657">sample_rate</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">sample_rate)</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(</span><span style="color: #A5D6FF">&quot;Streaming</span><span style="color: #79C0FF">\\n</span><span style="color: #A5D6FF">&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   stream_iterator </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> streamer.stream(</span><span style="color: #FFA657">timeout</span><span style="color: #FF7B72">=-</span><span style="color: #79C0FF">1</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">backoff</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">1.0</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> _ </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">range</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">ITERATIONS</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">       (chunk,) </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">next</span><span style="color: #C9D1D9">(stream_iterator)</span></span>\n<span class="line"><span style="color: #C9D1D9">       queue.put(chunk)</span></span>\n<span class="line"><span style="color: #C9D1D9"> </span></span>\n<span class="line"><span style="color: #FF7B72">class</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">InferencePipeline</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #A5D6FF">&#39;&#39;&#39;Creates an inference pipeline for streaming audio data&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__init__</span><span style="color: #C9D1D9">(self,</span></span>\n<span class="line"><span style="color: #C9D1D9">       pipeline: torchaudio.pipelines.RNNTBundle,</span></span>\n<span class="line"><span style="color: #C9D1D9">       beam_width: </span><span style="color: #79C0FF">int</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">10</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #A5D6FF">&#39;&#39;&#39;Initializes TorchAudio RNNT Pipeline</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">           pipeline: TorchAudio Pipeline to use</span></span>\n<span class="line"><span style="color: #A5D6FF">           beam_width: Beam width</span></span>\n<span class="line"><span style="color: #A5D6FF"> </span></span>\n<span class="line"><span style="color: #A5D6FF">       Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">           None&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9"> </span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.pipeline </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.feature_extractor </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline.get_streaming_feature_extractor()</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.decoder </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline.get_decoder()</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.token_processor </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline.get_token_processor()</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.beam_width </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> beam_width</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.state </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">None</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.hypothesis </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">None</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">infer</span><span style="color: #C9D1D9">(self, segment: torch.Tensor) -&gt; </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #A5D6FF">&#39;&#39;&#39;Runs inference using the initialized pipeline</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">           segment: Torch tensor with features to extract</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">           Transcript as string type&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9"> </span></span>\n<span class="line"><span style="color: #C9D1D9">       features, length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.feature_extractor(segment)</span></span>\n<span class="line"><span style="color: #C9D1D9">       predictions, </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.state </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.decoder.infer(</span></span>\n<span class="line"><span style="color: #C9D1D9">           features, length, </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.beam_width, </span><span style="color: #FFA657">state</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.state,</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #FFA657">hypothesis</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.hypothesis</span></span>\n<span class="line"><span style="color: #C9D1D9">       )</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.hypothesis </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> predictions[</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">]</span></span>\n<span class="line"><span style="color: #C9D1D9">       transcript </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.token_processor(</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.hypothesis[</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">], </span><span style="color: #FFA657">lstrip</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">False</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> transcript</span></span>\n<span class="line"><span style="color: #C9D1D9"> </span></span>\n<span class="line"><span style="color: #FF7B72">class</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">ContextCacher</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__init__</span><span style="color: #C9D1D9">(self, segment_length: </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">, context_length: </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #A5D6FF">&#39;&#39;&#39;Creates initial context cache</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">           segment_length: length of one audio segment</span></span>\n<span class="line"><span style="color: #A5D6FF">           context_length: length of the context</span></span>\n<span class="line"><span style="color: #A5D6FF"> </span></span>\n<span class="line"><span style="color: #A5D6FF">       Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">           None&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.segment_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> segment_length</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> context_length</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> torch.zeros([context_length])</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__call__</span><span style="color: #C9D1D9">(self, chunk: torch.Tensor):</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #A5D6FF">&#39;&#39;&#39;Adds chunk to context and returns it</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">           chunk: chunk of audio data to process</span></span>\n<span class="line"><span style="color: #A5D6FF">      </span></span>\n<span class="line"><span style="color: #A5D6FF">       Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">           Tensor&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> chunk.size(</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">) </span><span style="color: #FF7B72">&lt;</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.segment_length:</span></span>\n<span class="line"><span style="color: #C9D1D9">           chunk </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> torch.nn.functional.pad(chunk,</span></span>\n<span class="line"><span style="color: #C9D1D9">               (</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.segment_length </span><span style="color: #FF7B72">-</span><span style="color: #C9D1D9"> chunk.size(</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">)))</span></span>\n<span class="line"><span style="color: #C9D1D9">       chunk_with_context </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> torch.cat((</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context, chunk))</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> chunk[</span><span style="color: #FF7B72">-</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context_length :]</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> chunk_with_context</span></span>\n<span class="line"><span style="color: #C9D1D9"> </span></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">main</span><span style="color: #C9D1D9">(device: </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">, src: </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">, bundle: torchaudio.pipelines):</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #A5D6FF">&#39;&#39;&#39;Transcribed audio data from the mic</span></span>\n<span class="line"><span style="color: #A5D6FF">  </span></span>\n<span class="line"><span style="color: #A5D6FF">   Parameters:</span></span>\n<span class="line"><span style="color: #A5D6FF">       device: Input device name</span></span>\n<span class="line"><span style="color: #A5D6FF">       src: Source from input</span></span>\n<span class="line"><span style="color: #A5D6FF">       bundle: TorchAudio pipeline</span></span>\n<span class="line"><span style="color: #A5D6FF">  </span></span>\n<span class="line"><span style="color: #A5D6FF">   Returns:</span></span>\n<span class="line"><span style="color: #A5D6FF">       None&#39;&#39;&#39;</span></span>\n<span class="line"><span style="color: #C9D1D9">   pipeline </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> InferencePipeline(bundle)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   sample_rate </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> bundle.sample_rate</span></span>\n<span class="line"><span style="color: #C9D1D9">   segment_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> bundle.segment_length </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> bundle.hop_length</span></span>\n<span class="line"><span style="color: #C9D1D9">   context_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> bundle.right_context_length </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> bundle.hop_length</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   cacher </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> ContextCacher(segment_length, context_length)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #D2A8FF">@torch.inference_mode</span><span style="color: #C9D1D9">()</span></span>\n<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">infer</span><span style="color: #C9D1D9">():</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> _ </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">range</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">ITERATIONS</span><span style="color: #C9D1D9">):</span></span>\n<span class="line"><span style="color: #C9D1D9">           chunk </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> q.get()</span></span>\n<span class="line"><span style="color: #C9D1D9">           segment </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> cacher(chunk[:,</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">])</span></span>\n<span class="line"><span style="color: #C9D1D9">           transcript </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline.infer(segment)</span></span>\n<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcript, </span><span style="color: #FFA657">end</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">flush</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">True</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span></span>\n<span class="line"><span style="color: #C9D1D9">   ctx </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> mp.get_context(</span><span style="color: #A5D6FF">&quot;spawn&quot;</span><span style="color: #C9D1D9">)</span></span>\n<span class="line"><span style="color: #C9D1D9">   q </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> ctx.Queue()</span></span>\n<span class="line"><span style="color: #C9D1D9">   p </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> ctx.Process(</span><span style="color: #FFA657">target</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">stream, </span><span style="color: #FFA657">args</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">(q, device, src, segment_length, sample_rate))</span></span>\n<span class="line"><span style="color: #C9D1D9">   p.start()</span></span>\n<span class="line"><span style="color: #C9D1D9">   infer()</span></span>\n<span class="line"><span style="color: #C9D1D9">   p.join()</span></span>\n<span class="line"><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__name__</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">==</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;__main__&quot;</span><span style="color: #C9D1D9">:</span></span>\n<span class="line"><span style="color: #C9D1D9">   main(</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FFA657">device</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;avfoundation&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FFA657">src</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;:1&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FFA657">bundle</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">torchaudio.pipelines.</span><span style="color: #79C0FF">EMFORMER_RNNT_BASE_LIBRISPEECH</span></span>\n<span class="line"><span style="color: #C9D1D9">   )</span></span></code></pre>\n<h2 id="summary-of-python-speech-recognition-locally-with-pytorch">Summary of Python Speech Recognition Locally with PyTorch</h2>\n<p>Speech recognition has come a long way from its first inception. We can now use Python to do speech recognition in many ways, including with the TorchAudio library from PyTorch. The TorchAudio library comes with many pre-built models we can use for automatic speech recognition as well as many audio data manipulation tools.</p>\n<p>In this post, we covered how to run speech recognition locally with their Emformer RNN-T. First, we created an audio data streaming function. Next, we defined a pipeline object which can run speech recognition on tensors and turn them into text. Then, we created a cache to cache the audio data as it came in. We also created a main function to handle running the audio data stream as a subprocess and run inference on it while it ran. Finally, we ran that main function with varying parameters to let it know we are streaming data in from the mic and using an Emformer RNN-T model.</p>\n<p>Want to do speech recognition without all that code? <a href="https://console.deepgram.com">Sign up for Deepgram</a> today and be up and running in just a few minutes.</p>';
}
const $$Astro = createAstro("/Users/sandrarodgers/web-next/blog/src/content/blog/posts/python-speech-recognition-locally-torchaudio/index.md", "https://blog.deepgram.com/", "file:///Users/sandrarodgers/web-next/blog/");
const $$Index = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro, $$props, $$slots);
  Astro2.self = $$Index;
  new Slugger();
  return renderTemplate`<head>${renderHead($$result)}</head><p>“Your call may be recorded for quality assurance purposes.”</p>
<p>We’ve all heard this when calling customer service. What are they doing with that call? How are they transcribing it? Back in the early 2000s and late 1990s, companies were using people to transcribe these. Now, <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> has come far along enough that we can use Python and machine learning to do automatic speech recognition.</p>
<p>In this post, we’ll focus on how to do speech recognition locally on your device using TorchAudio’s pre-built Emformer RNN-T model. We will cover:</p>
<ul>
<li><a href="#understanding-pytorch-torchaudio">Understanding PyTorch TorchAudio</a></li>
<li><a href="#setting-up-torchaudio-for-speech-recognition">Setting Up TorchAudio for Speech Recognition</a></li>
<li><a href="#building-a-python-audio-data-streaming-function-for-speech-recognition">Building a Python Audio Data Streaming Function for Speech Recognition</a></li>
<li><a href="#setting-up-python-speech-recognition-inference-pipeline">Setting up Python Speech Recognition Inference Pipeline</a></li>
<li><a href="#creating-a-context-cache-to-store-audio-data-for-speech-recognition">Creating a Context Cache to Store Audio Data for Speech Recognition</a></li>
<li><a href="#using-torchaudios-emformer-model-for-local-speech-recognition-in-python">Using TorchAudio’s Emformer Model for Local Speech Recognition in Python</a></li>
<li><a href="#in-summary">In Summary</a></li>
</ul>
<h2 id="understanding-pytorch-torchaudio">Understanding PyTorch TorchAudio</h2>
<p>PyTorch is a versatile and powerful Python library for quickly developing machine learning models. Is it the open-source Python version of the Torch library (built on Lua) and primarily developed by Meta/Facebook. TorchAudio is an additional library to PyTorch that handles dealing with audio data for machine learning models.</p>
<p>TorchAudio isn’t just for creating machine learning models, you can also use it to do some audio data manipulation. We previously covered how to <a href="https://blog.deepgram.com/pytorch-intro-with-torchaudio/">use TorchAudio to manipulate audio data in Python</a>. In this piece, we’re going to use it to build an inference pipeline to do speech recognition in real time.</p>
<h2 id="setting-up-torchaudio-for-speech-recognition">Setting Up TorchAudio for Speech Recognition</h2>
<p>Before we can dive into the program for doing speech recognition with TorchAudio, we need to set up our system. We need to install the <code>pytorch</code>, <code>torchaudio</code>, <code>sentencepiece</code>, and <code>ffmpeg-python</code> libraries. We can install all of these with the command <code>pip install pytorch torchaudio sentencepiece ffmpeg-python</code>. If you encounter errors, you may need to upgrade <code>pip</code>, which you can do using <code>pip install -U pip</code>.</p>
<p>If you already have PyTorch and TorchAudio installed and you encounter an error importing <code>torchaudio</code> as I did. To get around this, force update both libraries with <code>pip install -U torch torchaudio --no-cache-dir</code>. Next, we’re going to build our script. It’s going to consist of an audio data function, the machine learning model pipeline, a context cache, and a main function to run.</p>
<h2 id="building-a-python-audio-data-streaming-function-for-speech-recognition">Building a Python Audio Data Streaming Function for Speech Recognition</h2>
<p>Now that our system is set up to work with PyTorch and Torchaudio for speech recognition, let’s build our script. The first thing we need is an audio data streaming function, we’ll call ours <code>stream</code>. We also declare a constant number of iterations for the streaming function to stream.</p>
<p>Our <code>stream</code> function needs five parameters. It needs a queue, a format or device (different for Mac vs Windows), a source, a segment length, and a sample rate. There are some print statements in our function that I’ve commented out. You can uncomment these to show more information about the program as it runs.</p>
<p>The first thing we’re going to do in our <code>stream</code> function is create a <code>StreamReader</code> instance from <code>torchaudio.io</code>. Next, we’ll populate that stream reader with an audio stream of the passed in segment length and sample rate. I would leave the <code>"Streaming"</code> and blank <code>print</code> statements uncommented so you know when you can start talking into the mic.</p>
<p>Next, we’ll create an iterator to iterate across our stream. Finally, we’ll loop through the number of predefined iterations, in our case 100, and get the next chunk of audio data from the stream iterator and put it in the queue.</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> torch</span></span>
<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> torch.multiprocessing </span><span style="color: #FF7B72">as</span><span style="color: #C9D1D9"> mp</span></span>
<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> torchaudio</span></span>
<span class="line"><span style="color: #FF7B72">from</span><span style="color: #C9D1D9"> torchaudio.io </span><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> StreamReader</span></span>
<span class="line"><span style="color: #C9D1D9"> </span></span>
<span class="line"><span style="color: #79C0FF">ITERATIONS</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">100</span></span>
<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">stream</span><span style="color: #C9D1D9">(queue: mp.Queue(),</span></span>
<span class="line"><span style="color: #C9D1D9">   format: </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">,</span></span>
<span class="line"><span style="color: #C9D1D9">   src: </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">,</span></span>
<span class="line"><span style="color: #C9D1D9">   frames_per_chunk: </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">,</span></span>
<span class="line"><span style="color: #C9D1D9">   sample_rate: </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">):</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #A5D6FF">&#39;&#39;&#39;Streams audio data</span></span>
<span class="line"><span style="color: #A5D6FF">  </span></span>
<span class="line"><span style="color: #A5D6FF">   Parameters:</span></span>
<span class="line"><span style="color: #A5D6FF">       queue: Queue of data chunks</span></span>
<span class="line"><span style="color: #A5D6FF">       format: Format</span></span>
<span class="line"><span style="color: #A5D6FF">       src: Source</span></span>
<span class="line"><span style="color: #A5D6FF">       frames_per_chunk: How many frames are in each data chunk</span></span>
<span class="line"><span style="color: #A5D6FF">       sample_rate: Sample rate</span></span>
<span class="line"><span style="color: #A5D6FF"> </span></span>
<span class="line"><span style="color: #A5D6FF">   Returns:</span></span>
<span class="line"><span style="color: #A5D6FF">       None&#39;&#39;&#39;</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(</span><span style="color: #A5D6FF">&quot;Initializing Audio Stream&quot;</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">   streamer </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> StreamReader(src, </span><span style="color: #FFA657">format</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">format</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">   streamer.add_basic_audio_stream(</span><span style="color: #FFA657">frames_per_chunk</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">frames_per_chunk,</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FFA657">sample_rate</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">sample_rate)</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(</span><span style="color: #A5D6FF">&quot;Streaming</span><span style="color: #79C0FF">\\n</span><span style="color: #A5D6FF">&quot;</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">   stream_iterator </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> streamer.stream(</span><span style="color: #FFA657">timeout</span><span style="color: #FF7B72">=-</span><span style="color: #79C0FF">1</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">backoff</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">1.0</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> _ </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">range</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">ITERATIONS</span><span style="color: #C9D1D9">):</span></span>
<span class="line"><span style="color: #C9D1D9">       (chunk,) </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">next</span><span style="color: #C9D1D9">(stream_iterator)</span></span>
<span class="line"><span style="color: #C9D1D9">       queue.put(chunk)</span></span></code></pre>
<h2 id="setting-up-python-speech-recognition-inference-pipeline">Setting up Python Speech Recognition Inference Pipeline</h2>
<p>Now that we have a way to smoothly stream the audio data from a microphone, we can run predictions on it. Let’s build a pipeline to do speech recognition with. Unlike the part above, this time we will be creating a <code>Class</code> and not a function.</p>
<p>We require our class to be initialized with one variable, an RNN-T model from TorchAudio’s Pipelines library. We also allow a second, optional variable to define beam width, which is set at 10 by default. Our <code>__init__</code> function sets the instance’s <code>bundle</code> value to the model we passed in. Then, it uses that model to get the feature extractor, decoder, and token processor. Next, it sets the beam width, the state of the pipeline, and the predictions.</p>
<p>The other function that our <code>Pipeline</code> object has is an inference function. This function takes a PyTorch <code>Tensor</code> object and returns a string, the predicted text. This function uses the feature extractor to get the features and the length of the input tensor. Next, call the inference function from the decoder to get the hypothesis and set the state of the Pipeline. We set the instance’s <code>hypothesis</code> attribute to the value in the first index of the returned hypothesis. Finally, we use the token processor on the first entry in the instance’s hypothesis attribute and return that value.</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">class</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">InferencePipeline</span><span style="color: #C9D1D9">:</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #A5D6FF">&#39;&#39;&#39;Creates an inference pipeline for streaming audio data&#39;&#39;&#39;</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__init__</span><span style="color: #C9D1D9">(self,</span></span>
<span class="line"><span style="color: #C9D1D9">       pipeline: torchaudio.pipelines.RNNTBundle,</span></span>
<span class="line"><span style="color: #C9D1D9">       beam_width: </span><span style="color: #79C0FF">int</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">10</span><span style="color: #C9D1D9">):</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #A5D6FF">&#39;&#39;&#39;Initializes TorchAudio RNNT Pipeline</span></span>
<span class="line"><span style="color: #A5D6FF">      </span></span>
<span class="line"><span style="color: #A5D6FF">       Parameters:</span></span>
<span class="line"><span style="color: #A5D6FF">           pipeline: TorchAudio Pipeline to use</span></span>
<span class="line"><span style="color: #A5D6FF">           beam_width: Beam width</span></span>
<span class="line"><span style="color: #A5D6FF"> </span></span>
<span class="line"><span style="color: #A5D6FF">       Returns:</span></span>
<span class="line"><span style="color: #A5D6FF">           None&#39;&#39;&#39;</span></span>
<span class="line"><span style="color: #C9D1D9"> </span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.pipeline </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.feature_extractor </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline.get_streaming_feature_extractor()</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.decoder </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline.get_decoder()</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.token_processor </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline.get_token_processor()</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.beam_width </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> beam_width</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.state </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">None</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.hypothesis </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">None</span></span>
<span class="line"><span style="color: #C9D1D9">  </span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">infer</span><span style="color: #C9D1D9">(self, segment: torch.Tensor) -&gt; </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">:</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #A5D6FF">&#39;&#39;&#39;Runs inference using the initialized pipeline</span></span>
<span class="line"><span style="color: #A5D6FF">      </span></span>
<span class="line"><span style="color: #A5D6FF">       Parameters:</span></span>
<span class="line"><span style="color: #A5D6FF">           segment: Torch tensor with features to extract</span></span>
<span class="line"><span style="color: #A5D6FF">      </span></span>
<span class="line"><span style="color: #A5D6FF">       Returns:</span></span>
<span class="line"><span style="color: #A5D6FF">           Transcript as string type&#39;&#39;&#39;</span></span>
<span class="line"><span style="color: #C9D1D9"> </span></span>
<span class="line"><span style="color: #C9D1D9">       features, length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.feature_extractor(segment)</span></span>
<span class="line"><span style="color: #C9D1D9">       predictions, </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.state </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.decoder.infer(</span></span>
<span class="line"><span style="color: #C9D1D9">           features, length, </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.beam_width, </span><span style="color: #FFA657">state</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.state,</span></span>
<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #FFA657">hypothesis</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.hypothesis</span></span>
<span class="line"><span style="color: #C9D1D9">       )</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.hypothesis </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> predictions[</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">]</span></span>
<span class="line"><span style="color: #C9D1D9">       transcript </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.token_processor(</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.hypothesis[</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">], </span><span style="color: #FFA657">lstrip</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">False</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> transcript</span></span></code></pre>
<h2 id="creating-a-context-cache-to-store-audio-data-for-speech-recognition">Creating a Context Cache to Store Audio Data for Speech Recognition</h2>
<p>We need a context cache to batch store the audio data frames as we read them. The context cache requires two parameters to initialize. First, a segment length, then a context length. The initialization function sets the instance’s attributes to the passed in values and initializes a tensor of zeros of the context length as the instance’s current context.</p>
<p>The second function we’ll define is the <code>call</code> function. This is another automatic function that we’re overwriting. Whenever you see a class function preceded and followed by two underscores, that’s a default function. In this case, we require the call function to intake a PyTorch tensor object.</p>
<p>If the size of the first object in the tensor is less than the set segment length for the cache, we’ll pad that chunk with 0s. Next, we use the concatenate function from <code>torch</code> to add that chunk to the current context. Then, we set the instance’s context attribute to the last entries in the chunk equivalent to the context length. Finally, we return the concatenate context.</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">class</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">ContextCacher</span><span style="color: #C9D1D9">:</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__init__</span><span style="color: #C9D1D9">(self, segment_length: </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">, context_length: </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">):</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #A5D6FF">&#39;&#39;&#39;Creates initial context cache</span></span>
<span class="line"><span style="color: #A5D6FF">      </span></span>
<span class="line"><span style="color: #A5D6FF">       Parameters:</span></span>
<span class="line"><span style="color: #A5D6FF">           segment_length: length of one audio segment</span></span>
<span class="line"><span style="color: #A5D6FF">           context_length: length of the context</span></span>
<span class="line"><span style="color: #A5D6FF"> </span></span>
<span class="line"><span style="color: #A5D6FF">       Returns:</span></span>
<span class="line"><span style="color: #A5D6FF">           None&#39;&#39;&#39;</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.segment_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> segment_length</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> context_length</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> torch.zeros([context_length])</span></span>
<span class="line"><span style="color: #C9D1D9">  </span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__call__</span><span style="color: #C9D1D9">(self, chunk: torch.Tensor):</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #A5D6FF">&#39;&#39;&#39;Adds chunk to context and returns it</span></span>
<span class="line"><span style="color: #A5D6FF">      </span></span>
<span class="line"><span style="color: #A5D6FF">       Parameters:</span></span>
<span class="line"><span style="color: #A5D6FF">           chunk: chunk of audio data to process</span></span>
<span class="line"><span style="color: #A5D6FF">      </span></span>
<span class="line"><span style="color: #A5D6FF">       Returns:</span></span>
<span class="line"><span style="color: #A5D6FF">           Tensor&#39;&#39;&#39;</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> chunk.size(</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">) </span><span style="color: #FF7B72">&lt;</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.segment_length:</span></span>
<span class="line"><span style="color: #C9D1D9">           chunk </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> torch.nn.functional.pad(chunk,</span></span>
<span class="line"><span style="color: #C9D1D9">               (</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.segment_length </span><span style="color: #FF7B72">-</span><span style="color: #C9D1D9"> chunk.size(</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">)))</span></span>
<span class="line"><span style="color: #C9D1D9">       chunk_with_context </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> torch.cat((</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context, chunk))</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> chunk[</span><span style="color: #FF7B72">-</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context_length :]</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> chunk_with_context</span></span></code></pre>
<h2 id="using-torchaudios-emformer-model-for-local-speech-recognition-in-python">Using TorchAudio’s Emformer Model for Local Speech Recognition in Python</h2>
<p>Finally, all our helper functions have been made. It is now time to create the main function to orchestrate the audio data streamer, the machine learning pipeline, and the cache. Our main function will take three inputs: the input device, the source, and the RNN-T model. As before, I’ve left some print statements in here commented out, uncomment them if you’d like.</p>
<p>First, we create an instance of our Pipeline. This is what turns the audio data into text. Next, we use the RNN-T model to set the sample rate, the segment length, and the context length. Then, we instantiate the context with the newly gotten segment and context lengths.</p>
<p>Now things get funky. We use an annotation on the <code>infer</code> function we’re creating that signals to <code>torch</code> to turn on inference mode while this function runs. The infer function does not need any parameters. It loops through the preset number of iterations. For each iteration, it gets a chunk of data from the queue, calls the cache to return the concatenated first column entry of that chunk of data and the context, calls the pipeline to do speech recognition on that cached segment of data, and finally prints the transcript and flushes the I/O buffer.</p>
<p>We won’t actually call that <code>infer</code> function just yet. We’re just defining it. Next, we import PyTorch’s multiprocessing capability to spawn subprocesses. We need to spawn the audio data capture as a subprocess for correct functionality. Otherwise, it will block I/O.</p>
<p>The first thing we do with the multiprocessing function is to signal we are spawning a new process. Second, we create a queue object in the context. Next, we create a process that runs the <code>stream</code> function from above in the context with the created queue, the passed in device, source, and model, the segment length, and the sample rate. Once we start that subprocess, we run our infer function until we end the subprocess.</p>
<p>When the script actually runs (in the <code>if __name__ == "__main__"</code> section), we call the main function. For Mac’s we pass the “avfoundation” for the device and the second entry for the source. The first entry in a Mac’s setup is the video.</p>
<p>For Windows, see <a href="https://pytorch.org/audio/main/tutorials/device_asr.html">the original notebook</a>.</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">main</span><span style="color: #C9D1D9">(device: </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">, src: </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">, bundle: torchaudio.pipelines):</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #A5D6FF">&#39;&#39;&#39;Transcribed audio data from the mic</span></span>
<span class="line"><span style="color: #A5D6FF">  </span></span>
<span class="line"><span style="color: #A5D6FF">   Parameters:</span></span>
<span class="line"><span style="color: #A5D6FF">       device: Input device name</span></span>
<span class="line"><span style="color: #A5D6FF">       src: Source from input</span></span>
<span class="line"><span style="color: #A5D6FF">       bundle: TorchAudio pipeline</span></span>
<span class="line"><span style="color: #A5D6FF">  </span></span>
<span class="line"><span style="color: #A5D6FF">   Returns:</span></span>
<span class="line"><span style="color: #A5D6FF">       None&#39;&#39;&#39;</span></span>
<span class="line"><span style="color: #C9D1D9">   pipeline </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> InferencePipeline(bundle)</span></span>
<span class="line"><span style="color: #C9D1D9">  </span></span>
<span class="line"><span style="color: #C9D1D9">   sample_rate </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> bundle.sample_rate</span></span>
<span class="line"><span style="color: #C9D1D9">   segment_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> bundle.segment_length </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> bundle.hop_length</span></span>
<span class="line"><span style="color: #C9D1D9">   context_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> bundle.right_context_length </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> bundle.hop_length</span></span>
<span class="line"><span style="color: #C9D1D9">  </span></span>
<span class="line"><span style="color: #C9D1D9">   cacher </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> ContextCacher(segment_length, context_length)</span></span>
<span class="line"><span style="color: #C9D1D9">  </span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #D2A8FF">@torch.inference_mode</span><span style="color: #C9D1D9">()</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">infer</span><span style="color: #C9D1D9">():</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> _ </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">range</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">ITERATIONS</span><span style="color: #C9D1D9">):</span></span>
<span class="line"><span style="color: #C9D1D9">           chunk </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> q.get()</span></span>
<span class="line"><span style="color: #C9D1D9">           segment </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> cacher(chunk[:,</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">])</span></span>
<span class="line"><span style="color: #C9D1D9">           transcript </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline.infer(segment)</span></span>
<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcript, </span><span style="color: #FFA657">end</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">flush</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">True</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">  </span></span>
<span class="line"><span style="color: #C9D1D9">   ctx </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> mp.get_context(</span><span style="color: #A5D6FF">&quot;spawn&quot;</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">   q </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> ctx.Queue()</span></span>
<span class="line"><span style="color: #C9D1D9">   p </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> ctx.Process(</span><span style="color: #FFA657">target</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">stream, </span><span style="color: #FFA657">args</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">(q, device, src, segment_length, sample_rate))</span></span>
<span class="line"><span style="color: #C9D1D9">   p.start()</span></span>
<span class="line"><span style="color: #C9D1D9">   infer()</span></span>
<span class="line"><span style="color: #C9D1D9">   p.join()</span></span>
<span class="line"><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__name__</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">==</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;__main__&quot;</span><span style="color: #C9D1D9">:</span></span>
<span class="line"><span style="color: #C9D1D9">   main(</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FFA657">device</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;avfoundation&quot;</span><span style="color: #C9D1D9">,</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FFA657">src</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;:1&quot;</span><span style="color: #C9D1D9">,</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FFA657">bundle</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">torchaudio.pipelines.</span><span style="color: #79C0FF">EMFORMER_RNNT_BASE_LIBRISPEECH</span></span>
<span class="line"><span style="color: #C9D1D9">   )</span></span></code></pre>
<h2 id="full-code-for-building-a-local-streaming-audio-transcription-tool-with-pytorch-torchaudio">Full Code for Building a Local Streaming Audio Transcription Tool with PyTorch TorchAudio</h2>
<p>That was a lot of code. Let’s see how it looks all together in one file.</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> torch</span></span>
<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> torch.multiprocessing </span><span style="color: #FF7B72">as</span><span style="color: #C9D1D9"> mp</span></span>
<span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> torchaudio</span></span>
<span class="line"><span style="color: #FF7B72">from</span><span style="color: #C9D1D9"> torchaudio.io </span><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> StreamReader</span></span>
<span class="line"><span style="color: #C9D1D9"> </span></span>
<span class="line"><span style="color: #79C0FF">ITERATIONS</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">100</span></span>
<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">stream</span><span style="color: #C9D1D9">(queue: mp.Queue(),</span></span>
<span class="line"><span style="color: #C9D1D9">   format: </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">,</span></span>
<span class="line"><span style="color: #C9D1D9">   src: </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">,</span></span>
<span class="line"><span style="color: #C9D1D9">   frames_per_chunk: </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">,</span></span>
<span class="line"><span style="color: #C9D1D9">   sample_rate: </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">):</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #A5D6FF">&#39;&#39;&#39;Streams audio data</span></span>
<span class="line"><span style="color: #A5D6FF">  </span></span>
<span class="line"><span style="color: #A5D6FF">   Parameters:</span></span>
<span class="line"><span style="color: #A5D6FF">       queue: Queue of data chunks</span></span>
<span class="line"><span style="color: #A5D6FF">       format: Format</span></span>
<span class="line"><span style="color: #A5D6FF">       src: Source</span></span>
<span class="line"><span style="color: #A5D6FF">       frames_per_chunk: How many frames are in each data chunk</span></span>
<span class="line"><span style="color: #A5D6FF">       sample_rate: Sample rate</span></span>
<span class="line"><span style="color: #A5D6FF"> </span></span>
<span class="line"><span style="color: #A5D6FF">   Returns:</span></span>
<span class="line"><span style="color: #A5D6FF">       None&#39;&#39;&#39;</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(</span><span style="color: #A5D6FF">&quot;Initializing Audio Stream&quot;</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">   streamer </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> StreamReader(src, </span><span style="color: #FFA657">format</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">format</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">   streamer.add_basic_audio_stream(</span><span style="color: #FFA657">frames_per_chunk</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">frames_per_chunk,</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FFA657">sample_rate</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">sample_rate)</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(</span><span style="color: #A5D6FF">&quot;Streaming</span><span style="color: #79C0FF">\\n</span><span style="color: #A5D6FF">&quot;</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">   stream_iterator </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> streamer.stream(</span><span style="color: #FFA657">timeout</span><span style="color: #FF7B72">=-</span><span style="color: #79C0FF">1</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">backoff</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">1.0</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> _ </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">range</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">ITERATIONS</span><span style="color: #C9D1D9">):</span></span>
<span class="line"><span style="color: #C9D1D9">       (chunk,) </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">next</span><span style="color: #C9D1D9">(stream_iterator)</span></span>
<span class="line"><span style="color: #C9D1D9">       queue.put(chunk)</span></span>
<span class="line"><span style="color: #C9D1D9"> </span></span>
<span class="line"><span style="color: #FF7B72">class</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">InferencePipeline</span><span style="color: #C9D1D9">:</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #A5D6FF">&#39;&#39;&#39;Creates an inference pipeline for streaming audio data&#39;&#39;&#39;</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__init__</span><span style="color: #C9D1D9">(self,</span></span>
<span class="line"><span style="color: #C9D1D9">       pipeline: torchaudio.pipelines.RNNTBundle,</span></span>
<span class="line"><span style="color: #C9D1D9">       beam_width: </span><span style="color: #79C0FF">int</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">10</span><span style="color: #C9D1D9">):</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #A5D6FF">&#39;&#39;&#39;Initializes TorchAudio RNNT Pipeline</span></span>
<span class="line"><span style="color: #A5D6FF">      </span></span>
<span class="line"><span style="color: #A5D6FF">       Parameters:</span></span>
<span class="line"><span style="color: #A5D6FF">           pipeline: TorchAudio Pipeline to use</span></span>
<span class="line"><span style="color: #A5D6FF">           beam_width: Beam width</span></span>
<span class="line"><span style="color: #A5D6FF"> </span></span>
<span class="line"><span style="color: #A5D6FF">       Returns:</span></span>
<span class="line"><span style="color: #A5D6FF">           None&#39;&#39;&#39;</span></span>
<span class="line"><span style="color: #C9D1D9"> </span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.pipeline </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.feature_extractor </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline.get_streaming_feature_extractor()</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.decoder </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline.get_decoder()</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.token_processor </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline.get_token_processor()</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.beam_width </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> beam_width</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.state </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">None</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.hypothesis </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">None</span></span>
<span class="line"><span style="color: #C9D1D9">  </span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">infer</span><span style="color: #C9D1D9">(self, segment: torch.Tensor) -&gt; </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">:</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #A5D6FF">&#39;&#39;&#39;Runs inference using the initialized pipeline</span></span>
<span class="line"><span style="color: #A5D6FF">      </span></span>
<span class="line"><span style="color: #A5D6FF">       Parameters:</span></span>
<span class="line"><span style="color: #A5D6FF">           segment: Torch tensor with features to extract</span></span>
<span class="line"><span style="color: #A5D6FF">      </span></span>
<span class="line"><span style="color: #A5D6FF">       Returns:</span></span>
<span class="line"><span style="color: #A5D6FF">           Transcript as string type&#39;&#39;&#39;</span></span>
<span class="line"><span style="color: #C9D1D9"> </span></span>
<span class="line"><span style="color: #C9D1D9">       features, length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.feature_extractor(segment)</span></span>
<span class="line"><span style="color: #C9D1D9">       predictions, </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.state </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.decoder.infer(</span></span>
<span class="line"><span style="color: #C9D1D9">           features, length, </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.beam_width, </span><span style="color: #FFA657">state</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.state,</span></span>
<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #FFA657">hypothesis</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.hypothesis</span></span>
<span class="line"><span style="color: #C9D1D9">       )</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.hypothesis </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> predictions[</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">]</span></span>
<span class="line"><span style="color: #C9D1D9">       transcript </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.token_processor(</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.hypothesis[</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">], </span><span style="color: #FFA657">lstrip</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">False</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> transcript</span></span>
<span class="line"><span style="color: #C9D1D9"> </span></span>
<span class="line"><span style="color: #FF7B72">class</span><span style="color: #C9D1D9"> </span><span style="color: #FFA657">ContextCacher</span><span style="color: #C9D1D9">:</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__init__</span><span style="color: #C9D1D9">(self, segment_length: </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">, context_length: </span><span style="color: #79C0FF">int</span><span style="color: #C9D1D9">):</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #A5D6FF">&#39;&#39;&#39;Creates initial context cache</span></span>
<span class="line"><span style="color: #A5D6FF">      </span></span>
<span class="line"><span style="color: #A5D6FF">       Parameters:</span></span>
<span class="line"><span style="color: #A5D6FF">           segment_length: length of one audio segment</span></span>
<span class="line"><span style="color: #A5D6FF">           context_length: length of the context</span></span>
<span class="line"><span style="color: #A5D6FF"> </span></span>
<span class="line"><span style="color: #A5D6FF">       Returns:</span></span>
<span class="line"><span style="color: #A5D6FF">           None&#39;&#39;&#39;</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.segment_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> segment_length</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> context_length</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> torch.zeros([context_length])</span></span>
<span class="line"><span style="color: #C9D1D9">  </span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__call__</span><span style="color: #C9D1D9">(self, chunk: torch.Tensor):</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #A5D6FF">&#39;&#39;&#39;Adds chunk to context and returns it</span></span>
<span class="line"><span style="color: #A5D6FF">      </span></span>
<span class="line"><span style="color: #A5D6FF">       Parameters:</span></span>
<span class="line"><span style="color: #A5D6FF">           chunk: chunk of audio data to process</span></span>
<span class="line"><span style="color: #A5D6FF">      </span></span>
<span class="line"><span style="color: #A5D6FF">       Returns:</span></span>
<span class="line"><span style="color: #A5D6FF">           Tensor&#39;&#39;&#39;</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> chunk.size(</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">) </span><span style="color: #FF7B72">&lt;</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.segment_length:</span></span>
<span class="line"><span style="color: #C9D1D9">           chunk </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> torch.nn.functional.pad(chunk,</span></span>
<span class="line"><span style="color: #C9D1D9">               (</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">, </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.segment_length </span><span style="color: #FF7B72">-</span><span style="color: #C9D1D9"> chunk.size(</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">)))</span></span>
<span class="line"><span style="color: #C9D1D9">       chunk_with_context </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> torch.cat((</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context, chunk))</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> chunk[</span><span style="color: #FF7B72">-</span><span style="color: #79C0FF">self</span><span style="color: #C9D1D9">.context_length :]</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> chunk_with_context</span></span>
<span class="line"><span style="color: #C9D1D9"> </span></span>
<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">main</span><span style="color: #C9D1D9">(device: </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">, src: </span><span style="color: #79C0FF">str</span><span style="color: #C9D1D9">, bundle: torchaudio.pipelines):</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #A5D6FF">&#39;&#39;&#39;Transcribed audio data from the mic</span></span>
<span class="line"><span style="color: #A5D6FF">  </span></span>
<span class="line"><span style="color: #A5D6FF">   Parameters:</span></span>
<span class="line"><span style="color: #A5D6FF">       device: Input device name</span></span>
<span class="line"><span style="color: #A5D6FF">       src: Source from input</span></span>
<span class="line"><span style="color: #A5D6FF">       bundle: TorchAudio pipeline</span></span>
<span class="line"><span style="color: #A5D6FF">  </span></span>
<span class="line"><span style="color: #A5D6FF">   Returns:</span></span>
<span class="line"><span style="color: #A5D6FF">       None&#39;&#39;&#39;</span></span>
<span class="line"><span style="color: #C9D1D9">   pipeline </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> InferencePipeline(bundle)</span></span>
<span class="line"><span style="color: #C9D1D9">  </span></span>
<span class="line"><span style="color: #C9D1D9">   sample_rate </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> bundle.sample_rate</span></span>
<span class="line"><span style="color: #C9D1D9">   segment_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> bundle.segment_length </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> bundle.hop_length</span></span>
<span class="line"><span style="color: #C9D1D9">   context_length </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> bundle.right_context_length </span><span style="color: #FF7B72">*</span><span style="color: #C9D1D9"> bundle.hop_length</span></span>
<span class="line"><span style="color: #C9D1D9">  </span></span>
<span class="line"><span style="color: #C9D1D9">   cacher </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> ContextCacher(segment_length, context_length)</span></span>
<span class="line"><span style="color: #C9D1D9">  </span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #D2A8FF">@torch.inference_mode</span><span style="color: #C9D1D9">()</span></span>
<span class="line"><span style="color: #C9D1D9">   </span><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">infer</span><span style="color: #C9D1D9">():</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FF7B72">for</span><span style="color: #C9D1D9"> _ </span><span style="color: #FF7B72">in</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">range</span><span style="color: #C9D1D9">(</span><span style="color: #79C0FF">ITERATIONS</span><span style="color: #C9D1D9">):</span></span>
<span class="line"><span style="color: #C9D1D9">           chunk </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> q.get()</span></span>
<span class="line"><span style="color: #C9D1D9">           segment </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> cacher(chunk[:,</span><span style="color: #79C0FF">0</span><span style="color: #C9D1D9">])</span></span>
<span class="line"><span style="color: #C9D1D9">           transcript </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> pipeline.infer(segment)</span></span>
<span class="line"><span style="color: #C9D1D9">           </span><span style="color: #79C0FF">print</span><span style="color: #C9D1D9">(transcript, </span><span style="color: #FFA657">end</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;&quot;</span><span style="color: #C9D1D9">, </span><span style="color: #FFA657">flush</span><span style="color: #FF7B72">=</span><span style="color: #79C0FF">True</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">  </span></span>
<span class="line"><span style="color: #C9D1D9">   ctx </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> mp.get_context(</span><span style="color: #A5D6FF">&quot;spawn&quot;</span><span style="color: #C9D1D9">)</span></span>
<span class="line"><span style="color: #C9D1D9">   q </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> ctx.Queue()</span></span>
<span class="line"><span style="color: #C9D1D9">   p </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> ctx.Process(</span><span style="color: #FFA657">target</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">stream, </span><span style="color: #FFA657">args</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">(q, device, src, segment_length, sample_rate))</span></span>
<span class="line"><span style="color: #C9D1D9">   p.start()</span></span>
<span class="line"><span style="color: #C9D1D9">   infer()</span></span>
<span class="line"><span style="color: #C9D1D9">   p.join()</span></span>
<span class="line"><span style="color: #FF7B72">if</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">__name__</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">==</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;__main__&quot;</span><span style="color: #C9D1D9">:</span></span>
<span class="line"><span style="color: #C9D1D9">   main(</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FFA657">device</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;avfoundation&quot;</span><span style="color: #C9D1D9">,</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FFA657">src</span><span style="color: #FF7B72">=</span><span style="color: #A5D6FF">&quot;:1&quot;</span><span style="color: #C9D1D9">,</span></span>
<span class="line"><span style="color: #C9D1D9">       </span><span style="color: #FFA657">bundle</span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9">torchaudio.pipelines.</span><span style="color: #79C0FF">EMFORMER_RNNT_BASE_LIBRISPEECH</span></span>
<span class="line"><span style="color: #C9D1D9">   )</span></span></code></pre>
<h2 id="summary-of-python-speech-recognition-locally-with-pytorch">Summary of Python Speech Recognition Locally with PyTorch</h2>
<p>Speech recognition has come a long way from its first inception. We can now use Python to do speech recognition in many ways, including with the TorchAudio library from PyTorch. The TorchAudio library comes with many pre-built models we can use for automatic speech recognition as well as many audio data manipulation tools.</p>
<p>In this post, we covered how to run speech recognition locally with their Emformer RNN-T. First, we created an audio data streaming function. Next, we defined a pipeline object which can run speech recognition on tensors and turn them into text. Then, we created a cache to cache the audio data as it came in. We also created a main function to handle running the audio data stream as a subprocess and run inference on it while it ran. Finally, we ran that main function with varying parameters to let it know we are streaming data in from the mic and using an Emformer RNN-T model.</p>
<p>Want to do speech recognition without all that code? <a href="https://console.deepgram.com">Sign up for Deepgram</a> today and be up and running in just a few minutes.</p>`;
}, "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/python-speech-recognition-locally-torchaudio/index.md");

export { compiledContent, $$Index as default, frontmatter, metadata, rawContent };
