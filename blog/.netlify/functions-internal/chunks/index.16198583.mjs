import { c as createAstro, a as createComponent, r as renderTemplate, b as renderHead } from '../entry.mjs';
import Slugger from 'github-slugger';
import '@astrojs/netlify/netlify-functions.js';
import 'preact';
import 'preact-render-to-string';
import 'vue';
import 'vue/server-renderer';
import 'html-escaper';
import 'node-html-parser';
/* empty css                           */import 'axios';
/* empty css                          *//* empty css                           *//* empty css                          *//* empty css                              *//* empty css                              */import 'clone-deep';
import 'slugify';
import 'shiki';
/* empty css                           *//* empty css                              */import '@astrojs/rss';
/* empty css                           */import 'mime';
import 'cookie';
import 'kleur/colors';
import 'string-width';
import 'path-browserify';
import 'path-to-regexp';

const metadata = { "headings": [{ "depth": 2, "slug": "how-it-works", "text": "How It Works" }, { "depth": 2, "slug": "try-it-out", "text": "Try It Out" }], "source": "\r\nThe team behind yack! wanted to use Deepgram and computer vision to build a fun and novel project. What came out of it was an automatic video to comic generator, which has more complexity than you might think. I sat down with [Allan Zhang](https://github.com/WeixuanZ), [Andreas Economides](https://github.com/antroseco/), [Felix Chippendale](https://github.com/FChippendale), and [Tom Grant](https://github.com/DaveDuck321/) to ask them about their project.\r\n\r\nyack! takes a video and restyles it as a classic comic book using Deepgram's Speech Recognition API and computer vision. The output looks a bit like this:\r\n\r\n![A comic book with four panes. In each pane is Mark Zuckerberg. In the first and third is a speech box with text.](https://res.cloudinary.com/deepgram/image/upload/v1646083983/blog/2022/03/comic-books-videos-yack/screenshot.jpg)\r\n\r\nOnce a video is provided, yack! generates a transcript with Deepgram. Then, keyframes in the video are chosen. Frames are cropped, the image has some comic book styling applied, and captions are overlaid as speech bubbles. Finally, each 'tile' is placed in a dynamic SVG element which is rendered on the page.\r\n\r\nThat's... a lot.\r\n\r\n## How It Works\r\n\r\nThe team got Deepgram working within the first hour of building, which freed the team to focus on more complex parts of the project. To make the returned transcript as useful as possible, they used our [utterances](https://developers.deepgram.com/documentation/features/utterances/) feature to understand what keyframes to show and [diarization](https://developers.deepgram.com/documentation/features/diarize/) to color text when different speakers are detected.\r\n\r\nOnce a key frame is chosen, computer vision is used to detect a speaker's location in the frame. It is then cropped to ensure faces are seen, that there's enough space for text to be overlaid, and that the aspect ratio is roughly maintained. During development, the face detection algorithm was one of the slowest parts -- taking up to 20 seconds -- though the team managed to speed this up slightly.\r\n\r\nThe style transfer then took place -- a set of simple visual tricks to make a real-life image look more comic-like -- reducing colors, finding edges and making them darker/bolder, and stacking. This was by far the slowest bit of the overall processing time - accounting for around 60%. Given more time this could be done with machine learning.\r\n\r\nFinally, the text is overlaid, and a dynamic SVG is created. The placement of tiles is, in itself, an engineering challenge. The team used a block-claiming algorithm to have times 'claim' space on the page.\r\n\r\n## Try It Out\r\n\r\nThe yack! team built a website for users to interact with and a Docker image to create a portable, scalable, and easily-deployable server.\r\n\r\nYou can try out yack! at [yack.ml](https://yack.ml)\r\n\r\n        ", "html": '<p>The team behind yack! wanted to use Deepgram and computer vision to build a fun and novel project. What came out of it was an automatic video to comic generator, which has more complexity than you might think. I sat down with <a href="https://github.com/WeixuanZ">Allan Zhang</a>, <a href="https://github.com/antroseco/">Andreas Economides</a>, <a href="https://github.com/FChippendale">Felix Chippendale</a>, and <a href="https://github.com/DaveDuck321/">Tom Grant</a> to ask them about their project.</p>\n<p>yack! takes a video and restyles it as a classic comic book using Deepgram\u2019s Speech Recognition API and computer vision. The output looks a bit like this:</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1646083983/blog/2022/03/comic-books-videos-yack/screenshot.jpg" alt="A comic book with four panes. In each pane is Mark Zuckerberg. In the first and third is a speech box with text."></p>\n<p>Once a video is provided, yack! generates a transcript with Deepgram. Then, keyframes in the video are chosen. Frames are cropped, the image has some comic book styling applied, and captions are overlaid as speech bubbles. Finally, each \u2018tile\u2019 is placed in a dynamic SVG element which is rendered on the page.</p>\n<p>That\u2019s\u2026 a lot.</p>\n<h2 id="how-it-works">How It Works</h2>\n<p>The team got Deepgram working within the first hour of building, which freed the team to focus on more complex parts of the project. To make the returned transcript as useful as possible, they used our <a href="https://developers.deepgram.com/documentation/features/utterances/">utterances</a> feature to understand what keyframes to show and <a href="https://developers.deepgram.com/documentation/features/diarize/">diarization</a> to color text when different speakers are detected.</p>\n<p>Once a key frame is chosen, computer vision is used to detect a speaker\u2019s location in the frame. It is then cropped to ensure faces are seen, that there\u2019s enough space for text to be overlaid, and that the aspect ratio is roughly maintained. During development, the face detection algorithm was one of the slowest parts \u2014 taking up to 20 seconds \u2014 though the team managed to speed this up slightly.</p>\n<p>The style transfer then took place \u2014 a set of simple visual tricks to make a real-life image look more comic-like \u2014 reducing colors, finding edges and making them darker/bolder, and stacking. This was by far the slowest bit of the overall processing time - accounting for around 60%. Given more time this could be done with machine learning.</p>\n<p>Finally, the text is overlaid, and a dynamic SVG is created. The placement of tiles is, in itself, an engineering challenge. The team used a block-claiming algorithm to have times \u2018claim\u2019 space on the page.</p>\n<h2 id="try-it-out">Try It Out</h2>\n<p>The yack! team built a website for users to interact with and a Docker image to create a portable, scalable, and easily-deployable server.</p>\n<p>You can try out yack! at <a href="https://yack.ml">yack.ml</a></p>' };
const frontmatter = { "title": "Create Comic Books From Videos with yack!", "description": "Developers built a video and restyle it as a classic comic book using Deepgram's Speech Recognition API and computer vision. See how here!", "date": "2022-03-09T00:00:00.000Z", "cover": "https://res.cloudinary.com/deepgram/image/upload/v1646083965/blog/2022/03/comic-books-videos-yack/yack.jpg", "authors": ["kevin-lewis"], "category": "project-showcase", "tags": ["computer-vision", "hackathon"], "seo": { "title": "Create Comic Books From Videos with yack!", "description": "Developers built a video and restyle it as a classic comic book using Deepgram's Speech Recognition API and computer vision. See how here!" }, "shorturls": { "share": "https://dpgr.am/f0ae853", "twitter": "https://dpgr.am/4bb8f13", "linkedin": "https://dpgr.am/ed4384a", "reddit": "https://dpgr.am/3530152", "facebook": "https://dpgr.am/c7e98e3" }, "og": { "image": "https://res.cloudinary.com/deepgram/image/upload/v1661454016/blog/comic-books-videos-yack/ograph.png" }, "astro": { "headings": [{ "depth": 2, "slug": "how-it-works", "text": "How It Works" }, { "depth": 2, "slug": "try-it-out", "text": "Try It Out" }], "source": "\r\nThe team behind yack! wanted to use Deepgram and computer vision to build a fun and novel project. What came out of it was an automatic video to comic generator, which has more complexity than you might think. I sat down with [Allan Zhang](https://github.com/WeixuanZ), [Andreas Economides](https://github.com/antroseco/), [Felix Chippendale](https://github.com/FChippendale), and [Tom Grant](https://github.com/DaveDuck321/) to ask them about their project.\r\n\r\nyack! takes a video and restyles it as a classic comic book using Deepgram's Speech Recognition API and computer vision. The output looks a bit like this:\r\n\r\n![A comic book with four panes. In each pane is Mark Zuckerberg. In the first and third is a speech box with text.](https://res.cloudinary.com/deepgram/image/upload/v1646083983/blog/2022/03/comic-books-videos-yack/screenshot.jpg)\r\n\r\nOnce a video is provided, yack! generates a transcript with Deepgram. Then, keyframes in the video are chosen. Frames are cropped, the image has some comic book styling applied, and captions are overlaid as speech bubbles. Finally, each 'tile' is placed in a dynamic SVG element which is rendered on the page.\r\n\r\nThat's... a lot.\r\n\r\n## How It Works\r\n\r\nThe team got Deepgram working within the first hour of building, which freed the team to focus on more complex parts of the project. To make the returned transcript as useful as possible, they used our [utterances](https://developers.deepgram.com/documentation/features/utterances/) feature to understand what keyframes to show and [diarization](https://developers.deepgram.com/documentation/features/diarize/) to color text when different speakers are detected.\r\n\r\nOnce a key frame is chosen, computer vision is used to detect a speaker's location in the frame. It is then cropped to ensure faces are seen, that there's enough space for text to be overlaid, and that the aspect ratio is roughly maintained. During development, the face detection algorithm was one of the slowest parts -- taking up to 20 seconds -- though the team managed to speed this up slightly.\r\n\r\nThe style transfer then took place -- a set of simple visual tricks to make a real-life image look more comic-like -- reducing colors, finding edges and making them darker/bolder, and stacking. This was by far the slowest bit of the overall processing time - accounting for around 60%. Given more time this could be done with machine learning.\r\n\r\nFinally, the text is overlaid, and a dynamic SVG is created. The placement of tiles is, in itself, an engineering challenge. The team used a block-claiming algorithm to have times 'claim' space on the page.\r\n\r\n## Try It Out\r\n\r\nThe yack! team built a website for users to interact with and a Docker image to create a portable, scalable, and easily-deployable server.\r\n\r\nYou can try out yack! at [yack.ml](https://yack.ml)\r\n\r\n        ", "html": '<p>The team behind yack! wanted to use Deepgram and computer vision to build a fun and novel project. What came out of it was an automatic video to comic generator, which has more complexity than you might think. I sat down with <a href="https://github.com/WeixuanZ">Allan Zhang</a>, <a href="https://github.com/antroseco/">Andreas Economides</a>, <a href="https://github.com/FChippendale">Felix Chippendale</a>, and <a href="https://github.com/DaveDuck321/">Tom Grant</a> to ask them about their project.</p>\n<p>yack! takes a video and restyles it as a classic comic book using Deepgram\u2019s Speech Recognition API and computer vision. The output looks a bit like this:</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1646083983/blog/2022/03/comic-books-videos-yack/screenshot.jpg" alt="A comic book with four panes. In each pane is Mark Zuckerberg. In the first and third is a speech box with text."></p>\n<p>Once a video is provided, yack! generates a transcript with Deepgram. Then, keyframes in the video are chosen. Frames are cropped, the image has some comic book styling applied, and captions are overlaid as speech bubbles. Finally, each \u2018tile\u2019 is placed in a dynamic SVG element which is rendered on the page.</p>\n<p>That\u2019s\u2026 a lot.</p>\n<h2 id="how-it-works">How It Works</h2>\n<p>The team got Deepgram working within the first hour of building, which freed the team to focus on more complex parts of the project. To make the returned transcript as useful as possible, they used our <a href="https://developers.deepgram.com/documentation/features/utterances/">utterances</a> feature to understand what keyframes to show and <a href="https://developers.deepgram.com/documentation/features/diarize/">diarization</a> to color text when different speakers are detected.</p>\n<p>Once a key frame is chosen, computer vision is used to detect a speaker\u2019s location in the frame. It is then cropped to ensure faces are seen, that there\u2019s enough space for text to be overlaid, and that the aspect ratio is roughly maintained. During development, the face detection algorithm was one of the slowest parts \u2014 taking up to 20 seconds \u2014 though the team managed to speed this up slightly.</p>\n<p>The style transfer then took place \u2014 a set of simple visual tricks to make a real-life image look more comic-like \u2014 reducing colors, finding edges and making them darker/bolder, and stacking. This was by far the slowest bit of the overall processing time - accounting for around 60%. Given more time this could be done with machine learning.</p>\n<p>Finally, the text is overlaid, and a dynamic SVG is created. The placement of tiles is, in itself, an engineering challenge. The team used a block-claiming algorithm to have times \u2018claim\u2019 space on the page.</p>\n<h2 id="try-it-out">Try It Out</h2>\n<p>The yack! team built a website for users to interact with and a Docker image to create a portable, scalable, and easily-deployable server.</p>\n<p>You can try out yack! at <a href="https://yack.ml">yack.ml</a></p>' }, "file": "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/comic-books-videos-yack/index.md" };
function rawContent() {
  return "\r\nThe team behind yack! wanted to use Deepgram and computer vision to build a fun and novel project. What came out of it was an automatic video to comic generator, which has more complexity than you might think. I sat down with [Allan Zhang](https://github.com/WeixuanZ), [Andreas Economides](https://github.com/antroseco/), [Felix Chippendale](https://github.com/FChippendale), and [Tom Grant](https://github.com/DaveDuck321/) to ask them about their project.\r\n\r\nyack! takes a video and restyles it as a classic comic book using Deepgram's Speech Recognition API and computer vision. The output looks a bit like this:\r\n\r\n![A comic book with four panes. In each pane is Mark Zuckerberg. In the first and third is a speech box with text.](https://res.cloudinary.com/deepgram/image/upload/v1646083983/blog/2022/03/comic-books-videos-yack/screenshot.jpg)\r\n\r\nOnce a video is provided, yack! generates a transcript with Deepgram. Then, keyframes in the video are chosen. Frames are cropped, the image has some comic book styling applied, and captions are overlaid as speech bubbles. Finally, each 'tile' is placed in a dynamic SVG element which is rendered on the page.\r\n\r\nThat's... a lot.\r\n\r\n## How It Works\r\n\r\nThe team got Deepgram working within the first hour of building, which freed the team to focus on more complex parts of the project. To make the returned transcript as useful as possible, they used our [utterances](https://developers.deepgram.com/documentation/features/utterances/) feature to understand what keyframes to show and [diarization](https://developers.deepgram.com/documentation/features/diarize/) to color text when different speakers are detected.\r\n\r\nOnce a key frame is chosen, computer vision is used to detect a speaker's location in the frame. It is then cropped to ensure faces are seen, that there's enough space for text to be overlaid, and that the aspect ratio is roughly maintained. During development, the face detection algorithm was one of the slowest parts -- taking up to 20 seconds -- though the team managed to speed this up slightly.\r\n\r\nThe style transfer then took place -- a set of simple visual tricks to make a real-life image look more comic-like -- reducing colors, finding edges and making them darker/bolder, and stacking. This was by far the slowest bit of the overall processing time - accounting for around 60%. Given more time this could be done with machine learning.\r\n\r\nFinally, the text is overlaid, and a dynamic SVG is created. The placement of tiles is, in itself, an engineering challenge. The team used a block-claiming algorithm to have times 'claim' space on the page.\r\n\r\n## Try It Out\r\n\r\nThe yack! team built a website for users to interact with and a Docker image to create a portable, scalable, and easily-deployable server.\r\n\r\nYou can try out yack! at [yack.ml](https://yack.ml)\r\n\r\n        ";
}
function compiledContent() {
  return '<p>The team behind yack! wanted to use Deepgram and computer vision to build a fun and novel project. What came out of it was an automatic video to comic generator, which has more complexity than you might think. I sat down with <a href="https://github.com/WeixuanZ">Allan Zhang</a>, <a href="https://github.com/antroseco/">Andreas Economides</a>, <a href="https://github.com/FChippendale">Felix Chippendale</a>, and <a href="https://github.com/DaveDuck321/">Tom Grant</a> to ask them about their project.</p>\n<p>yack! takes a video and restyles it as a classic comic book using Deepgram\u2019s Speech Recognition API and computer vision. The output looks a bit like this:</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1646083983/blog/2022/03/comic-books-videos-yack/screenshot.jpg" alt="A comic book with four panes. In each pane is Mark Zuckerberg. In the first and third is a speech box with text."></p>\n<p>Once a video is provided, yack! generates a transcript with Deepgram. Then, keyframes in the video are chosen. Frames are cropped, the image has some comic book styling applied, and captions are overlaid as speech bubbles. Finally, each \u2018tile\u2019 is placed in a dynamic SVG element which is rendered on the page.</p>\n<p>That\u2019s\u2026 a lot.</p>\n<h2 id="how-it-works">How It Works</h2>\n<p>The team got Deepgram working within the first hour of building, which freed the team to focus on more complex parts of the project. To make the returned transcript as useful as possible, they used our <a href="https://developers.deepgram.com/documentation/features/utterances/">utterances</a> feature to understand what keyframes to show and <a href="https://developers.deepgram.com/documentation/features/diarize/">diarization</a> to color text when different speakers are detected.</p>\n<p>Once a key frame is chosen, computer vision is used to detect a speaker\u2019s location in the frame. It is then cropped to ensure faces are seen, that there\u2019s enough space for text to be overlaid, and that the aspect ratio is roughly maintained. During development, the face detection algorithm was one of the slowest parts \u2014 taking up to 20 seconds \u2014 though the team managed to speed this up slightly.</p>\n<p>The style transfer then took place \u2014 a set of simple visual tricks to make a real-life image look more comic-like \u2014 reducing colors, finding edges and making them darker/bolder, and stacking. This was by far the slowest bit of the overall processing time - accounting for around 60%. Given more time this could be done with machine learning.</p>\n<p>Finally, the text is overlaid, and a dynamic SVG is created. The placement of tiles is, in itself, an engineering challenge. The team used a block-claiming algorithm to have times \u2018claim\u2019 space on the page.</p>\n<h2 id="try-it-out">Try It Out</h2>\n<p>The yack! team built a website for users to interact with and a Docker image to create a portable, scalable, and easily-deployable server.</p>\n<p>You can try out yack! at <a href="https://yack.ml">yack.ml</a></p>';
}
const $$Astro = createAstro("/Users/sandrarodgers/web-next/blog/src/content/blog/posts/comic-books-videos-yack/index.md", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Index = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro, $$props, $$slots);
  Astro2.self = $$Index;
  new Slugger();
  return renderTemplate`<head>${renderHead($$result)}</head><p>The team behind yack! wanted to use Deepgram and computer vision to build a fun and novel project. What came out of it was an automatic video to comic generator, which has more complexity than you might think. I sat down with <a href="https://github.com/WeixuanZ">Allan Zhang</a>, <a href="https://github.com/antroseco/">Andreas Economides</a>, <a href="https://github.com/FChippendale">Felix Chippendale</a>, and <a href="https://github.com/DaveDuck321/">Tom Grant</a> to ask them about their project.</p>
<p>yack! takes a video and restyles it as a classic comic book using Deepgram’s Speech Recognition API and computer vision. The output looks a bit like this:</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1646083983/blog/2022/03/comic-books-videos-yack/screenshot.jpg" alt="A comic book with four panes. In each pane is Mark Zuckerberg. In the first and third is a speech box with text."></p>
<p>Once a video is provided, yack! generates a transcript with Deepgram. Then, keyframes in the video are chosen. Frames are cropped, the image has some comic book styling applied, and captions are overlaid as speech bubbles. Finally, each ‘tile’ is placed in a dynamic SVG element which is rendered on the page.</p>
<p>That’s… a lot.</p>
<h2 id="how-it-works">How It Works</h2>
<p>The team got Deepgram working within the first hour of building, which freed the team to focus on more complex parts of the project. To make the returned transcript as useful as possible, they used our <a href="https://developers.deepgram.com/documentation/features/utterances/">utterances</a> feature to understand what keyframes to show and <a href="https://developers.deepgram.com/documentation/features/diarize/">diarization</a> to color text when different speakers are detected.</p>
<p>Once a key frame is chosen, computer vision is used to detect a speaker’s location in the frame. It is then cropped to ensure faces are seen, that there’s enough space for text to be overlaid, and that the aspect ratio is roughly maintained. During development, the face detection algorithm was one of the slowest parts — taking up to 20 seconds — though the team managed to speed this up slightly.</p>
<p>The style transfer then took place — a set of simple visual tricks to make a real-life image look more comic-like — reducing colors, finding edges and making them darker/bolder, and stacking. This was by far the slowest bit of the overall processing time - accounting for around 60%. Given more time this could be done with machine learning.</p>
<p>Finally, the text is overlaid, and a dynamic SVG is created. The placement of tiles is, in itself, an engineering challenge. The team used a block-claiming algorithm to have times ‘claim’ space on the page.</p>
<h2 id="try-it-out">Try It Out</h2>
<p>The yack! team built a website for users to interact with and a Docker image to create a portable, scalable, and easily-deployable server.</p>
<p>You can try out yack! at <a href="https://yack.ml">yack.ml</a></p>`;
});

export { compiledContent, $$Index as default, frontmatter, metadata, rawContent };
