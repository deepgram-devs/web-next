import { c as createAstro, a as createComponent, r as renderTemplate, b as renderHead, d as renderComponent } from '../entry.mjs';
import Slugger from 'github-slugger';
import '@astrojs/netlify/netlify-functions.js';
import 'preact';
import 'preact-render-to-string';
import 'vue';
import 'vue/server-renderer';
import 'html-escaper';
import 'node-html-parser';
import 'axios';
/* empty css                           *//* empty css                           *//* empty css                           *//* empty css                           *//* empty css                          */import 'clone-deep';
import 'slugify';
import 'shiki';
/* empty css                           */import '@astrojs/rss';
/* empty css                           */import 'mime';
import 'cookie';
import 'kleur/colors';
import 'string-width';
import 'path-browserify';
import 'path-to-regexp';

const metadata = { "headings": [{ "depth": 2, "slug": "how-to-calculate-wer", "text": "How to Calculate WER" }, { "depth": 2, "slug": "what-wer-really-means", "text": "What WER Really Means" }, { "depth": 2, "slug": "a-low-wer-can-be-deceptive-depending-on-the-data", "text": "A Low WER can be Deceptive-Depending on the Data" }, { "depth": 2, "slug": "what-affects-the-word-error-rate", "text": "What Affects the Word Error Rate?" }, { "depth": 3, "slug": "technical-and-industry-specific-language", "text": "Technical and Industry-specific Language" }, { "depth": 3, "slug": "accented-language", "text": "Accented Language" }, { "depth": 3, "slug": "noisy-data", "text": "Noisy Data" }, { "depth": 2, "slug": "choosing-an-speech-recognition-api", "text": "Choosing an Speech Recognition API" }], "source": `Word Error Rate (WER) is a common metric used to compare the accuracy of the transcripts produced by speech recognition APIs. Speech recognition APIs are used to surface actionable insights from large volumes of audio data in addition to powering robust IVRs and voice-command-enabled devices such as the Amazon Echo. Product developers and data scientists can choose from many speech recognition APIs. How are they to judge which will be a good fit for their application? When evaluating speech recognition APIs, the first metric they'll consider is likely to be WER. However, a metric has no value unless we understand what it tells us. Let's break down WER to find out what it means and how useful a metric it is.

## How to Calculate WER

Word error rate is the most common metric used today to evaluate the effectiveness of an [automatic speech recognition system (ASR)](https://blog.deepgram.com/what-is-asr/). It is simply calculated as: 

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976771/blog/what-is-word-error-rate/wer-1.jpg)

**S** stands for substitutions (replacing a word). **I** stands for insertions (inserting a word). **D** stands for deletions (omitting a word). **N** is the number of words that were actually said *Note: WER will be calculated incorrectly if you forget to normalize capitalization, punctuation, numbers, etc. across all transcripts*

Imagine you are using speech recognition to find out why customers are calling. You have thousands of hours of calls, and you want to automatically categorize the calls. On playback, one such call starts as follows: 

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976772/blog/what-is-word-error-rate/creditcardcall-1.jpg)

However, when the machine transcribes this same call, the output may look like this: 

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976773/blog/what-is-word-error-rate/transcript_creditcardcall-1.jpg)

 If you compare this transcript with the one above, it's clear that the machine's one has problems. Let's analyze them in terms of our WER formula.

1. In line one, we see that the word "Upsilon", has been interpreted as "up silent". We will say that this represents (1) **substitution**-a wrong word in place of the correct word-and (1) **insertion**-adding of a word that was never said.
2. On line two, we have (1) **substitution:** "brat" instead of "Pratt."
3. On line three we have (1) **substitution:** "designed" instead of "declined."
4. On line four we have (2) **substitutions:** "cart" instead of "card" and "because" instead of cause. On this line we also have (1) **deletion:** the word "it" is gone.

The original phone call contained 36 words. With a total of 9 errors, the WER comes out to 25%.

## What WER Really Means

How well a speech recognition API can transcribe audio depends on a number of factors, which we will discuss below. Before we do, we must ask ourselves the most important question one can ask when assessing a speech recognition system: "is the transcript usable for my purposes?" Let's consider our example. This is a good transcription if you are trying to figure whether customers are calling to solve issues with credit cards or debit cards. However, if your goal is to figure why out each person called your call center (to deal with a declined card, lost card, etc.) then this phone call would likely get mis-sorted. **This is because the system did not properly transcribe a keyword: "declined."** <mark>When a speech recognition API fails to recognize words important to your analysis, it is not good enough-no matter what the WER is.</mark> Word error rate, as a metric, does not give us any information about how the errors will affect usability for users. As you can see, you the human can read the flawed transcript and still manage to figure out the problem. The only piece of information you might have trouble reconstructing is the name and location of the gas station. Unfamiliar proper names are troublesome for both humans and machines.

## A Low WER can be Deceptive-Depending on the Data

Depending on the data we want to look at, even low word error rate transcripts may prove less useful than expected. For example, notice how on line 4, "'cause" was transcribed as "because" and the object pronoun "it" was omitted. These two errors may not matter, especially if your goal is to find out why customers are calling. If the speech recognition API had not made these errors, we would have a 19.4% WER-almost as good as it gets for general, off-the-shelf speech recognition. But, as you can see, a low(er) error rate does not necessarily translate into a more useful transcript. This is because adverbs like "because" and object pronouns like "it" are not of much interest to us in this case.

> "You can have two systems with similar accuracy rates that produce wildly differently transcripts in terms of understandability. You can have two different systems that are similar in terms of accuracy but maybe one handles particular vocabulary that's germane to your application better than the other. There's more than just accuracy at the heart of it."
>
> \u2014Klint Kanopka Stanford Ph.D. Researcher

While WER is a good, first-blush metric for comparing the accuracy of speech recognition APIs, it is by no means the only metric which you should consider. **Importantly, you should understand how the speech recognition API will deal with *your* data. What words will it transcribe with ease? What words will give it trouble?** What words matter to you? In our example, the words "declined" and "credit card" are likely the ones we want to get right every time.

## What Affects the Word Error Rate?

A 25% word error rate is about average for "off the shelf" speech recognition APIs like Amazon, Google, IBM Watson, and Nuance. The more technical, the more industry-specific, the more "accented" and the more noisy your speech data is, the less likely that a general speech recognition API (or humans) will do as well.

### Technical and Industry-specific Language

There's a reason that human transcriptionists charge more for technical or industry-specific language. It simply takes more time and effort for human brains to reliably recognize niche terms. The language that is normal to call center manager, a lawyer or a business executive is rare elsewhere. As a result, speech recognition systems trained on "average" data also struggle with more specialized words. As you'd guess, the technical language was created for a reason and accordingly, it's the language that businesses care the most about.

### Accented Language

Accent is a highly relative, very human concept. This author has a strong accent in Dublin, but almost none in New York. The speech recognition systems built by large companies such as Google, Nuance and IBM are very familiar with the sort of English one hears on TV: "general American English" and RP (received pronunciation-the form of British English spoken by the Queen, the BBC and Oxford graduates). They are not necessarily familiar with the "real" English spoken in Palo Alto, CA; Athens, Georgia; New Dehli, India or Edinburgh, Scotland. However, companies are interested in the "real" language since a very tiny subset of their employees are TV anchors. 

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976774/blog/what-is-word-error-rate/raghu-nayyar-501556-unsplash-1.jpg)

*In New Delhi-English is spoken natively and non-natively by a large percentage of the population. Photo Credit: [Raghu Nayyar](https://unsplash.com/photos/EpAq2EE-shg).* 

Therefore, if your data has a wider variety of accents (it almost certainly does), or is limited to a set of accents not well represented in the data used to create general speech recognition APIs, then you probably need a [custom-built model](https://deepgram.com/product/train/) to really get a good look at your data.

### Noisy Data

Wouldn't it be nice if everyone who called us to do business did so from a sound studio? Wouldn't you love that crisp, bassy, noise-free audio? Better yet, how about they didn't call us over the phone, since VoIP and traditional phone systems compress audio, cut off many frequencies and add noise artifacts? The real world is full of noise. Phone calls are inherently bad quality, people call while rushing to work, or walking by a seemingly endless line of jackhammers, fire engines and screaming 4-month-olds.

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976774/blog/what-is-word-error-rate/baby-firetruck-jackhammer.jpg)

Somehow, human transcribers do okay with such noisy data, and speech recognition APIs, if properly trained, can do okay too. However, as you can imagine, when companies advertise super-low word error rates, these are not the WERs they get when transcribing audio captured at Iron Maiden concerts held in the middle of 16 lane interstate highways.

<WhitepaperPromo whitepaper="deepgram-whitepaper-how-deepgram-works"></WhitepaperPromo>

## Choosing an Speech Recognition API

Speech recognition APIs are fantastic tools that allow us to look into vast amounts of audio data in order to learn meaningful things about our customers, our operations and the world in general. WER is one metric that allows us to compare speech recognition APIs. However, as it is the case in any science, there is no one "best" metric.

I like analogies, so here is one: Asking which is the best metric to judge the quality of a bicycle could end in disaster. If your say "weight is the best metric, the lighter the better," then people like me who use their bikes to carry heavy groceries, 2 months of laundry and the occasional 2x4 would be in trouble. If you said "the number of pannier racks on a bike" is a good metric, then Tour de France cyclists would become a lot more winded, faster. All in all, you need to choose what's right for you. 

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976775/blog/what-is-word-error-rate/derek-thomson-271991-unsplash.jpg)

*This bike is a robust touring bike with pannier racks-great for shopping and 10,000 mile tours, bad for Tour de France. Photo credit: [Derek Thomson](https://unsplash.com/photos/AJ-7QpXV9U4)*

When you want to decide which speech recognition API to use, ask yourself:

* Are there particular audio types that you need the speech recognition API to perform well on (phone call, TV, radio, meetings)?
* Are there certain words or accents that the speech recognition API should do well on?
* Can you [customize the API](https://deepgram.com/product/train/) to perform better on your data?

For more, check out our [step by step guide on how to evaluate an ASR provider](https://blog.deepgram.com/how-to-test-automatic-speech-recognition-asr-providers-for-your-business/) or [have us evaluate](https://deepgram.com/contact-us) the ASR provider for you.`, "html": '<p>Word Error Rate (WER) is a common metric used to compare the accuracy of the transcripts produced by speech recognition APIs. Speech recognition APIs are used to surface actionable insights from large volumes of audio data in addition to powering robust IVRs and voice-command-enabled devices such as the Amazon Echo. Product developers and data scientists can choose from many speech recognition APIs. How are they to judge which will be a good fit for their application? When evaluating speech recognition APIs, the first metric they\u2019ll consider is likely to be WER. However, a metric has no value unless we understand what it tells us. Let\u2019s break down WER to find out what it means and how useful a metric it is.</p>\n<h2 id="how-to-calculate-wer">How to Calculate WER</h2>\n<p>Word error rate is the most common metric used today to evaluate the effectiveness of an <a href="https://blog.deepgram.com/what-is-asr/">automatic speech recognition system (ASR)</a>. It is simply calculated as:</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976771/blog/what-is-word-error-rate/wer-1.jpg" alt="Alt"></p>\n<p><strong>S</strong> stands for substitutions (replacing a word). <strong>I</strong> stands for insertions (inserting a word). <strong>D</strong> stands for deletions (omitting a word). <strong>N</strong> is the number of words that were actually said <em>Note: WER will be calculated incorrectly if you forget to normalize capitalization, punctuation, numbers, etc. across all transcripts</em></p>\n<p>Imagine you are using speech recognition to find out why customers are calling. You have thousands of hours of calls, and you want to automatically categorize the calls. On playback, one such call starts as follows:</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976772/blog/what-is-word-error-rate/creditcardcall-1.jpg" alt="Alt"></p>\n<p>However, when the machine transcribes this same call, the output may look like this:</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976773/blog/what-is-word-error-rate/transcript_creditcardcall-1.jpg" alt="Alt"></p>\n<p>If you compare this transcript with the one above, it\u2019s clear that the machine\u2019s one has problems. Let\u2019s analyze them in terms of our WER formula.</p>\n<ol>\n<li>In line one, we see that the word \u201CUpsilon\u201D, has been interpreted as \u201Cup silent\u201D. We will say that this represents (1) <strong>substitution</strong>-a wrong word in place of the correct word-and (1) <strong>insertion</strong>-adding of a word that was never said.</li>\n<li>On line two, we have (1) <strong>substitution:</strong> \u201Cbrat\u201D instead of \u201CPratt.\u201D</li>\n<li>On line three we have (1) <strong>substitution:</strong> \u201Cdesigned\u201D instead of \u201Cdeclined.\u201D</li>\n<li>On line four we have (2) <strong>substitutions:</strong> \u201Ccart\u201D instead of \u201Ccard\u201D and \u201Cbecause\u201D instead of cause. On this line we also have (1) <strong>deletion:</strong> the word \u201Cit\u201D is gone.</li>\n</ol>\n<p>The original phone call contained 36 words. With a total of 9 errors, the WER comes out to 25%.</p>\n<h2 id="what-wer-really-means">What WER Really Means</h2>\n<p>How well a speech recognition API can transcribe audio depends on a number of factors, which we will discuss below. Before we do, we must ask ourselves the most important question one can ask when assessing a speech recognition system: \u201Cis the transcript usable for my purposes?\u201D Let\u2019s consider our example. This is a good transcription if you are trying to figure whether customers are calling to solve issues with credit cards or debit cards. However, if your goal is to figure why out each person called your call center (to deal with a declined card, lost card, etc.) then this phone call would likely get mis-sorted. <strong>This is because the system did not properly transcribe a keyword: \u201Cdeclined.\u201D</strong> <mark>When a speech recognition API fails to recognize words important to your analysis, it is not good enough-no matter what the WER is.</mark> Word error rate, as a metric, does not give us any information about how the errors will affect usability for users. As you can see, you the human can read the flawed transcript and still manage to figure out the problem. The only piece of information you might have trouble reconstructing is the name and location of the gas station. Unfamiliar proper names are troublesome for both humans and machines.</p>\n<h2 id="a-low-wer-can-be-deceptive-depending-on-the-data">A Low WER can be Deceptive-Depending on the Data</h2>\n<p>Depending on the data we want to look at, even low word error rate transcripts may prove less useful than expected. For example, notice how on line 4, \u201C\u2018cause\u201D was transcribed as \u201Cbecause\u201D and the object pronoun \u201Cit\u201D was omitted. These two errors may not matter, especially if your goal is to find out why customers are calling. If the speech recognition API had not made these errors, we would have a 19.4% WER-almost as good as it gets for general, off-the-shelf speech recognition. But, as you can see, a low(er) error rate does not necessarily translate into a more useful transcript. This is because adverbs like \u201Cbecause\u201D and object pronouns like \u201Cit\u201D are not of much interest to us in this case.</p>\n<blockquote>\n<p>\u201CYou can have two systems with similar accuracy rates that produce wildly differently transcripts in terms of understandability. You can have two different systems that are similar in terms of accuracy but maybe one handles particular vocabulary that\u2019s germane to your application better than the other. There\u2019s more than just accuracy at the heart of it.\u201D</p>\n<p>\u2014Klint Kanopka Stanford Ph.D. Researcher</p>\n</blockquote>\n<p>While WER is a good, first-blush metric for comparing the accuracy of speech recognition APIs, it is by no means the only metric which you should consider. <strong>Importantly, you should understand how the speech recognition API will deal with <em>your</em> data. What words will it transcribe with ease? What words will give it trouble?</strong> What words matter to you? In our example, the words \u201Cdeclined\u201D and \u201Ccredit card\u201D are likely the ones we want to get right every time.</p>\n<h2 id="what-affects-the-word-error-rate">What Affects the Word Error Rate?</h2>\n<p>A 25% word error rate is about average for \u201Coff the shelf\u201D speech recognition APIs like Amazon, Google, IBM Watson, and Nuance. The more technical, the more industry-specific, the more \u201Caccented\u201D and the more noisy your speech data is, the less likely that a general speech recognition API (or humans) will do as well.</p>\n<h3 id="technical-and-industry-specific-language">Technical and Industry-specific Language</h3>\n<p>There\u2019s a reason that human transcriptionists charge more for technical or industry-specific language. It simply takes more time and effort for human brains to reliably recognize niche terms. The language that is normal to call center manager, a lawyer or a business executive is rare elsewhere. As a result, speech recognition systems trained on \u201Caverage\u201D data also struggle with more specialized words. As you\u2019d guess, the technical language was created for a reason and accordingly, it\u2019s the language that businesses care the most about.</p>\n<h3 id="accented-language">Accented Language</h3>\n<p>Accent is a highly relative, very human concept. This author has a strong accent in Dublin, but almost none in New York. The speech recognition systems built by large companies such as Google, Nuance and IBM are very familiar with the sort of English one hears on TV: \u201Cgeneral American English\u201D and RP (received pronunciation-the form of British English spoken by the Queen, the BBC and Oxford graduates). They are not necessarily familiar with the \u201Creal\u201D English spoken in Palo Alto, CA; Athens, Georgia; New Dehli, India or Edinburgh, Scotland. However, companies are interested in the \u201Creal\u201D language since a very tiny subset of their employees are TV anchors.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976774/blog/what-is-word-error-rate/raghu-nayyar-501556-unsplash-1.jpg" alt="Alt"></p>\n<p><em>In New Delhi-English is spoken natively and non-natively by a large percentage of the population. Photo Credit: <a href="https://unsplash.com/photos/EpAq2EE-shg">Raghu Nayyar</a>.</em></p>\n<p>Therefore, if your data has a wider variety of accents (it almost certainly does), or is limited to a set of accents not well represented in the data used to create general speech recognition APIs, then you probably need a <a href="https://deepgram.com/product/train/">custom-built model</a> to really get a good look at your data.</p>\n<h3 id="noisy-data">Noisy Data</h3>\n<p>Wouldn\u2019t it be nice if everyone who called us to do business did so from a sound studio? Wouldn\u2019t you love that crisp, bassy, noise-free audio? Better yet, how about they didn\u2019t call us over the phone, since VoIP and traditional phone systems compress audio, cut off many frequencies and add noise artifacts? The real world is full of noise. Phone calls are inherently bad quality, people call while rushing to work, or walking by a seemingly endless line of jackhammers, fire engines and screaming 4-month-olds.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976774/blog/what-is-word-error-rate/baby-firetruck-jackhammer.jpg" alt="Alt"></p>\n<p>Somehow, human transcribers do okay with such noisy data, and speech recognition APIs, if properly trained, can do okay too. However, as you can imagine, when companies advertise super-low word error rates, these are not the WERs they get when transcribing audio captured at Iron Maiden concerts held in the middle of 16 lane interstate highways.</p>\n<WhitepaperPromo whitepaper="deepgram-whitepaper-how-deepgram-works" />\n<h2 id="choosing-an-speech-recognition-api">Choosing an Speech Recognition API</h2>\n<p>Speech recognition APIs are fantastic tools that allow us to look into vast amounts of audio data in order to learn meaningful things about our customers, our operations and the world in general. WER is one metric that allows us to compare speech recognition APIs. However, as it is the case in any science, there is no one \u201Cbest\u201D metric.</p>\n<p>I like analogies, so here is one: Asking which is the best metric to judge the quality of a bicycle could end in disaster. If your say \u201Cweight is the best metric, the lighter the better,\u201D then people like me who use their bikes to carry heavy groceries, 2 months of laundry and the occasional 2x4 would be in trouble. If you said \u201Cthe number of pannier racks on a bike\u201D is a good metric, then Tour de France cyclists would become a lot more winded, faster. All in all, you need to choose what\u2019s right for you.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976775/blog/what-is-word-error-rate/derek-thomson-271991-unsplash.jpg" alt="Alt"></p>\n<p><em>This bike is a robust touring bike with pannier racks-great for shopping and 10,000 mile tours, bad for Tour de France. Photo credit: <a href="https://unsplash.com/photos/AJ-7QpXV9U4">Derek Thomson</a></em></p>\n<p>When you want to decide which speech recognition API to use, ask yourself:</p>\n<ul>\n<li>Are there particular audio types that you need the speech recognition API to perform well on (phone call, TV, radio, meetings)?</li>\n<li>Are there certain words or accents that the speech recognition API should do well on?</li>\n<li>Can you <a href="https://deepgram.com/product/train/">customize the API</a> to perform better on your data?</li>\n</ul>\n<p>For more, check out our <a href="https://blog.deepgram.com/how-to-test-automatic-speech-recognition-asr-providers-for-your-business/">step by step guide on how to evaluate an ASR provider</a> or <a href="https://deepgram.com/contact-us">have us evaluate</a> the ASR provider for you.</p>' };
const frontmatter = { "title": "What is Word Error Rate (WER)?", "description": "Learn all about what word error rate is, what it means, and how it's calculated here.", "date": "2018-12-04T00:00:00.000Z", "cover": "https://res.cloudinary.com/deepgram/image/upload/v1661981330/blog/what-is-word-error-rate/what-is-wer%402x.jpg", "authors": ["morris-gevirtz"], "category": "ai-and-engineering", "tags": ["word-error-rate"], "seo": { "title": "What is Word Error Rate (WER)?", "description": "" }, "og": { "image": "https://res.cloudinary.com/deepgram/image/upload/v1661981330/blog/what-is-word-error-rate/what-is-wer%402x.jpg" }, "shorturls": { "share": "https://dpgr.am/3587a50", "twitter": "https://dpgr.am/24510e0", "linkedin": "https://dpgr.am/62e4f0e", "reddit": "https://dpgr.am/0675328", "facebook": "https://dpgr.am/9e5fe1f" }, "astro": { "headings": [{ "depth": 2, "slug": "how-to-calculate-wer", "text": "How to Calculate WER" }, { "depth": 2, "slug": "what-wer-really-means", "text": "What WER Really Means" }, { "depth": 2, "slug": "a-low-wer-can-be-deceptive-depending-on-the-data", "text": "A Low WER can be Deceptive-Depending on the Data" }, { "depth": 2, "slug": "what-affects-the-word-error-rate", "text": "What Affects the Word Error Rate?" }, { "depth": 3, "slug": "technical-and-industry-specific-language", "text": "Technical and Industry-specific Language" }, { "depth": 3, "slug": "accented-language", "text": "Accented Language" }, { "depth": 3, "slug": "noisy-data", "text": "Noisy Data" }, { "depth": 2, "slug": "choosing-an-speech-recognition-api", "text": "Choosing an Speech Recognition API" }], "source": `Word Error Rate (WER) is a common metric used to compare the accuracy of the transcripts produced by speech recognition APIs. Speech recognition APIs are used to surface actionable insights from large volumes of audio data in addition to powering robust IVRs and voice-command-enabled devices such as the Amazon Echo. Product developers and data scientists can choose from many speech recognition APIs. How are they to judge which will be a good fit for their application? When evaluating speech recognition APIs, the first metric they'll consider is likely to be WER. However, a metric has no value unless we understand what it tells us. Let's break down WER to find out what it means and how useful a metric it is.

## How to Calculate WER

Word error rate is the most common metric used today to evaluate the effectiveness of an [automatic speech recognition system (ASR)](https://blog.deepgram.com/what-is-asr/). It is simply calculated as: 

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976771/blog/what-is-word-error-rate/wer-1.jpg)

**S** stands for substitutions (replacing a word). **I** stands for insertions (inserting a word). **D** stands for deletions (omitting a word). **N** is the number of words that were actually said *Note: WER will be calculated incorrectly if you forget to normalize capitalization, punctuation, numbers, etc. across all transcripts*

Imagine you are using speech recognition to find out why customers are calling. You have thousands of hours of calls, and you want to automatically categorize the calls. On playback, one such call starts as follows: 

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976772/blog/what-is-word-error-rate/creditcardcall-1.jpg)

However, when the machine transcribes this same call, the output may look like this: 

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976773/blog/what-is-word-error-rate/transcript_creditcardcall-1.jpg)

 If you compare this transcript with the one above, it's clear that the machine's one has problems. Let's analyze them in terms of our WER formula.

1. In line one, we see that the word "Upsilon", has been interpreted as "up silent". We will say that this represents (1) **substitution**-a wrong word in place of the correct word-and (1) **insertion**-adding of a word that was never said.
2. On line two, we have (1) **substitution:** "brat" instead of "Pratt."
3. On line three we have (1) **substitution:** "designed" instead of "declined."
4. On line four we have (2) **substitutions:** "cart" instead of "card" and "because" instead of cause. On this line we also have (1) **deletion:** the word "it" is gone.

The original phone call contained 36 words. With a total of 9 errors, the WER comes out to 25%.

## What WER Really Means

How well a speech recognition API can transcribe audio depends on a number of factors, which we will discuss below. Before we do, we must ask ourselves the most important question one can ask when assessing a speech recognition system: "is the transcript usable for my purposes?" Let's consider our example. This is a good transcription if you are trying to figure whether customers are calling to solve issues with credit cards or debit cards. However, if your goal is to figure why out each person called your call center (to deal with a declined card, lost card, etc.) then this phone call would likely get mis-sorted. **This is because the system did not properly transcribe a keyword: "declined."** <mark>When a speech recognition API fails to recognize words important to your analysis, it is not good enough-no matter what the WER is.</mark> Word error rate, as a metric, does not give us any information about how the errors will affect usability for users. As you can see, you the human can read the flawed transcript and still manage to figure out the problem. The only piece of information you might have trouble reconstructing is the name and location of the gas station. Unfamiliar proper names are troublesome for both humans and machines.

## A Low WER can be Deceptive-Depending on the Data

Depending on the data we want to look at, even low word error rate transcripts may prove less useful than expected. For example, notice how on line 4, "'cause" was transcribed as "because" and the object pronoun "it" was omitted. These two errors may not matter, especially if your goal is to find out why customers are calling. If the speech recognition API had not made these errors, we would have a 19.4% WER-almost as good as it gets for general, off-the-shelf speech recognition. But, as you can see, a low(er) error rate does not necessarily translate into a more useful transcript. This is because adverbs like "because" and object pronouns like "it" are not of much interest to us in this case.

> "You can have two systems with similar accuracy rates that produce wildly differently transcripts in terms of understandability. You can have two different systems that are similar in terms of accuracy but maybe one handles particular vocabulary that's germane to your application better than the other. There's more than just accuracy at the heart of it."
>
> \u2014Klint Kanopka Stanford Ph.D. Researcher

While WER is a good, first-blush metric for comparing the accuracy of speech recognition APIs, it is by no means the only metric which you should consider. **Importantly, you should understand how the speech recognition API will deal with *your* data. What words will it transcribe with ease? What words will give it trouble?** What words matter to you? In our example, the words "declined" and "credit card" are likely the ones we want to get right every time.

## What Affects the Word Error Rate?

A 25% word error rate is about average for "off the shelf" speech recognition APIs like Amazon, Google, IBM Watson, and Nuance. The more technical, the more industry-specific, the more "accented" and the more noisy your speech data is, the less likely that a general speech recognition API (or humans) will do as well.

### Technical and Industry-specific Language

There's a reason that human transcriptionists charge more for technical or industry-specific language. It simply takes more time and effort for human brains to reliably recognize niche terms. The language that is normal to call center manager, a lawyer or a business executive is rare elsewhere. As a result, speech recognition systems trained on "average" data also struggle with more specialized words. As you'd guess, the technical language was created for a reason and accordingly, it's the language that businesses care the most about.

### Accented Language

Accent is a highly relative, very human concept. This author has a strong accent in Dublin, but almost none in New York. The speech recognition systems built by large companies such as Google, Nuance and IBM are very familiar with the sort of English one hears on TV: "general American English" and RP (received pronunciation-the form of British English spoken by the Queen, the BBC and Oxford graduates). They are not necessarily familiar with the "real" English spoken in Palo Alto, CA; Athens, Georgia; New Dehli, India or Edinburgh, Scotland. However, companies are interested in the "real" language since a very tiny subset of their employees are TV anchors. 

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976774/blog/what-is-word-error-rate/raghu-nayyar-501556-unsplash-1.jpg)

*In New Delhi-English is spoken natively and non-natively by a large percentage of the population. Photo Credit: [Raghu Nayyar](https://unsplash.com/photos/EpAq2EE-shg).* 

Therefore, if your data has a wider variety of accents (it almost certainly does), or is limited to a set of accents not well represented in the data used to create general speech recognition APIs, then you probably need a [custom-built model](https://deepgram.com/product/train/) to really get a good look at your data.

### Noisy Data

Wouldn't it be nice if everyone who called us to do business did so from a sound studio? Wouldn't you love that crisp, bassy, noise-free audio? Better yet, how about they didn't call us over the phone, since VoIP and traditional phone systems compress audio, cut off many frequencies and add noise artifacts? The real world is full of noise. Phone calls are inherently bad quality, people call while rushing to work, or walking by a seemingly endless line of jackhammers, fire engines and screaming 4-month-olds.

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976774/blog/what-is-word-error-rate/baby-firetruck-jackhammer.jpg)

Somehow, human transcribers do okay with such noisy data, and speech recognition APIs, if properly trained, can do okay too. However, as you can imagine, when companies advertise super-low word error rates, these are not the WERs they get when transcribing audio captured at Iron Maiden concerts held in the middle of 16 lane interstate highways.

<WhitepaperPromo whitepaper="deepgram-whitepaper-how-deepgram-works"></WhitepaperPromo>

## Choosing an Speech Recognition API

Speech recognition APIs are fantastic tools that allow us to look into vast amounts of audio data in order to learn meaningful things about our customers, our operations and the world in general. WER is one metric that allows us to compare speech recognition APIs. However, as it is the case in any science, there is no one "best" metric.

I like analogies, so here is one: Asking which is the best metric to judge the quality of a bicycle could end in disaster. If your say "weight is the best metric, the lighter the better," then people like me who use their bikes to carry heavy groceries, 2 months of laundry and the occasional 2x4 would be in trouble. If you said "the number of pannier racks on a bike" is a good metric, then Tour de France cyclists would become a lot more winded, faster. All in all, you need to choose what's right for you. 

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976775/blog/what-is-word-error-rate/derek-thomson-271991-unsplash.jpg)

*This bike is a robust touring bike with pannier racks-great for shopping and 10,000 mile tours, bad for Tour de France. Photo credit: [Derek Thomson](https://unsplash.com/photos/AJ-7QpXV9U4)*

When you want to decide which speech recognition API to use, ask yourself:

* Are there particular audio types that you need the speech recognition API to perform well on (phone call, TV, radio, meetings)?
* Are there certain words or accents that the speech recognition API should do well on?
* Can you [customize the API](https://deepgram.com/product/train/) to perform better on your data?

For more, check out our [step by step guide on how to evaluate an ASR provider](https://blog.deepgram.com/how-to-test-automatic-speech-recognition-asr-providers-for-your-business/) or [have us evaluate](https://deepgram.com/contact-us) the ASR provider for you.`, "html": '<p>Word Error Rate (WER) is a common metric used to compare the accuracy of the transcripts produced by speech recognition APIs. Speech recognition APIs are used to surface actionable insights from large volumes of audio data in addition to powering robust IVRs and voice-command-enabled devices such as the Amazon Echo. Product developers and data scientists can choose from many speech recognition APIs. How are they to judge which will be a good fit for their application? When evaluating speech recognition APIs, the first metric they\u2019ll consider is likely to be WER. However, a metric has no value unless we understand what it tells us. Let\u2019s break down WER to find out what it means and how useful a metric it is.</p>\n<h2 id="how-to-calculate-wer">How to Calculate WER</h2>\n<p>Word error rate is the most common metric used today to evaluate the effectiveness of an <a href="https://blog.deepgram.com/what-is-asr/">automatic speech recognition system (ASR)</a>. It is simply calculated as:</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976771/blog/what-is-word-error-rate/wer-1.jpg" alt="Alt"></p>\n<p><strong>S</strong> stands for substitutions (replacing a word). <strong>I</strong> stands for insertions (inserting a word). <strong>D</strong> stands for deletions (omitting a word). <strong>N</strong> is the number of words that were actually said <em>Note: WER will be calculated incorrectly if you forget to normalize capitalization, punctuation, numbers, etc. across all transcripts</em></p>\n<p>Imagine you are using speech recognition to find out why customers are calling. You have thousands of hours of calls, and you want to automatically categorize the calls. On playback, one such call starts as follows:</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976772/blog/what-is-word-error-rate/creditcardcall-1.jpg" alt="Alt"></p>\n<p>However, when the machine transcribes this same call, the output may look like this:</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976773/blog/what-is-word-error-rate/transcript_creditcardcall-1.jpg" alt="Alt"></p>\n<p>If you compare this transcript with the one above, it\u2019s clear that the machine\u2019s one has problems. Let\u2019s analyze them in terms of our WER formula.</p>\n<ol>\n<li>In line one, we see that the word \u201CUpsilon\u201D, has been interpreted as \u201Cup silent\u201D. We will say that this represents (1) <strong>substitution</strong>-a wrong word in place of the correct word-and (1) <strong>insertion</strong>-adding of a word that was never said.</li>\n<li>On line two, we have (1) <strong>substitution:</strong> \u201Cbrat\u201D instead of \u201CPratt.\u201D</li>\n<li>On line three we have (1) <strong>substitution:</strong> \u201Cdesigned\u201D instead of \u201Cdeclined.\u201D</li>\n<li>On line four we have (2) <strong>substitutions:</strong> \u201Ccart\u201D instead of \u201Ccard\u201D and \u201Cbecause\u201D instead of cause. On this line we also have (1) <strong>deletion:</strong> the word \u201Cit\u201D is gone.</li>\n</ol>\n<p>The original phone call contained 36 words. With a total of 9 errors, the WER comes out to 25%.</p>\n<h2 id="what-wer-really-means">What WER Really Means</h2>\n<p>How well a speech recognition API can transcribe audio depends on a number of factors, which we will discuss below. Before we do, we must ask ourselves the most important question one can ask when assessing a speech recognition system: \u201Cis the transcript usable for my purposes?\u201D Let\u2019s consider our example. This is a good transcription if you are trying to figure whether customers are calling to solve issues with credit cards or debit cards. However, if your goal is to figure why out each person called your call center (to deal with a declined card, lost card, etc.) then this phone call would likely get mis-sorted. <strong>This is because the system did not properly transcribe a keyword: \u201Cdeclined.\u201D</strong> <mark>When a speech recognition API fails to recognize words important to your analysis, it is not good enough-no matter what the WER is.</mark> Word error rate, as a metric, does not give us any information about how the errors will affect usability for users. As you can see, you the human can read the flawed transcript and still manage to figure out the problem. The only piece of information you might have trouble reconstructing is the name and location of the gas station. Unfamiliar proper names are troublesome for both humans and machines.</p>\n<h2 id="a-low-wer-can-be-deceptive-depending-on-the-data">A Low WER can be Deceptive-Depending on the Data</h2>\n<p>Depending on the data we want to look at, even low word error rate transcripts may prove less useful than expected. For example, notice how on line 4, \u201C\u2018cause\u201D was transcribed as \u201Cbecause\u201D and the object pronoun \u201Cit\u201D was omitted. These two errors may not matter, especially if your goal is to find out why customers are calling. If the speech recognition API had not made these errors, we would have a 19.4% WER-almost as good as it gets for general, off-the-shelf speech recognition. But, as you can see, a low(er) error rate does not necessarily translate into a more useful transcript. This is because adverbs like \u201Cbecause\u201D and object pronouns like \u201Cit\u201D are not of much interest to us in this case.</p>\n<blockquote>\n<p>\u201CYou can have two systems with similar accuracy rates that produce wildly differently transcripts in terms of understandability. You can have two different systems that are similar in terms of accuracy but maybe one handles particular vocabulary that\u2019s germane to your application better than the other. There\u2019s more than just accuracy at the heart of it.\u201D</p>\n<p>\u2014Klint Kanopka Stanford Ph.D. Researcher</p>\n</blockquote>\n<p>While WER is a good, first-blush metric for comparing the accuracy of speech recognition APIs, it is by no means the only metric which you should consider. <strong>Importantly, you should understand how the speech recognition API will deal with <em>your</em> data. What words will it transcribe with ease? What words will give it trouble?</strong> What words matter to you? In our example, the words \u201Cdeclined\u201D and \u201Ccredit card\u201D are likely the ones we want to get right every time.</p>\n<h2 id="what-affects-the-word-error-rate">What Affects the Word Error Rate?</h2>\n<p>A 25% word error rate is about average for \u201Coff the shelf\u201D speech recognition APIs like Amazon, Google, IBM Watson, and Nuance. The more technical, the more industry-specific, the more \u201Caccented\u201D and the more noisy your speech data is, the less likely that a general speech recognition API (or humans) will do as well.</p>\n<h3 id="technical-and-industry-specific-language">Technical and Industry-specific Language</h3>\n<p>There\u2019s a reason that human transcriptionists charge more for technical or industry-specific language. It simply takes more time and effort for human brains to reliably recognize niche terms. The language that is normal to call center manager, a lawyer or a business executive is rare elsewhere. As a result, speech recognition systems trained on \u201Caverage\u201D data also struggle with more specialized words. As you\u2019d guess, the technical language was created for a reason and accordingly, it\u2019s the language that businesses care the most about.</p>\n<h3 id="accented-language">Accented Language</h3>\n<p>Accent is a highly relative, very human concept. This author has a strong accent in Dublin, but almost none in New York. The speech recognition systems built by large companies such as Google, Nuance and IBM are very familiar with the sort of English one hears on TV: \u201Cgeneral American English\u201D and RP (received pronunciation-the form of British English spoken by the Queen, the BBC and Oxford graduates). They are not necessarily familiar with the \u201Creal\u201D English spoken in Palo Alto, CA; Athens, Georgia; New Dehli, India or Edinburgh, Scotland. However, companies are interested in the \u201Creal\u201D language since a very tiny subset of their employees are TV anchors.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976774/blog/what-is-word-error-rate/raghu-nayyar-501556-unsplash-1.jpg" alt="Alt"></p>\n<p><em>In New Delhi-English is spoken natively and non-natively by a large percentage of the population. Photo Credit: <a href="https://unsplash.com/photos/EpAq2EE-shg">Raghu Nayyar</a>.</em></p>\n<p>Therefore, if your data has a wider variety of accents (it almost certainly does), or is limited to a set of accents not well represented in the data used to create general speech recognition APIs, then you probably need a <a href="https://deepgram.com/product/train/">custom-built model</a> to really get a good look at your data.</p>\n<h3 id="noisy-data">Noisy Data</h3>\n<p>Wouldn\u2019t it be nice if everyone who called us to do business did so from a sound studio? Wouldn\u2019t you love that crisp, bassy, noise-free audio? Better yet, how about they didn\u2019t call us over the phone, since VoIP and traditional phone systems compress audio, cut off many frequencies and add noise artifacts? The real world is full of noise. Phone calls are inherently bad quality, people call while rushing to work, or walking by a seemingly endless line of jackhammers, fire engines and screaming 4-month-olds.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976774/blog/what-is-word-error-rate/baby-firetruck-jackhammer.jpg" alt="Alt"></p>\n<p>Somehow, human transcribers do okay with such noisy data, and speech recognition APIs, if properly trained, can do okay too. However, as you can imagine, when companies advertise super-low word error rates, these are not the WERs they get when transcribing audio captured at Iron Maiden concerts held in the middle of 16 lane interstate highways.</p>\n<WhitepaperPromo whitepaper="deepgram-whitepaper-how-deepgram-works" />\n<h2 id="choosing-an-speech-recognition-api">Choosing an Speech Recognition API</h2>\n<p>Speech recognition APIs are fantastic tools that allow us to look into vast amounts of audio data in order to learn meaningful things about our customers, our operations and the world in general. WER is one metric that allows us to compare speech recognition APIs. However, as it is the case in any science, there is no one \u201Cbest\u201D metric.</p>\n<p>I like analogies, so here is one: Asking which is the best metric to judge the quality of a bicycle could end in disaster. If your say \u201Cweight is the best metric, the lighter the better,\u201D then people like me who use their bikes to carry heavy groceries, 2 months of laundry and the occasional 2x4 would be in trouble. If you said \u201Cthe number of pannier racks on a bike\u201D is a good metric, then Tour de France cyclists would become a lot more winded, faster. All in all, you need to choose what\u2019s right for you.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976775/blog/what-is-word-error-rate/derek-thomson-271991-unsplash.jpg" alt="Alt"></p>\n<p><em>This bike is a robust touring bike with pannier racks-great for shopping and 10,000 mile tours, bad for Tour de France. Photo credit: <a href="https://unsplash.com/photos/AJ-7QpXV9U4">Derek Thomson</a></em></p>\n<p>When you want to decide which speech recognition API to use, ask yourself:</p>\n<ul>\n<li>Are there particular audio types that you need the speech recognition API to perform well on (phone call, TV, radio, meetings)?</li>\n<li>Are there certain words or accents that the speech recognition API should do well on?</li>\n<li>Can you <a href="https://deepgram.com/product/train/">customize the API</a> to perform better on your data?</li>\n</ul>\n<p>For more, check out our <a href="https://blog.deepgram.com/how-to-test-automatic-speech-recognition-asr-providers-for-your-business/">step by step guide on how to evaluate an ASR provider</a> or <a href="https://deepgram.com/contact-us">have us evaluate</a> the ASR provider for you.</p>' }, "file": "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-is-word-error-rate/index.md" };
function rawContent() {
  return `Word Error Rate (WER) is a common metric used to compare the accuracy of the transcripts produced by speech recognition APIs. Speech recognition APIs are used to surface actionable insights from large volumes of audio data in addition to powering robust IVRs and voice-command-enabled devices such as the Amazon Echo. Product developers and data scientists can choose from many speech recognition APIs. How are they to judge which will be a good fit for their application? When evaluating speech recognition APIs, the first metric they'll consider is likely to be WER. However, a metric has no value unless we understand what it tells us. Let's break down WER to find out what it means and how useful a metric it is.

## How to Calculate WER

Word error rate is the most common metric used today to evaluate the effectiveness of an [automatic speech recognition system (ASR)](https://blog.deepgram.com/what-is-asr/). It is simply calculated as: 

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976771/blog/what-is-word-error-rate/wer-1.jpg)

**S** stands for substitutions (replacing a word). **I** stands for insertions (inserting a word). **D** stands for deletions (omitting a word). **N** is the number of words that were actually said *Note: WER will be calculated incorrectly if you forget to normalize capitalization, punctuation, numbers, etc. across all transcripts*

Imagine you are using speech recognition to find out why customers are calling. You have thousands of hours of calls, and you want to automatically categorize the calls. On playback, one such call starts as follows: 

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976772/blog/what-is-word-error-rate/creditcardcall-1.jpg)

However, when the machine transcribes this same call, the output may look like this: 

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976773/blog/what-is-word-error-rate/transcript_creditcardcall-1.jpg)

 If you compare this transcript with the one above, it's clear that the machine's one has problems. Let's analyze them in terms of our WER formula.

1. In line one, we see that the word "Upsilon", has been interpreted as "up silent". We will say that this represents (1) **substitution**-a wrong word in place of the correct word-and (1) **insertion**-adding of a word that was never said.
2. On line two, we have (1) **substitution:** "brat" instead of "Pratt."
3. On line three we have (1) **substitution:** "designed" instead of "declined."
4. On line four we have (2) **substitutions:** "cart" instead of "card" and "because" instead of cause. On this line we also have (1) **deletion:** the word "it" is gone.

The original phone call contained 36 words. With a total of 9 errors, the WER comes out to 25%.

## What WER Really Means

How well a speech recognition API can transcribe audio depends on a number of factors, which we will discuss below. Before we do, we must ask ourselves the most important question one can ask when assessing a speech recognition system: "is the transcript usable for my purposes?" Let's consider our example. This is a good transcription if you are trying to figure whether customers are calling to solve issues with credit cards or debit cards. However, if your goal is to figure why out each person called your call center (to deal with a declined card, lost card, etc.) then this phone call would likely get mis-sorted. **This is because the system did not properly transcribe a keyword: "declined."** <mark>When a speech recognition API fails to recognize words important to your analysis, it is not good enough-no matter what the WER is.</mark> Word error rate, as a metric, does not give us any information about how the errors will affect usability for users. As you can see, you the human can read the flawed transcript and still manage to figure out the problem. The only piece of information you might have trouble reconstructing is the name and location of the gas station. Unfamiliar proper names are troublesome for both humans and machines.

## A Low WER can be Deceptive-Depending on the Data

Depending on the data we want to look at, even low word error rate transcripts may prove less useful than expected. For example, notice how on line 4, "'cause" was transcribed as "because" and the object pronoun "it" was omitted. These two errors may not matter, especially if your goal is to find out why customers are calling. If the speech recognition API had not made these errors, we would have a 19.4% WER-almost as good as it gets for general, off-the-shelf speech recognition. But, as you can see, a low(er) error rate does not necessarily translate into a more useful transcript. This is because adverbs like "because" and object pronouns like "it" are not of much interest to us in this case.

> "You can have two systems with similar accuracy rates that produce wildly differently transcripts in terms of understandability. You can have two different systems that are similar in terms of accuracy but maybe one handles particular vocabulary that's germane to your application better than the other. There's more than just accuracy at the heart of it."
>
> \u2014Klint Kanopka Stanford Ph.D. Researcher

While WER is a good, first-blush metric for comparing the accuracy of speech recognition APIs, it is by no means the only metric which you should consider. **Importantly, you should understand how the speech recognition API will deal with *your* data. What words will it transcribe with ease? What words will give it trouble?** What words matter to you? In our example, the words "declined" and "credit card" are likely the ones we want to get right every time.

## What Affects the Word Error Rate?

A 25% word error rate is about average for "off the shelf" speech recognition APIs like Amazon, Google, IBM Watson, and Nuance. The more technical, the more industry-specific, the more "accented" and the more noisy your speech data is, the less likely that a general speech recognition API (or humans) will do as well.

### Technical and Industry-specific Language

There's a reason that human transcriptionists charge more for technical or industry-specific language. It simply takes more time and effort for human brains to reliably recognize niche terms. The language that is normal to call center manager, a lawyer or a business executive is rare elsewhere. As a result, speech recognition systems trained on "average" data also struggle with more specialized words. As you'd guess, the technical language was created for a reason and accordingly, it's the language that businesses care the most about.

### Accented Language

Accent is a highly relative, very human concept. This author has a strong accent in Dublin, but almost none in New York. The speech recognition systems built by large companies such as Google, Nuance and IBM are very familiar with the sort of English one hears on TV: "general American English" and RP (received pronunciation-the form of British English spoken by the Queen, the BBC and Oxford graduates). They are not necessarily familiar with the "real" English spoken in Palo Alto, CA; Athens, Georgia; New Dehli, India or Edinburgh, Scotland. However, companies are interested in the "real" language since a very tiny subset of their employees are TV anchors. 

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976774/blog/what-is-word-error-rate/raghu-nayyar-501556-unsplash-1.jpg)

*In New Delhi-English is spoken natively and non-natively by a large percentage of the population. Photo Credit: [Raghu Nayyar](https://unsplash.com/photos/EpAq2EE-shg).* 

Therefore, if your data has a wider variety of accents (it almost certainly does), or is limited to a set of accents not well represented in the data used to create general speech recognition APIs, then you probably need a [custom-built model](https://deepgram.com/product/train/) to really get a good look at your data.

### Noisy Data

Wouldn't it be nice if everyone who called us to do business did so from a sound studio? Wouldn't you love that crisp, bassy, noise-free audio? Better yet, how about they didn't call us over the phone, since VoIP and traditional phone systems compress audio, cut off many frequencies and add noise artifacts? The real world is full of noise. Phone calls are inherently bad quality, people call while rushing to work, or walking by a seemingly endless line of jackhammers, fire engines and screaming 4-month-olds.

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976774/blog/what-is-word-error-rate/baby-firetruck-jackhammer.jpg)

Somehow, human transcribers do okay with such noisy data, and speech recognition APIs, if properly trained, can do okay too. However, as you can imagine, when companies advertise super-low word error rates, these are not the WERs they get when transcribing audio captured at Iron Maiden concerts held in the middle of 16 lane interstate highways.

<WhitepaperPromo whitepaper="deepgram-whitepaper-how-deepgram-works"></WhitepaperPromo>

## Choosing an Speech Recognition API

Speech recognition APIs are fantastic tools that allow us to look into vast amounts of audio data in order to learn meaningful things about our customers, our operations and the world in general. WER is one metric that allows us to compare speech recognition APIs. However, as it is the case in any science, there is no one "best" metric.

I like analogies, so here is one: Asking which is the best metric to judge the quality of a bicycle could end in disaster. If your say "weight is the best metric, the lighter the better," then people like me who use their bikes to carry heavy groceries, 2 months of laundry and the occasional 2x4 would be in trouble. If you said "the number of pannier racks on a bike" is a good metric, then Tour de France cyclists would become a lot more winded, faster. All in all, you need to choose what's right for you. 

![Alt](https://res.cloudinary.com/deepgram/image/upload/v1661976775/blog/what-is-word-error-rate/derek-thomson-271991-unsplash.jpg)

*This bike is a robust touring bike with pannier racks-great for shopping and 10,000 mile tours, bad for Tour de France. Photo credit: [Derek Thomson](https://unsplash.com/photos/AJ-7QpXV9U4)*

When you want to decide which speech recognition API to use, ask yourself:

* Are there particular audio types that you need the speech recognition API to perform well on (phone call, TV, radio, meetings)?
* Are there certain words or accents that the speech recognition API should do well on?
* Can you [customize the API](https://deepgram.com/product/train/) to perform better on your data?

For more, check out our [step by step guide on how to evaluate an ASR provider](https://blog.deepgram.com/how-to-test-automatic-speech-recognition-asr-providers-for-your-business/) or [have us evaluate](https://deepgram.com/contact-us) the ASR provider for you.`;
}
function compiledContent() {
  return '<p>Word Error Rate (WER) is a common metric used to compare the accuracy of the transcripts produced by speech recognition APIs. Speech recognition APIs are used to surface actionable insights from large volumes of audio data in addition to powering robust IVRs and voice-command-enabled devices such as the Amazon Echo. Product developers and data scientists can choose from many speech recognition APIs. How are they to judge which will be a good fit for their application? When evaluating speech recognition APIs, the first metric they\u2019ll consider is likely to be WER. However, a metric has no value unless we understand what it tells us. Let\u2019s break down WER to find out what it means and how useful a metric it is.</p>\n<h2 id="how-to-calculate-wer">How to Calculate WER</h2>\n<p>Word error rate is the most common metric used today to evaluate the effectiveness of an <a href="https://blog.deepgram.com/what-is-asr/">automatic speech recognition system (ASR)</a>. It is simply calculated as:</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976771/blog/what-is-word-error-rate/wer-1.jpg" alt="Alt"></p>\n<p><strong>S</strong> stands for substitutions (replacing a word). <strong>I</strong> stands for insertions (inserting a word). <strong>D</strong> stands for deletions (omitting a word). <strong>N</strong> is the number of words that were actually said <em>Note: WER will be calculated incorrectly if you forget to normalize capitalization, punctuation, numbers, etc. across all transcripts</em></p>\n<p>Imagine you are using speech recognition to find out why customers are calling. You have thousands of hours of calls, and you want to automatically categorize the calls. On playback, one such call starts as follows:</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976772/blog/what-is-word-error-rate/creditcardcall-1.jpg" alt="Alt"></p>\n<p>However, when the machine transcribes this same call, the output may look like this:</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976773/blog/what-is-word-error-rate/transcript_creditcardcall-1.jpg" alt="Alt"></p>\n<p>If you compare this transcript with the one above, it\u2019s clear that the machine\u2019s one has problems. Let\u2019s analyze them in terms of our WER formula.</p>\n<ol>\n<li>In line one, we see that the word \u201CUpsilon\u201D, has been interpreted as \u201Cup silent\u201D. We will say that this represents (1) <strong>substitution</strong>-a wrong word in place of the correct word-and (1) <strong>insertion</strong>-adding of a word that was never said.</li>\n<li>On line two, we have (1) <strong>substitution:</strong> \u201Cbrat\u201D instead of \u201CPratt.\u201D</li>\n<li>On line three we have (1) <strong>substitution:</strong> \u201Cdesigned\u201D instead of \u201Cdeclined.\u201D</li>\n<li>On line four we have (2) <strong>substitutions:</strong> \u201Ccart\u201D instead of \u201Ccard\u201D and \u201Cbecause\u201D instead of cause. On this line we also have (1) <strong>deletion:</strong> the word \u201Cit\u201D is gone.</li>\n</ol>\n<p>The original phone call contained 36 words. With a total of 9 errors, the WER comes out to 25%.</p>\n<h2 id="what-wer-really-means">What WER Really Means</h2>\n<p>How well a speech recognition API can transcribe audio depends on a number of factors, which we will discuss below. Before we do, we must ask ourselves the most important question one can ask when assessing a speech recognition system: \u201Cis the transcript usable for my purposes?\u201D Let\u2019s consider our example. This is a good transcription if you are trying to figure whether customers are calling to solve issues with credit cards or debit cards. However, if your goal is to figure why out each person called your call center (to deal with a declined card, lost card, etc.) then this phone call would likely get mis-sorted. <strong>This is because the system did not properly transcribe a keyword: \u201Cdeclined.\u201D</strong> <mark>When a speech recognition API fails to recognize words important to your analysis, it is not good enough-no matter what the WER is.</mark> Word error rate, as a metric, does not give us any information about how the errors will affect usability for users. As you can see, you the human can read the flawed transcript and still manage to figure out the problem. The only piece of information you might have trouble reconstructing is the name and location of the gas station. Unfamiliar proper names are troublesome for both humans and machines.</p>\n<h2 id="a-low-wer-can-be-deceptive-depending-on-the-data">A Low WER can be Deceptive-Depending on the Data</h2>\n<p>Depending on the data we want to look at, even low word error rate transcripts may prove less useful than expected. For example, notice how on line 4, \u201C\u2018cause\u201D was transcribed as \u201Cbecause\u201D and the object pronoun \u201Cit\u201D was omitted. These two errors may not matter, especially if your goal is to find out why customers are calling. If the speech recognition API had not made these errors, we would have a 19.4% WER-almost as good as it gets for general, off-the-shelf speech recognition. But, as you can see, a low(er) error rate does not necessarily translate into a more useful transcript. This is because adverbs like \u201Cbecause\u201D and object pronouns like \u201Cit\u201D are not of much interest to us in this case.</p>\n<blockquote>\n<p>\u201CYou can have two systems with similar accuracy rates that produce wildly differently transcripts in terms of understandability. You can have two different systems that are similar in terms of accuracy but maybe one handles particular vocabulary that\u2019s germane to your application better than the other. There\u2019s more than just accuracy at the heart of it.\u201D</p>\n<p>\u2014Klint Kanopka Stanford Ph.D. Researcher</p>\n</blockquote>\n<p>While WER is a good, first-blush metric for comparing the accuracy of speech recognition APIs, it is by no means the only metric which you should consider. <strong>Importantly, you should understand how the speech recognition API will deal with <em>your</em> data. What words will it transcribe with ease? What words will give it trouble?</strong> What words matter to you? In our example, the words \u201Cdeclined\u201D and \u201Ccredit card\u201D are likely the ones we want to get right every time.</p>\n<h2 id="what-affects-the-word-error-rate">What Affects the Word Error Rate?</h2>\n<p>A 25% word error rate is about average for \u201Coff the shelf\u201D speech recognition APIs like Amazon, Google, IBM Watson, and Nuance. The more technical, the more industry-specific, the more \u201Caccented\u201D and the more noisy your speech data is, the less likely that a general speech recognition API (or humans) will do as well.</p>\n<h3 id="technical-and-industry-specific-language">Technical and Industry-specific Language</h3>\n<p>There\u2019s a reason that human transcriptionists charge more for technical or industry-specific language. It simply takes more time and effort for human brains to reliably recognize niche terms. The language that is normal to call center manager, a lawyer or a business executive is rare elsewhere. As a result, speech recognition systems trained on \u201Caverage\u201D data also struggle with more specialized words. As you\u2019d guess, the technical language was created for a reason and accordingly, it\u2019s the language that businesses care the most about.</p>\n<h3 id="accented-language">Accented Language</h3>\n<p>Accent is a highly relative, very human concept. This author has a strong accent in Dublin, but almost none in New York. The speech recognition systems built by large companies such as Google, Nuance and IBM are very familiar with the sort of English one hears on TV: \u201Cgeneral American English\u201D and RP (received pronunciation-the form of British English spoken by the Queen, the BBC and Oxford graduates). They are not necessarily familiar with the \u201Creal\u201D English spoken in Palo Alto, CA; Athens, Georgia; New Dehli, India or Edinburgh, Scotland. However, companies are interested in the \u201Creal\u201D language since a very tiny subset of their employees are TV anchors.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976774/blog/what-is-word-error-rate/raghu-nayyar-501556-unsplash-1.jpg" alt="Alt"></p>\n<p><em>In New Delhi-English is spoken natively and non-natively by a large percentage of the population. Photo Credit: <a href="https://unsplash.com/photos/EpAq2EE-shg">Raghu Nayyar</a>.</em></p>\n<p>Therefore, if your data has a wider variety of accents (it almost certainly does), or is limited to a set of accents not well represented in the data used to create general speech recognition APIs, then you probably need a <a href="https://deepgram.com/product/train/">custom-built model</a> to really get a good look at your data.</p>\n<h3 id="noisy-data">Noisy Data</h3>\n<p>Wouldn\u2019t it be nice if everyone who called us to do business did so from a sound studio? Wouldn\u2019t you love that crisp, bassy, noise-free audio? Better yet, how about they didn\u2019t call us over the phone, since VoIP and traditional phone systems compress audio, cut off many frequencies and add noise artifacts? The real world is full of noise. Phone calls are inherently bad quality, people call while rushing to work, or walking by a seemingly endless line of jackhammers, fire engines and screaming 4-month-olds.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976774/blog/what-is-word-error-rate/baby-firetruck-jackhammer.jpg" alt="Alt"></p>\n<p>Somehow, human transcribers do okay with such noisy data, and speech recognition APIs, if properly trained, can do okay too. However, as you can imagine, when companies advertise super-low word error rates, these are not the WERs they get when transcribing audio captured at Iron Maiden concerts held in the middle of 16 lane interstate highways.</p>\n<WhitepaperPromo whitepaper="deepgram-whitepaper-how-deepgram-works" />\n<h2 id="choosing-an-speech-recognition-api">Choosing an Speech Recognition API</h2>\n<p>Speech recognition APIs are fantastic tools that allow us to look into vast amounts of audio data in order to learn meaningful things about our customers, our operations and the world in general. WER is one metric that allows us to compare speech recognition APIs. However, as it is the case in any science, there is no one \u201Cbest\u201D metric.</p>\n<p>I like analogies, so here is one: Asking which is the best metric to judge the quality of a bicycle could end in disaster. If your say \u201Cweight is the best metric, the lighter the better,\u201D then people like me who use their bikes to carry heavy groceries, 2 months of laundry and the occasional 2x4 would be in trouble. If you said \u201Cthe number of pannier racks on a bike\u201D is a good metric, then Tour de France cyclists would become a lot more winded, faster. All in all, you need to choose what\u2019s right for you.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976775/blog/what-is-word-error-rate/derek-thomson-271991-unsplash.jpg" alt="Alt"></p>\n<p><em>This bike is a robust touring bike with pannier racks-great for shopping and 10,000 mile tours, bad for Tour de France. Photo credit: <a href="https://unsplash.com/photos/AJ-7QpXV9U4">Derek Thomson</a></em></p>\n<p>When you want to decide which speech recognition API to use, ask yourself:</p>\n<ul>\n<li>Are there particular audio types that you need the speech recognition API to perform well on (phone call, TV, radio, meetings)?</li>\n<li>Are there certain words or accents that the speech recognition API should do well on?</li>\n<li>Can you <a href="https://deepgram.com/product/train/">customize the API</a> to perform better on your data?</li>\n</ul>\n<p>For more, check out our <a href="https://blog.deepgram.com/how-to-test-automatic-speech-recognition-asr-providers-for-your-business/">step by step guide on how to evaluate an ASR provider</a> or <a href="https://deepgram.com/contact-us">have us evaluate</a> the ASR provider for you.</p>';
}
const $$Astro = createAstro("/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-is-word-error-rate/index.md", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Index = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro, $$props, $$slots);
  Astro2.self = $$Index;
  new Slugger();
  return renderTemplate`<head>${renderHead($$result)}</head><p>Word Error Rate (WER) is a common metric used to compare the accuracy of the transcripts produced by speech recognition APIs. Speech recognition APIs are used to surface actionable insights from large volumes of audio data in addition to powering robust IVRs and voice-command-enabled devices such as the Amazon Echo. Product developers and data scientists can choose from many speech recognition APIs. How are they to judge which will be a good fit for their application? When evaluating speech recognition APIs, the first metric they’ll consider is likely to be WER. However, a metric has no value unless we understand what it tells us. Let’s break down WER to find out what it means and how useful a metric it is.</p>
<h2 id="how-to-calculate-wer">How to Calculate WER</h2>
<p>Word error rate is the most common metric used today to evaluate the effectiveness of an <a href="https://blog.deepgram.com/what-is-asr/">automatic speech recognition system (ASR)</a>. It is simply calculated as:</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976771/blog/what-is-word-error-rate/wer-1.jpg" alt="Alt"></p>
<p><strong>S</strong> stands for substitutions (replacing a word). <strong>I</strong> stands for insertions (inserting a word). <strong>D</strong> stands for deletions (omitting a word). <strong>N</strong> is the number of words that were actually said <em>Note: WER will be calculated incorrectly if you forget to normalize capitalization, punctuation, numbers, etc. across all transcripts</em></p>
<p>Imagine you are using speech recognition to find out why customers are calling. You have thousands of hours of calls, and you want to automatically categorize the calls. On playback, one such call starts as follows:</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976772/blog/what-is-word-error-rate/creditcardcall-1.jpg" alt="Alt"></p>
<p>However, when the machine transcribes this same call, the output may look like this:</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976773/blog/what-is-word-error-rate/transcript_creditcardcall-1.jpg" alt="Alt"></p>
<p>If you compare this transcript with the one above, it’s clear that the machine’s one has problems. Let’s analyze them in terms of our WER formula.</p>
<ol>
<li>In line one, we see that the word “Upsilon”, has been interpreted as “up silent”. We will say that this represents (1) <strong>substitution</strong>-a wrong word in place of the correct word-and (1) <strong>insertion</strong>-adding of a word that was never said.</li>
<li>On line two, we have (1) <strong>substitution:</strong> “brat” instead of “Pratt.”</li>
<li>On line three we have (1) <strong>substitution:</strong> “designed” instead of “declined.”</li>
<li>On line four we have (2) <strong>substitutions:</strong> “cart” instead of “card” and “because” instead of cause. On this line we also have (1) <strong>deletion:</strong> the word “it” is gone.</li>
</ol>
<p>The original phone call contained 36 words. With a total of 9 errors, the WER comes out to 25%.</p>
<h2 id="what-wer-really-means">What WER Really Means</h2>
<p>How well a speech recognition API can transcribe audio depends on a number of factors, which we will discuss below. Before we do, we must ask ourselves the most important question one can ask when assessing a speech recognition system: “is the transcript usable for my purposes?” Let’s consider our example. This is a good transcription if you are trying to figure whether customers are calling to solve issues with credit cards or debit cards. However, if your goal is to figure why out each person called your call center (to deal with a declined card, lost card, etc.) then this phone call would likely get mis-sorted. <strong>This is because the system did not properly transcribe a keyword: “declined.”</strong> <mark>When a speech recognition API fails to recognize words important to your analysis, it is not good enough-no matter what the WER is.</mark> Word error rate, as a metric, does not give us any information about how the errors will affect usability for users. As you can see, you the human can read the flawed transcript and still manage to figure out the problem. The only piece of information you might have trouble reconstructing is the name and location of the gas station. Unfamiliar proper names are troublesome for both humans and machines.</p>
<h2 id="a-low-wer-can-be-deceptive-depending-on-the-data">A Low WER can be Deceptive-Depending on the Data</h2>
<p>Depending on the data we want to look at, even low word error rate transcripts may prove less useful than expected. For example, notice how on line 4, “‘cause” was transcribed as “because” and the object pronoun “it” was omitted. These two errors may not matter, especially if your goal is to find out why customers are calling. If the speech recognition API had not made these errors, we would have a 19.4% WER-almost as good as it gets for general, off-the-shelf speech recognition. But, as you can see, a low(er) error rate does not necessarily translate into a more useful transcript. This is because adverbs like “because” and object pronouns like “it” are not of much interest to us in this case.</p>
<blockquote>
<p>“You can have two systems with similar accuracy rates that produce wildly differently transcripts in terms of understandability. You can have two different systems that are similar in terms of accuracy but maybe one handles particular vocabulary that’s germane to your application better than the other. There’s more than just accuracy at the heart of it.”</p>
<p>—Klint Kanopka Stanford Ph.D. Researcher</p>
</blockquote>
<p>While WER is a good, first-blush metric for comparing the accuracy of speech recognition APIs, it is by no means the only metric which you should consider. <strong>Importantly, you should understand how the speech recognition API will deal with <em>your</em> data. What words will it transcribe with ease? What words will give it trouble?</strong> What words matter to you? In our example, the words “declined” and “credit card” are likely the ones we want to get right every time.</p>
<h2 id="what-affects-the-word-error-rate">What Affects the Word Error Rate?</h2>
<p>A 25% word error rate is about average for “off the shelf” speech recognition APIs like Amazon, Google, IBM Watson, and Nuance. The more technical, the more industry-specific, the more “accented” and the more noisy your speech data is, the less likely that a general speech recognition API (or humans) will do as well.</p>
<h3 id="technical-and-industry-specific-language">Technical and Industry-specific Language</h3>
<p>There’s a reason that human transcriptionists charge more for technical or industry-specific language. It simply takes more time and effort for human brains to reliably recognize niche terms. The language that is normal to call center manager, a lawyer or a business executive is rare elsewhere. As a result, speech recognition systems trained on “average” data also struggle with more specialized words. As you’d guess, the technical language was created for a reason and accordingly, it’s the language that businesses care the most about.</p>
<h3 id="accented-language">Accented Language</h3>
<p>Accent is a highly relative, very human concept. This author has a strong accent in Dublin, but almost none in New York. The speech recognition systems built by large companies such as Google, Nuance and IBM are very familiar with the sort of English one hears on TV: “general American English” and RP (received pronunciation-the form of British English spoken by the Queen, the BBC and Oxford graduates). They are not necessarily familiar with the “real” English spoken in Palo Alto, CA; Athens, Georgia; New Dehli, India or Edinburgh, Scotland. However, companies are interested in the “real” language since a very tiny subset of their employees are TV anchors.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976774/blog/what-is-word-error-rate/raghu-nayyar-501556-unsplash-1.jpg" alt="Alt"></p>
<p><em>In New Delhi-English is spoken natively and non-natively by a large percentage of the population. Photo Credit: <a href="https://unsplash.com/photos/EpAq2EE-shg">Raghu Nayyar</a>.</em></p>
<p>Therefore, if your data has a wider variety of accents (it almost certainly does), or is limited to a set of accents not well represented in the data used to create general speech recognition APIs, then you probably need a <a href="https://deepgram.com/product/train/">custom-built model</a> to really get a good look at your data.</p>
<h3 id="noisy-data">Noisy Data</h3>
<p>Wouldn’t it be nice if everyone who called us to do business did so from a sound studio? Wouldn’t you love that crisp, bassy, noise-free audio? Better yet, how about they didn’t call us over the phone, since VoIP and traditional phone systems compress audio, cut off many frequencies and add noise artifacts? The real world is full of noise. Phone calls are inherently bad quality, people call while rushing to work, or walking by a seemingly endless line of jackhammers, fire engines and screaming 4-month-olds.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976774/blog/what-is-word-error-rate/baby-firetruck-jackhammer.jpg" alt="Alt"></p>
<p>Somehow, human transcribers do okay with such noisy data, and speech recognition APIs, if properly trained, can do okay too. However, as you can imagine, when companies advertise super-low word error rates, these are not the WERs they get when transcribing audio captured at Iron Maiden concerts held in the middle of 16 lane interstate highways.</p>
${renderComponent($$result, "WhitepaperPromo", WhitepaperPromo, { "whitepaper": "deepgram-whitepaper-how-deepgram-works" })}
<h2 id="choosing-an-speech-recognition-api">Choosing an Speech Recognition API</h2>
<p>Speech recognition APIs are fantastic tools that allow us to look into vast amounts of audio data in order to learn meaningful things about our customers, our operations and the world in general. WER is one metric that allows us to compare speech recognition APIs. However, as it is the case in any science, there is no one “best” metric.</p>
<p>I like analogies, so here is one: Asking which is the best metric to judge the quality of a bicycle could end in disaster. If your say “weight is the best metric, the lighter the better,” then people like me who use their bikes to carry heavy groceries, 2 months of laundry and the occasional 2x4 would be in trouble. If you said “the number of pannier racks on a bike” is a good metric, then Tour de France cyclists would become a lot more winded, faster. All in all, you need to choose what’s right for you.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976775/blog/what-is-word-error-rate/derek-thomson-271991-unsplash.jpg" alt="Alt"></p>
<p><em>This bike is a robust touring bike with pannier racks-great for shopping and 10,000 mile tours, bad for Tour de France. Photo credit: <a href="https://unsplash.com/photos/AJ-7QpXV9U4">Derek Thomson</a></em></p>
<p>When you want to decide which speech recognition API to use, ask yourself:</p>
<ul>
<li>Are there particular audio types that you need the speech recognition API to perform well on (phone call, TV, radio, meetings)?</li>
<li>Are there certain words or accents that the speech recognition API should do well on?</li>
<li>Can you <a href="https://deepgram.com/product/train/">customize the API</a> to perform better on your data?</li>
</ul>
<p>For more, check out our <a href="https://blog.deepgram.com/how-to-test-automatic-speech-recognition-asr-providers-for-your-business/">step by step guide on how to evaluate an ASR provider</a> or <a href="https://deepgram.com/contact-us">have us evaluate</a> the ASR provider for you.</p>`;
}, "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/what-is-word-error-rate/index.md");

export { compiledContent, $$Index as default, frontmatter, metadata, rawContent };
