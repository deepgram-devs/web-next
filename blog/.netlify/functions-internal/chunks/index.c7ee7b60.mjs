import { c as createAstro, a as createComponent, r as renderTemplate, b as renderHead, d as renderComponent } from '../entry.mjs';
import Slugger from 'github-slugger';
import '@astrojs/netlify/netlify-functions.js';
import 'preact';
import 'preact-render-to-string';
import 'vue';
import 'vue/server-renderer';
import 'html-escaper';
import 'node-html-parser';
import 'axios';
/* empty css                           *//* empty css                           *//* empty css                           *//* empty css                           *//* empty css                          */import 'clone-deep';
import 'slugify';
import 'shiki';
/* empty css                           */import '@astrojs/rss';
/* empty css                           */import 'mime';
import 'cookie';
import 'kleur/colors';
import 'string-width';
import 'path-browserify';
import 'path-to-regexp';

const metadata = { "headings": [{ "depth": 3, "slug": "issues-with-legacy-speech-recognition", "text": "Issues With Legacy Speech Recognition" }, { "depth": 2, "slug": "new-methods-and-easier-access", "text": "New Methods and Easier Access" }, { "depth": 2, "slug": "features-available", "text": "Features Available" }], "source": `We are excited to introduce Phase One of our Developer-First initiative to help voice technology developers implement our revolutionary End-to-End AI Speech Platform more easily**.**

### **Issues With Legacy Speech Recognition**

Voice has been one of the last unstructured data sources to be fully used and mined for insights. According to [Deloitte](https://www.deloittedigital.com/content/dam/deloittedigital/us/documents/blog/blog-20190513-2019%20globalcontactcentersurvey.pdf), fully 90% of all complex conversations in contact centers are done through the voice channel and the number is not going to change anytime soon. With the pandemic, the voice channel has become even more important. In the [2021 State of ASR Report](https://deepgram.com/state-of-asr-report/) by Opus Research, 85% of organizations note that automatic speech recognition is "important or very important" to their future enterprise strategy. If audio is so important, why has it not been fully utilized in business?  We believe three main issues prevent wider use of Automatic Speech Recognition (ASR) or Speech-to-Text (STT).

1. **Costs** - The cost of speech-to-text applications has gone down in the last decade because of competition, but has bottomed out due to the use of legacy speech processing that is inefficient and requires huge amounts of computing resources.  The bottom of the legacy cost curve is still too high to seriously consider transcribing all of an organization's voice data.
2. **Speed** - The speed of transcriptions has also gone down, but it still takes more than one day to transcribe one day of contact center calls.  Instead, organizations sample their calls to try to get some customer insights, but they often miss a gem of a product idea, can't respond quickly enough to churn signals, or can't assist sales in upselling a customer.  For applications requiring real-time transcriptions, like Conversational AI voicebots, the 2-4 second legacy STT lag time does not meet their needs.  You notice a one-second lag on video calls or streaming movies, think about waiting up to 4 seconds before a voicebot answers you.
3. **Accuracy** - We don't mean general out-of-the-box accuracy of all words, but the accuracy of the important keywords.  Do you care if you get articles, prepositions, and filler words correct?  Maybe, if a readable transcript is most important to you.  But if you are doing data analysis or finding knowledge base responses, you care more about the product names, phone numbers, government ID numbers, terminology, sentiment words, and acronyms because that is where you can find the insights and intent of the conversation.  Most one-size-fits-all, legacy STT solutions max out at around 80% accuracy. It's readable but unusable for things like transcribing phone numbers or social security numbers that require 100% accuracy.  One digit off means incorrect answers or insights.

Deepgram reinvented STT from the ground up specifically to solve these legacy tech issues.

## **New Methods and Easier Access**

Deepgram's STT is built with [End-to-End Deep Learning](https://offers.deepgram.com/how-deepgram-works-whitepaper), a neural network that can be trained and learn how to improve accuracy, remove noise, and focus on the important keywords.  We do not use any of the legacy speech recognition processes, therefore, their cost, speed, and accuracy limitations do not apply to us. A better foundation of STT allows us to expand our focus to improve access and use of our technology. With that in mind, we are releasing the following enhancements for a better developer experience with our platform:

* Two software development kits (SDKs) for [Python](https://pypi.org/project/deepgram-sdk/) and [Node.js](https://www.npmjs.com/package/@deepgram/sdk), with more languages to come
* A new Developer Console for better API, user, project, billing, and usage management.
* 200 hours of free pre-recorded transcription or 150 hours of real-time streaming transcriptions with initial [Developer Console sign up](https://console.deepgram.com/).

<WhitepaperPromo whitepaper="deepgram-whitepaper-how-deepgram-works"></WhitepaperPromo>

## **Features Available**

With this new Developer Console, you can try out all the following features that are included under our batch (pre-recorded) or real-time streaming [price](https://deepgram.com/pricing/).

* Interim results
* Punctuation
* Foreign languages
* Numeral formatting
* Utterance formatting
* Find and replace
* Profanity filter
* Keyword boosting

Descriptions of these features can be found on our [Product Overview](https://deepgram.com/product/overview/) page For a limited time, we are also opening up these two features at no charge.

* **Deep Search**: Text-based search is highly inaccurate, and has big implications if it's used for NLU data classification, analytics, and automated experiences. Deep Search drastically increases accuracy with acoustic pattern matching. 
* **Diarization:** As more companies move to digital communication channels, more voices need to be identified within recordings. Diarization labels audio transcripts with specific identities to allow for better transcripts.

Happy building and [keep in touch](https://deepgram.com/contact-us/). We'd love to hear how we can keep improving your experience with Deepgram.`, "html": '<p>We are excited to introduce Phase One of our Developer-First initiative to help voice technology developers implement our revolutionary End-to-End AI Speech Platform more easily**.**</p>\n<h3 id="issues-with-legacy-speech-recognition"><strong>Issues With Legacy Speech Recognition</strong></h3>\n<p>Voice has been one of the last unstructured data sources to be fully used and mined for insights. According to <a href="https://www.deloittedigital.com/content/dam/deloittedigital/us/documents/blog/blog-20190513-2019%20globalcontactcentersurvey.pdf">Deloitte</a>, fully 90% of all complex conversations in contact centers are done through the voice channel and the number is not going to change anytime soon. With the pandemic, the voice channel has become even more important. In the <a href="https://deepgram.com/state-of-asr-report/">2021 State of ASR Report</a> by Opus Research, 85% of organizations note that automatic speech recognition is \u201Cimportant or very important\u201D to their future enterprise strategy. If audio is so important, why has it not been fully utilized in business?  We believe three main issues prevent wider use of Automatic Speech Recognition (ASR) or Speech-to-Text (STT).</p>\n<ol>\n<li><strong>Costs</strong> - The cost of speech-to-text applications has gone down in the last decade because of competition, but has bottomed out due to the use of legacy speech processing that is inefficient and requires huge amounts of computing resources.  The bottom of the legacy cost curve is still too high to seriously consider transcribing all of an organization\u2019s voice data.</li>\n<li><strong>Speed</strong> - The speed of transcriptions has also gone down, but it still takes more than one day to transcribe one day of contact center calls.  Instead, organizations sample their calls to try to get some customer insights, but they often miss a gem of a product idea, can\u2019t respond quickly enough to churn signals, or can\u2019t assist sales in upselling a customer.  For applications requiring real-time transcriptions, like Conversational AI voicebots, the 2-4 second legacy STT lag time does not meet their needs.  You notice a one-second lag on video calls or streaming movies, think about waiting up to 4 seconds before a voicebot answers you.</li>\n<li><strong>Accuracy</strong> - We don\u2019t mean general out-of-the-box accuracy of all words, but the accuracy of the important keywords.  Do you care if you get articles, prepositions, and filler words correct?  Maybe, if a readable transcript is most important to you.  But if you are doing data analysis or finding knowledge base responses, you care more about the product names, phone numbers, government ID numbers, terminology, sentiment words, and acronyms because that is where you can find the insights and intent of the conversation.  Most one-size-fits-all, legacy STT solutions max out at around 80% accuracy. It\u2019s readable but unusable for things like transcribing phone numbers or social security numbers that require 100% accuracy.  One digit off means incorrect answers or insights.</li>\n</ol>\n<p>Deepgram reinvented STT from the ground up specifically to solve these legacy tech issues.</p>\n<h2 id="new-methods-and-easier-access"><strong>New Methods and Easier Access</strong></h2>\n<p>Deepgram\u2019s STT is built with <a href="https://offers.deepgram.com/how-deepgram-works-whitepaper">End-to-End Deep Learning</a>, a neural network that can be trained and learn how to improve accuracy, remove noise, and focus on the important keywords.  We do not use any of the legacy speech recognition processes, therefore, their cost, speed, and accuracy limitations do not apply to us. A better foundation of STT allows us to expand our focus to improve access and use of our technology. With that in mind, we are releasing the following enhancements for a better developer experience with our platform:</p>\n<ul>\n<li>Two software development kits (SDKs) for <a href="https://pypi.org/project/deepgram-sdk/">Python</a> and <a href="https://www.npmjs.com/package/@deepgram/sdk">Node.js</a>, with more languages to come</li>\n<li>A new Developer Console for better API, user, project, billing, and usage management.</li>\n<li>200 hours of free pre-recorded transcription or 150 hours of real-time streaming transcriptions with initial <a href="https://console.deepgram.com/">Developer Console sign up</a>.</li>\n</ul>\n<WhitepaperPromo whitepaper="deepgram-whitepaper-how-deepgram-works" />\n<h2 id="features-available"><strong>Features Available</strong></h2>\n<p>With this new Developer Console, you can try out all the following features that are included under our batch (pre-recorded) or real-time streaming <a href="https://deepgram.com/pricing/">price</a>.</p>\n<ul>\n<li>Interim results</li>\n<li>Punctuation</li>\n<li>Foreign languages</li>\n<li>Numeral formatting</li>\n<li>Utterance formatting</li>\n<li>Find and replace</li>\n<li>Profanity filter</li>\n<li>Keyword boosting</li>\n</ul>\n<p>Descriptions of these features can be found on our <a href="https://deepgram.com/product/overview/">Product Overview</a> page For a limited time, we are also opening up these two features at no charge.</p>\n<ul>\n<li><strong>Deep Search</strong>: Text-based search is highly inaccurate, and has big implications if it\u2019s used for NLU data classification, analytics, and automated experiences. Deep Search drastically increases accuracy with acoustic pattern matching.</li>\n<li><strong>Diarization:</strong> As more companies move to digital communication channels, more voices need to be identified within recordings. Diarization labels audio transcripts with specific identities to allow for better transcripts.</li>\n</ul>\n<p>Happy building and <a href="https://deepgram.com/contact-us/">keep in touch</a>. We\u2019d love to hear how we can keep improving your experience with Deepgram.</p>' };
const frontmatter = { "title": "Hell Yes, We Have SDKs, APIs, and Docs", "description": "Phase One of our Developer-First initiative to help voice technology developers more easily implement our revolutionary End-to-End AI Speech Platform.", "date": "2021-08-03T00:00:00.000Z", "cover": "https://res.cloudinary.com/deepgram/image/upload/v1661981380/blog/hell-yes-we-have-sdks-apis-and-docs/hell-yes-sdks%402x.jpg", "authors": ["keith-lam"], "category": "product-news", "tags": ["sdk"], "seo": { "title": "Hell Yes, We Have SDKs, APIs, and Docs", "description": "Phase One of our Developer-First initiative to help voice technology developers more easily implement our revolutionary End-to-End AI Speech Platform." }, "og": { "image": "https://res.cloudinary.com/deepgram/image/upload/v1661981380/blog/hell-yes-we-have-sdks-apis-and-docs/hell-yes-sdks%402x.jpg" }, "shorturls": { "share": "https://dpgr.am/3d69218", "twitter": "https://dpgr.am/19425e4", "linkedin": "https://dpgr.am/2bcb0a1", "reddit": "https://dpgr.am/1cc2dc5", "facebook": "https://dpgr.am/440ed0e" }, "astro": { "headings": [{ "depth": 3, "slug": "issues-with-legacy-speech-recognition", "text": "Issues With Legacy Speech Recognition" }, { "depth": 2, "slug": "new-methods-and-easier-access", "text": "New Methods and Easier Access" }, { "depth": 2, "slug": "features-available", "text": "Features Available" }], "source": `We are excited to introduce Phase One of our Developer-First initiative to help voice technology developers implement our revolutionary End-to-End AI Speech Platform more easily**.**

### **Issues With Legacy Speech Recognition**

Voice has been one of the last unstructured data sources to be fully used and mined for insights. According to [Deloitte](https://www.deloittedigital.com/content/dam/deloittedigital/us/documents/blog/blog-20190513-2019%20globalcontactcentersurvey.pdf), fully 90% of all complex conversations in contact centers are done through the voice channel and the number is not going to change anytime soon. With the pandemic, the voice channel has become even more important. In the [2021 State of ASR Report](https://deepgram.com/state-of-asr-report/) by Opus Research, 85% of organizations note that automatic speech recognition is "important or very important" to their future enterprise strategy. If audio is so important, why has it not been fully utilized in business?  We believe three main issues prevent wider use of Automatic Speech Recognition (ASR) or Speech-to-Text (STT).

1. **Costs** - The cost of speech-to-text applications has gone down in the last decade because of competition, but has bottomed out due to the use of legacy speech processing that is inefficient and requires huge amounts of computing resources.  The bottom of the legacy cost curve is still too high to seriously consider transcribing all of an organization's voice data.
2. **Speed** - The speed of transcriptions has also gone down, but it still takes more than one day to transcribe one day of contact center calls.  Instead, organizations sample their calls to try to get some customer insights, but they often miss a gem of a product idea, can't respond quickly enough to churn signals, or can't assist sales in upselling a customer.  For applications requiring real-time transcriptions, like Conversational AI voicebots, the 2-4 second legacy STT lag time does not meet their needs.  You notice a one-second lag on video calls or streaming movies, think about waiting up to 4 seconds before a voicebot answers you.
3. **Accuracy** - We don't mean general out-of-the-box accuracy of all words, but the accuracy of the important keywords.  Do you care if you get articles, prepositions, and filler words correct?  Maybe, if a readable transcript is most important to you.  But if you are doing data analysis or finding knowledge base responses, you care more about the product names, phone numbers, government ID numbers, terminology, sentiment words, and acronyms because that is where you can find the insights and intent of the conversation.  Most one-size-fits-all, legacy STT solutions max out at around 80% accuracy. It's readable but unusable for things like transcribing phone numbers or social security numbers that require 100% accuracy.  One digit off means incorrect answers or insights.

Deepgram reinvented STT from the ground up specifically to solve these legacy tech issues.

## **New Methods and Easier Access**

Deepgram's STT is built with [End-to-End Deep Learning](https://offers.deepgram.com/how-deepgram-works-whitepaper), a neural network that can be trained and learn how to improve accuracy, remove noise, and focus on the important keywords.  We do not use any of the legacy speech recognition processes, therefore, their cost, speed, and accuracy limitations do not apply to us. A better foundation of STT allows us to expand our focus to improve access and use of our technology. With that in mind, we are releasing the following enhancements for a better developer experience with our platform:

* Two software development kits (SDKs) for [Python](https://pypi.org/project/deepgram-sdk/) and [Node.js](https://www.npmjs.com/package/@deepgram/sdk), with more languages to come
* A new Developer Console for better API, user, project, billing, and usage management.
* 200 hours of free pre-recorded transcription or 150 hours of real-time streaming transcriptions with initial [Developer Console sign up](https://console.deepgram.com/).

<WhitepaperPromo whitepaper="deepgram-whitepaper-how-deepgram-works"></WhitepaperPromo>

## **Features Available**

With this new Developer Console, you can try out all the following features that are included under our batch (pre-recorded) or real-time streaming [price](https://deepgram.com/pricing/).

* Interim results
* Punctuation
* Foreign languages
* Numeral formatting
* Utterance formatting
* Find and replace
* Profanity filter
* Keyword boosting

Descriptions of these features can be found on our [Product Overview](https://deepgram.com/product/overview/) page For a limited time, we are also opening up these two features at no charge.

* **Deep Search**: Text-based search is highly inaccurate, and has big implications if it's used for NLU data classification, analytics, and automated experiences. Deep Search drastically increases accuracy with acoustic pattern matching. 
* **Diarization:** As more companies move to digital communication channels, more voices need to be identified within recordings. Diarization labels audio transcripts with specific identities to allow for better transcripts.

Happy building and [keep in touch](https://deepgram.com/contact-us/). We'd love to hear how we can keep improving your experience with Deepgram.`, "html": '<p>We are excited to introduce Phase One of our Developer-First initiative to help voice technology developers implement our revolutionary End-to-End AI Speech Platform more easily**.**</p>\n<h3 id="issues-with-legacy-speech-recognition"><strong>Issues With Legacy Speech Recognition</strong></h3>\n<p>Voice has been one of the last unstructured data sources to be fully used and mined for insights. According to <a href="https://www.deloittedigital.com/content/dam/deloittedigital/us/documents/blog/blog-20190513-2019%20globalcontactcentersurvey.pdf">Deloitte</a>, fully 90% of all complex conversations in contact centers are done through the voice channel and the number is not going to change anytime soon. With the pandemic, the voice channel has become even more important. In the <a href="https://deepgram.com/state-of-asr-report/">2021 State of ASR Report</a> by Opus Research, 85% of organizations note that automatic speech recognition is \u201Cimportant or very important\u201D to their future enterprise strategy. If audio is so important, why has it not been fully utilized in business?  We believe three main issues prevent wider use of Automatic Speech Recognition (ASR) or Speech-to-Text (STT).</p>\n<ol>\n<li><strong>Costs</strong> - The cost of speech-to-text applications has gone down in the last decade because of competition, but has bottomed out due to the use of legacy speech processing that is inefficient and requires huge amounts of computing resources.  The bottom of the legacy cost curve is still too high to seriously consider transcribing all of an organization\u2019s voice data.</li>\n<li><strong>Speed</strong> - The speed of transcriptions has also gone down, but it still takes more than one day to transcribe one day of contact center calls.  Instead, organizations sample their calls to try to get some customer insights, but they often miss a gem of a product idea, can\u2019t respond quickly enough to churn signals, or can\u2019t assist sales in upselling a customer.  For applications requiring real-time transcriptions, like Conversational AI voicebots, the 2-4 second legacy STT lag time does not meet their needs.  You notice a one-second lag on video calls or streaming movies, think about waiting up to 4 seconds before a voicebot answers you.</li>\n<li><strong>Accuracy</strong> - We don\u2019t mean general out-of-the-box accuracy of all words, but the accuracy of the important keywords.  Do you care if you get articles, prepositions, and filler words correct?  Maybe, if a readable transcript is most important to you.  But if you are doing data analysis or finding knowledge base responses, you care more about the product names, phone numbers, government ID numbers, terminology, sentiment words, and acronyms because that is where you can find the insights and intent of the conversation.  Most one-size-fits-all, legacy STT solutions max out at around 80% accuracy. It\u2019s readable but unusable for things like transcribing phone numbers or social security numbers that require 100% accuracy.  One digit off means incorrect answers or insights.</li>\n</ol>\n<p>Deepgram reinvented STT from the ground up specifically to solve these legacy tech issues.</p>\n<h2 id="new-methods-and-easier-access"><strong>New Methods and Easier Access</strong></h2>\n<p>Deepgram\u2019s STT is built with <a href="https://offers.deepgram.com/how-deepgram-works-whitepaper">End-to-End Deep Learning</a>, a neural network that can be trained and learn how to improve accuracy, remove noise, and focus on the important keywords.  We do not use any of the legacy speech recognition processes, therefore, their cost, speed, and accuracy limitations do not apply to us. A better foundation of STT allows us to expand our focus to improve access and use of our technology. With that in mind, we are releasing the following enhancements for a better developer experience with our platform:</p>\n<ul>\n<li>Two software development kits (SDKs) for <a href="https://pypi.org/project/deepgram-sdk/">Python</a> and <a href="https://www.npmjs.com/package/@deepgram/sdk">Node.js</a>, with more languages to come</li>\n<li>A new Developer Console for better API, user, project, billing, and usage management.</li>\n<li>200 hours of free pre-recorded transcription or 150 hours of real-time streaming transcriptions with initial <a href="https://console.deepgram.com/">Developer Console sign up</a>.</li>\n</ul>\n<WhitepaperPromo whitepaper="deepgram-whitepaper-how-deepgram-works" />\n<h2 id="features-available"><strong>Features Available</strong></h2>\n<p>With this new Developer Console, you can try out all the following features that are included under our batch (pre-recorded) or real-time streaming <a href="https://deepgram.com/pricing/">price</a>.</p>\n<ul>\n<li>Interim results</li>\n<li>Punctuation</li>\n<li>Foreign languages</li>\n<li>Numeral formatting</li>\n<li>Utterance formatting</li>\n<li>Find and replace</li>\n<li>Profanity filter</li>\n<li>Keyword boosting</li>\n</ul>\n<p>Descriptions of these features can be found on our <a href="https://deepgram.com/product/overview/">Product Overview</a> page For a limited time, we are also opening up these two features at no charge.</p>\n<ul>\n<li><strong>Deep Search</strong>: Text-based search is highly inaccurate, and has big implications if it\u2019s used for NLU data classification, analytics, and automated experiences. Deep Search drastically increases accuracy with acoustic pattern matching.</li>\n<li><strong>Diarization:</strong> As more companies move to digital communication channels, more voices need to be identified within recordings. Diarization labels audio transcripts with specific identities to allow for better transcripts.</li>\n</ul>\n<p>Happy building and <a href="https://deepgram.com/contact-us/">keep in touch</a>. We\u2019d love to hear how we can keep improving your experience with Deepgram.</p>' }, "file": "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/hell-yes-we-have-sdks-apis-and-docs/index.md" };
function rawContent() {
  return `We are excited to introduce Phase One of our Developer-First initiative to help voice technology developers implement our revolutionary End-to-End AI Speech Platform more easily**.**

### **Issues With Legacy Speech Recognition**

Voice has been one of the last unstructured data sources to be fully used and mined for insights. According to [Deloitte](https://www.deloittedigital.com/content/dam/deloittedigital/us/documents/blog/blog-20190513-2019%20globalcontactcentersurvey.pdf), fully 90% of all complex conversations in contact centers are done through the voice channel and the number is not going to change anytime soon. With the pandemic, the voice channel has become even more important. In the [2021 State of ASR Report](https://deepgram.com/state-of-asr-report/) by Opus Research, 85% of organizations note that automatic speech recognition is "important or very important" to their future enterprise strategy. If audio is so important, why has it not been fully utilized in business?  We believe three main issues prevent wider use of Automatic Speech Recognition (ASR) or Speech-to-Text (STT).

1. **Costs** - The cost of speech-to-text applications has gone down in the last decade because of competition, but has bottomed out due to the use of legacy speech processing that is inefficient and requires huge amounts of computing resources.  The bottom of the legacy cost curve is still too high to seriously consider transcribing all of an organization's voice data.
2. **Speed** - The speed of transcriptions has also gone down, but it still takes more than one day to transcribe one day of contact center calls.  Instead, organizations sample their calls to try to get some customer insights, but they often miss a gem of a product idea, can't respond quickly enough to churn signals, or can't assist sales in upselling a customer.  For applications requiring real-time transcriptions, like Conversational AI voicebots, the 2-4 second legacy STT lag time does not meet their needs.  You notice a one-second lag on video calls or streaming movies, think about waiting up to 4 seconds before a voicebot answers you.
3. **Accuracy** - We don't mean general out-of-the-box accuracy of all words, but the accuracy of the important keywords.  Do you care if you get articles, prepositions, and filler words correct?  Maybe, if a readable transcript is most important to you.  But if you are doing data analysis or finding knowledge base responses, you care more about the product names, phone numbers, government ID numbers, terminology, sentiment words, and acronyms because that is where you can find the insights and intent of the conversation.  Most one-size-fits-all, legacy STT solutions max out at around 80% accuracy. It's readable but unusable for things like transcribing phone numbers or social security numbers that require 100% accuracy.  One digit off means incorrect answers or insights.

Deepgram reinvented STT from the ground up specifically to solve these legacy tech issues.

## **New Methods and Easier Access**

Deepgram's STT is built with [End-to-End Deep Learning](https://offers.deepgram.com/how-deepgram-works-whitepaper), a neural network that can be trained and learn how to improve accuracy, remove noise, and focus on the important keywords.  We do not use any of the legacy speech recognition processes, therefore, their cost, speed, and accuracy limitations do not apply to us. A better foundation of STT allows us to expand our focus to improve access and use of our technology. With that in mind, we are releasing the following enhancements for a better developer experience with our platform:

* Two software development kits (SDKs) for [Python](https://pypi.org/project/deepgram-sdk/) and [Node.js](https://www.npmjs.com/package/@deepgram/sdk), with more languages to come
* A new Developer Console for better API, user, project, billing, and usage management.
* 200 hours of free pre-recorded transcription or 150 hours of real-time streaming transcriptions with initial [Developer Console sign up](https://console.deepgram.com/).

<WhitepaperPromo whitepaper="deepgram-whitepaper-how-deepgram-works"></WhitepaperPromo>

## **Features Available**

With this new Developer Console, you can try out all the following features that are included under our batch (pre-recorded) or real-time streaming [price](https://deepgram.com/pricing/).

* Interim results
* Punctuation
* Foreign languages
* Numeral formatting
* Utterance formatting
* Find and replace
* Profanity filter
* Keyword boosting

Descriptions of these features can be found on our [Product Overview](https://deepgram.com/product/overview/) page For a limited time, we are also opening up these two features at no charge.

* **Deep Search**: Text-based search is highly inaccurate, and has big implications if it's used for NLU data classification, analytics, and automated experiences. Deep Search drastically increases accuracy with acoustic pattern matching. 
* **Diarization:** As more companies move to digital communication channels, more voices need to be identified within recordings. Diarization labels audio transcripts with specific identities to allow for better transcripts.

Happy building and [keep in touch](https://deepgram.com/contact-us/). We'd love to hear how we can keep improving your experience with Deepgram.`;
}
function compiledContent() {
  return '<p>We are excited to introduce Phase One of our Developer-First initiative to help voice technology developers implement our revolutionary End-to-End AI Speech Platform more easily**.**</p>\n<h3 id="issues-with-legacy-speech-recognition"><strong>Issues With Legacy Speech Recognition</strong></h3>\n<p>Voice has been one of the last unstructured data sources to be fully used and mined for insights. According to <a href="https://www.deloittedigital.com/content/dam/deloittedigital/us/documents/blog/blog-20190513-2019%20globalcontactcentersurvey.pdf">Deloitte</a>, fully 90% of all complex conversations in contact centers are done through the voice channel and the number is not going to change anytime soon. With the pandemic, the voice channel has become even more important. In the <a href="https://deepgram.com/state-of-asr-report/">2021 State of ASR Report</a> by Opus Research, 85% of organizations note that automatic speech recognition is \u201Cimportant or very important\u201D to their future enterprise strategy. If audio is so important, why has it not been fully utilized in business?  We believe three main issues prevent wider use of Automatic Speech Recognition (ASR) or Speech-to-Text (STT).</p>\n<ol>\n<li><strong>Costs</strong> - The cost of speech-to-text applications has gone down in the last decade because of competition, but has bottomed out due to the use of legacy speech processing that is inefficient and requires huge amounts of computing resources.  The bottom of the legacy cost curve is still too high to seriously consider transcribing all of an organization\u2019s voice data.</li>\n<li><strong>Speed</strong> - The speed of transcriptions has also gone down, but it still takes more than one day to transcribe one day of contact center calls.  Instead, organizations sample their calls to try to get some customer insights, but they often miss a gem of a product idea, can\u2019t respond quickly enough to churn signals, or can\u2019t assist sales in upselling a customer.  For applications requiring real-time transcriptions, like Conversational AI voicebots, the 2-4 second legacy STT lag time does not meet their needs.  You notice a one-second lag on video calls or streaming movies, think about waiting up to 4 seconds before a voicebot answers you.</li>\n<li><strong>Accuracy</strong> - We don\u2019t mean general out-of-the-box accuracy of all words, but the accuracy of the important keywords.  Do you care if you get articles, prepositions, and filler words correct?  Maybe, if a readable transcript is most important to you.  But if you are doing data analysis or finding knowledge base responses, you care more about the product names, phone numbers, government ID numbers, terminology, sentiment words, and acronyms because that is where you can find the insights and intent of the conversation.  Most one-size-fits-all, legacy STT solutions max out at around 80% accuracy. It\u2019s readable but unusable for things like transcribing phone numbers or social security numbers that require 100% accuracy.  One digit off means incorrect answers or insights.</li>\n</ol>\n<p>Deepgram reinvented STT from the ground up specifically to solve these legacy tech issues.</p>\n<h2 id="new-methods-and-easier-access"><strong>New Methods and Easier Access</strong></h2>\n<p>Deepgram\u2019s STT is built with <a href="https://offers.deepgram.com/how-deepgram-works-whitepaper">End-to-End Deep Learning</a>, a neural network that can be trained and learn how to improve accuracy, remove noise, and focus on the important keywords.  We do not use any of the legacy speech recognition processes, therefore, their cost, speed, and accuracy limitations do not apply to us. A better foundation of STT allows us to expand our focus to improve access and use of our technology. With that in mind, we are releasing the following enhancements for a better developer experience with our platform:</p>\n<ul>\n<li>Two software development kits (SDKs) for <a href="https://pypi.org/project/deepgram-sdk/">Python</a> and <a href="https://www.npmjs.com/package/@deepgram/sdk">Node.js</a>, with more languages to come</li>\n<li>A new Developer Console for better API, user, project, billing, and usage management.</li>\n<li>200 hours of free pre-recorded transcription or 150 hours of real-time streaming transcriptions with initial <a href="https://console.deepgram.com/">Developer Console sign up</a>.</li>\n</ul>\n<WhitepaperPromo whitepaper="deepgram-whitepaper-how-deepgram-works" />\n<h2 id="features-available"><strong>Features Available</strong></h2>\n<p>With this new Developer Console, you can try out all the following features that are included under our batch (pre-recorded) or real-time streaming <a href="https://deepgram.com/pricing/">price</a>.</p>\n<ul>\n<li>Interim results</li>\n<li>Punctuation</li>\n<li>Foreign languages</li>\n<li>Numeral formatting</li>\n<li>Utterance formatting</li>\n<li>Find and replace</li>\n<li>Profanity filter</li>\n<li>Keyword boosting</li>\n</ul>\n<p>Descriptions of these features can be found on our <a href="https://deepgram.com/product/overview/">Product Overview</a> page For a limited time, we are also opening up these two features at no charge.</p>\n<ul>\n<li><strong>Deep Search</strong>: Text-based search is highly inaccurate, and has big implications if it\u2019s used for NLU data classification, analytics, and automated experiences. Deep Search drastically increases accuracy with acoustic pattern matching.</li>\n<li><strong>Diarization:</strong> As more companies move to digital communication channels, more voices need to be identified within recordings. Diarization labels audio transcripts with specific identities to allow for better transcripts.</li>\n</ul>\n<p>Happy building and <a href="https://deepgram.com/contact-us/">keep in touch</a>. We\u2019d love to hear how we can keep improving your experience with Deepgram.</p>';
}
const $$Astro = createAstro("/Users/sandrarodgers/web-next/blog/src/content/blog/posts/hell-yes-we-have-sdks-apis-and-docs/index.md", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Index = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro, $$props, $$slots);
  Astro2.self = $$Index;
  new Slugger();
  return renderTemplate`<head>${renderHead($$result)}</head><p>We are excited to introduce Phase One of our Developer-First initiative to help voice technology developers implement our revolutionary End-to-End AI Speech Platform more easily**.**</p>
<h3 id="issues-with-legacy-speech-recognition"><strong>Issues With Legacy Speech Recognition</strong></h3>
<p>Voice has been one of the last unstructured data sources to be fully used and mined for insights. According to <a href="https://www.deloittedigital.com/content/dam/deloittedigital/us/documents/blog/blog-20190513-2019%20globalcontactcentersurvey.pdf">Deloitte</a>, fully 90% of all complex conversations in contact centers are done through the voice channel and the number is not going to change anytime soon. With the pandemic, the voice channel has become even more important. In the <a href="https://deepgram.com/state-of-asr-report/">2021 State of ASR Report</a> by Opus Research, 85% of organizations note that automatic speech recognition is “important or very important” to their future enterprise strategy. If audio is so important, why has it not been fully utilized in business?  We believe three main issues prevent wider use of Automatic Speech Recognition (ASR) or Speech-to-Text (STT).</p>
<ol>
<li><strong>Costs</strong> - The cost of speech-to-text applications has gone down in the last decade because of competition, but has bottomed out due to the use of legacy speech processing that is inefficient and requires huge amounts of computing resources.  The bottom of the legacy cost curve is still too high to seriously consider transcribing all of an organization’s voice data.</li>
<li><strong>Speed</strong> - The speed of transcriptions has also gone down, but it still takes more than one day to transcribe one day of contact center calls.  Instead, organizations sample their calls to try to get some customer insights, but they often miss a gem of a product idea, can’t respond quickly enough to churn signals, or can’t assist sales in upselling a customer.  For applications requiring real-time transcriptions, like Conversational AI voicebots, the 2-4 second legacy STT lag time does not meet their needs.  You notice a one-second lag on video calls or streaming movies, think about waiting up to 4 seconds before a voicebot answers you.</li>
<li><strong>Accuracy</strong> - We don’t mean general out-of-the-box accuracy of all words, but the accuracy of the important keywords.  Do you care if you get articles, prepositions, and filler words correct?  Maybe, if a readable transcript is most important to you.  But if you are doing data analysis or finding knowledge base responses, you care more about the product names, phone numbers, government ID numbers, terminology, sentiment words, and acronyms because that is where you can find the insights and intent of the conversation.  Most one-size-fits-all, legacy STT solutions max out at around 80% accuracy. It’s readable but unusable for things like transcribing phone numbers or social security numbers that require 100% accuracy.  One digit off means incorrect answers or insights.</li>
</ol>
<p>Deepgram reinvented STT from the ground up specifically to solve these legacy tech issues.</p>
<h2 id="new-methods-and-easier-access"><strong>New Methods and Easier Access</strong></h2>
<p>Deepgram’s STT is built with <a href="https://offers.deepgram.com/how-deepgram-works-whitepaper">End-to-End Deep Learning</a>, a neural network that can be trained and learn how to improve accuracy, remove noise, and focus on the important keywords.  We do not use any of the legacy speech recognition processes, therefore, their cost, speed, and accuracy limitations do not apply to us. A better foundation of STT allows us to expand our focus to improve access and use of our technology. With that in mind, we are releasing the following enhancements for a better developer experience with our platform:</p>
<ul>
<li>Two software development kits (SDKs) for <a href="https://pypi.org/project/deepgram-sdk/">Python</a> and <a href="https://www.npmjs.com/package/@deepgram/sdk">Node.js</a>, with more languages to come</li>
<li>A new Developer Console for better API, user, project, billing, and usage management.</li>
<li>200 hours of free pre-recorded transcription or 150 hours of real-time streaming transcriptions with initial <a href="https://console.deepgram.com/">Developer Console sign up</a>.</li>
</ul>
${renderComponent($$result, "WhitepaperPromo", WhitepaperPromo, { "whitepaper": "deepgram-whitepaper-how-deepgram-works" })}
<h2 id="features-available"><strong>Features Available</strong></h2>
<p>With this new Developer Console, you can try out all the following features that are included under our batch (pre-recorded) or real-time streaming <a href="https://deepgram.com/pricing/">price</a>.</p>
<ul>
<li>Interim results</li>
<li>Punctuation</li>
<li>Foreign languages</li>
<li>Numeral formatting</li>
<li>Utterance formatting</li>
<li>Find and replace</li>
<li>Profanity filter</li>
<li>Keyword boosting</li>
</ul>
<p>Descriptions of these features can be found on our <a href="https://deepgram.com/product/overview/">Product Overview</a> page For a limited time, we are also opening up these two features at no charge.</p>
<ul>
<li><strong>Deep Search</strong>: Text-based search is highly inaccurate, and has big implications if it’s used for NLU data classification, analytics, and automated experiences. Deep Search drastically increases accuracy with acoustic pattern matching.</li>
<li><strong>Diarization:</strong> As more companies move to digital communication channels, more voices need to be identified within recordings. Diarization labels audio transcripts with specific identities to allow for better transcripts.</li>
</ul>
<p>Happy building and <a href="https://deepgram.com/contact-us/">keep in touch</a>. We’d love to hear how we can keep improving your experience with Deepgram.</p>`;
}, "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/hell-yes-we-have-sdks-apis-and-docs/index.md");

export { compiledContent, $$Index as default, frontmatter, metadata, rawContent };
