import { c as createAstro, a as createComponent, r as renderTemplate, b as renderHead } from '../entry.mjs';
import Slugger from 'github-slugger';
import '@astrojs/netlify/netlify-functions.js';
import 'preact';
import 'preact-render-to-string';
import 'vue';
import 'vue/server-renderer';
import 'html-escaper';
import 'node-html-parser';
import 'axios';
/* empty css                           *//* empty css                           *//* empty css                           */import '@storyblok/js';
/* empty css                           *//* empty css                          */import 'clone-deep';
import 'slugify';
import 'shiki';
/* empty css                           */import 'camelcase';
import '@astrojs/rss';
/* empty css                           */import 'mime';
import 'cookie';
import 'kleur/colors';
import 'string-width';
import 'path-browserify';
import 'path-to-regexp';

const metadata = { "headings": [], "source": "*This is the transcript for the opening keynote presented by Scott Stephenson, CEO of Deepgram, presented on day one of Project Voice X.*\xA0\n\n*The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was\xA0**94% accurate.**\xA0 Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram\u2019s API.\xA0 If you want to see if Deepgram is right for your use case,\xA0[contact us](https://deepgram.com/contact-us/).*\n\n\\[Scott Stephenson:] Hey, everybody. Thank you for for listening and coming to the event. I know it\u2019s kind of interesting. Yeah. This\u2026 for us, at least, at Deepgram, this is a first in-person event back. Maybe for most people, that that might be the case. But, yeah, I\u2019m happy to be here, happy to meet all the new people. And I just wanna thank all the other speakers before me as well, a lot of great discussion today. And I\u2019m also going to talk about the future of voice, not the distant, distant future. I\u2019ll talk about the next couple years. But I\u2019ll give you a little bit of background on myself, though, before we do that.\n\nSo I\u2019m Scott Stephenson. I\u2019m a CEO and cofounder of Deepgram. I\u2019m also\u2026 I\u2019m a technical CEO. So if people out there wanna talk, you know, shop with me, that\u2019s totally fine. I I was a particle physicist before, so I I built deep underground dark matter detectors and and that kind of thing. So if you\u2019re interested in that, happy to talk about that as well. But but, yeah, Deepgram is a company that builds APIs. You just you just saw some of the performance of it, but we build speech APIs in order to enable the next generation of voice products to be built. And that can be in real time. It can be in batch mode, but we want to enable developers to build that piece. So think of Deepgram, like, you\u2026 in the same category that you would Stripe or Twilio or something like that in order to enable developers to build voice products.\n\nI\u2019m gonna I\u2019m gonna talk about something in particular, and then we\u2019ll maybe go a little bit broader after that. But I\u2019ll I\u2019ll talk about something that\u2019s happening right now, which is we\u2019re starting to see the adoption of automation and AI in the call center space and in other areas as well. But, in particular, you see it, like, really ramping up in meetings and call center. But one thing I see is this this this this trend of the old way, which was if you wanted to figure out if you had happy customers or how well things are going or that type of thing, what you would do is ask them. You you would send out a survey. You\u2019d try to corner them. You know, you try to do these very unscalable things or things that maybe had questionable statistics behind them on on how how well they worked. But you basically didn\u2019t have an alternative otherwise.\n\nAnd so I\u2019m here to talk about that a little bit about what\u2019s actually happening now is, yes, some of that old stuff is still happening, but you already hear me saying old stuff. Because what\u2019s actually happening now is people are saying, hey. We already have piles of data. There\u2019s phone calls happening. There\u2019s voice bot interactions. There\u2019s emails. There\u2019s other things that are coming in here.\n\nLooks like we lost our connection. I\u2019m not sure why. K. Sorry about that. We\u2019ll see if I can\u2026 I don\u2019t think I hit anything, but let\u2019s see.\n\nOk. So so it can be text interactions. It can be voice interactions. There\u2019s there\u2019s a a number of ways that customers will interact with your product. Could just be clicks on your website, that type of thing. But if if you have to ask for the survey, if you have to, you know, send a text afterward, that type of thing, then you\u2019re gonna get a very biased response. People that hate you or love you. What about the in between? What about what they were thinking in the moment? You know, that type of thing. And so I\u2019m I\u2019m\n\n> I\u2019m saying, hey. That was the old way. The new way is if if you want the act\u2026 if you want to figure out what\u2019s going on, doing product development with your customers, see how all that goes, stop asking them and just start listening to them.\n\nSo what do I mean by that? I mean, just take the data as it sits and be able to analyze it at scale to figure out what\u2019s going on. And so this could be a retail space, where you have microphones and people are talking about certain clothes that they want to try on. Hey. I hate this. That looks great, etcetera, that type of thing. Just listen to them in situ. If it\u2019s a call center, somebody calling in saying, I hate this. I like this, etcetera, like, right in right in situ in in in the data, looking inside.\n\nAnd there\u2019s\u2026 essentially, for data scientists, like myself and lots of other people, they look at this huge data lake, dark dataset that previously just was, like, untouched. And maybe if you\u2019re in the call center space, you know that there would be, like, a QA sampling kind of thing that would happen. Maybe one percent of calls would be listened to by a team of, like, ten people, where each person would listen to, like, ten calls a day or something like that. And they would try to randomly sample and and see what they could get out of it, but it\u2019s a really, really small percentage. And, you know, it had marginal success, had some success but marginal.\n\nAnd so the next step in obvious evolution there is if you can automate this process and make it actually good and and and, again, see previous demo, then you can learn a lot about it, about what they\u2019re actually saying for real. And so now the question would be, though. Alright. That\u2019s that\u2019s great. You told us there was something previously that was really hard to do, and now it\u2019s really easy to do or it seems like it\u2019s easy to do. How does that all work? It seems hard. Well, it is hard. Audio is a really hard problem, and I\u2019m just going to describe that a little bit to you.\n\nSo take five seconds and look at this image, and I won\u2019t say anything. Ok. Does anybody remember what happened in that image? There\u2019s there\u2019s a dog, has a blue Frisbee. There\u2019s a girl probably fifty feet in the background with a\u2026 with, like, a bodyboard. There\u2019s another bodyboard. And I don\u2019t know why I keep going out here. But from the from the image perspective, you can glean a lot of information from a very small amount of time. And this makes labeling images really easy and inexpensive in a lot of ways. It\u2026 trust me. It\u2019s still expensive. But compared to audio and and many other sources, it\u2019s it\u2019s, like, a lot easier. But you can get this whole story just from that quick five seconds. But now what if what if you were asked to look at this hour-long lecture for five seconds and then tell me, like, what happened. Right? You know, you get five seconds. Like, I\u2019m not gonna play, you know, biochemistry lecture for you. But but but, nevertheless, five seconds with that, you\u2019ll get five words, you know, ten, fifteen words, like, whatever. Right? You\u2019re\u2026 there isn\u2019t much you can do. Right? And so, hopefully, that illustrates a little bit about audio that\u2019s happening. It\u2019s real time. It\u2019s very hard for a person to parallel process.\n\nYou can you can recognize that when you\u2019re in a cocktail-party problem, which we\u2019ll all have later today when we\u2019re all sitting around trying to talk to each other, many conversations going on, etcetera. You can only really track one, maybe one and a half. You know? That kinda thing. But but part part of the reason\u2026 or\u2026 you know, part of the reason for that is humans are actually pretty good in real time in conversation. They can understand what what other humans are doing, but we have specialized hardware for it. You know? We have our brain that we\u2026 and ears and everything that is tuned just for listening to other humans talk and computers don\u2019t. They they see the image like what is on the right here. And it\u2019s\u2026 that\u2019s that\u2019s not the whole, like, sixty hour lecture. That\u2019s, like, a couple seconds of it. You know? So it\u2019s a whole mess. Right? And if you were to if you were to try to represent that entire lecture that way, you, as a human, would have no idea what\u2019s going on. You might be able to say, a person was talking in this area or not. That that might be a question you can answer, but but, otherwise, you have no idea what they\u2019re saying.\n\nBut if you run it through our ears and through our brain in real time, then we can understand it pretty well. But now how do you get a machine to actually do that? And that\u2019s the real trick. Right? And if you have a machine that can do it, then it\u2019s probably very valuable because now you can understand humans at scale. And that\u2019s the type of thing that we build. I\u2019ll just point out a few of these problems again, like, in a in a call center space. Maybe you have specific product names or company names or industry terms and jargon or certain acronyms that you\u2019re using if you\u2019re a bank or whatever it is. There\u2019s also how it said, so maybe you\u2019re speaking in a specific language. English, for instance. You could have a certain accent, dialect, etcetera. You could be upset or not. There could be background noise. You could be using a phone or some other device to actually talk to people, like calling in on a meeting or something. And, generally, when you\u2019re having these conversations, they\u2019re in a conversational style.\n\nIt\u2019s a fast type of speaking, like I\u2019m doing right now. Sorry about that. But but, nevertheless, it\u2019s very conversational. You\u2019re not trying to purposefully slow down and enunciate so that the machine can understand you. You know? And so that\u2019s a challenge you have to deal with. And then audio\u2026 actually, a lot of people might think of audio as not that big of, like, a file, but it\u2019s about the third the size of video. So there\u2019s a lot of information packed in audio in order to make it sound good enough for you to understand. And there\u2019s different encoding and different compression techniques, and, you know, there could be several channels, like one person talking on one\u2026 like like an agent and then, like, the customer that\u2019s calling in, so multiple channels.\n\nAnd so, anyway, audio is very varied and very difficult. But the the way that you have to handle this, and I I\u2026 I\u2019ll talk technically just for a second here. But as you need a lot of data that is labeled, but it\u2019s not just a lot of it. Like, the hour count matters, but it doesn\u2019t matter as much as you might think. But it definitely matters. You need a lot in different areas, so young, old, male, female, low noise, high noise, in the car, not, upset, not upset, you know, very happy, etcetera. And you need this in multiple languages, multiple accents, etcetera, and you all\u2026 you need that all labeled as, like, a a a data plus truth pair, essentially. So here\u2019s the audio and here\u2019s the transcript that goes with that audio, and then the machine can learn by example. And you can kind of see that if you can read it here.\n\nAudio comes in labeled, and that\u2019s converted into training data. And then that makes a trained model, which you use, like, convolutional neural networks, dense dense layers or recurrent neural networks or attention layers or that type of thing is just kind of\u2026 I think of it like a like a periodic table of chemical elements, but, instead, it\u2019s just for deep learning. You have these different things that care about spatial or temporal or that type of thing. But, nevertheless, mix those up, put them in the right order, expose them to a bunch of training data. And then you have a general model and\u2026 which is if many, if people in here have used APIs before or, you know, you\u2019ve used your, like, keyboard on your phone, then, generally, what you\u2019re using is is a general model, which is a a model that\u2019s a one size fits all or jack-of-all-trades. Not necessarily a master of one, but it\u2026 it\u2019s it\u2019s designed to hopefully do pretty ok with the general population.\n\nBut if you have a specific task, like you\u2019re our customer, like Citibank, and you\u2019re like, hey. I have bankers, and they\u2019re talking about bank stuff. And we wanna make sure that they\u2019re compliant in the things that they\u2019re doing, then you can go through this bottom loop here that says taking new data and label that to do transfer learning on it. So you have a\u2026 your original model that is already really good at English or other languages, and then now you expose it to a specific domain and then serve that in order to accomplish to your task.\n\nSo, anyway, if you wanna talk about the technical side of it later, I\u2019m happy to happy to discuss, but that\u2019s how it all works under the hood. But now you can get wet\u2026 you can get rid of the old way, or keep doing it if if you find value in it, but add the new way, which is doing a hundred percent analysis across all your data. Do it in a reliable way. Do it in a scalable way and with high accuracy. And so I think you you guys\u2026 Cyrano, one of our customers, you just saw the demo. We\u2019re we\u2019re pumped to have them, and we can discuss a little bit later. Again, we have a booth back there. But\u2026 about the use case there, so I I won\u2019t take too much time talking about them, but another company that maybe some have heard heard of in the audience is Valyant.\n\nAnd Valyant does fast-food ordering. So\u2026 well, it doesn\u2019t have to be fast food. It could be many other things, but, typically, it\u2019s fast food. So think of driving through and you\u2019re in your car or you\u2019re on your mobile phone or you\u2019re at a kiosk, and you\u2019re trying to order food inside a mall or something like that. And you have a very specific menu items, like Baconator or something like that. And you\u2019d like your model to be very good at those things in this very noisy environment. And how do you do that? Well, you just saw magic. You have a general model, and then you transfer learn that model into their domain, and then it works really well. And another place that this works really well is in space.\n\nSo NASA is one of our customers. They they do space-to-ground communication. So we worked with the CTO and their team at the Johnson Space Center in Houston in Houston. And what they\u2019re trying to do is under\u2026 essentially, there\u2019s always a link between the international space station and ground. And there\u2019s people listening all the time, like actual humans. Three of them sitting down and transcribing everything that\u2019s happening. And, hey, can we, like, reduce that number to, like, two people or one person or zero people or something like that. And this is the problem that they\u2019re working on, and it\u2019s it\u2019s a\u2026 it\u2019s a big challenge, and their their CTO says, hey. The problem\u2019s right in our faces. We have we have all of seventy five hundred pieces of jargon and acronyms, and it\u2019s a scratchy, you know, radio signal, and it\u2019s it\u2019s it\u2019s not a pleasant thing to try to transcribe. But, hey, if there were a machine that could listen to this and do a good job of it, then that would be very valuable to them. And so, anyway, they they tried a lot of vendors.\n\nAnd as you can maybe imagine using a general model approach, which is what pretty much everybody else does, then it doesn\u2019t work that well. But if you use this transfer-learning approach, then it works really well handling background noise, jargon, multi speaker, etcetera. And, yeah, happy to talk about these use cases later if you want to. But\u2026 so, yeah, moving on to act three here a little bit, which is just a comment on what\u2019s happening in the world. We\u2019re in the middle of the intelligence revolution, like it or not. There was the agricultural revolution, industrial revolution, information revolution. We\u2019re in the intelligence revolution right now. Automation is going to happen. Thirty years from now we\u2019re gonna look back and be like, oh, yeah. Those were the beginning days. You know? Just like if anybody in here was around when the Internet was first starting to form. You remember, you know, when you got your first email account, and you\u2019re like, holy shit. This is crazy. You know, you can talk to anybody anywhere.\n\nAnd, anyway, same\u2026 we\u2019re we\u2019re in that we\u2019re in that point in time right now. And another comment is that real time is finally real. A couple of years ago, it wasn\u2019t, and now it actually is. That\u2019s partially because the models are getting good enough to do things in real time, but it\u2019s also partially because the computational techniques have become sophisticated enough, but not as heavy lifting plus other\u2026 like, NVIDIA has gotten good essentially at processing larger model sizes and that type of thing. So, essentially, confluence of hardware plus software plus new techniques being implemented in software can make real time\u2026 high-accuracy real time actually real. And then these techniques, if you\u2019re if you\u2019re in a general domain, you can get, like, ninety percent plus accuracy with just a general model.\n\nOr if you if you need that kind of accuracy, but it\u2019s kind of a niche domain, then you can go after a custom-trained model. And this stuff all works now, basically. So a lot a lot of times come\u2026 people come to me and say, like, hey. Is it possible too? And the the\u2026 in audio, the answer is almost like it\u2026 it\u2019s almost always yes, unless it\u2019s something that a human really can\u2019t do. Like, can you build a lie detector? Like, well, I mean, kind of. Yes. But as as well as maybe you could as a human. You know? May\u2026 I mean, not quite that well, but close. Right? But think about all the other things that a human can do. Like, they can jump into a conversation, tell you how many people are speaking when they\u2019re speaking, what words they\u2019re talking about, what topic they\u2019re talking about, that type of thing.\n\nThese things are all possible, but it\u2019s still kind of, like, the railroad was built across the United States, but there\u2019s a whole bunch of other work that has to be done and so all that stuff is happening now. So, yeah, the the people who win the west, you know, over the next couple decades will be the ones that empower the developers to build this architecture to\u2026 you know, the the railroad builders and the towns that sprout up along those railroad lines. But it\u2019s going to be in compliance spaces, voice bots, call centers, the meetings, podcast. It\u2019s gonna be all over the place. I mean, for voice, it\u2019s just like a\u2026 it\u2019s the natural human commit.\n\nI\u2019m up here talking right now. Right? Like, this is the natural communication mechanism, and it just previously was enabled by connectivity. So you could call somebody. You could do that you could do that type of thing, but there was never any automation or intelligence behind it, and now that\u2019s all changing. And, you know, our lives are gonna change massively because of it. And I think the productivity of the world is gonna go up significantly, which previous, you know, talks that we heard today we\u2019re all hinting at that. You know, our lives are gonna change a lot. So I\u2019d\u2026 I I don\u2019t know if there\u2019s\u2026 if I if\u2026 maybe people are tired of hands. I don\u2019t know. But are there start-up founders in the room here? Like, any\u2026 ok. Cool. So if you\u2019re if if you\u2019re into voice and you\u2019re a start-up founder, you might wanna know about this.\n\nSo Deepgram has a start-up program that we gave away ten million dollars in speech-recognition credit. And what you can do is apply. Get up to a hundred thousand dollars for your start-up. And the reason we do this is to help push forward the the new the new products being built. I I don\u2019t I don\u2019t know if people are quite aware of what happened with AWS, GCP, Azure, etcetera over the last ten, fifteen years. But they gave away a lot of credit, and a lot of tech companies benefited as they grew their company on that initial credit. And, of course, you know, for Deepgram, the goal here is if you grow a whole bunch and become awesome then, you know, great. You\u2019re you\u2019re a customer of Deepgram, and we\u2019re happy to help you. And another one.\n\nSo tomorrow evening, we have\u2026 how far away is it? A mile, but we have buses. We have party buses. Ok. So tomorrow evening, we\u2019re going to\u2026 we\u2026 we\u2019re gonna have a private screening of Dune. So if if you haven\u2019t seen Dune yet, now\u2019s an opportunity to do it at a place called Suds n n Cinema Suds n Cinema, which is a pretty dope theater, and we could all watch Dune together if you wanna come hang out and do that. We also have a booth. And we have a special\u2026 a a new a new member of our Deepgram team. We call him our VP of Intrigue. If you want to come meet him, come talk to us at the booth. I won\u2019t say anymore. But thank you, everybody, for listening to the talk, and I I appreciate everybody coming out to hear it.\n\n\n\n ![](https://res.cloudinary.com/deepgram/image/upload/v1661976850/blog/building-the-future-of-voice-scott-stephenson-ceo-deepgram-project-voice-x/scott-stephenson-script-analysis-chart%402x-1-300x16.png)\n\n\n\n*To learn more about real-time voice analysis, check out [Cyrano.ai](https://www.cyrano.ai/).*", "html": '<p><em>This is the transcript for the opening keynote presented by Scott Stephenson, CEO of Deepgram, presented on day one of Project Voice X.</em>\xA0</p>\n<p><em>The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was\xA0<strong>94% accurate.</strong>\xA0 Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram\u2019s API.\xA0 If you want to see if Deepgram is right for your use case,\xA0<a href="https://deepgram.com/contact-us/">contact us</a>.</em></p>\n<p>[Scott Stephenson:] Hey, everybody. Thank you for for listening and coming to the event. I know it\u2019s kind of interesting. Yeah. This\u2026 for us, at least, at Deepgram, this is a first in-person event back. Maybe for most people, that that might be the case. But, yeah, I\u2019m happy to be here, happy to meet all the new people. And I just wanna thank all the other speakers before me as well, a lot of great discussion today. And I\u2019m also going to talk about the future of voice, not the distant, distant future. I\u2019ll talk about the next couple years. But I\u2019ll give you a little bit of background on myself, though, before we do that.</p>\n<p>So I\u2019m Scott Stephenson. I\u2019m a CEO and cofounder of Deepgram. I\u2019m also\u2026 I\u2019m a technical CEO. So if people out there wanna talk, you know, shop with me, that\u2019s totally fine. I I was a particle physicist before, so I I built deep underground dark matter detectors and and that kind of thing. So if you\u2019re interested in that, happy to talk about that as well. But but, yeah, Deepgram is a company that builds APIs. You just you just saw some of the performance of it, but we build speech APIs in order to enable the next generation of voice products to be built. And that can be in real time. It can be in batch mode, but we want to enable developers to build that piece. So think of Deepgram, like, you\u2026 in the same category that you would Stripe or Twilio or something like that in order to enable developers to build voice products.</p>\n<p>I\u2019m gonna I\u2019m gonna talk about something in particular, and then we\u2019ll maybe go a little bit broader after that. But I\u2019ll I\u2019ll talk about something that\u2019s happening right now, which is we\u2019re starting to see the adoption of automation and AI in the call center space and in other areas as well. But, in particular, you see it, like, really ramping up in meetings and call center. But one thing I see is this this this this trend of the old way, which was if you wanted to figure out if you had happy customers or how well things are going or that type of thing, what you would do is ask them. You you would send out a survey. You\u2019d try to corner them. You know, you try to do these very unscalable things or things that maybe had questionable statistics behind them on on how how well they worked. But you basically didn\u2019t have an alternative otherwise.</p>\n<p>And so I\u2019m here to talk about that a little bit about what\u2019s actually happening now is, yes, some of that old stuff is still happening, but you already hear me saying old stuff. Because what\u2019s actually happening now is people are saying, hey. We already have piles of data. There\u2019s phone calls happening. There\u2019s voice bot interactions. There\u2019s emails. There\u2019s other things that are coming in here.</p>\n<p>Looks like we lost our connection. I\u2019m not sure why. K. Sorry about that. We\u2019ll see if I can\u2026 I don\u2019t think I hit anything, but let\u2019s see.</p>\n<p>Ok. So so it can be text interactions. It can be voice interactions. There\u2019s there\u2019s a a number of ways that customers will interact with your product. Could just be clicks on your website, that type of thing. But if if you have to ask for the survey, if you have to, you know, send a text afterward, that type of thing, then you\u2019re gonna get a very biased response. People that hate you or love you. What about the in between? What about what they were thinking in the moment? You know, that type of thing. And so I\u2019m I\u2019m</p>\n<blockquote>\n<p>I\u2019m saying, hey. That was the old way. The new way is if if you want the act\u2026 if you want to figure out what\u2019s going on, doing product development with your customers, see how all that goes, stop asking them and just start listening to them.</p>\n</blockquote>\n<p>So what do I mean by that? I mean, just take the data as it sits and be able to analyze it at scale to figure out what\u2019s going on. And so this could be a retail space, where you have microphones and people are talking about certain clothes that they want to try on. Hey. I hate this. That looks great, etcetera, that type of thing. Just listen to them in situ. If it\u2019s a call center, somebody calling in saying, I hate this. I like this, etcetera, like, right in right in situ in in in the data, looking inside.</p>\n<p>And there\u2019s\u2026 essentially, for data scientists, like myself and lots of other people, they look at this huge data lake, dark dataset that previously just was, like, untouched. And maybe if you\u2019re in the call center space, you know that there would be, like, a QA sampling kind of thing that would happen. Maybe one percent of calls would be listened to by a team of, like, ten people, where each person would listen to, like, ten calls a day or something like that. And they would try to randomly sample and and see what they could get out of it, but it\u2019s a really, really small percentage. And, you know, it had marginal success, had some success but marginal.</p>\n<p>And so the next step in obvious evolution there is if you can automate this process and make it actually good and and and, again, see previous demo, then you can learn a lot about it, about what they\u2019re actually saying for real. And so now the question would be, though. Alright. That\u2019s that\u2019s great. You told us there was something previously that was really hard to do, and now it\u2019s really easy to do or it seems like it\u2019s easy to do. How does that all work? It seems hard. Well, it is hard. Audio is a really hard problem, and I\u2019m just going to describe that a little bit to you.</p>\n<p>So take five seconds and look at this image, and I won\u2019t say anything. Ok. Does anybody remember what happened in that image? There\u2019s there\u2019s a dog, has a blue Frisbee. There\u2019s a girl probably fifty feet in the background with a\u2026 with, like, a bodyboard. There\u2019s another bodyboard. And I don\u2019t know why I keep going out here. But from the from the image perspective, you can glean a lot of information from a very small amount of time. And this makes labeling images really easy and inexpensive in a lot of ways. It\u2026 trust me. It\u2019s still expensive. But compared to audio and and many other sources, it\u2019s it\u2019s, like, a lot easier. But you can get this whole story just from that quick five seconds. But now what if what if you were asked to look at this hour-long lecture for five seconds and then tell me, like, what happened. Right? You know, you get five seconds. Like, I\u2019m not gonna play, you know, biochemistry lecture for you. But but but, nevertheless, five seconds with that, you\u2019ll get five words, you know, ten, fifteen words, like, whatever. Right? You\u2019re\u2026 there isn\u2019t much you can do. Right? And so, hopefully, that illustrates a little bit about audio that\u2019s happening. It\u2019s real time. It\u2019s very hard for a person to parallel process.</p>\n<p>You can you can recognize that when you\u2019re in a cocktail-party problem, which we\u2019ll all have later today when we\u2019re all sitting around trying to talk to each other, many conversations going on, etcetera. You can only really track one, maybe one and a half. You know? That kinda thing. But but part part of the reason\u2026 or\u2026 you know, part of the reason for that is humans are actually pretty good in real time in conversation. They can understand what what other humans are doing, but we have specialized hardware for it. You know? We have our brain that we\u2026 and ears and everything that is tuned just for listening to other humans talk and computers don\u2019t. They they see the image like what is on the right here. And it\u2019s\u2026 that\u2019s that\u2019s not the whole, like, sixty hour lecture. That\u2019s, like, a couple seconds of it. You know? So it\u2019s a whole mess. Right? And if you were to if you were to try to represent that entire lecture that way, you, as a human, would have no idea what\u2019s going on. You might be able to say, a person was talking in this area or not. That that might be a question you can answer, but but, otherwise, you have no idea what they\u2019re saying.</p>\n<p>But if you run it through our ears and through our brain in real time, then we can understand it pretty well. But now how do you get a machine to actually do that? And that\u2019s the real trick. Right? And if you have a machine that can do it, then it\u2019s probably very valuable because now you can understand humans at scale. And that\u2019s the type of thing that we build. I\u2019ll just point out a few of these problems again, like, in a in a call center space. Maybe you have specific product names or company names or industry terms and jargon or certain acronyms that you\u2019re using if you\u2019re a bank or whatever it is. There\u2019s also how it said, so maybe you\u2019re speaking in a specific language. English, for instance. You could have a certain accent, dialect, etcetera. You could be upset or not. There could be background noise. You could be using a phone or some other device to actually talk to people, like calling in on a meeting or something. And, generally, when you\u2019re having these conversations, they\u2019re in a conversational style.</p>\n<p>It\u2019s a fast type of speaking, like I\u2019m doing right now. Sorry about that. But but, nevertheless, it\u2019s very conversational. You\u2019re not trying to purposefully slow down and enunciate so that the machine can understand you. You know? And so that\u2019s a challenge you have to deal with. And then audio\u2026 actually, a lot of people might think of audio as not that big of, like, a file, but it\u2019s about the third the size of video. So there\u2019s a lot of information packed in audio in order to make it sound good enough for you to understand. And there\u2019s different encoding and different compression techniques, and, you know, there could be several channels, like one person talking on one\u2026 like like an agent and then, like, the customer that\u2019s calling in, so multiple channels.</p>\n<p>And so, anyway, audio is very varied and very difficult. But the the way that you have to handle this, and I I\u2026 I\u2019ll talk technically just for a second here. But as you need a lot of data that is labeled, but it\u2019s not just a lot of it. Like, the hour count matters, but it doesn\u2019t matter as much as you might think. But it definitely matters. You need a lot in different areas, so young, old, male, female, low noise, high noise, in the car, not, upset, not upset, you know, very happy, etcetera. And you need this in multiple languages, multiple accents, etcetera, and you all\u2026 you need that all labeled as, like, a a a data plus truth pair, essentially. So here\u2019s the audio and here\u2019s the transcript that goes with that audio, and then the machine can learn by example. And you can kind of see that if you can read it here.</p>\n<p>Audio comes in labeled, and that\u2019s converted into training data. And then that makes a trained model, which you use, like, convolutional neural networks, dense dense layers or recurrent neural networks or attention layers or that type of thing is just kind of\u2026 I think of it like a like a periodic table of chemical elements, but, instead, it\u2019s just for deep learning. You have these different things that care about spatial or temporal or that type of thing. But, nevertheless, mix those up, put them in the right order, expose them to a bunch of training data. And then you have a general model and\u2026 which is if many, if people in here have used APIs before or, you know, you\u2019ve used your, like, keyboard on your phone, then, generally, what you\u2019re using is is a general model, which is a a model that\u2019s a one size fits all or jack-of-all-trades. Not necessarily a master of one, but it\u2026 it\u2019s it\u2019s designed to hopefully do pretty ok with the general population.</p>\n<p>But if you have a specific task, like you\u2019re our customer, like Citibank, and you\u2019re like, hey. I have bankers, and they\u2019re talking about bank stuff. And we wanna make sure that they\u2019re compliant in the things that they\u2019re doing, then you can go through this bottom loop here that says taking new data and label that to do transfer learning on it. So you have a\u2026 your original model that is already really good at English or other languages, and then now you expose it to a specific domain and then serve that in order to accomplish to your task.</p>\n<p>So, anyway, if you wanna talk about the technical side of it later, I\u2019m happy to happy to discuss, but that\u2019s how it all works under the hood. But now you can get wet\u2026 you can get rid of the old way, or keep doing it if if you find value in it, but add the new way, which is doing a hundred percent analysis across all your data. Do it in a reliable way. Do it in a scalable way and with high accuracy. And so I think you you guys\u2026 Cyrano, one of our customers, you just saw the demo. We\u2019re we\u2019re pumped to have them, and we can discuss a little bit later. Again, we have a booth back there. But\u2026 about the use case there, so I I won\u2019t take too much time talking about them, but another company that maybe some have heard heard of in the audience is Valyant.</p>\n<p>And Valyant does fast-food ordering. So\u2026 well, it doesn\u2019t have to be fast food. It could be many other things, but, typically, it\u2019s fast food. So think of driving through and you\u2019re in your car or you\u2019re on your mobile phone or you\u2019re at a kiosk, and you\u2019re trying to order food inside a mall or something like that. And you have a very specific menu items, like Baconator or something like that. And you\u2019d like your model to be very good at those things in this very noisy environment. And how do you do that? Well, you just saw magic. You have a general model, and then you transfer learn that model into their domain, and then it works really well. And another place that this works really well is in space.</p>\n<p>So NASA is one of our customers. They they do space-to-ground communication. So we worked with the CTO and their team at the Johnson Space Center in Houston in Houston. And what they\u2019re trying to do is under\u2026 essentially, there\u2019s always a link between the international space station and ground. And there\u2019s people listening all the time, like actual humans. Three of them sitting down and transcribing everything that\u2019s happening. And, hey, can we, like, reduce that number to, like, two people or one person or zero people or something like that. And this is the problem that they\u2019re working on, and it\u2019s it\u2019s a\u2026 it\u2019s a big challenge, and their their CTO says, hey. The problem\u2019s right in our faces. We have we have all of seventy five hundred pieces of jargon and acronyms, and it\u2019s a scratchy, you know, radio signal, and it\u2019s it\u2019s it\u2019s not a pleasant thing to try to transcribe. But, hey, if there were a machine that could listen to this and do a good job of it, then that would be very valuable to them. And so, anyway, they they tried a lot of vendors.</p>\n<p>And as you can maybe imagine using a general model approach, which is what pretty much everybody else does, then it doesn\u2019t work that well. But if you use this transfer-learning approach, then it works really well handling background noise, jargon, multi speaker, etcetera. And, yeah, happy to talk about these use cases later if you want to. But\u2026 so, yeah, moving on to act three here a little bit, which is just a comment on what\u2019s happening in the world. We\u2019re in the middle of the intelligence revolution, like it or not. There was the agricultural revolution, industrial revolution, information revolution. We\u2019re in the intelligence revolution right now. Automation is going to happen. Thirty years from now we\u2019re gonna look back and be like, oh, yeah. Those were the beginning days. You know? Just like if anybody in here was around when the Internet was first starting to form. You remember, you know, when you got your first email account, and you\u2019re like, holy shit. This is crazy. You know, you can talk to anybody anywhere.</p>\n<p>And, anyway, same\u2026 we\u2019re we\u2019re in that we\u2019re in that point in time right now. And another comment is that real time is finally real. A couple of years ago, it wasn\u2019t, and now it actually is. That\u2019s partially because the models are getting good enough to do things in real time, but it\u2019s also partially because the computational techniques have become sophisticated enough, but not as heavy lifting plus other\u2026 like, NVIDIA has gotten good essentially at processing larger model sizes and that type of thing. So, essentially, confluence of hardware plus software plus new techniques being implemented in software can make real time\u2026 high-accuracy real time actually real. And then these techniques, if you\u2019re if you\u2019re in a general domain, you can get, like, ninety percent plus accuracy with just a general model.</p>\n<p>Or if you if you need that kind of accuracy, but it\u2019s kind of a niche domain, then you can go after a custom-trained model. And this stuff all works now, basically. So a lot a lot of times come\u2026 people come to me and say, like, hey. Is it possible too? And the the\u2026 in audio, the answer is almost like it\u2026 it\u2019s almost always yes, unless it\u2019s something that a human really can\u2019t do. Like, can you build a lie detector? Like, well, I mean, kind of. Yes. But as as well as maybe you could as a human. You know? May\u2026 I mean, not quite that well, but close. Right? But think about all the other things that a human can do. Like, they can jump into a conversation, tell you how many people are speaking when they\u2019re speaking, what words they\u2019re talking about, what topic they\u2019re talking about, that type of thing.</p>\n<p>These things are all possible, but it\u2019s still kind of, like, the railroad was built across the United States, but there\u2019s a whole bunch of other work that has to be done and so all that stuff is happening now. So, yeah, the the people who win the west, you know, over the next couple decades will be the ones that empower the developers to build this architecture to\u2026 you know, the the railroad builders and the towns that sprout up along those railroad lines. But it\u2019s going to be in compliance spaces, voice bots, call centers, the meetings, podcast. It\u2019s gonna be all over the place. I mean, for voice, it\u2019s just like a\u2026 it\u2019s the natural human commit.</p>\n<p>I\u2019m up here talking right now. Right? Like, this is the natural communication mechanism, and it just previously was enabled by connectivity. So you could call somebody. You could do that you could do that type of thing, but there was never any automation or intelligence behind it, and now that\u2019s all changing. And, you know, our lives are gonna change massively because of it. And I think the productivity of the world is gonna go up significantly, which previous, you know, talks that we heard today we\u2019re all hinting at that. You know, our lives are gonna change a lot. So I\u2019d\u2026 I I don\u2019t know if there\u2019s\u2026 if I if\u2026 maybe people are tired of hands. I don\u2019t know. But are there start-up founders in the room here? Like, any\u2026 ok. Cool. So if you\u2019re if if you\u2019re into voice and you\u2019re a start-up founder, you might wanna know about this.</p>\n<p>So Deepgram has a start-up program that we gave away ten million dollars in speech-recognition credit. And what you can do is apply. Get up to a hundred thousand dollars for your start-up. And the reason we do this is to help push forward the the new the new products being built. I I don\u2019t I don\u2019t know if people are quite aware of what happened with AWS, GCP, Azure, etcetera over the last ten, fifteen years. But they gave away a lot of credit, and a lot of tech companies benefited as they grew their company on that initial credit. And, of course, you know, for Deepgram, the goal here is if you grow a whole bunch and become awesome then, you know, great. You\u2019re you\u2019re a customer of Deepgram, and we\u2019re happy to help you. And another one.</p>\n<p>So tomorrow evening, we have\u2026 how far away is it? A mile, but we have buses. We have party buses. Ok. So tomorrow evening, we\u2019re going to\u2026 we\u2026 we\u2019re gonna have a private screening of Dune. So if if you haven\u2019t seen Dune yet, now\u2019s an opportunity to do it at a place called Suds n n Cinema Suds n Cinema, which is a pretty dope theater, and we could all watch Dune together if you wanna come hang out and do that. We also have a booth. And we have a special\u2026 a a new a new member of our Deepgram team. We call him our VP of Intrigue. If you want to come meet him, come talk to us at the booth. I won\u2019t say anymore. But thank you, everybody, for listening to the talk, and I I appreciate everybody coming out to hear it.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976850/blog/building-the-future-of-voice-scott-stephenson-ceo-deepgram-project-voice-x/scott-stephenson-script-analysis-chart%402x-1-300x16.png" alt=""></p>\n<p><em>To learn more about real-time voice analysis, check out <a href="https://www.cyrano.ai/">Cyrano.ai</a>.</em></p>' };
const frontmatter = { "title": "Building the Future of Voice - Scott Stephenson, CEO, Deepgram - Project Voice X", "description": "Building the future of Voice presented by Scott Stephenson, CEO of Deepgram, presented on day one of Project Voice X.\xA0", "date": "2021-12-09T00:00:00.000Z", "cover": "https://res.cloudinary.com/deepgram/image/upload/v1661981393/blog/building-the-future-of-voice-scott-stephenson-ceo-deepgram-project-voice-x/proj-voice-x-session-scott-stephenson-blog-thumb-5.png", "authors": ["claudia-ring"], "category": "speech-trends", "tags": ["project-voice-x"], "seo": { "title": "Building the Future of Voice - Scott Stephenson, CEO, Deepgram - Project Voice X", "description": "Building the future of Voice presented by Scott Stephenson, CEO of Deepgram, presented on day one of Project Voice X.\xA0" }, "og": { "image": "https://res.cloudinary.com/deepgram/image/upload/v1661981393/blog/building-the-future-of-voice-scott-stephenson-ceo-deepgram-project-voice-x/proj-voice-x-session-scott-stephenson-blog-thumb-5.png" }, "shorturls": { "share": "https://dpgr.am/124b7f6", "twitter": "https://dpgr.am/90b1aa4", "linkedin": "https://dpgr.am/350b8f6", "reddit": "https://dpgr.am/b61b7c2", "facebook": "https://dpgr.am/fcbf36f" }, "astro": { "headings": [], "source": "*This is the transcript for the opening keynote presented by Scott Stephenson, CEO of Deepgram, presented on day one of Project Voice X.*\xA0\n\n*The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was\xA0**94% accurate.**\xA0 Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram\u2019s API.\xA0 If you want to see if Deepgram is right for your use case,\xA0[contact us](https://deepgram.com/contact-us/).*\n\n\\[Scott Stephenson:] Hey, everybody. Thank you for for listening and coming to the event. I know it\u2019s kind of interesting. Yeah. This\u2026 for us, at least, at Deepgram, this is a first in-person event back. Maybe for most people, that that might be the case. But, yeah, I\u2019m happy to be here, happy to meet all the new people. And I just wanna thank all the other speakers before me as well, a lot of great discussion today. And I\u2019m also going to talk about the future of voice, not the distant, distant future. I\u2019ll talk about the next couple years. But I\u2019ll give you a little bit of background on myself, though, before we do that.\n\nSo I\u2019m Scott Stephenson. I\u2019m a CEO and cofounder of Deepgram. I\u2019m also\u2026 I\u2019m a technical CEO. So if people out there wanna talk, you know, shop with me, that\u2019s totally fine. I I was a particle physicist before, so I I built deep underground dark matter detectors and and that kind of thing. So if you\u2019re interested in that, happy to talk about that as well. But but, yeah, Deepgram is a company that builds APIs. You just you just saw some of the performance of it, but we build speech APIs in order to enable the next generation of voice products to be built. And that can be in real time. It can be in batch mode, but we want to enable developers to build that piece. So think of Deepgram, like, you\u2026 in the same category that you would Stripe or Twilio or something like that in order to enable developers to build voice products.\n\nI\u2019m gonna I\u2019m gonna talk about something in particular, and then we\u2019ll maybe go a little bit broader after that. But I\u2019ll I\u2019ll talk about something that\u2019s happening right now, which is we\u2019re starting to see the adoption of automation and AI in the call center space and in other areas as well. But, in particular, you see it, like, really ramping up in meetings and call center. But one thing I see is this this this this trend of the old way, which was if you wanted to figure out if you had happy customers or how well things are going or that type of thing, what you would do is ask them. You you would send out a survey. You\u2019d try to corner them. You know, you try to do these very unscalable things or things that maybe had questionable statistics behind them on on how how well they worked. But you basically didn\u2019t have an alternative otherwise.\n\nAnd so I\u2019m here to talk about that a little bit about what\u2019s actually happening now is, yes, some of that old stuff is still happening, but you already hear me saying old stuff. Because what\u2019s actually happening now is people are saying, hey. We already have piles of data. There\u2019s phone calls happening. There\u2019s voice bot interactions. There\u2019s emails. There\u2019s other things that are coming in here.\n\nLooks like we lost our connection. I\u2019m not sure why. K. Sorry about that. We\u2019ll see if I can\u2026 I don\u2019t think I hit anything, but let\u2019s see.\n\nOk. So so it can be text interactions. It can be voice interactions. There\u2019s there\u2019s a a number of ways that customers will interact with your product. Could just be clicks on your website, that type of thing. But if if you have to ask for the survey, if you have to, you know, send a text afterward, that type of thing, then you\u2019re gonna get a very biased response. People that hate you or love you. What about the in between? What about what they were thinking in the moment? You know, that type of thing. And so I\u2019m I\u2019m\n\n> I\u2019m saying, hey. That was the old way. The new way is if if you want the act\u2026 if you want to figure out what\u2019s going on, doing product development with your customers, see how all that goes, stop asking them and just start listening to them.\n\nSo what do I mean by that? I mean, just take the data as it sits and be able to analyze it at scale to figure out what\u2019s going on. And so this could be a retail space, where you have microphones and people are talking about certain clothes that they want to try on. Hey. I hate this. That looks great, etcetera, that type of thing. Just listen to them in situ. If it\u2019s a call center, somebody calling in saying, I hate this. I like this, etcetera, like, right in right in situ in in in the data, looking inside.\n\nAnd there\u2019s\u2026 essentially, for data scientists, like myself and lots of other people, they look at this huge data lake, dark dataset that previously just was, like, untouched. And maybe if you\u2019re in the call center space, you know that there would be, like, a QA sampling kind of thing that would happen. Maybe one percent of calls would be listened to by a team of, like, ten people, where each person would listen to, like, ten calls a day or something like that. And they would try to randomly sample and and see what they could get out of it, but it\u2019s a really, really small percentage. And, you know, it had marginal success, had some success but marginal.\n\nAnd so the next step in obvious evolution there is if you can automate this process and make it actually good and and and, again, see previous demo, then you can learn a lot about it, about what they\u2019re actually saying for real. And so now the question would be, though. Alright. That\u2019s that\u2019s great. You told us there was something previously that was really hard to do, and now it\u2019s really easy to do or it seems like it\u2019s easy to do. How does that all work? It seems hard. Well, it is hard. Audio is a really hard problem, and I\u2019m just going to describe that a little bit to you.\n\nSo take five seconds and look at this image, and I won\u2019t say anything. Ok. Does anybody remember what happened in that image? There\u2019s there\u2019s a dog, has a blue Frisbee. There\u2019s a girl probably fifty feet in the background with a\u2026 with, like, a bodyboard. There\u2019s another bodyboard. And I don\u2019t know why I keep going out here. But from the from the image perspective, you can glean a lot of information from a very small amount of time. And this makes labeling images really easy and inexpensive in a lot of ways. It\u2026 trust me. It\u2019s still expensive. But compared to audio and and many other sources, it\u2019s it\u2019s, like, a lot easier. But you can get this whole story just from that quick five seconds. But now what if what if you were asked to look at this hour-long lecture for five seconds and then tell me, like, what happened. Right? You know, you get five seconds. Like, I\u2019m not gonna play, you know, biochemistry lecture for you. But but but, nevertheless, five seconds with that, you\u2019ll get five words, you know, ten, fifteen words, like, whatever. Right? You\u2019re\u2026 there isn\u2019t much you can do. Right? And so, hopefully, that illustrates a little bit about audio that\u2019s happening. It\u2019s real time. It\u2019s very hard for a person to parallel process.\n\nYou can you can recognize that when you\u2019re in a cocktail-party problem, which we\u2019ll all have later today when we\u2019re all sitting around trying to talk to each other, many conversations going on, etcetera. You can only really track one, maybe one and a half. You know? That kinda thing. But but part part of the reason\u2026 or\u2026 you know, part of the reason for that is humans are actually pretty good in real time in conversation. They can understand what what other humans are doing, but we have specialized hardware for it. You know? We have our brain that we\u2026 and ears and everything that is tuned just for listening to other humans talk and computers don\u2019t. They they see the image like what is on the right here. And it\u2019s\u2026 that\u2019s that\u2019s not the whole, like, sixty hour lecture. That\u2019s, like, a couple seconds of it. You know? So it\u2019s a whole mess. Right? And if you were to if you were to try to represent that entire lecture that way, you, as a human, would have no idea what\u2019s going on. You might be able to say, a person was talking in this area or not. That that might be a question you can answer, but but, otherwise, you have no idea what they\u2019re saying.\n\nBut if you run it through our ears and through our brain in real time, then we can understand it pretty well. But now how do you get a machine to actually do that? And that\u2019s the real trick. Right? And if you have a machine that can do it, then it\u2019s probably very valuable because now you can understand humans at scale. And that\u2019s the type of thing that we build. I\u2019ll just point out a few of these problems again, like, in a in a call center space. Maybe you have specific product names or company names or industry terms and jargon or certain acronyms that you\u2019re using if you\u2019re a bank or whatever it is. There\u2019s also how it said, so maybe you\u2019re speaking in a specific language. English, for instance. You could have a certain accent, dialect, etcetera. You could be upset or not. There could be background noise. You could be using a phone or some other device to actually talk to people, like calling in on a meeting or something. And, generally, when you\u2019re having these conversations, they\u2019re in a conversational style.\n\nIt\u2019s a fast type of speaking, like I\u2019m doing right now. Sorry about that. But but, nevertheless, it\u2019s very conversational. You\u2019re not trying to purposefully slow down and enunciate so that the machine can understand you. You know? And so that\u2019s a challenge you have to deal with. And then audio\u2026 actually, a lot of people might think of audio as not that big of, like, a file, but it\u2019s about the third the size of video. So there\u2019s a lot of information packed in audio in order to make it sound good enough for you to understand. And there\u2019s different encoding and different compression techniques, and, you know, there could be several channels, like one person talking on one\u2026 like like an agent and then, like, the customer that\u2019s calling in, so multiple channels.\n\nAnd so, anyway, audio is very varied and very difficult. But the the way that you have to handle this, and I I\u2026 I\u2019ll talk technically just for a second here. But as you need a lot of data that is labeled, but it\u2019s not just a lot of it. Like, the hour count matters, but it doesn\u2019t matter as much as you might think. But it definitely matters. You need a lot in different areas, so young, old, male, female, low noise, high noise, in the car, not, upset, not upset, you know, very happy, etcetera. And you need this in multiple languages, multiple accents, etcetera, and you all\u2026 you need that all labeled as, like, a a a data plus truth pair, essentially. So here\u2019s the audio and here\u2019s the transcript that goes with that audio, and then the machine can learn by example. And you can kind of see that if you can read it here.\n\nAudio comes in labeled, and that\u2019s converted into training data. And then that makes a trained model, which you use, like, convolutional neural networks, dense dense layers or recurrent neural networks or attention layers or that type of thing is just kind of\u2026 I think of it like a like a periodic table of chemical elements, but, instead, it\u2019s just for deep learning. You have these different things that care about spatial or temporal or that type of thing. But, nevertheless, mix those up, put them in the right order, expose them to a bunch of training data. And then you have a general model and\u2026 which is if many, if people in here have used APIs before or, you know, you\u2019ve used your, like, keyboard on your phone, then, generally, what you\u2019re using is is a general model, which is a a model that\u2019s a one size fits all or jack-of-all-trades. Not necessarily a master of one, but it\u2026 it\u2019s it\u2019s designed to hopefully do pretty ok with the general population.\n\nBut if you have a specific task, like you\u2019re our customer, like Citibank, and you\u2019re like, hey. I have bankers, and they\u2019re talking about bank stuff. And we wanna make sure that they\u2019re compliant in the things that they\u2019re doing, then you can go through this bottom loop here that says taking new data and label that to do transfer learning on it. So you have a\u2026 your original model that is already really good at English or other languages, and then now you expose it to a specific domain and then serve that in order to accomplish to your task.\n\nSo, anyway, if you wanna talk about the technical side of it later, I\u2019m happy to happy to discuss, but that\u2019s how it all works under the hood. But now you can get wet\u2026 you can get rid of the old way, or keep doing it if if you find value in it, but add the new way, which is doing a hundred percent analysis across all your data. Do it in a reliable way. Do it in a scalable way and with high accuracy. And so I think you you guys\u2026 Cyrano, one of our customers, you just saw the demo. We\u2019re we\u2019re pumped to have them, and we can discuss a little bit later. Again, we have a booth back there. But\u2026 about the use case there, so I I won\u2019t take too much time talking about them, but another company that maybe some have heard heard of in the audience is Valyant.\n\nAnd Valyant does fast-food ordering. So\u2026 well, it doesn\u2019t have to be fast food. It could be many other things, but, typically, it\u2019s fast food. So think of driving through and you\u2019re in your car or you\u2019re on your mobile phone or you\u2019re at a kiosk, and you\u2019re trying to order food inside a mall or something like that. And you have a very specific menu items, like Baconator or something like that. And you\u2019d like your model to be very good at those things in this very noisy environment. And how do you do that? Well, you just saw magic. You have a general model, and then you transfer learn that model into their domain, and then it works really well. And another place that this works really well is in space.\n\nSo NASA is one of our customers. They they do space-to-ground communication. So we worked with the CTO and their team at the Johnson Space Center in Houston in Houston. And what they\u2019re trying to do is under\u2026 essentially, there\u2019s always a link between the international space station and ground. And there\u2019s people listening all the time, like actual humans. Three of them sitting down and transcribing everything that\u2019s happening. And, hey, can we, like, reduce that number to, like, two people or one person or zero people or something like that. And this is the problem that they\u2019re working on, and it\u2019s it\u2019s a\u2026 it\u2019s a big challenge, and their their CTO says, hey. The problem\u2019s right in our faces. We have we have all of seventy five hundred pieces of jargon and acronyms, and it\u2019s a scratchy, you know, radio signal, and it\u2019s it\u2019s it\u2019s not a pleasant thing to try to transcribe. But, hey, if there were a machine that could listen to this and do a good job of it, then that would be very valuable to them. And so, anyway, they they tried a lot of vendors.\n\nAnd as you can maybe imagine using a general model approach, which is what pretty much everybody else does, then it doesn\u2019t work that well. But if you use this transfer-learning approach, then it works really well handling background noise, jargon, multi speaker, etcetera. And, yeah, happy to talk about these use cases later if you want to. But\u2026 so, yeah, moving on to act three here a little bit, which is just a comment on what\u2019s happening in the world. We\u2019re in the middle of the intelligence revolution, like it or not. There was the agricultural revolution, industrial revolution, information revolution. We\u2019re in the intelligence revolution right now. Automation is going to happen. Thirty years from now we\u2019re gonna look back and be like, oh, yeah. Those were the beginning days. You know? Just like if anybody in here was around when the Internet was first starting to form. You remember, you know, when you got your first email account, and you\u2019re like, holy shit. This is crazy. You know, you can talk to anybody anywhere.\n\nAnd, anyway, same\u2026 we\u2019re we\u2019re in that we\u2019re in that point in time right now. And another comment is that real time is finally real. A couple of years ago, it wasn\u2019t, and now it actually is. That\u2019s partially because the models are getting good enough to do things in real time, but it\u2019s also partially because the computational techniques have become sophisticated enough, but not as heavy lifting plus other\u2026 like, NVIDIA has gotten good essentially at processing larger model sizes and that type of thing. So, essentially, confluence of hardware plus software plus new techniques being implemented in software can make real time\u2026 high-accuracy real time actually real. And then these techniques, if you\u2019re if you\u2019re in a general domain, you can get, like, ninety percent plus accuracy with just a general model.\n\nOr if you if you need that kind of accuracy, but it\u2019s kind of a niche domain, then you can go after a custom-trained model. And this stuff all works now, basically. So a lot a lot of times come\u2026 people come to me and say, like, hey. Is it possible too? And the the\u2026 in audio, the answer is almost like it\u2026 it\u2019s almost always yes, unless it\u2019s something that a human really can\u2019t do. Like, can you build a lie detector? Like, well, I mean, kind of. Yes. But as as well as maybe you could as a human. You know? May\u2026 I mean, not quite that well, but close. Right? But think about all the other things that a human can do. Like, they can jump into a conversation, tell you how many people are speaking when they\u2019re speaking, what words they\u2019re talking about, what topic they\u2019re talking about, that type of thing.\n\nThese things are all possible, but it\u2019s still kind of, like, the railroad was built across the United States, but there\u2019s a whole bunch of other work that has to be done and so all that stuff is happening now. So, yeah, the the people who win the west, you know, over the next couple decades will be the ones that empower the developers to build this architecture to\u2026 you know, the the railroad builders and the towns that sprout up along those railroad lines. But it\u2019s going to be in compliance spaces, voice bots, call centers, the meetings, podcast. It\u2019s gonna be all over the place. I mean, for voice, it\u2019s just like a\u2026 it\u2019s the natural human commit.\n\nI\u2019m up here talking right now. Right? Like, this is the natural communication mechanism, and it just previously was enabled by connectivity. So you could call somebody. You could do that you could do that type of thing, but there was never any automation or intelligence behind it, and now that\u2019s all changing. And, you know, our lives are gonna change massively because of it. And I think the productivity of the world is gonna go up significantly, which previous, you know, talks that we heard today we\u2019re all hinting at that. You know, our lives are gonna change a lot. So I\u2019d\u2026 I I don\u2019t know if there\u2019s\u2026 if I if\u2026 maybe people are tired of hands. I don\u2019t know. But are there start-up founders in the room here? Like, any\u2026 ok. Cool. So if you\u2019re if if you\u2019re into voice and you\u2019re a start-up founder, you might wanna know about this.\n\nSo Deepgram has a start-up program that we gave away ten million dollars in speech-recognition credit. And what you can do is apply. Get up to a hundred thousand dollars for your start-up. And the reason we do this is to help push forward the the new the new products being built. I I don\u2019t I don\u2019t know if people are quite aware of what happened with AWS, GCP, Azure, etcetera over the last ten, fifteen years. But they gave away a lot of credit, and a lot of tech companies benefited as they grew their company on that initial credit. And, of course, you know, for Deepgram, the goal here is if you grow a whole bunch and become awesome then, you know, great. You\u2019re you\u2019re a customer of Deepgram, and we\u2019re happy to help you. And another one.\n\nSo tomorrow evening, we have\u2026 how far away is it? A mile, but we have buses. We have party buses. Ok. So tomorrow evening, we\u2019re going to\u2026 we\u2026 we\u2019re gonna have a private screening of Dune. So if if you haven\u2019t seen Dune yet, now\u2019s an opportunity to do it at a place called Suds n n Cinema Suds n Cinema, which is a pretty dope theater, and we could all watch Dune together if you wanna come hang out and do that. We also have a booth. And we have a special\u2026 a a new a new member of our Deepgram team. We call him our VP of Intrigue. If you want to come meet him, come talk to us at the booth. I won\u2019t say anymore. But thank you, everybody, for listening to the talk, and I I appreciate everybody coming out to hear it.\n\n\n\n ![](https://res.cloudinary.com/deepgram/image/upload/v1661976850/blog/building-the-future-of-voice-scott-stephenson-ceo-deepgram-project-voice-x/scott-stephenson-script-analysis-chart%402x-1-300x16.png)\n\n\n\n*To learn more about real-time voice analysis, check out [Cyrano.ai](https://www.cyrano.ai/).*", "html": '<p><em>This is the transcript for the opening keynote presented by Scott Stephenson, CEO of Deepgram, presented on day one of Project Voice X.</em>\xA0</p>\n<p><em>The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was\xA0<strong>94% accurate.</strong>\xA0 Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram\u2019s API.\xA0 If you want to see if Deepgram is right for your use case,\xA0<a href="https://deepgram.com/contact-us/">contact us</a>.</em></p>\n<p>[Scott Stephenson:] Hey, everybody. Thank you for for listening and coming to the event. I know it\u2019s kind of interesting. Yeah. This\u2026 for us, at least, at Deepgram, this is a first in-person event back. Maybe for most people, that that might be the case. But, yeah, I\u2019m happy to be here, happy to meet all the new people. And I just wanna thank all the other speakers before me as well, a lot of great discussion today. And I\u2019m also going to talk about the future of voice, not the distant, distant future. I\u2019ll talk about the next couple years. But I\u2019ll give you a little bit of background on myself, though, before we do that.</p>\n<p>So I\u2019m Scott Stephenson. I\u2019m a CEO and cofounder of Deepgram. I\u2019m also\u2026 I\u2019m a technical CEO. So if people out there wanna talk, you know, shop with me, that\u2019s totally fine. I I was a particle physicist before, so I I built deep underground dark matter detectors and and that kind of thing. So if you\u2019re interested in that, happy to talk about that as well. But but, yeah, Deepgram is a company that builds APIs. You just you just saw some of the performance of it, but we build speech APIs in order to enable the next generation of voice products to be built. And that can be in real time. It can be in batch mode, but we want to enable developers to build that piece. So think of Deepgram, like, you\u2026 in the same category that you would Stripe or Twilio or something like that in order to enable developers to build voice products.</p>\n<p>I\u2019m gonna I\u2019m gonna talk about something in particular, and then we\u2019ll maybe go a little bit broader after that. But I\u2019ll I\u2019ll talk about something that\u2019s happening right now, which is we\u2019re starting to see the adoption of automation and AI in the call center space and in other areas as well. But, in particular, you see it, like, really ramping up in meetings and call center. But one thing I see is this this this this trend of the old way, which was if you wanted to figure out if you had happy customers or how well things are going or that type of thing, what you would do is ask them. You you would send out a survey. You\u2019d try to corner them. You know, you try to do these very unscalable things or things that maybe had questionable statistics behind them on on how how well they worked. But you basically didn\u2019t have an alternative otherwise.</p>\n<p>And so I\u2019m here to talk about that a little bit about what\u2019s actually happening now is, yes, some of that old stuff is still happening, but you already hear me saying old stuff. Because what\u2019s actually happening now is people are saying, hey. We already have piles of data. There\u2019s phone calls happening. There\u2019s voice bot interactions. There\u2019s emails. There\u2019s other things that are coming in here.</p>\n<p>Looks like we lost our connection. I\u2019m not sure why. K. Sorry about that. We\u2019ll see if I can\u2026 I don\u2019t think I hit anything, but let\u2019s see.</p>\n<p>Ok. So so it can be text interactions. It can be voice interactions. There\u2019s there\u2019s a a number of ways that customers will interact with your product. Could just be clicks on your website, that type of thing. But if if you have to ask for the survey, if you have to, you know, send a text afterward, that type of thing, then you\u2019re gonna get a very biased response. People that hate you or love you. What about the in between? What about what they were thinking in the moment? You know, that type of thing. And so I\u2019m I\u2019m</p>\n<blockquote>\n<p>I\u2019m saying, hey. That was the old way. The new way is if if you want the act\u2026 if you want to figure out what\u2019s going on, doing product development with your customers, see how all that goes, stop asking them and just start listening to them.</p>\n</blockquote>\n<p>So what do I mean by that? I mean, just take the data as it sits and be able to analyze it at scale to figure out what\u2019s going on. And so this could be a retail space, where you have microphones and people are talking about certain clothes that they want to try on. Hey. I hate this. That looks great, etcetera, that type of thing. Just listen to them in situ. If it\u2019s a call center, somebody calling in saying, I hate this. I like this, etcetera, like, right in right in situ in in in the data, looking inside.</p>\n<p>And there\u2019s\u2026 essentially, for data scientists, like myself and lots of other people, they look at this huge data lake, dark dataset that previously just was, like, untouched. And maybe if you\u2019re in the call center space, you know that there would be, like, a QA sampling kind of thing that would happen. Maybe one percent of calls would be listened to by a team of, like, ten people, where each person would listen to, like, ten calls a day or something like that. And they would try to randomly sample and and see what they could get out of it, but it\u2019s a really, really small percentage. And, you know, it had marginal success, had some success but marginal.</p>\n<p>And so the next step in obvious evolution there is if you can automate this process and make it actually good and and and, again, see previous demo, then you can learn a lot about it, about what they\u2019re actually saying for real. And so now the question would be, though. Alright. That\u2019s that\u2019s great. You told us there was something previously that was really hard to do, and now it\u2019s really easy to do or it seems like it\u2019s easy to do. How does that all work? It seems hard. Well, it is hard. Audio is a really hard problem, and I\u2019m just going to describe that a little bit to you.</p>\n<p>So take five seconds and look at this image, and I won\u2019t say anything. Ok. Does anybody remember what happened in that image? There\u2019s there\u2019s a dog, has a blue Frisbee. There\u2019s a girl probably fifty feet in the background with a\u2026 with, like, a bodyboard. There\u2019s another bodyboard. And I don\u2019t know why I keep going out here. But from the from the image perspective, you can glean a lot of information from a very small amount of time. And this makes labeling images really easy and inexpensive in a lot of ways. It\u2026 trust me. It\u2019s still expensive. But compared to audio and and many other sources, it\u2019s it\u2019s, like, a lot easier. But you can get this whole story just from that quick five seconds. But now what if what if you were asked to look at this hour-long lecture for five seconds and then tell me, like, what happened. Right? You know, you get five seconds. Like, I\u2019m not gonna play, you know, biochemistry lecture for you. But but but, nevertheless, five seconds with that, you\u2019ll get five words, you know, ten, fifteen words, like, whatever. Right? You\u2019re\u2026 there isn\u2019t much you can do. Right? And so, hopefully, that illustrates a little bit about audio that\u2019s happening. It\u2019s real time. It\u2019s very hard for a person to parallel process.</p>\n<p>You can you can recognize that when you\u2019re in a cocktail-party problem, which we\u2019ll all have later today when we\u2019re all sitting around trying to talk to each other, many conversations going on, etcetera. You can only really track one, maybe one and a half. You know? That kinda thing. But but part part of the reason\u2026 or\u2026 you know, part of the reason for that is humans are actually pretty good in real time in conversation. They can understand what what other humans are doing, but we have specialized hardware for it. You know? We have our brain that we\u2026 and ears and everything that is tuned just for listening to other humans talk and computers don\u2019t. They they see the image like what is on the right here. And it\u2019s\u2026 that\u2019s that\u2019s not the whole, like, sixty hour lecture. That\u2019s, like, a couple seconds of it. You know? So it\u2019s a whole mess. Right? And if you were to if you were to try to represent that entire lecture that way, you, as a human, would have no idea what\u2019s going on. You might be able to say, a person was talking in this area or not. That that might be a question you can answer, but but, otherwise, you have no idea what they\u2019re saying.</p>\n<p>But if you run it through our ears and through our brain in real time, then we can understand it pretty well. But now how do you get a machine to actually do that? And that\u2019s the real trick. Right? And if you have a machine that can do it, then it\u2019s probably very valuable because now you can understand humans at scale. And that\u2019s the type of thing that we build. I\u2019ll just point out a few of these problems again, like, in a in a call center space. Maybe you have specific product names or company names or industry terms and jargon or certain acronyms that you\u2019re using if you\u2019re a bank or whatever it is. There\u2019s also how it said, so maybe you\u2019re speaking in a specific language. English, for instance. You could have a certain accent, dialect, etcetera. You could be upset or not. There could be background noise. You could be using a phone or some other device to actually talk to people, like calling in on a meeting or something. And, generally, when you\u2019re having these conversations, they\u2019re in a conversational style.</p>\n<p>It\u2019s a fast type of speaking, like I\u2019m doing right now. Sorry about that. But but, nevertheless, it\u2019s very conversational. You\u2019re not trying to purposefully slow down and enunciate so that the machine can understand you. You know? And so that\u2019s a challenge you have to deal with. And then audio\u2026 actually, a lot of people might think of audio as not that big of, like, a file, but it\u2019s about the third the size of video. So there\u2019s a lot of information packed in audio in order to make it sound good enough for you to understand. And there\u2019s different encoding and different compression techniques, and, you know, there could be several channels, like one person talking on one\u2026 like like an agent and then, like, the customer that\u2019s calling in, so multiple channels.</p>\n<p>And so, anyway, audio is very varied and very difficult. But the the way that you have to handle this, and I I\u2026 I\u2019ll talk technically just for a second here. But as you need a lot of data that is labeled, but it\u2019s not just a lot of it. Like, the hour count matters, but it doesn\u2019t matter as much as you might think. But it definitely matters. You need a lot in different areas, so young, old, male, female, low noise, high noise, in the car, not, upset, not upset, you know, very happy, etcetera. And you need this in multiple languages, multiple accents, etcetera, and you all\u2026 you need that all labeled as, like, a a a data plus truth pair, essentially. So here\u2019s the audio and here\u2019s the transcript that goes with that audio, and then the machine can learn by example. And you can kind of see that if you can read it here.</p>\n<p>Audio comes in labeled, and that\u2019s converted into training data. And then that makes a trained model, which you use, like, convolutional neural networks, dense dense layers or recurrent neural networks or attention layers or that type of thing is just kind of\u2026 I think of it like a like a periodic table of chemical elements, but, instead, it\u2019s just for deep learning. You have these different things that care about spatial or temporal or that type of thing. But, nevertheless, mix those up, put them in the right order, expose them to a bunch of training data. And then you have a general model and\u2026 which is if many, if people in here have used APIs before or, you know, you\u2019ve used your, like, keyboard on your phone, then, generally, what you\u2019re using is is a general model, which is a a model that\u2019s a one size fits all or jack-of-all-trades. Not necessarily a master of one, but it\u2026 it\u2019s it\u2019s designed to hopefully do pretty ok with the general population.</p>\n<p>But if you have a specific task, like you\u2019re our customer, like Citibank, and you\u2019re like, hey. I have bankers, and they\u2019re talking about bank stuff. And we wanna make sure that they\u2019re compliant in the things that they\u2019re doing, then you can go through this bottom loop here that says taking new data and label that to do transfer learning on it. So you have a\u2026 your original model that is already really good at English or other languages, and then now you expose it to a specific domain and then serve that in order to accomplish to your task.</p>\n<p>So, anyway, if you wanna talk about the technical side of it later, I\u2019m happy to happy to discuss, but that\u2019s how it all works under the hood. But now you can get wet\u2026 you can get rid of the old way, or keep doing it if if you find value in it, but add the new way, which is doing a hundred percent analysis across all your data. Do it in a reliable way. Do it in a scalable way and with high accuracy. And so I think you you guys\u2026 Cyrano, one of our customers, you just saw the demo. We\u2019re we\u2019re pumped to have them, and we can discuss a little bit later. Again, we have a booth back there. But\u2026 about the use case there, so I I won\u2019t take too much time talking about them, but another company that maybe some have heard heard of in the audience is Valyant.</p>\n<p>And Valyant does fast-food ordering. So\u2026 well, it doesn\u2019t have to be fast food. It could be many other things, but, typically, it\u2019s fast food. So think of driving through and you\u2019re in your car or you\u2019re on your mobile phone or you\u2019re at a kiosk, and you\u2019re trying to order food inside a mall or something like that. And you have a very specific menu items, like Baconator or something like that. And you\u2019d like your model to be very good at those things in this very noisy environment. And how do you do that? Well, you just saw magic. You have a general model, and then you transfer learn that model into their domain, and then it works really well. And another place that this works really well is in space.</p>\n<p>So NASA is one of our customers. They they do space-to-ground communication. So we worked with the CTO and their team at the Johnson Space Center in Houston in Houston. And what they\u2019re trying to do is under\u2026 essentially, there\u2019s always a link between the international space station and ground. And there\u2019s people listening all the time, like actual humans. Three of them sitting down and transcribing everything that\u2019s happening. And, hey, can we, like, reduce that number to, like, two people or one person or zero people or something like that. And this is the problem that they\u2019re working on, and it\u2019s it\u2019s a\u2026 it\u2019s a big challenge, and their their CTO says, hey. The problem\u2019s right in our faces. We have we have all of seventy five hundred pieces of jargon and acronyms, and it\u2019s a scratchy, you know, radio signal, and it\u2019s it\u2019s it\u2019s not a pleasant thing to try to transcribe. But, hey, if there were a machine that could listen to this and do a good job of it, then that would be very valuable to them. And so, anyway, they they tried a lot of vendors.</p>\n<p>And as you can maybe imagine using a general model approach, which is what pretty much everybody else does, then it doesn\u2019t work that well. But if you use this transfer-learning approach, then it works really well handling background noise, jargon, multi speaker, etcetera. And, yeah, happy to talk about these use cases later if you want to. But\u2026 so, yeah, moving on to act three here a little bit, which is just a comment on what\u2019s happening in the world. We\u2019re in the middle of the intelligence revolution, like it or not. There was the agricultural revolution, industrial revolution, information revolution. We\u2019re in the intelligence revolution right now. Automation is going to happen. Thirty years from now we\u2019re gonna look back and be like, oh, yeah. Those were the beginning days. You know? Just like if anybody in here was around when the Internet was first starting to form. You remember, you know, when you got your first email account, and you\u2019re like, holy shit. This is crazy. You know, you can talk to anybody anywhere.</p>\n<p>And, anyway, same\u2026 we\u2019re we\u2019re in that we\u2019re in that point in time right now. And another comment is that real time is finally real. A couple of years ago, it wasn\u2019t, and now it actually is. That\u2019s partially because the models are getting good enough to do things in real time, but it\u2019s also partially because the computational techniques have become sophisticated enough, but not as heavy lifting plus other\u2026 like, NVIDIA has gotten good essentially at processing larger model sizes and that type of thing. So, essentially, confluence of hardware plus software plus new techniques being implemented in software can make real time\u2026 high-accuracy real time actually real. And then these techniques, if you\u2019re if you\u2019re in a general domain, you can get, like, ninety percent plus accuracy with just a general model.</p>\n<p>Or if you if you need that kind of accuracy, but it\u2019s kind of a niche domain, then you can go after a custom-trained model. And this stuff all works now, basically. So a lot a lot of times come\u2026 people come to me and say, like, hey. Is it possible too? And the the\u2026 in audio, the answer is almost like it\u2026 it\u2019s almost always yes, unless it\u2019s something that a human really can\u2019t do. Like, can you build a lie detector? Like, well, I mean, kind of. Yes. But as as well as maybe you could as a human. You know? May\u2026 I mean, not quite that well, but close. Right? But think about all the other things that a human can do. Like, they can jump into a conversation, tell you how many people are speaking when they\u2019re speaking, what words they\u2019re talking about, what topic they\u2019re talking about, that type of thing.</p>\n<p>These things are all possible, but it\u2019s still kind of, like, the railroad was built across the United States, but there\u2019s a whole bunch of other work that has to be done and so all that stuff is happening now. So, yeah, the the people who win the west, you know, over the next couple decades will be the ones that empower the developers to build this architecture to\u2026 you know, the the railroad builders and the towns that sprout up along those railroad lines. But it\u2019s going to be in compliance spaces, voice bots, call centers, the meetings, podcast. It\u2019s gonna be all over the place. I mean, for voice, it\u2019s just like a\u2026 it\u2019s the natural human commit.</p>\n<p>I\u2019m up here talking right now. Right? Like, this is the natural communication mechanism, and it just previously was enabled by connectivity. So you could call somebody. You could do that you could do that type of thing, but there was never any automation or intelligence behind it, and now that\u2019s all changing. And, you know, our lives are gonna change massively because of it. And I think the productivity of the world is gonna go up significantly, which previous, you know, talks that we heard today we\u2019re all hinting at that. You know, our lives are gonna change a lot. So I\u2019d\u2026 I I don\u2019t know if there\u2019s\u2026 if I if\u2026 maybe people are tired of hands. I don\u2019t know. But are there start-up founders in the room here? Like, any\u2026 ok. Cool. So if you\u2019re if if you\u2019re into voice and you\u2019re a start-up founder, you might wanna know about this.</p>\n<p>So Deepgram has a start-up program that we gave away ten million dollars in speech-recognition credit. And what you can do is apply. Get up to a hundred thousand dollars for your start-up. And the reason we do this is to help push forward the the new the new products being built. I I don\u2019t I don\u2019t know if people are quite aware of what happened with AWS, GCP, Azure, etcetera over the last ten, fifteen years. But they gave away a lot of credit, and a lot of tech companies benefited as they grew their company on that initial credit. And, of course, you know, for Deepgram, the goal here is if you grow a whole bunch and become awesome then, you know, great. You\u2019re you\u2019re a customer of Deepgram, and we\u2019re happy to help you. And another one.</p>\n<p>So tomorrow evening, we have\u2026 how far away is it? A mile, but we have buses. We have party buses. Ok. So tomorrow evening, we\u2019re going to\u2026 we\u2026 we\u2019re gonna have a private screening of Dune. So if if you haven\u2019t seen Dune yet, now\u2019s an opportunity to do it at a place called Suds n n Cinema Suds n Cinema, which is a pretty dope theater, and we could all watch Dune together if you wanna come hang out and do that. We also have a booth. And we have a special\u2026 a a new a new member of our Deepgram team. We call him our VP of Intrigue. If you want to come meet him, come talk to us at the booth. I won\u2019t say anymore. But thank you, everybody, for listening to the talk, and I I appreciate everybody coming out to hear it.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976850/blog/building-the-future-of-voice-scott-stephenson-ceo-deepgram-project-voice-x/scott-stephenson-script-analysis-chart%402x-1-300x16.png" alt=""></p>\n<p><em>To learn more about real-time voice analysis, check out <a href="https://www.cyrano.ai/">Cyrano.ai</a>.</em></p>' }, "file": "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/building-the-future-of-voice-scott-stephenson-ceo-deepgram-project-voice-x/index.md" };
function rawContent() {
  return "*This is the transcript for the opening keynote presented by Scott Stephenson, CEO of Deepgram, presented on day one of Project Voice X.*\xA0\n\n*The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was\xA0**94% accurate.**\xA0 Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram\u2019s API.\xA0 If you want to see if Deepgram is right for your use case,\xA0[contact us](https://deepgram.com/contact-us/).*\n\n\\[Scott Stephenson:] Hey, everybody. Thank you for for listening and coming to the event. I know it\u2019s kind of interesting. Yeah. This\u2026 for us, at least, at Deepgram, this is a first in-person event back. Maybe for most people, that that might be the case. But, yeah, I\u2019m happy to be here, happy to meet all the new people. And I just wanna thank all the other speakers before me as well, a lot of great discussion today. And I\u2019m also going to talk about the future of voice, not the distant, distant future. I\u2019ll talk about the next couple years. But I\u2019ll give you a little bit of background on myself, though, before we do that.\n\nSo I\u2019m Scott Stephenson. I\u2019m a CEO and cofounder of Deepgram. I\u2019m also\u2026 I\u2019m a technical CEO. So if people out there wanna talk, you know, shop with me, that\u2019s totally fine. I I was a particle physicist before, so I I built deep underground dark matter detectors and and that kind of thing. So if you\u2019re interested in that, happy to talk about that as well. But but, yeah, Deepgram is a company that builds APIs. You just you just saw some of the performance of it, but we build speech APIs in order to enable the next generation of voice products to be built. And that can be in real time. It can be in batch mode, but we want to enable developers to build that piece. So think of Deepgram, like, you\u2026 in the same category that you would Stripe or Twilio or something like that in order to enable developers to build voice products.\n\nI\u2019m gonna I\u2019m gonna talk about something in particular, and then we\u2019ll maybe go a little bit broader after that. But I\u2019ll I\u2019ll talk about something that\u2019s happening right now, which is we\u2019re starting to see the adoption of automation and AI in the call center space and in other areas as well. But, in particular, you see it, like, really ramping up in meetings and call center. But one thing I see is this this this this trend of the old way, which was if you wanted to figure out if you had happy customers or how well things are going or that type of thing, what you would do is ask them. You you would send out a survey. You\u2019d try to corner them. You know, you try to do these very unscalable things or things that maybe had questionable statistics behind them on on how how well they worked. But you basically didn\u2019t have an alternative otherwise.\n\nAnd so I\u2019m here to talk about that a little bit about what\u2019s actually happening now is, yes, some of that old stuff is still happening, but you already hear me saying old stuff. Because what\u2019s actually happening now is people are saying, hey. We already have piles of data. There\u2019s phone calls happening. There\u2019s voice bot interactions. There\u2019s emails. There\u2019s other things that are coming in here.\n\nLooks like we lost our connection. I\u2019m not sure why. K. Sorry about that. We\u2019ll see if I can\u2026 I don\u2019t think I hit anything, but let\u2019s see.\n\nOk. So so it can be text interactions. It can be voice interactions. There\u2019s there\u2019s a a number of ways that customers will interact with your product. Could just be clicks on your website, that type of thing. But if if you have to ask for the survey, if you have to, you know, send a text afterward, that type of thing, then you\u2019re gonna get a very biased response. People that hate you or love you. What about the in between? What about what they were thinking in the moment? You know, that type of thing. And so I\u2019m I\u2019m\n\n> I\u2019m saying, hey. That was the old way. The new way is if if you want the act\u2026 if you want to figure out what\u2019s going on, doing product development with your customers, see how all that goes, stop asking them and just start listening to them.\n\nSo what do I mean by that? I mean, just take the data as it sits and be able to analyze it at scale to figure out what\u2019s going on. And so this could be a retail space, where you have microphones and people are talking about certain clothes that they want to try on. Hey. I hate this. That looks great, etcetera, that type of thing. Just listen to them in situ. If it\u2019s a call center, somebody calling in saying, I hate this. I like this, etcetera, like, right in right in situ in in in the data, looking inside.\n\nAnd there\u2019s\u2026 essentially, for data scientists, like myself and lots of other people, they look at this huge data lake, dark dataset that previously just was, like, untouched. And maybe if you\u2019re in the call center space, you know that there would be, like, a QA sampling kind of thing that would happen. Maybe one percent of calls would be listened to by a team of, like, ten people, where each person would listen to, like, ten calls a day or something like that. And they would try to randomly sample and and see what they could get out of it, but it\u2019s a really, really small percentage. And, you know, it had marginal success, had some success but marginal.\n\nAnd so the next step in obvious evolution there is if you can automate this process and make it actually good and and and, again, see previous demo, then you can learn a lot about it, about what they\u2019re actually saying for real. And so now the question would be, though. Alright. That\u2019s that\u2019s great. You told us there was something previously that was really hard to do, and now it\u2019s really easy to do or it seems like it\u2019s easy to do. How does that all work? It seems hard. Well, it is hard. Audio is a really hard problem, and I\u2019m just going to describe that a little bit to you.\n\nSo take five seconds and look at this image, and I won\u2019t say anything. Ok. Does anybody remember what happened in that image? There\u2019s there\u2019s a dog, has a blue Frisbee. There\u2019s a girl probably fifty feet in the background with a\u2026 with, like, a bodyboard. There\u2019s another bodyboard. And I don\u2019t know why I keep going out here. But from the from the image perspective, you can glean a lot of information from a very small amount of time. And this makes labeling images really easy and inexpensive in a lot of ways. It\u2026 trust me. It\u2019s still expensive. But compared to audio and and many other sources, it\u2019s it\u2019s, like, a lot easier. But you can get this whole story just from that quick five seconds. But now what if what if you were asked to look at this hour-long lecture for five seconds and then tell me, like, what happened. Right? You know, you get five seconds. Like, I\u2019m not gonna play, you know, biochemistry lecture for you. But but but, nevertheless, five seconds with that, you\u2019ll get five words, you know, ten, fifteen words, like, whatever. Right? You\u2019re\u2026 there isn\u2019t much you can do. Right? And so, hopefully, that illustrates a little bit about audio that\u2019s happening. It\u2019s real time. It\u2019s very hard for a person to parallel process.\n\nYou can you can recognize that when you\u2019re in a cocktail-party problem, which we\u2019ll all have later today when we\u2019re all sitting around trying to talk to each other, many conversations going on, etcetera. You can only really track one, maybe one and a half. You know? That kinda thing. But but part part of the reason\u2026 or\u2026 you know, part of the reason for that is humans are actually pretty good in real time in conversation. They can understand what what other humans are doing, but we have specialized hardware for it. You know? We have our brain that we\u2026 and ears and everything that is tuned just for listening to other humans talk and computers don\u2019t. They they see the image like what is on the right here. And it\u2019s\u2026 that\u2019s that\u2019s not the whole, like, sixty hour lecture. That\u2019s, like, a couple seconds of it. You know? So it\u2019s a whole mess. Right? And if you were to if you were to try to represent that entire lecture that way, you, as a human, would have no idea what\u2019s going on. You might be able to say, a person was talking in this area or not. That that might be a question you can answer, but but, otherwise, you have no idea what they\u2019re saying.\n\nBut if you run it through our ears and through our brain in real time, then we can understand it pretty well. But now how do you get a machine to actually do that? And that\u2019s the real trick. Right? And if you have a machine that can do it, then it\u2019s probably very valuable because now you can understand humans at scale. And that\u2019s the type of thing that we build. I\u2019ll just point out a few of these problems again, like, in a in a call center space. Maybe you have specific product names or company names or industry terms and jargon or certain acronyms that you\u2019re using if you\u2019re a bank or whatever it is. There\u2019s also how it said, so maybe you\u2019re speaking in a specific language. English, for instance. You could have a certain accent, dialect, etcetera. You could be upset or not. There could be background noise. You could be using a phone or some other device to actually talk to people, like calling in on a meeting or something. And, generally, when you\u2019re having these conversations, they\u2019re in a conversational style.\n\nIt\u2019s a fast type of speaking, like I\u2019m doing right now. Sorry about that. But but, nevertheless, it\u2019s very conversational. You\u2019re not trying to purposefully slow down and enunciate so that the machine can understand you. You know? And so that\u2019s a challenge you have to deal with. And then audio\u2026 actually, a lot of people might think of audio as not that big of, like, a file, but it\u2019s about the third the size of video. So there\u2019s a lot of information packed in audio in order to make it sound good enough for you to understand. And there\u2019s different encoding and different compression techniques, and, you know, there could be several channels, like one person talking on one\u2026 like like an agent and then, like, the customer that\u2019s calling in, so multiple channels.\n\nAnd so, anyway, audio is very varied and very difficult. But the the way that you have to handle this, and I I\u2026 I\u2019ll talk technically just for a second here. But as you need a lot of data that is labeled, but it\u2019s not just a lot of it. Like, the hour count matters, but it doesn\u2019t matter as much as you might think. But it definitely matters. You need a lot in different areas, so young, old, male, female, low noise, high noise, in the car, not, upset, not upset, you know, very happy, etcetera. And you need this in multiple languages, multiple accents, etcetera, and you all\u2026 you need that all labeled as, like, a a a data plus truth pair, essentially. So here\u2019s the audio and here\u2019s the transcript that goes with that audio, and then the machine can learn by example. And you can kind of see that if you can read it here.\n\nAudio comes in labeled, and that\u2019s converted into training data. And then that makes a trained model, which you use, like, convolutional neural networks, dense dense layers or recurrent neural networks or attention layers or that type of thing is just kind of\u2026 I think of it like a like a periodic table of chemical elements, but, instead, it\u2019s just for deep learning. You have these different things that care about spatial or temporal or that type of thing. But, nevertheless, mix those up, put them in the right order, expose them to a bunch of training data. And then you have a general model and\u2026 which is if many, if people in here have used APIs before or, you know, you\u2019ve used your, like, keyboard on your phone, then, generally, what you\u2019re using is is a general model, which is a a model that\u2019s a one size fits all or jack-of-all-trades. Not necessarily a master of one, but it\u2026 it\u2019s it\u2019s designed to hopefully do pretty ok with the general population.\n\nBut if you have a specific task, like you\u2019re our customer, like Citibank, and you\u2019re like, hey. I have bankers, and they\u2019re talking about bank stuff. And we wanna make sure that they\u2019re compliant in the things that they\u2019re doing, then you can go through this bottom loop here that says taking new data and label that to do transfer learning on it. So you have a\u2026 your original model that is already really good at English or other languages, and then now you expose it to a specific domain and then serve that in order to accomplish to your task.\n\nSo, anyway, if you wanna talk about the technical side of it later, I\u2019m happy to happy to discuss, but that\u2019s how it all works under the hood. But now you can get wet\u2026 you can get rid of the old way, or keep doing it if if you find value in it, but add the new way, which is doing a hundred percent analysis across all your data. Do it in a reliable way. Do it in a scalable way and with high accuracy. And so I think you you guys\u2026 Cyrano, one of our customers, you just saw the demo. We\u2019re we\u2019re pumped to have them, and we can discuss a little bit later. Again, we have a booth back there. But\u2026 about the use case there, so I I won\u2019t take too much time talking about them, but another company that maybe some have heard heard of in the audience is Valyant.\n\nAnd Valyant does fast-food ordering. So\u2026 well, it doesn\u2019t have to be fast food. It could be many other things, but, typically, it\u2019s fast food. So think of driving through and you\u2019re in your car or you\u2019re on your mobile phone or you\u2019re at a kiosk, and you\u2019re trying to order food inside a mall or something like that. And you have a very specific menu items, like Baconator or something like that. And you\u2019d like your model to be very good at those things in this very noisy environment. And how do you do that? Well, you just saw magic. You have a general model, and then you transfer learn that model into their domain, and then it works really well. And another place that this works really well is in space.\n\nSo NASA is one of our customers. They they do space-to-ground communication. So we worked with the CTO and their team at the Johnson Space Center in Houston in Houston. And what they\u2019re trying to do is under\u2026 essentially, there\u2019s always a link between the international space station and ground. And there\u2019s people listening all the time, like actual humans. Three of them sitting down and transcribing everything that\u2019s happening. And, hey, can we, like, reduce that number to, like, two people or one person or zero people or something like that. And this is the problem that they\u2019re working on, and it\u2019s it\u2019s a\u2026 it\u2019s a big challenge, and their their CTO says, hey. The problem\u2019s right in our faces. We have we have all of seventy five hundred pieces of jargon and acronyms, and it\u2019s a scratchy, you know, radio signal, and it\u2019s it\u2019s it\u2019s not a pleasant thing to try to transcribe. But, hey, if there were a machine that could listen to this and do a good job of it, then that would be very valuable to them. And so, anyway, they they tried a lot of vendors.\n\nAnd as you can maybe imagine using a general model approach, which is what pretty much everybody else does, then it doesn\u2019t work that well. But if you use this transfer-learning approach, then it works really well handling background noise, jargon, multi speaker, etcetera. And, yeah, happy to talk about these use cases later if you want to. But\u2026 so, yeah, moving on to act three here a little bit, which is just a comment on what\u2019s happening in the world. We\u2019re in the middle of the intelligence revolution, like it or not. There was the agricultural revolution, industrial revolution, information revolution. We\u2019re in the intelligence revolution right now. Automation is going to happen. Thirty years from now we\u2019re gonna look back and be like, oh, yeah. Those were the beginning days. You know? Just like if anybody in here was around when the Internet was first starting to form. You remember, you know, when you got your first email account, and you\u2019re like, holy shit. This is crazy. You know, you can talk to anybody anywhere.\n\nAnd, anyway, same\u2026 we\u2019re we\u2019re in that we\u2019re in that point in time right now. And another comment is that real time is finally real. A couple of years ago, it wasn\u2019t, and now it actually is. That\u2019s partially because the models are getting good enough to do things in real time, but it\u2019s also partially because the computational techniques have become sophisticated enough, but not as heavy lifting plus other\u2026 like, NVIDIA has gotten good essentially at processing larger model sizes and that type of thing. So, essentially, confluence of hardware plus software plus new techniques being implemented in software can make real time\u2026 high-accuracy real time actually real. And then these techniques, if you\u2019re if you\u2019re in a general domain, you can get, like, ninety percent plus accuracy with just a general model.\n\nOr if you if you need that kind of accuracy, but it\u2019s kind of a niche domain, then you can go after a custom-trained model. And this stuff all works now, basically. So a lot a lot of times come\u2026 people come to me and say, like, hey. Is it possible too? And the the\u2026 in audio, the answer is almost like it\u2026 it\u2019s almost always yes, unless it\u2019s something that a human really can\u2019t do. Like, can you build a lie detector? Like, well, I mean, kind of. Yes. But as as well as maybe you could as a human. You know? May\u2026 I mean, not quite that well, but close. Right? But think about all the other things that a human can do. Like, they can jump into a conversation, tell you how many people are speaking when they\u2019re speaking, what words they\u2019re talking about, what topic they\u2019re talking about, that type of thing.\n\nThese things are all possible, but it\u2019s still kind of, like, the railroad was built across the United States, but there\u2019s a whole bunch of other work that has to be done and so all that stuff is happening now. So, yeah, the the people who win the west, you know, over the next couple decades will be the ones that empower the developers to build this architecture to\u2026 you know, the the railroad builders and the towns that sprout up along those railroad lines. But it\u2019s going to be in compliance spaces, voice bots, call centers, the meetings, podcast. It\u2019s gonna be all over the place. I mean, for voice, it\u2019s just like a\u2026 it\u2019s the natural human commit.\n\nI\u2019m up here talking right now. Right? Like, this is the natural communication mechanism, and it just previously was enabled by connectivity. So you could call somebody. You could do that you could do that type of thing, but there was never any automation or intelligence behind it, and now that\u2019s all changing. And, you know, our lives are gonna change massively because of it. And I think the productivity of the world is gonna go up significantly, which previous, you know, talks that we heard today we\u2019re all hinting at that. You know, our lives are gonna change a lot. So I\u2019d\u2026 I I don\u2019t know if there\u2019s\u2026 if I if\u2026 maybe people are tired of hands. I don\u2019t know. But are there start-up founders in the room here? Like, any\u2026 ok. Cool. So if you\u2019re if if you\u2019re into voice and you\u2019re a start-up founder, you might wanna know about this.\n\nSo Deepgram has a start-up program that we gave away ten million dollars in speech-recognition credit. And what you can do is apply. Get up to a hundred thousand dollars for your start-up. And the reason we do this is to help push forward the the new the new products being built. I I don\u2019t I don\u2019t know if people are quite aware of what happened with AWS, GCP, Azure, etcetera over the last ten, fifteen years. But they gave away a lot of credit, and a lot of tech companies benefited as they grew their company on that initial credit. And, of course, you know, for Deepgram, the goal here is if you grow a whole bunch and become awesome then, you know, great. You\u2019re you\u2019re a customer of Deepgram, and we\u2019re happy to help you. And another one.\n\nSo tomorrow evening, we have\u2026 how far away is it? A mile, but we have buses. We have party buses. Ok. So tomorrow evening, we\u2019re going to\u2026 we\u2026 we\u2019re gonna have a private screening of Dune. So if if you haven\u2019t seen Dune yet, now\u2019s an opportunity to do it at a place called Suds n n Cinema Suds n Cinema, which is a pretty dope theater, and we could all watch Dune together if you wanna come hang out and do that. We also have a booth. And we have a special\u2026 a a new a new member of our Deepgram team. We call him our VP of Intrigue. If you want to come meet him, come talk to us at the booth. I won\u2019t say anymore. But thank you, everybody, for listening to the talk, and I I appreciate everybody coming out to hear it.\n\n\n\n ![](https://res.cloudinary.com/deepgram/image/upload/v1661976850/blog/building-the-future-of-voice-scott-stephenson-ceo-deepgram-project-voice-x/scott-stephenson-script-analysis-chart%402x-1-300x16.png)\n\n\n\n*To learn more about real-time voice analysis, check out [Cyrano.ai](https://www.cyrano.ai/).*";
}
function compiledContent() {
  return '<p><em>This is the transcript for the opening keynote presented by Scott Stephenson, CEO of Deepgram, presented on day one of Project Voice X.</em>\xA0</p>\n<p><em>The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was\xA0<strong>94% accurate.</strong>\xA0 Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram\u2019s API.\xA0 If you want to see if Deepgram is right for your use case,\xA0<a href="https://deepgram.com/contact-us/">contact us</a>.</em></p>\n<p>[Scott Stephenson:] Hey, everybody. Thank you for for listening and coming to the event. I know it\u2019s kind of interesting. Yeah. This\u2026 for us, at least, at Deepgram, this is a first in-person event back. Maybe for most people, that that might be the case. But, yeah, I\u2019m happy to be here, happy to meet all the new people. And I just wanna thank all the other speakers before me as well, a lot of great discussion today. And I\u2019m also going to talk about the future of voice, not the distant, distant future. I\u2019ll talk about the next couple years. But I\u2019ll give you a little bit of background on myself, though, before we do that.</p>\n<p>So I\u2019m Scott Stephenson. I\u2019m a CEO and cofounder of Deepgram. I\u2019m also\u2026 I\u2019m a technical CEO. So if people out there wanna talk, you know, shop with me, that\u2019s totally fine. I I was a particle physicist before, so I I built deep underground dark matter detectors and and that kind of thing. So if you\u2019re interested in that, happy to talk about that as well. But but, yeah, Deepgram is a company that builds APIs. You just you just saw some of the performance of it, but we build speech APIs in order to enable the next generation of voice products to be built. And that can be in real time. It can be in batch mode, but we want to enable developers to build that piece. So think of Deepgram, like, you\u2026 in the same category that you would Stripe or Twilio or something like that in order to enable developers to build voice products.</p>\n<p>I\u2019m gonna I\u2019m gonna talk about something in particular, and then we\u2019ll maybe go a little bit broader after that. But I\u2019ll I\u2019ll talk about something that\u2019s happening right now, which is we\u2019re starting to see the adoption of automation and AI in the call center space and in other areas as well. But, in particular, you see it, like, really ramping up in meetings and call center. But one thing I see is this this this this trend of the old way, which was if you wanted to figure out if you had happy customers or how well things are going or that type of thing, what you would do is ask them. You you would send out a survey. You\u2019d try to corner them. You know, you try to do these very unscalable things or things that maybe had questionable statistics behind them on on how how well they worked. But you basically didn\u2019t have an alternative otherwise.</p>\n<p>And so I\u2019m here to talk about that a little bit about what\u2019s actually happening now is, yes, some of that old stuff is still happening, but you already hear me saying old stuff. Because what\u2019s actually happening now is people are saying, hey. We already have piles of data. There\u2019s phone calls happening. There\u2019s voice bot interactions. There\u2019s emails. There\u2019s other things that are coming in here.</p>\n<p>Looks like we lost our connection. I\u2019m not sure why. K. Sorry about that. We\u2019ll see if I can\u2026 I don\u2019t think I hit anything, but let\u2019s see.</p>\n<p>Ok. So so it can be text interactions. It can be voice interactions. There\u2019s there\u2019s a a number of ways that customers will interact with your product. Could just be clicks on your website, that type of thing. But if if you have to ask for the survey, if you have to, you know, send a text afterward, that type of thing, then you\u2019re gonna get a very biased response. People that hate you or love you. What about the in between? What about what they were thinking in the moment? You know, that type of thing. And so I\u2019m I\u2019m</p>\n<blockquote>\n<p>I\u2019m saying, hey. That was the old way. The new way is if if you want the act\u2026 if you want to figure out what\u2019s going on, doing product development with your customers, see how all that goes, stop asking them and just start listening to them.</p>\n</blockquote>\n<p>So what do I mean by that? I mean, just take the data as it sits and be able to analyze it at scale to figure out what\u2019s going on. And so this could be a retail space, where you have microphones and people are talking about certain clothes that they want to try on. Hey. I hate this. That looks great, etcetera, that type of thing. Just listen to them in situ. If it\u2019s a call center, somebody calling in saying, I hate this. I like this, etcetera, like, right in right in situ in in in the data, looking inside.</p>\n<p>And there\u2019s\u2026 essentially, for data scientists, like myself and lots of other people, they look at this huge data lake, dark dataset that previously just was, like, untouched. And maybe if you\u2019re in the call center space, you know that there would be, like, a QA sampling kind of thing that would happen. Maybe one percent of calls would be listened to by a team of, like, ten people, where each person would listen to, like, ten calls a day or something like that. And they would try to randomly sample and and see what they could get out of it, but it\u2019s a really, really small percentage. And, you know, it had marginal success, had some success but marginal.</p>\n<p>And so the next step in obvious evolution there is if you can automate this process and make it actually good and and and, again, see previous demo, then you can learn a lot about it, about what they\u2019re actually saying for real. And so now the question would be, though. Alright. That\u2019s that\u2019s great. You told us there was something previously that was really hard to do, and now it\u2019s really easy to do or it seems like it\u2019s easy to do. How does that all work? It seems hard. Well, it is hard. Audio is a really hard problem, and I\u2019m just going to describe that a little bit to you.</p>\n<p>So take five seconds and look at this image, and I won\u2019t say anything. Ok. Does anybody remember what happened in that image? There\u2019s there\u2019s a dog, has a blue Frisbee. There\u2019s a girl probably fifty feet in the background with a\u2026 with, like, a bodyboard. There\u2019s another bodyboard. And I don\u2019t know why I keep going out here. But from the from the image perspective, you can glean a lot of information from a very small amount of time. And this makes labeling images really easy and inexpensive in a lot of ways. It\u2026 trust me. It\u2019s still expensive. But compared to audio and and many other sources, it\u2019s it\u2019s, like, a lot easier. But you can get this whole story just from that quick five seconds. But now what if what if you were asked to look at this hour-long lecture for five seconds and then tell me, like, what happened. Right? You know, you get five seconds. Like, I\u2019m not gonna play, you know, biochemistry lecture for you. But but but, nevertheless, five seconds with that, you\u2019ll get five words, you know, ten, fifteen words, like, whatever. Right? You\u2019re\u2026 there isn\u2019t much you can do. Right? And so, hopefully, that illustrates a little bit about audio that\u2019s happening. It\u2019s real time. It\u2019s very hard for a person to parallel process.</p>\n<p>You can you can recognize that when you\u2019re in a cocktail-party problem, which we\u2019ll all have later today when we\u2019re all sitting around trying to talk to each other, many conversations going on, etcetera. You can only really track one, maybe one and a half. You know? That kinda thing. But but part part of the reason\u2026 or\u2026 you know, part of the reason for that is humans are actually pretty good in real time in conversation. They can understand what what other humans are doing, but we have specialized hardware for it. You know? We have our brain that we\u2026 and ears and everything that is tuned just for listening to other humans talk and computers don\u2019t. They they see the image like what is on the right here. And it\u2019s\u2026 that\u2019s that\u2019s not the whole, like, sixty hour lecture. That\u2019s, like, a couple seconds of it. You know? So it\u2019s a whole mess. Right? And if you were to if you were to try to represent that entire lecture that way, you, as a human, would have no idea what\u2019s going on. You might be able to say, a person was talking in this area or not. That that might be a question you can answer, but but, otherwise, you have no idea what they\u2019re saying.</p>\n<p>But if you run it through our ears and through our brain in real time, then we can understand it pretty well. But now how do you get a machine to actually do that? And that\u2019s the real trick. Right? And if you have a machine that can do it, then it\u2019s probably very valuable because now you can understand humans at scale. And that\u2019s the type of thing that we build. I\u2019ll just point out a few of these problems again, like, in a in a call center space. Maybe you have specific product names or company names or industry terms and jargon or certain acronyms that you\u2019re using if you\u2019re a bank or whatever it is. There\u2019s also how it said, so maybe you\u2019re speaking in a specific language. English, for instance. You could have a certain accent, dialect, etcetera. You could be upset or not. There could be background noise. You could be using a phone or some other device to actually talk to people, like calling in on a meeting or something. And, generally, when you\u2019re having these conversations, they\u2019re in a conversational style.</p>\n<p>It\u2019s a fast type of speaking, like I\u2019m doing right now. Sorry about that. But but, nevertheless, it\u2019s very conversational. You\u2019re not trying to purposefully slow down and enunciate so that the machine can understand you. You know? And so that\u2019s a challenge you have to deal with. And then audio\u2026 actually, a lot of people might think of audio as not that big of, like, a file, but it\u2019s about the third the size of video. So there\u2019s a lot of information packed in audio in order to make it sound good enough for you to understand. And there\u2019s different encoding and different compression techniques, and, you know, there could be several channels, like one person talking on one\u2026 like like an agent and then, like, the customer that\u2019s calling in, so multiple channels.</p>\n<p>And so, anyway, audio is very varied and very difficult. But the the way that you have to handle this, and I I\u2026 I\u2019ll talk technically just for a second here. But as you need a lot of data that is labeled, but it\u2019s not just a lot of it. Like, the hour count matters, but it doesn\u2019t matter as much as you might think. But it definitely matters. You need a lot in different areas, so young, old, male, female, low noise, high noise, in the car, not, upset, not upset, you know, very happy, etcetera. And you need this in multiple languages, multiple accents, etcetera, and you all\u2026 you need that all labeled as, like, a a a data plus truth pair, essentially. So here\u2019s the audio and here\u2019s the transcript that goes with that audio, and then the machine can learn by example. And you can kind of see that if you can read it here.</p>\n<p>Audio comes in labeled, and that\u2019s converted into training data. And then that makes a trained model, which you use, like, convolutional neural networks, dense dense layers or recurrent neural networks or attention layers or that type of thing is just kind of\u2026 I think of it like a like a periodic table of chemical elements, but, instead, it\u2019s just for deep learning. You have these different things that care about spatial or temporal or that type of thing. But, nevertheless, mix those up, put them in the right order, expose them to a bunch of training data. And then you have a general model and\u2026 which is if many, if people in here have used APIs before or, you know, you\u2019ve used your, like, keyboard on your phone, then, generally, what you\u2019re using is is a general model, which is a a model that\u2019s a one size fits all or jack-of-all-trades. Not necessarily a master of one, but it\u2026 it\u2019s it\u2019s designed to hopefully do pretty ok with the general population.</p>\n<p>But if you have a specific task, like you\u2019re our customer, like Citibank, and you\u2019re like, hey. I have bankers, and they\u2019re talking about bank stuff. And we wanna make sure that they\u2019re compliant in the things that they\u2019re doing, then you can go through this bottom loop here that says taking new data and label that to do transfer learning on it. So you have a\u2026 your original model that is already really good at English or other languages, and then now you expose it to a specific domain and then serve that in order to accomplish to your task.</p>\n<p>So, anyway, if you wanna talk about the technical side of it later, I\u2019m happy to happy to discuss, but that\u2019s how it all works under the hood. But now you can get wet\u2026 you can get rid of the old way, or keep doing it if if you find value in it, but add the new way, which is doing a hundred percent analysis across all your data. Do it in a reliable way. Do it in a scalable way and with high accuracy. And so I think you you guys\u2026 Cyrano, one of our customers, you just saw the demo. We\u2019re we\u2019re pumped to have them, and we can discuss a little bit later. Again, we have a booth back there. But\u2026 about the use case there, so I I won\u2019t take too much time talking about them, but another company that maybe some have heard heard of in the audience is Valyant.</p>\n<p>And Valyant does fast-food ordering. So\u2026 well, it doesn\u2019t have to be fast food. It could be many other things, but, typically, it\u2019s fast food. So think of driving through and you\u2019re in your car or you\u2019re on your mobile phone or you\u2019re at a kiosk, and you\u2019re trying to order food inside a mall or something like that. And you have a very specific menu items, like Baconator or something like that. And you\u2019d like your model to be very good at those things in this very noisy environment. And how do you do that? Well, you just saw magic. You have a general model, and then you transfer learn that model into their domain, and then it works really well. And another place that this works really well is in space.</p>\n<p>So NASA is one of our customers. They they do space-to-ground communication. So we worked with the CTO and their team at the Johnson Space Center in Houston in Houston. And what they\u2019re trying to do is under\u2026 essentially, there\u2019s always a link between the international space station and ground. And there\u2019s people listening all the time, like actual humans. Three of them sitting down and transcribing everything that\u2019s happening. And, hey, can we, like, reduce that number to, like, two people or one person or zero people or something like that. And this is the problem that they\u2019re working on, and it\u2019s it\u2019s a\u2026 it\u2019s a big challenge, and their their CTO says, hey. The problem\u2019s right in our faces. We have we have all of seventy five hundred pieces of jargon and acronyms, and it\u2019s a scratchy, you know, radio signal, and it\u2019s it\u2019s it\u2019s not a pleasant thing to try to transcribe. But, hey, if there were a machine that could listen to this and do a good job of it, then that would be very valuable to them. And so, anyway, they they tried a lot of vendors.</p>\n<p>And as you can maybe imagine using a general model approach, which is what pretty much everybody else does, then it doesn\u2019t work that well. But if you use this transfer-learning approach, then it works really well handling background noise, jargon, multi speaker, etcetera. And, yeah, happy to talk about these use cases later if you want to. But\u2026 so, yeah, moving on to act three here a little bit, which is just a comment on what\u2019s happening in the world. We\u2019re in the middle of the intelligence revolution, like it or not. There was the agricultural revolution, industrial revolution, information revolution. We\u2019re in the intelligence revolution right now. Automation is going to happen. Thirty years from now we\u2019re gonna look back and be like, oh, yeah. Those were the beginning days. You know? Just like if anybody in here was around when the Internet was first starting to form. You remember, you know, when you got your first email account, and you\u2019re like, holy shit. This is crazy. You know, you can talk to anybody anywhere.</p>\n<p>And, anyway, same\u2026 we\u2019re we\u2019re in that we\u2019re in that point in time right now. And another comment is that real time is finally real. A couple of years ago, it wasn\u2019t, and now it actually is. That\u2019s partially because the models are getting good enough to do things in real time, but it\u2019s also partially because the computational techniques have become sophisticated enough, but not as heavy lifting plus other\u2026 like, NVIDIA has gotten good essentially at processing larger model sizes and that type of thing. So, essentially, confluence of hardware plus software plus new techniques being implemented in software can make real time\u2026 high-accuracy real time actually real. And then these techniques, if you\u2019re if you\u2019re in a general domain, you can get, like, ninety percent plus accuracy with just a general model.</p>\n<p>Or if you if you need that kind of accuracy, but it\u2019s kind of a niche domain, then you can go after a custom-trained model. And this stuff all works now, basically. So a lot a lot of times come\u2026 people come to me and say, like, hey. Is it possible too? And the the\u2026 in audio, the answer is almost like it\u2026 it\u2019s almost always yes, unless it\u2019s something that a human really can\u2019t do. Like, can you build a lie detector? Like, well, I mean, kind of. Yes. But as as well as maybe you could as a human. You know? May\u2026 I mean, not quite that well, but close. Right? But think about all the other things that a human can do. Like, they can jump into a conversation, tell you how many people are speaking when they\u2019re speaking, what words they\u2019re talking about, what topic they\u2019re talking about, that type of thing.</p>\n<p>These things are all possible, but it\u2019s still kind of, like, the railroad was built across the United States, but there\u2019s a whole bunch of other work that has to be done and so all that stuff is happening now. So, yeah, the the people who win the west, you know, over the next couple decades will be the ones that empower the developers to build this architecture to\u2026 you know, the the railroad builders and the towns that sprout up along those railroad lines. But it\u2019s going to be in compliance spaces, voice bots, call centers, the meetings, podcast. It\u2019s gonna be all over the place. I mean, for voice, it\u2019s just like a\u2026 it\u2019s the natural human commit.</p>\n<p>I\u2019m up here talking right now. Right? Like, this is the natural communication mechanism, and it just previously was enabled by connectivity. So you could call somebody. You could do that you could do that type of thing, but there was never any automation or intelligence behind it, and now that\u2019s all changing. And, you know, our lives are gonna change massively because of it. And I think the productivity of the world is gonna go up significantly, which previous, you know, talks that we heard today we\u2019re all hinting at that. You know, our lives are gonna change a lot. So I\u2019d\u2026 I I don\u2019t know if there\u2019s\u2026 if I if\u2026 maybe people are tired of hands. I don\u2019t know. But are there start-up founders in the room here? Like, any\u2026 ok. Cool. So if you\u2019re if if you\u2019re into voice and you\u2019re a start-up founder, you might wanna know about this.</p>\n<p>So Deepgram has a start-up program that we gave away ten million dollars in speech-recognition credit. And what you can do is apply. Get up to a hundred thousand dollars for your start-up. And the reason we do this is to help push forward the the new the new products being built. I I don\u2019t I don\u2019t know if people are quite aware of what happened with AWS, GCP, Azure, etcetera over the last ten, fifteen years. But they gave away a lot of credit, and a lot of tech companies benefited as they grew their company on that initial credit. And, of course, you know, for Deepgram, the goal here is if you grow a whole bunch and become awesome then, you know, great. You\u2019re you\u2019re a customer of Deepgram, and we\u2019re happy to help you. And another one.</p>\n<p>So tomorrow evening, we have\u2026 how far away is it? A mile, but we have buses. We have party buses. Ok. So tomorrow evening, we\u2019re going to\u2026 we\u2026 we\u2019re gonna have a private screening of Dune. So if if you haven\u2019t seen Dune yet, now\u2019s an opportunity to do it at a place called Suds n n Cinema Suds n Cinema, which is a pretty dope theater, and we could all watch Dune together if you wanna come hang out and do that. We also have a booth. And we have a special\u2026 a a new a new member of our Deepgram team. We call him our VP of Intrigue. If you want to come meet him, come talk to us at the booth. I won\u2019t say anymore. But thank you, everybody, for listening to the talk, and I I appreciate everybody coming out to hear it.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976850/blog/building-the-future-of-voice-scott-stephenson-ceo-deepgram-project-voice-x/scott-stephenson-script-analysis-chart%402x-1-300x16.png" alt=""></p>\n<p><em>To learn more about real-time voice analysis, check out <a href="https://www.cyrano.ai/">Cyrano.ai</a>.</em></p>';
}
const $$Astro = createAstro("/Users/sandrarodgers/web-next/blog/src/content/blog/posts/building-the-future-of-voice-scott-stephenson-ceo-deepgram-project-voice-x/index.md", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Index = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro, $$props, $$slots);
  Astro2.self = $$Index;
  new Slugger();
  return renderTemplate`<head>${renderHead($$result)}</head><p><em>This is the transcript for the opening keynote presented by Scott Stephenson, CEO of Deepgram, presented on day one of Project Voice X.</em> </p>
<p><em>The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was <strong>94% accurate.</strong>  Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram’s API.  If you want to see if Deepgram is right for your use case, <a href="https://deepgram.com/contact-us/">contact us</a>.</em></p>
<p>[Scott Stephenson:] Hey, everybody. Thank you for for listening and coming to the event. I know it’s kind of interesting. Yeah. This… for us, at least, at Deepgram, this is a first in-person event back. Maybe for most people, that that might be the case. But, yeah, I’m happy to be here, happy to meet all the new people. And I just wanna thank all the other speakers before me as well, a lot of great discussion today. And I’m also going to talk about the future of voice, not the distant, distant future. I’ll talk about the next couple years. But I’ll give you a little bit of background on myself, though, before we do that.</p>
<p>So I’m Scott Stephenson. I’m a CEO and cofounder of Deepgram. I’m also… I’m a technical CEO. So if people out there wanna talk, you know, shop with me, that’s totally fine. I I was a particle physicist before, so I I built deep underground dark matter detectors and and that kind of thing. So if you’re interested in that, happy to talk about that as well. But but, yeah, Deepgram is a company that builds APIs. You just you just saw some of the performance of it, but we build speech APIs in order to enable the next generation of voice products to be built. And that can be in real time. It can be in batch mode, but we want to enable developers to build that piece. So think of Deepgram, like, you… in the same category that you would Stripe or Twilio or something like that in order to enable developers to build voice products.</p>
<p>I’m gonna I’m gonna talk about something in particular, and then we’ll maybe go a little bit broader after that. But I’ll I’ll talk about something that’s happening right now, which is we’re starting to see the adoption of automation and AI in the call center space and in other areas as well. But, in particular, you see it, like, really ramping up in meetings and call center. But one thing I see is this this this this trend of the old way, which was if you wanted to figure out if you had happy customers or how well things are going or that type of thing, what you would do is ask them. You you would send out a survey. You’d try to corner them. You know, you try to do these very unscalable things or things that maybe had questionable statistics behind them on on how how well they worked. But you basically didn’t have an alternative otherwise.</p>
<p>And so I’m here to talk about that a little bit about what’s actually happening now is, yes, some of that old stuff is still happening, but you already hear me saying old stuff. Because what’s actually happening now is people are saying, hey. We already have piles of data. There’s phone calls happening. There’s voice bot interactions. There’s emails. There’s other things that are coming in here.</p>
<p>Looks like we lost our connection. I’m not sure why. K. Sorry about that. We’ll see if I can… I don’t think I hit anything, but let’s see.</p>
<p>Ok. So so it can be text interactions. It can be voice interactions. There’s there’s a a number of ways that customers will interact with your product. Could just be clicks on your website, that type of thing. But if if you have to ask for the survey, if you have to, you know, send a text afterward, that type of thing, then you’re gonna get a very biased response. People that hate you or love you. What about the in between? What about what they were thinking in the moment? You know, that type of thing. And so I’m I’m</p>
<blockquote>
<p>I’m saying, hey. That was the old way. The new way is if if you want the act… if you want to figure out what’s going on, doing product development with your customers, see how all that goes, stop asking them and just start listening to them.</p>
</blockquote>
<p>So what do I mean by that? I mean, just take the data as it sits and be able to analyze it at scale to figure out what’s going on. And so this could be a retail space, where you have microphones and people are talking about certain clothes that they want to try on. Hey. I hate this. That looks great, etcetera, that type of thing. Just listen to them in situ. If it’s a call center, somebody calling in saying, I hate this. I like this, etcetera, like, right in right in situ in in in the data, looking inside.</p>
<p>And there’s… essentially, for data scientists, like myself and lots of other people, they look at this huge data lake, dark dataset that previously just was, like, untouched. And maybe if you’re in the call center space, you know that there would be, like, a QA sampling kind of thing that would happen. Maybe one percent of calls would be listened to by a team of, like, ten people, where each person would listen to, like, ten calls a day or something like that. And they would try to randomly sample and and see what they could get out of it, but it’s a really, really small percentage. And, you know, it had marginal success, had some success but marginal.</p>
<p>And so the next step in obvious evolution there is if you can automate this process and make it actually good and and and, again, see previous demo, then you can learn a lot about it, about what they’re actually saying for real. And so now the question would be, though. Alright. That’s that’s great. You told us there was something previously that was really hard to do, and now it’s really easy to do or it seems like it’s easy to do. How does that all work? It seems hard. Well, it is hard. Audio is a really hard problem, and I’m just going to describe that a little bit to you.</p>
<p>So take five seconds and look at this image, and I won’t say anything. Ok. Does anybody remember what happened in that image? There’s there’s a dog, has a blue Frisbee. There’s a girl probably fifty feet in the background with a… with, like, a bodyboard. There’s another bodyboard. And I don’t know why I keep going out here. But from the from the image perspective, you can glean a lot of information from a very small amount of time. And this makes labeling images really easy and inexpensive in a lot of ways. It… trust me. It’s still expensive. But compared to audio and and many other sources, it’s it’s, like, a lot easier. But you can get this whole story just from that quick five seconds. But now what if what if you were asked to look at this hour-long lecture for five seconds and then tell me, like, what happened. Right? You know, you get five seconds. Like, I’m not gonna play, you know, biochemistry lecture for you. But but but, nevertheless, five seconds with that, you’ll get five words, you know, ten, fifteen words, like, whatever. Right? You’re… there isn’t much you can do. Right? And so, hopefully, that illustrates a little bit about audio that’s happening. It’s real time. It’s very hard for a person to parallel process.</p>
<p>You can you can recognize that when you’re in a cocktail-party problem, which we’ll all have later today when we’re all sitting around trying to talk to each other, many conversations going on, etcetera. You can only really track one, maybe one and a half. You know? That kinda thing. But but part part of the reason… or… you know, part of the reason for that is humans are actually pretty good in real time in conversation. They can understand what what other humans are doing, but we have specialized hardware for it. You know? We have our brain that we… and ears and everything that is tuned just for listening to other humans talk and computers don’t. They they see the image like what is on the right here. And it’s… that’s that’s not the whole, like, sixty hour lecture. That’s, like, a couple seconds of it. You know? So it’s a whole mess. Right? And if you were to if you were to try to represent that entire lecture that way, you, as a human, would have no idea what’s going on. You might be able to say, a person was talking in this area or not. That that might be a question you can answer, but but, otherwise, you have no idea what they’re saying.</p>
<p>But if you run it through our ears and through our brain in real time, then we can understand it pretty well. But now how do you get a machine to actually do that? And that’s the real trick. Right? And if you have a machine that can do it, then it’s probably very valuable because now you can understand humans at scale. And that’s the type of thing that we build. I’ll just point out a few of these problems again, like, in a in a call center space. Maybe you have specific product names or company names or industry terms and jargon or certain acronyms that you’re using if you’re a bank or whatever it is. There’s also how it said, so maybe you’re speaking in a specific language. English, for instance. You could have a certain accent, dialect, etcetera. You could be upset or not. There could be background noise. You could be using a phone or some other device to actually talk to people, like calling in on a meeting or something. And, generally, when you’re having these conversations, they’re in a conversational style.</p>
<p>It’s a fast type of speaking, like I’m doing right now. Sorry about that. But but, nevertheless, it’s very conversational. You’re not trying to purposefully slow down and enunciate so that the machine can understand you. You know? And so that’s a challenge you have to deal with. And then audio… actually, a lot of people might think of audio as not that big of, like, a file, but it’s about the third the size of video. So there’s a lot of information packed in audio in order to make it sound good enough for you to understand. And there’s different encoding and different compression techniques, and, you know, there could be several channels, like one person talking on one… like like an agent and then, like, the customer that’s calling in, so multiple channels.</p>
<p>And so, anyway, audio is very varied and very difficult. But the the way that you have to handle this, and I I… I’ll talk technically just for a second here. But as you need a lot of data that is labeled, but it’s not just a lot of it. Like, the hour count matters, but it doesn’t matter as much as you might think. But it definitely matters. You need a lot in different areas, so young, old, male, female, low noise, high noise, in the car, not, upset, not upset, you know, very happy, etcetera. And you need this in multiple languages, multiple accents, etcetera, and you all… you need that all labeled as, like, a a a data plus truth pair, essentially. So here’s the audio and here’s the transcript that goes with that audio, and then the machine can learn by example. And you can kind of see that if you can read it here.</p>
<p>Audio comes in labeled, and that’s converted into training data. And then that makes a trained model, which you use, like, convolutional neural networks, dense dense layers or recurrent neural networks or attention layers or that type of thing is just kind of… I think of it like a like a periodic table of chemical elements, but, instead, it’s just for deep learning. You have these different things that care about spatial or temporal or that type of thing. But, nevertheless, mix those up, put them in the right order, expose them to a bunch of training data. And then you have a general model and… which is if many, if people in here have used APIs before or, you know, you’ve used your, like, keyboard on your phone, then, generally, what you’re using is is a general model, which is a a model that’s a one size fits all or jack-of-all-trades. Not necessarily a master of one, but it… it’s it’s designed to hopefully do pretty ok with the general population.</p>
<p>But if you have a specific task, like you’re our customer, like Citibank, and you’re like, hey. I have bankers, and they’re talking about bank stuff. And we wanna make sure that they’re compliant in the things that they’re doing, then you can go through this bottom loop here that says taking new data and label that to do transfer learning on it. So you have a… your original model that is already really good at English or other languages, and then now you expose it to a specific domain and then serve that in order to accomplish to your task.</p>
<p>So, anyway, if you wanna talk about the technical side of it later, I’m happy to happy to discuss, but that’s how it all works under the hood. But now you can get wet… you can get rid of the old way, or keep doing it if if you find value in it, but add the new way, which is doing a hundred percent analysis across all your data. Do it in a reliable way. Do it in a scalable way and with high accuracy. And so I think you you guys… Cyrano, one of our customers, you just saw the demo. We’re we’re pumped to have them, and we can discuss a little bit later. Again, we have a booth back there. But… about the use case there, so I I won’t take too much time talking about them, but another company that maybe some have heard heard of in the audience is Valyant.</p>
<p>And Valyant does fast-food ordering. So… well, it doesn’t have to be fast food. It could be many other things, but, typically, it’s fast food. So think of driving through and you’re in your car or you’re on your mobile phone or you’re at a kiosk, and you’re trying to order food inside a mall or something like that. And you have a very specific menu items, like Baconator or something like that. And you’d like your model to be very good at those things in this very noisy environment. And how do you do that? Well, you just saw magic. You have a general model, and then you transfer learn that model into their domain, and then it works really well. And another place that this works really well is in space.</p>
<p>So NASA is one of our customers. They they do space-to-ground communication. So we worked with the CTO and their team at the Johnson Space Center in Houston in Houston. And what they’re trying to do is under… essentially, there’s always a link between the international space station and ground. And there’s people listening all the time, like actual humans. Three of them sitting down and transcribing everything that’s happening. And, hey, can we, like, reduce that number to, like, two people or one person or zero people or something like that. And this is the problem that they’re working on, and it’s it’s a… it’s a big challenge, and their their CTO says, hey. The problem’s right in our faces. We have we have all of seventy five hundred pieces of jargon and acronyms, and it’s a scratchy, you know, radio signal, and it’s it’s it’s not a pleasant thing to try to transcribe. But, hey, if there were a machine that could listen to this and do a good job of it, then that would be very valuable to them. And so, anyway, they they tried a lot of vendors.</p>
<p>And as you can maybe imagine using a general model approach, which is what pretty much everybody else does, then it doesn’t work that well. But if you use this transfer-learning approach, then it works really well handling background noise, jargon, multi speaker, etcetera. And, yeah, happy to talk about these use cases later if you want to. But… so, yeah, moving on to act three here a little bit, which is just a comment on what’s happening in the world. We’re in the middle of the intelligence revolution, like it or not. There was the agricultural revolution, industrial revolution, information revolution. We’re in the intelligence revolution right now. Automation is going to happen. Thirty years from now we’re gonna look back and be like, oh, yeah. Those were the beginning days. You know? Just like if anybody in here was around when the Internet was first starting to form. You remember, you know, when you got your first email account, and you’re like, holy shit. This is crazy. You know, you can talk to anybody anywhere.</p>
<p>And, anyway, same… we’re we’re in that we’re in that point in time right now. And another comment is that real time is finally real. A couple of years ago, it wasn’t, and now it actually is. That’s partially because the models are getting good enough to do things in real time, but it’s also partially because the computational techniques have become sophisticated enough, but not as heavy lifting plus other… like, NVIDIA has gotten good essentially at processing larger model sizes and that type of thing. So, essentially, confluence of hardware plus software plus new techniques being implemented in software can make real time… high-accuracy real time actually real. And then these techniques, if you’re if you’re in a general domain, you can get, like, ninety percent plus accuracy with just a general model.</p>
<p>Or if you if you need that kind of accuracy, but it’s kind of a niche domain, then you can go after a custom-trained model. And this stuff all works now, basically. So a lot a lot of times come… people come to me and say, like, hey. Is it possible too? And the the… in audio, the answer is almost like it… it’s almost always yes, unless it’s something that a human really can’t do. Like, can you build a lie detector? Like, well, I mean, kind of. Yes. But as as well as maybe you could as a human. You know? May… I mean, not quite that well, but close. Right? But think about all the other things that a human can do. Like, they can jump into a conversation, tell you how many people are speaking when they’re speaking, what words they’re talking about, what topic they’re talking about, that type of thing.</p>
<p>These things are all possible, but it’s still kind of, like, the railroad was built across the United States, but there’s a whole bunch of other work that has to be done and so all that stuff is happening now. So, yeah, the the people who win the west, you know, over the next couple decades will be the ones that empower the developers to build this architecture to… you know, the the railroad builders and the towns that sprout up along those railroad lines. But it’s going to be in compliance spaces, voice bots, call centers, the meetings, podcast. It’s gonna be all over the place. I mean, for voice, it’s just like a… it’s the natural human commit.</p>
<p>I’m up here talking right now. Right? Like, this is the natural communication mechanism, and it just previously was enabled by connectivity. So you could call somebody. You could do that you could do that type of thing, but there was never any automation or intelligence behind it, and now that’s all changing. And, you know, our lives are gonna change massively because of it. And I think the productivity of the world is gonna go up significantly, which previous, you know, talks that we heard today we’re all hinting at that. You know, our lives are gonna change a lot. So I’d… I I don’t know if there’s… if I if… maybe people are tired of hands. I don’t know. But are there start-up founders in the room here? Like, any… ok. Cool. So if you’re if if you’re into voice and you’re a start-up founder, you might wanna know about this.</p>
<p>So Deepgram has a start-up program that we gave away ten million dollars in speech-recognition credit. And what you can do is apply. Get up to a hundred thousand dollars for your start-up. And the reason we do this is to help push forward the the new the new products being built. I I don’t I don’t know if people are quite aware of what happened with AWS, GCP, Azure, etcetera over the last ten, fifteen years. But they gave away a lot of credit, and a lot of tech companies benefited as they grew their company on that initial credit. And, of course, you know, for Deepgram, the goal here is if you grow a whole bunch and become awesome then, you know, great. You’re you’re a customer of Deepgram, and we’re happy to help you. And another one.</p>
<p>So tomorrow evening, we have… how far away is it? A mile, but we have buses. We have party buses. Ok. So tomorrow evening, we’re going to… we… we’re gonna have a private screening of Dune. So if if you haven’t seen Dune yet, now’s an opportunity to do it at a place called Suds n n Cinema Suds n Cinema, which is a pretty dope theater, and we could all watch Dune together if you wanna come hang out and do that. We also have a booth. And we have a special… a a new a new member of our Deepgram team. We call him our VP of Intrigue. If you want to come meet him, come talk to us at the booth. I won’t say anymore. But thank you, everybody, for listening to the talk, and I I appreciate everybody coming out to hear it.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976850/blog/building-the-future-of-voice-scott-stephenson-ceo-deepgram-project-voice-x/scott-stephenson-script-analysis-chart%402x-1-300x16.png" alt=""></p>
<p><em>To learn more about real-time voice analysis, check out <a href="https://www.cyrano.ai/">Cyrano.ai</a>.</em></p>`;
}, "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/building-the-future-of-voice-scott-stephenson-ceo-deepgram-project-voice-x/index.md");

export { compiledContent, $$Index as default, frontmatter, metadata, rawContent };
