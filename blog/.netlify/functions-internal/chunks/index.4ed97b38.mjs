import { c as createAstro, a as createComponent, r as renderTemplate, b as renderHead } from '../entry.mjs';
import Slugger from 'github-slugger';
import '@astrojs/netlify/netlify-functions.js';
import 'preact';
import 'preact-render-to-string';
import 'vue';
import 'vue/server-renderer';
import 'html-escaper';
import 'node-html-parser';
/* empty css                           */import 'axios';
/* empty css                          *//* empty css                           *//* empty css                          *//* empty css                              *//* empty css                              */import 'clone-deep';
import 'slugify';
import 'shiki';
/* empty css                           *//* empty css                              */import '@astrojs/rss';
/* empty css                           */import 'mime';
import 'cookie';
import 'kleur/colors';
import 'string-width';
import 'path-browserify';
import 'path-to-regexp';

const metadata = { "headings": [{ "depth": 2, "slug": "you-want-to-train-a-deep-neural-network-for-speech-recognition", "text": "You want to train a Deep Neural Network for Speech Recognition?" }, { "depth": 2, "slug": "turn-this-input-audio", "text": "Turn this input audio \u2B07\u2B07\u2B07" }, { "depth": 2, "slug": "into-this-text", "text": "Into this text \u2B07\u2B07\u2B07" }, { "depth": 1, "slug": "why-we-did-it", "text": "Why we did it" }, { "depth": 4, "slug": "to-get-this-working-download-and-install-kur", "text": "To get this working, download and install Kur" }, { "depth": 4, "slug": "run-the-deepspeech-example", "text": "Run the Deepspeech Example" }, { "depth": 4, "slug": "your-model-will-start-training", "text": "Your model will start training" }, { "depth": 5, "slug": "training-and-validation-loss-of-kur-deepspeech-model", "text": "Training and Validation Loss of Kur Deepspeech Model" }, { "depth": 4, "slug": "theres-a-lot-of-things-to-try", "text": "There\u2019s a lot of things to try" }, { "depth": 2, "slug": "overview-of-how-it-works", "text": "Overview of How it Works" }, { "depth": 4, "slug": "this-is-how-deepspeech-works-in-kur", "text": "This is how Deepspeech works in Kur" }, { "depth": 2, "slug": "tell-us-what-you-think", "text": "Tell us what you think" }], "source": "## You want to train a Deep Neural Network for Speech Recognition?\n\nMe too. It was two years ago and I was a particle physicist finishing a PhD at University of Michigan. I could code a little in C/C++ and Python and I knew Noah Shutty. Noah's my cofounder at [Deepgram](https://deepgram.com/) and an all around powerhouse of steep learning curve-ness. We both had no speech recognition background (at all), a little programming and applied machine learning (boosted decision trees and NN), and a lot of data juggling/system hacking experience. We found ourselves building the world's first deep learning based speech search engine. To get moving we needed a DNN that could understand speech. Here's the basic problem.\n\n## Turn this input audio \u2B07\u2B07\u2B07\n\n![missing](https://res.cloudinary.com/deepgram/image/upload/v1661725768/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-03-at-8.56.51-PM.png)\n\n<div style=\"text-align: center;\">A spectrogram of an ordinary squishy human saying \"I am a human saying human things.\"</div>\n\nListen to the audio.\n\n<iframe src=\"https://www.youtube.com/embed/TOZVpWL3ZGA\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\n## Into this text \u2B07\u2B07\u2B07\n\n![missing](https://res.cloudinary.com/deepgram/image/upload/v1661725769/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-02-at-12.51.52-PM.png)\n\n<div style=\"text-align: center; styl;margin-top: 0px;\">The prediction from a DNN that just heard the \"I am a human saying human things\" audio file.</div>\n\n# Why we did it\n\nWe'll probably write a \"This is Deepgram\" post sometime, but suffice to say: we are building a Google for Audio and **we needed a deep learning model for speech recognition** to accomplish that goal. Good thing Baidu had just released the first of the Deepspeech papers two years ago when we were starting \\[[1]](https://arxiv.org/abs/1412.5567). This gave us the push we needed to figure out how deep learning can work for audio search. Here's a picture of the Deepspeech RNN for inspiration. \n\n![](https://res.cloudinary.com/deepgram/image/upload/v1661725769/blog/how-to-train-baidus-deepspeech-model-with-kur/NVIDIA_Baidu_Deep_Speech_Neural_Network_600-1.jpg)\n\n*Baidu's Andrew Ng at NVidia's GTC conference talking about Deepspeech*\n\nDeep Learning is hard. There are a few reasons why that is but some obvious ones are that models and frameworks involving differentiable tensor graphs are complicated, the hardware to run them efficiently is finicky (GPUs), and it's difficult to get data. Getting a working Deepspeech model is pretty hard too, even with a paper outlining it. **The first step was to build an end-to-end deep learning speech recognition system.** We started working on that and based the DNN on the Baidu Deepspeech paper. After a lot of toil, we put together a genuinely good end-to-end DNN speech recognition model. We've [open sourced](https://kur.deepgram.com/) the Deepspeech model in the [Kur](https://kur.deepgram.com) framework running on [TensorFlow](https://www.github.com/tensorflow/tensorflow).\n\n> *Quick Aside:* We had to build Kur for Deepgram's survival. It's the wild west out here in A.I. and it's not possible to quickly build cutting edge models unless you have a simple way to do it.\n\nWe find that Kur lets you *describe your model* and then *it works* without having to do a lot of the plumbing that slows projects down. The [Kur](https://github.com/deepgram/kur) software package was [just released](https://techcrunch.com/2017/01/18/deepgram-open-sources-kur-to-make-diy-deep-learning-less-painful/). *It's free. It's open source. It's named after [the first mythical dragon](https://en.wikipedia.org/wiki/Kur).* Kur was crafted by the whole Deepgram A.I. team and we hope it helps the deep learning community in some small way.\n\n#### To get this working, download and install [Kur](https://github.com/deepgram/kur)\n\nTo install, all you really need to do is run `$ pip install kur` in your terminal if you have python 3.4 or above installed. If you need guidance or want easy environments to work in, we have an entire installation page at [kur.deepgram.com](https://kur.deepgram.com).\n\n#### Run the Deepspeech Example\n\nOnce Kur is installed, fire up your fingers and run `$ kur -v train speech.yml` from your `kur/examples/` directory. You can omit the `-v` if you want Kur to quietly train without outlining what it's up to in your terminal's standard out. We find that running with `-v` the first few times gives you an idea of how Kur works, however. Turn on `-vv` if you're really craving gory details.\n\n#### Your model will start training\n\nAt first, the outputs will be gibberish. But they get better :)\n\nHour 1:\n\n**True transcript:** `these vast buildings what were they`\n\n**DNN prediction:** `he s ma tol ln wt r hett jzxzjxzjqzjqjxzq`\n\nHour 6:\n\n**True transcript:** `the valkyrie kept off the coast steering to the westward`\n\n**DNN prediction:** `the bak gerly cap dof the cost stkuarinte the west werd`\n\nHour 24:\n\n**True transcript:** `it was a theatre ready made`\n\n**DNN prediction:** `it was it theater readi made`\n\n*Real English is spilling out.* I trained for 48 hours in total then ran the *\"i am a human saying human things\"* audio file through the network.\n\nListen to the audio.\n\n<iframe src=\"https://www.youtube.com/embed/TOZVpWL3ZGA\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\nHour 48:\n\n**True transcript:** `i am a human saying human things`\n\n**DNN prediction:** `i am a human saying human things`\n\nIt's just two days old and didn't make a single mistake on that utterance. **Our Speech A.I. is doing pretty well.**\n\n##### Training and Validation Loss of Kur Deepspeech Model\n\n![missing](https://res.cloudinary.com/deepgram/image/upload/v1661725770/blog/how-to-train-baidus-deepspeech-model-with-kur/loss-kur-up.png)\n\n<div style=\"text-align: center;\">Loss as a function of batch for both training and validation data in the [Kur](http://github.com/deepgram/kur) 'speech.yml' example. The validation data seems a little easier.</div>\n\n#### There's a lot of things to try\n\nWe abstracted away some of the time consuming bits. A little help comes from the descriptive nature of Kur, too. You can write down what you mean.\n\n![missing](https://res.cloudinary.com/deepgram/image/upload/v1661725771/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-02-at-10.15.51-AM.png)\n\n<div style=\"text-align: center;\">Hyperparameters for Deepspeech in the example Kurfile</div>\n\nThese are the handful of hyperparameters needed to construct the DNN. There's a single one dimensional CNN that operates on a time slice of FFT outputs. Then there's an RNN stack which is 3 layers deep and 1000 nodes wide each. The vocab size is how many 'letters' we'll be choosing from (`a` to `z`, a space and an apostrophe `'`-that's 28 total). The hyperparameters are grabbed in the model section of the Kurfile (that's the `speech.yml`). The CNN layer is built like this.\n\n![missing](https://res.cloudinary.com/deepgram/image/upload/v1661725772/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-02-at-10.20.24-AM.png)\n\n<div style=\"text-align: center;\">The CNN layer specification</div>\n\nThis puts in a single [CNN layer](https://en.wikipedia.org/wiki/Convolutional_neural_network#Convolutional_layer) with a few sensible hyperparameters and slaps on a [Rectified Linear Unit](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) activation layer. *Note: The hyperparameters are filled in using the [Jinja2](http://jinja.pocoo.org/docs/2.9/) templating engine.*\n\n> You can read more about defining Kurfiles in the docs at [kur.deepgram.com](https://kur.deepgram.com/).\n\nThe stack of [RNN layers](https://en.wikipedia.org/wiki/Recurrent_neural_network) is built with a `for` loop that stamps out three layers in a row-*three* because of the `depth` hyperparameter.\n\n![missing](https://res.cloudinary.com/deepgram/image/upload/v1661725772/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-09-at-5.05.32-PM.png)\n\n<div style=\"text-align: center;\">The RNN stack specification</div>\n\nThe batch normalization layer uses a technique to keep layer weights distributed in a non-crazy way, speeding up training. The rnn `sequence` hyperparameter just means you want the full sequence of outputs passed along (every single time slice of the audio) while generating a guess for the transcript. *Quick Summary:* The CNN layer ingests the FFT input then connects to RNNs and eventually to a fully connected layer which predicts from 28 letters. That's Deepspeech.\n\n## Overview of How it Works\n\nWhen training modern speech DNNs you generally slice up the audio into ~20 millisecond chunks, do something like a fast fourier transform (FFT) on the chunk of audio, feed those chunked FFTs into the DNN sequentially, and generate a prediction for the current chunk. You do that until you've digested the whole file (while remembering your chunk predictions the whole way) and end up with a sequence.\n\n#### This is how Deepspeech works in Kur\n\nKur takes in normal `wav` audio files. Then it grabs the spectrogram (FFT over time) of the file and jams it into a DNN with a CNN layer and a stack of three RNN layers. Out pops probabilities of latin characters, which (when read by a human) form words. As the model trains, there will be validation steps that give you an updated prediction on a random test audio file. You'll see the true text listed next to each prediction. You can watch the predicted text outputs get better as the network trains. *It's learning.* At first it will learn about spaces (ya know, this ), then it'll figure out good ratios for vowels and consonants, then it'll learn common easy words like `the`, `it`, `a`, `good` and build up it's vocabulary from there. It's fascinating to watch.\n\n![missing](https://res.cloudinary.com/deepgram/image/upload/v1661725773/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-02-at-12.21.53-PM.png)\n\n<div style=\"text-align: center;\">Take in time slices of audio frequencies and infer the letters that are being spoken. Time goes to the right. Image by Baidu.</div>\n\n## Tell us what you think\n\nAt Deepgram, we're really open about what we're working on. We know that A.I. is going to be huuuge and there are not enough trained people or good tools in the world to help it along ... yet. We hope [Kur](http://kur.deepgram.com), [KurHub](http://www.kurhub.com), our upcoming [Deep Learning Hackathon](http://www.deeplearninghackathon.com), and blog posts like this help out the community, gets people excited, and shows that the good stuff can now be used by everyone. We're a startup and our research team can only produce so much helpful material per unit time. The best way to help us is to implement your [favorite Deep Learning papers](https://github.com/terryum/awesome-deep-learning-papers) in [Kur](https://www.github.com/deepgram/kur) and upload it to [KurHub](http://www.kurhub.com/). You can also contribute to the Kur framework directly on [GitHub](https://github.com/deepgram/kur). You'll be showered with thanks from us and a pile of others that are hungry for good implementations.", "html": `<h2 id="you-want-to-train-a-deep-neural-network-for-speech-recognition">You want to train a Deep Neural Network for Speech Recognition?</h2>
<p>Me too. It was two years ago and I was a particle physicist finishing a PhD at University of Michigan. I could code a little in C/C++ and Python and I knew Noah Shutty. Noah\u2019s my cofounder at <a href="https://deepgram.com/">Deepgram</a> and an all around powerhouse of steep learning curve-ness. We both had no speech recognition background (at all), a little programming and applied machine learning (boosted decision trees and NN), and a lot of data juggling/system hacking experience. We found ourselves building the world\u2019s first deep learning based speech search engine. To get moving we needed a DNN that could understand speech. Here\u2019s the basic problem.</p>
<h2 id="turn-this-input-audio">Turn this input audio \u2B07\u2B07\u2B07</h2>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661725768/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-03-at-8.56.51-PM.png" alt="missing"></p>
<div style="text-align: center;">A spectrogram of an ordinary squishy human saying \u201CI am a human saying human things.\u201D</div>
<p>Listen to the audio.</p>
<iframe src="https://www.youtube.com/embed/TOZVpWL3ZGA" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen" />
<h2 id="into-this-text">Into this text \u2B07\u2B07\u2B07</h2>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661725769/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-02-at-12.51.52-PM.png" alt="missing"></p>
<div style="text-align: center; styl;margin-top: 0px;">The prediction from a DNN that just heard the \u201CI am a human saying human things\u201D audio file.</div>
<h1 id="why-we-did-it">Why we did it</h1>
<p>We\u2019ll probably write a \u201CThis is Deepgram\u201D post sometime, but suffice to say: we are building a Google for Audio and <strong>we needed a deep learning model for speech recognition</strong> to accomplish that goal. Good thing Baidu had just released the first of the Deepspeech papers two years ago when we were starting [[1]](<a href="https://arxiv.org/abs/1412.5567">https://arxiv.org/abs/1412.5567</a>). This gave us the push we needed to figure out how deep learning can work for audio search. Here\u2019s a picture of the Deepspeech RNN for inspiration.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661725769/blog/how-to-train-baidus-deepspeech-model-with-kur/NVIDIA_Baidu_Deep_Speech_Neural_Network_600-1.jpg" alt=""></p>
<p><em>Baidu\u2019s Andrew Ng at NVidia\u2019s GTC conference talking about Deepspeech</em></p>
<p>Deep Learning is hard. There are a few reasons why that is but some obvious ones are that models and frameworks involving differentiable tensor graphs are complicated, the hardware to run them efficiently is finicky (GPUs), and it\u2019s difficult to get data. Getting a working Deepspeech model is pretty hard too, even with a paper outlining it. <strong>The first step was to build an end-to-end deep learning speech recognition system.</strong> We started working on that and based the DNN on the Baidu Deepspeech paper. After a lot of toil, we put together a genuinely good end-to-end DNN speech recognition model. We\u2019ve <a href="https://kur.deepgram.com/">open sourced</a> the Deepspeech model in the <a href="https://kur.deepgram.com">Kur</a> framework running on <a href="https://www.github.com/tensorflow/tensorflow">TensorFlow</a>.</p>
<blockquote>
<p><em>Quick Aside:</em> We had to build Kur for Deepgram\u2019s survival. It\u2019s the wild west out here in A.I. and it\u2019s not possible to quickly build cutting edge models unless you have a simple way to do it.</p>
</blockquote>
<p>We find that Kur lets you <em>describe your model</em> and then <em>it works</em> without having to do a lot of the plumbing that slows projects down. The <a href="https://github.com/deepgram/kur">Kur</a> software package was <a href="https://techcrunch.com/2017/01/18/deepgram-open-sources-kur-to-make-diy-deep-learning-less-painful/">just released</a>. <em>It\u2019s free. It\u2019s open source. It\u2019s named after <a href="https://en.wikipedia.org/wiki/Kur">the first mythical dragon</a>.</em> Kur was crafted by the whole Deepgram A.I. team and we hope it helps the deep learning community in some small way.</p>
<h4 id="to-get-this-working-download-and-install-kur">To get this working, download and install <a href="https://github.com/deepgram/kur">Kur</a></h4>
<p>To install, all you really need to do is run <code is:raw>$ pip install kur</code> in your terminal if you have python 3.4 or above installed. If you need guidance or want easy environments to work in, we have an entire installation page at <a href="https://kur.deepgram.com">kur.deepgram.com</a>.</p>
<h4 id="run-the-deepspeech-example">Run the Deepspeech Example</h4>
<p>Once Kur is installed, fire up your fingers and run <code is:raw>$ kur -v train speech.yml</code> from your <code is:raw>kur/examples/</code> directory. You can omit the <code is:raw>-v</code> if you want Kur to quietly train without outlining what it\u2019s up to in your terminal\u2019s standard out. We find that running with <code is:raw>-v</code> the first few times gives you an idea of how Kur works, however. Turn on <code is:raw>-vv</code> if you\u2019re really craving gory details.</p>
<h4 id="your-model-will-start-training">Your model will start training</h4>
<p>At first, the outputs will be gibberish. But they get better :)</p>
<p>Hour 1:</p>
<p><strong>True transcript:</strong> <code is:raw>these vast buildings what were they</code></p>
<p><strong>DNN prediction:</strong> <code is:raw>he s ma tol ln wt r hett jzxzjxzjqzjqjxzq</code></p>
<p>Hour 6:</p>
<p><strong>True transcript:</strong> <code is:raw>the valkyrie kept off the coast steering to the westward</code></p>
<p><strong>DNN prediction:</strong> <code is:raw>the bak gerly cap dof the cost stkuarinte the west werd</code></p>
<p>Hour 24:</p>
<p><strong>True transcript:</strong> <code is:raw>it was a theatre ready made</code></p>
<p><strong>DNN prediction:</strong> <code is:raw>it was it theater readi made</code></p>
<p><em>Real English is spilling out.</em> I trained for 48 hours in total then ran the <em>\u201Ci am a human saying human things\u201D</em> audio file through the network.</p>
<p>Listen to the audio.</p>
<iframe src="https://www.youtube.com/embed/TOZVpWL3ZGA" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen" />
<p>Hour 48:</p>
<p><strong>True transcript:</strong> <code is:raw>i am a human saying human things</code></p>
<p><strong>DNN prediction:</strong> <code is:raw>i am a human saying human things</code></p>
<p>It\u2019s just two days old and didn\u2019t make a single mistake on that utterance. <strong>Our Speech A.I. is doing pretty well.</strong></p>
<h5 id="training-and-validation-loss-of-kur-deepspeech-model">Training and Validation Loss of Kur Deepspeech Model</h5>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661725770/blog/how-to-train-baidus-deepspeech-model-with-kur/loss-kur-up.png" alt="missing"></p>
<div style="text-align: center;">Loss as a function of batch for both training and validation data in the <a href="http://github.com/deepgram/kur">Kur</a> \u2018speech.yml\u2019 example. The validation data seems a little easier.</div>
<h4 id="theres-a-lot-of-things-to-try">There\u2019s a lot of things to try</h4>
<p>We abstracted away some of the time consuming bits. A little help comes from the descriptive nature of Kur, too. You can write down what you mean.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661725771/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-02-at-10.15.51-AM.png" alt="missing"></p>
<div style="text-align: center;">Hyperparameters for Deepspeech in the example Kurfile</div>
<p>These are the handful of hyperparameters needed to construct the DNN. There\u2019s a single one dimensional CNN that operates on a time slice of FFT outputs. Then there\u2019s an RNN stack which is 3 layers deep and 1000 nodes wide each. The vocab size is how many \u2018letters\u2019 we\u2019ll be choosing from (<code is:raw>a</code> to <code is:raw>z</code>, a space and an apostrophe <code is:raw>'</code>-that\u2019s 28 total). The hyperparameters are grabbed in the model section of the Kurfile (that\u2019s the <code is:raw>speech.yml</code>). The CNN layer is built like this.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661725772/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-02-at-10.20.24-AM.png" alt="missing"></p>
<div style="text-align: center;">The CNN layer specification</div>
<p>This puts in a single <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#Convolutional_layer">CNN layer</a> with a few sensible hyperparameters and slaps on a <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">Rectified Linear Unit</a> activation layer. <em>Note: The hyperparameters are filled in using the <a href="http://jinja.pocoo.org/docs/2.9/">Jinja2</a> templating engine.</em></p>
<blockquote>
<p>You can read more about defining Kurfiles in the docs at <a href="https://kur.deepgram.com/">kur.deepgram.com</a>.</p>
</blockquote>
<p>The stack of <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">RNN layers</a> is built with a <code is:raw>for</code> loop that stamps out three layers in a row-<em>three</em> because of the <code is:raw>depth</code> hyperparameter.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661725772/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-09-at-5.05.32-PM.png" alt="missing"></p>
<div style="text-align: center;">The RNN stack specification</div>
<p>The batch normalization layer uses a technique to keep layer weights distributed in a non-crazy way, speeding up training. The rnn <code is:raw>sequence</code> hyperparameter just means you want the full sequence of outputs passed along (every single time slice of the audio) while generating a guess for the transcript. <em>Quick Summary:</em> The CNN layer ingests the FFT input then connects to RNNs and eventually to a fully connected layer which predicts from 28 letters. That\u2019s Deepspeech.</p>
<h2 id="overview-of-how-it-works">Overview of How it Works</h2>
<p>When training modern speech DNNs you generally slice up the audio into ~20 millisecond chunks, do something like a fast fourier transform (FFT) on the chunk of audio, feed those chunked FFTs into the DNN sequentially, and generate a prediction for the current chunk. You do that until you\u2019ve digested the whole file (while remembering your chunk predictions the whole way) and end up with a sequence.</p>
<h4 id="this-is-how-deepspeech-works-in-kur">This is how Deepspeech works in Kur</h4>
<p>Kur takes in normal <code is:raw>wav</code> audio files. Then it grabs the spectrogram (FFT over time) of the file and jams it into a DNN with a CNN layer and a stack of three RNN layers. Out pops probabilities of latin characters, which (when read by a human) form words. As the model trains, there will be validation steps that give you an updated prediction on a random test audio file. You\u2019ll see the true text listed next to each prediction. You can watch the predicted text outputs get better as the network trains. <em>It\u2019s learning.</em> At first it will learn about spaces (ya know, this ), then it\u2019ll figure out good ratios for vowels and consonants, then it\u2019ll learn common easy words like <code is:raw>the</code>, <code is:raw>it</code>, <code is:raw>a</code>, <code is:raw>good</code> and build up it\u2019s vocabulary from there. It\u2019s fascinating to watch.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661725773/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-02-at-12.21.53-PM.png" alt="missing"></p>
<div style="text-align: center;">Take in time slices of audio frequencies and infer the letters that are being spoken. Time goes to the right. Image by Baidu.</div>
<h2 id="tell-us-what-you-think">Tell us what you think</h2>
<p>At Deepgram, we\u2019re really open about what we\u2019re working on. We know that A.I. is going to be huuuge and there are not enough trained people or good tools in the world to help it along \u2026 yet. We hope <a href="http://kur.deepgram.com">Kur</a>, <a href="http://www.kurhub.com">KurHub</a>, our upcoming <a href="http://www.deeplearninghackathon.com">Deep Learning Hackathon</a>, and blog posts like this help out the community, gets people excited, and shows that the good stuff can now be used by everyone. We\u2019re a startup and our research team can only produce so much helpful material per unit time. The best way to help us is to implement your <a href="https://github.com/terryum/awesome-deep-learning-papers">favorite Deep Learning papers</a> in <a href="https://www.github.com/deepgram/kur">Kur</a> and upload it to <a href="http://www.kurhub.com/">KurHub</a>. You can also contribute to the Kur framework directly on <a href="https://github.com/deepgram/kur">GitHub</a>. You\u2019ll be showered with thanks from us and a pile of others that are hungry for good implementations.</p>` };
const frontmatter = { "title": "How to train Baidus Deepspeech model", "description": "Learn how to train a deep neural network for speech recognition.", "date": "2017-02-21T00:00:00.000Z", "cover": "https://res.cloudinary.com/deepgram/image/upload/v1661981932/blog/how-to-train-baidus-deepspeech-model-with-kur/placeholder-post-image%402x.jpg", "authors": ["scott-stephenson"], "category": "ai-and-engineering", "tags": ["speech-models"], "seo": { "title": "How to train Baidus Deepspeech model", "description": "" }, "og": { "image": "https://res.cloudinary.com/deepgram/image/upload/v1661981932/blog/how-to-train-baidus-deepspeech-model-with-kur/placeholder-post-image%402x.jpg" }, "shorturls": { "share": "https://dpgr.am/85061c2", "twitter": "https://dpgr.am/febaff5", "linkedin": "https://dpgr.am/41bf9c7", "reddit": "https://dpgr.am/70fb579", "facebook": "https://dpgr.am/b693c50" }, "astro": { "headings": [{ "depth": 2, "slug": "you-want-to-train-a-deep-neural-network-for-speech-recognition", "text": "You want to train a Deep Neural Network for Speech Recognition?" }, { "depth": 2, "slug": "turn-this-input-audio", "text": "Turn this input audio \u2B07\u2B07\u2B07" }, { "depth": 2, "slug": "into-this-text", "text": "Into this text \u2B07\u2B07\u2B07" }, { "depth": 1, "slug": "why-we-did-it", "text": "Why we did it" }, { "depth": 4, "slug": "to-get-this-working-download-and-install-kur", "text": "To get this working, download and install Kur" }, { "depth": 4, "slug": "run-the-deepspeech-example", "text": "Run the Deepspeech Example" }, { "depth": 4, "slug": "your-model-will-start-training", "text": "Your model will start training" }, { "depth": 5, "slug": "training-and-validation-loss-of-kur-deepspeech-model", "text": "Training and Validation Loss of Kur Deepspeech Model" }, { "depth": 4, "slug": "theres-a-lot-of-things-to-try", "text": "There\u2019s a lot of things to try" }, { "depth": 2, "slug": "overview-of-how-it-works", "text": "Overview of How it Works" }, { "depth": 4, "slug": "this-is-how-deepspeech-works-in-kur", "text": "This is how Deepspeech works in Kur" }, { "depth": 2, "slug": "tell-us-what-you-think", "text": "Tell us what you think" }], "source": "## You want to train a Deep Neural Network for Speech Recognition?\n\nMe too. It was two years ago and I was a particle physicist finishing a PhD at University of Michigan. I could code a little in C/C++ and Python and I knew Noah Shutty. Noah's my cofounder at [Deepgram](https://deepgram.com/) and an all around powerhouse of steep learning curve-ness. We both had no speech recognition background (at all), a little programming and applied machine learning (boosted decision trees and NN), and a lot of data juggling/system hacking experience. We found ourselves building the world's first deep learning based speech search engine. To get moving we needed a DNN that could understand speech. Here's the basic problem.\n\n## Turn this input audio \u2B07\u2B07\u2B07\n\n![missing](https://res.cloudinary.com/deepgram/image/upload/v1661725768/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-03-at-8.56.51-PM.png)\n\n<div style=\"text-align: center;\">A spectrogram of an ordinary squishy human saying \"I am a human saying human things.\"</div>\n\nListen to the audio.\n\n<iframe src=\"https://www.youtube.com/embed/TOZVpWL3ZGA\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\n## Into this text \u2B07\u2B07\u2B07\n\n![missing](https://res.cloudinary.com/deepgram/image/upload/v1661725769/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-02-at-12.51.52-PM.png)\n\n<div style=\"text-align: center; styl;margin-top: 0px;\">The prediction from a DNN that just heard the \"I am a human saying human things\" audio file.</div>\n\n# Why we did it\n\nWe'll probably write a \"This is Deepgram\" post sometime, but suffice to say: we are building a Google for Audio and **we needed a deep learning model for speech recognition** to accomplish that goal. Good thing Baidu had just released the first of the Deepspeech papers two years ago when we were starting \\[[1]](https://arxiv.org/abs/1412.5567). This gave us the push we needed to figure out how deep learning can work for audio search. Here's a picture of the Deepspeech RNN for inspiration. \n\n![](https://res.cloudinary.com/deepgram/image/upload/v1661725769/blog/how-to-train-baidus-deepspeech-model-with-kur/NVIDIA_Baidu_Deep_Speech_Neural_Network_600-1.jpg)\n\n*Baidu's Andrew Ng at NVidia's GTC conference talking about Deepspeech*\n\nDeep Learning is hard. There are a few reasons why that is but some obvious ones are that models and frameworks involving differentiable tensor graphs are complicated, the hardware to run them efficiently is finicky (GPUs), and it's difficult to get data. Getting a working Deepspeech model is pretty hard too, even with a paper outlining it. **The first step was to build an end-to-end deep learning speech recognition system.** We started working on that and based the DNN on the Baidu Deepspeech paper. After a lot of toil, we put together a genuinely good end-to-end DNN speech recognition model. We've [open sourced](https://kur.deepgram.com/) the Deepspeech model in the [Kur](https://kur.deepgram.com) framework running on [TensorFlow](https://www.github.com/tensorflow/tensorflow).\n\n> *Quick Aside:* We had to build Kur for Deepgram's survival. It's the wild west out here in A.I. and it's not possible to quickly build cutting edge models unless you have a simple way to do it.\n\nWe find that Kur lets you *describe your model* and then *it works* without having to do a lot of the plumbing that slows projects down. The [Kur](https://github.com/deepgram/kur) software package was [just released](https://techcrunch.com/2017/01/18/deepgram-open-sources-kur-to-make-diy-deep-learning-less-painful/). *It's free. It's open source. It's named after [the first mythical dragon](https://en.wikipedia.org/wiki/Kur).* Kur was crafted by the whole Deepgram A.I. team and we hope it helps the deep learning community in some small way.\n\n#### To get this working, download and install [Kur](https://github.com/deepgram/kur)\n\nTo install, all you really need to do is run `$ pip install kur` in your terminal if you have python 3.4 or above installed. If you need guidance or want easy environments to work in, we have an entire installation page at [kur.deepgram.com](https://kur.deepgram.com).\n\n#### Run the Deepspeech Example\n\nOnce Kur is installed, fire up your fingers and run `$ kur -v train speech.yml` from your `kur/examples/` directory. You can omit the `-v` if you want Kur to quietly train without outlining what it's up to in your terminal's standard out. We find that running with `-v` the first few times gives you an idea of how Kur works, however. Turn on `-vv` if you're really craving gory details.\n\n#### Your model will start training\n\nAt first, the outputs will be gibberish. But they get better :)\n\nHour 1:\n\n**True transcript:** `these vast buildings what were they`\n\n**DNN prediction:** `he s ma tol ln wt r hett jzxzjxzjqzjqjxzq`\n\nHour 6:\n\n**True transcript:** `the valkyrie kept off the coast steering to the westward`\n\n**DNN prediction:** `the bak gerly cap dof the cost stkuarinte the west werd`\n\nHour 24:\n\n**True transcript:** `it was a theatre ready made`\n\n**DNN prediction:** `it was it theater readi made`\n\n*Real English is spilling out.* I trained for 48 hours in total then ran the *\"i am a human saying human things\"* audio file through the network.\n\nListen to the audio.\n\n<iframe src=\"https://www.youtube.com/embed/TOZVpWL3ZGA\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\nHour 48:\n\n**True transcript:** `i am a human saying human things`\n\n**DNN prediction:** `i am a human saying human things`\n\nIt's just two days old and didn't make a single mistake on that utterance. **Our Speech A.I. is doing pretty well.**\n\n##### Training and Validation Loss of Kur Deepspeech Model\n\n![missing](https://res.cloudinary.com/deepgram/image/upload/v1661725770/blog/how-to-train-baidus-deepspeech-model-with-kur/loss-kur-up.png)\n\n<div style=\"text-align: center;\">Loss as a function of batch for both training and validation data in the [Kur](http://github.com/deepgram/kur) 'speech.yml' example. The validation data seems a little easier.</div>\n\n#### There's a lot of things to try\n\nWe abstracted away some of the time consuming bits. A little help comes from the descriptive nature of Kur, too. You can write down what you mean.\n\n![missing](https://res.cloudinary.com/deepgram/image/upload/v1661725771/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-02-at-10.15.51-AM.png)\n\n<div style=\"text-align: center;\">Hyperparameters for Deepspeech in the example Kurfile</div>\n\nThese are the handful of hyperparameters needed to construct the DNN. There's a single one dimensional CNN that operates on a time slice of FFT outputs. Then there's an RNN stack which is 3 layers deep and 1000 nodes wide each. The vocab size is how many 'letters' we'll be choosing from (`a` to `z`, a space and an apostrophe `'`-that's 28 total). The hyperparameters are grabbed in the model section of the Kurfile (that's the `speech.yml`). The CNN layer is built like this.\n\n![missing](https://res.cloudinary.com/deepgram/image/upload/v1661725772/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-02-at-10.20.24-AM.png)\n\n<div style=\"text-align: center;\">The CNN layer specification</div>\n\nThis puts in a single [CNN layer](https://en.wikipedia.org/wiki/Convolutional_neural_network#Convolutional_layer) with a few sensible hyperparameters and slaps on a [Rectified Linear Unit](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) activation layer. *Note: The hyperparameters are filled in using the [Jinja2](http://jinja.pocoo.org/docs/2.9/) templating engine.*\n\n> You can read more about defining Kurfiles in the docs at [kur.deepgram.com](https://kur.deepgram.com/).\n\nThe stack of [RNN layers](https://en.wikipedia.org/wiki/Recurrent_neural_network) is built with a `for` loop that stamps out three layers in a row-*three* because of the `depth` hyperparameter.\n\n![missing](https://res.cloudinary.com/deepgram/image/upload/v1661725772/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-09-at-5.05.32-PM.png)\n\n<div style=\"text-align: center;\">The RNN stack specification</div>\n\nThe batch normalization layer uses a technique to keep layer weights distributed in a non-crazy way, speeding up training. The rnn `sequence` hyperparameter just means you want the full sequence of outputs passed along (every single time slice of the audio) while generating a guess for the transcript. *Quick Summary:* The CNN layer ingests the FFT input then connects to RNNs and eventually to a fully connected layer which predicts from 28 letters. That's Deepspeech.\n\n## Overview of How it Works\n\nWhen training modern speech DNNs you generally slice up the audio into ~20 millisecond chunks, do something like a fast fourier transform (FFT) on the chunk of audio, feed those chunked FFTs into the DNN sequentially, and generate a prediction for the current chunk. You do that until you've digested the whole file (while remembering your chunk predictions the whole way) and end up with a sequence.\n\n#### This is how Deepspeech works in Kur\n\nKur takes in normal `wav` audio files. Then it grabs the spectrogram (FFT over time) of the file and jams it into a DNN with a CNN layer and a stack of three RNN layers. Out pops probabilities of latin characters, which (when read by a human) form words. As the model trains, there will be validation steps that give you an updated prediction on a random test audio file. You'll see the true text listed next to each prediction. You can watch the predicted text outputs get better as the network trains. *It's learning.* At first it will learn about spaces (ya know, this ), then it'll figure out good ratios for vowels and consonants, then it'll learn common easy words like `the`, `it`, `a`, `good` and build up it's vocabulary from there. It's fascinating to watch.\n\n![missing](https://res.cloudinary.com/deepgram/image/upload/v1661725773/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-02-at-12.21.53-PM.png)\n\n<div style=\"text-align: center;\">Take in time slices of audio frequencies and infer the letters that are being spoken. Time goes to the right. Image by Baidu.</div>\n\n## Tell us what you think\n\nAt Deepgram, we're really open about what we're working on. We know that A.I. is going to be huuuge and there are not enough trained people or good tools in the world to help it along ... yet. We hope [Kur](http://kur.deepgram.com), [KurHub](http://www.kurhub.com), our upcoming [Deep Learning Hackathon](http://www.deeplearninghackathon.com), and blog posts like this help out the community, gets people excited, and shows that the good stuff can now be used by everyone. We're a startup and our research team can only produce so much helpful material per unit time. The best way to help us is to implement your [favorite Deep Learning papers](https://github.com/terryum/awesome-deep-learning-papers) in [Kur](https://www.github.com/deepgram/kur) and upload it to [KurHub](http://www.kurhub.com/). You can also contribute to the Kur framework directly on [GitHub](https://github.com/deepgram/kur). You'll be showered with thanks from us and a pile of others that are hungry for good implementations.", "html": `<h2 id="you-want-to-train-a-deep-neural-network-for-speech-recognition">You want to train a Deep Neural Network for Speech Recognition?</h2>
<p>Me too. It was two years ago and I was a particle physicist finishing a PhD at University of Michigan. I could code a little in C/C++ and Python and I knew Noah Shutty. Noah\u2019s my cofounder at <a href="https://deepgram.com/">Deepgram</a> and an all around powerhouse of steep learning curve-ness. We both had no speech recognition background (at all), a little programming and applied machine learning (boosted decision trees and NN), and a lot of data juggling/system hacking experience. We found ourselves building the world\u2019s first deep learning based speech search engine. To get moving we needed a DNN that could understand speech. Here\u2019s the basic problem.</p>
<h2 id="turn-this-input-audio">Turn this input audio \u2B07\u2B07\u2B07</h2>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661725768/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-03-at-8.56.51-PM.png" alt="missing"></p>
<div style="text-align: center;">A spectrogram of an ordinary squishy human saying \u201CI am a human saying human things.\u201D</div>
<p>Listen to the audio.</p>
<iframe src="https://www.youtube.com/embed/TOZVpWL3ZGA" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen" />
<h2 id="into-this-text">Into this text \u2B07\u2B07\u2B07</h2>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661725769/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-02-at-12.51.52-PM.png" alt="missing"></p>
<div style="text-align: center; styl;margin-top: 0px;">The prediction from a DNN that just heard the \u201CI am a human saying human things\u201D audio file.</div>
<h1 id="why-we-did-it">Why we did it</h1>
<p>We\u2019ll probably write a \u201CThis is Deepgram\u201D post sometime, but suffice to say: we are building a Google for Audio and <strong>we needed a deep learning model for speech recognition</strong> to accomplish that goal. Good thing Baidu had just released the first of the Deepspeech papers two years ago when we were starting [[1]](<a href="https://arxiv.org/abs/1412.5567">https://arxiv.org/abs/1412.5567</a>). This gave us the push we needed to figure out how deep learning can work for audio search. Here\u2019s a picture of the Deepspeech RNN for inspiration.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661725769/blog/how-to-train-baidus-deepspeech-model-with-kur/NVIDIA_Baidu_Deep_Speech_Neural_Network_600-1.jpg" alt=""></p>
<p><em>Baidu\u2019s Andrew Ng at NVidia\u2019s GTC conference talking about Deepspeech</em></p>
<p>Deep Learning is hard. There are a few reasons why that is but some obvious ones are that models and frameworks involving differentiable tensor graphs are complicated, the hardware to run them efficiently is finicky (GPUs), and it\u2019s difficult to get data. Getting a working Deepspeech model is pretty hard too, even with a paper outlining it. <strong>The first step was to build an end-to-end deep learning speech recognition system.</strong> We started working on that and based the DNN on the Baidu Deepspeech paper. After a lot of toil, we put together a genuinely good end-to-end DNN speech recognition model. We\u2019ve <a href="https://kur.deepgram.com/">open sourced</a> the Deepspeech model in the <a href="https://kur.deepgram.com">Kur</a> framework running on <a href="https://www.github.com/tensorflow/tensorflow">TensorFlow</a>.</p>
<blockquote>
<p><em>Quick Aside:</em> We had to build Kur for Deepgram\u2019s survival. It\u2019s the wild west out here in A.I. and it\u2019s not possible to quickly build cutting edge models unless you have a simple way to do it.</p>
</blockquote>
<p>We find that Kur lets you <em>describe your model</em> and then <em>it works</em> without having to do a lot of the plumbing that slows projects down. The <a href="https://github.com/deepgram/kur">Kur</a> software package was <a href="https://techcrunch.com/2017/01/18/deepgram-open-sources-kur-to-make-diy-deep-learning-less-painful/">just released</a>. <em>It\u2019s free. It\u2019s open source. It\u2019s named after <a href="https://en.wikipedia.org/wiki/Kur">the first mythical dragon</a>.</em> Kur was crafted by the whole Deepgram A.I. team and we hope it helps the deep learning community in some small way.</p>
<h4 id="to-get-this-working-download-and-install-kur">To get this working, download and install <a href="https://github.com/deepgram/kur">Kur</a></h4>
<p>To install, all you really need to do is run <code is:raw>$ pip install kur</code> in your terminal if you have python 3.4 or above installed. If you need guidance or want easy environments to work in, we have an entire installation page at <a href="https://kur.deepgram.com">kur.deepgram.com</a>.</p>
<h4 id="run-the-deepspeech-example">Run the Deepspeech Example</h4>
<p>Once Kur is installed, fire up your fingers and run <code is:raw>$ kur -v train speech.yml</code> from your <code is:raw>kur/examples/</code> directory. You can omit the <code is:raw>-v</code> if you want Kur to quietly train without outlining what it\u2019s up to in your terminal\u2019s standard out. We find that running with <code is:raw>-v</code> the first few times gives you an idea of how Kur works, however. Turn on <code is:raw>-vv</code> if you\u2019re really craving gory details.</p>
<h4 id="your-model-will-start-training">Your model will start training</h4>
<p>At first, the outputs will be gibberish. But they get better :)</p>
<p>Hour 1:</p>
<p><strong>True transcript:</strong> <code is:raw>these vast buildings what were they</code></p>
<p><strong>DNN prediction:</strong> <code is:raw>he s ma tol ln wt r hett jzxzjxzjqzjqjxzq</code></p>
<p>Hour 6:</p>
<p><strong>True transcript:</strong> <code is:raw>the valkyrie kept off the coast steering to the westward</code></p>
<p><strong>DNN prediction:</strong> <code is:raw>the bak gerly cap dof the cost stkuarinte the west werd</code></p>
<p>Hour 24:</p>
<p><strong>True transcript:</strong> <code is:raw>it was a theatre ready made</code></p>
<p><strong>DNN prediction:</strong> <code is:raw>it was it theater readi made</code></p>
<p><em>Real English is spilling out.</em> I trained for 48 hours in total then ran the <em>\u201Ci am a human saying human things\u201D</em> audio file through the network.</p>
<p>Listen to the audio.</p>
<iframe src="https://www.youtube.com/embed/TOZVpWL3ZGA" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen" />
<p>Hour 48:</p>
<p><strong>True transcript:</strong> <code is:raw>i am a human saying human things</code></p>
<p><strong>DNN prediction:</strong> <code is:raw>i am a human saying human things</code></p>
<p>It\u2019s just two days old and didn\u2019t make a single mistake on that utterance. <strong>Our Speech A.I. is doing pretty well.</strong></p>
<h5 id="training-and-validation-loss-of-kur-deepspeech-model">Training and Validation Loss of Kur Deepspeech Model</h5>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661725770/blog/how-to-train-baidus-deepspeech-model-with-kur/loss-kur-up.png" alt="missing"></p>
<div style="text-align: center;">Loss as a function of batch for both training and validation data in the <a href="http://github.com/deepgram/kur">Kur</a> \u2018speech.yml\u2019 example. The validation data seems a little easier.</div>
<h4 id="theres-a-lot-of-things-to-try">There\u2019s a lot of things to try</h4>
<p>We abstracted away some of the time consuming bits. A little help comes from the descriptive nature of Kur, too. You can write down what you mean.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661725771/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-02-at-10.15.51-AM.png" alt="missing"></p>
<div style="text-align: center;">Hyperparameters for Deepspeech in the example Kurfile</div>
<p>These are the handful of hyperparameters needed to construct the DNN. There\u2019s a single one dimensional CNN that operates on a time slice of FFT outputs. Then there\u2019s an RNN stack which is 3 layers deep and 1000 nodes wide each. The vocab size is how many \u2018letters\u2019 we\u2019ll be choosing from (<code is:raw>a</code> to <code is:raw>z</code>, a space and an apostrophe <code is:raw>'</code>-that\u2019s 28 total). The hyperparameters are grabbed in the model section of the Kurfile (that\u2019s the <code is:raw>speech.yml</code>). The CNN layer is built like this.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661725772/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-02-at-10.20.24-AM.png" alt="missing"></p>
<div style="text-align: center;">The CNN layer specification</div>
<p>This puts in a single <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#Convolutional_layer">CNN layer</a> with a few sensible hyperparameters and slaps on a <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">Rectified Linear Unit</a> activation layer. <em>Note: The hyperparameters are filled in using the <a href="http://jinja.pocoo.org/docs/2.9/">Jinja2</a> templating engine.</em></p>
<blockquote>
<p>You can read more about defining Kurfiles in the docs at <a href="https://kur.deepgram.com/">kur.deepgram.com</a>.</p>
</blockquote>
<p>The stack of <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">RNN layers</a> is built with a <code is:raw>for</code> loop that stamps out three layers in a row-<em>three</em> because of the <code is:raw>depth</code> hyperparameter.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661725772/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-09-at-5.05.32-PM.png" alt="missing"></p>
<div style="text-align: center;">The RNN stack specification</div>
<p>The batch normalization layer uses a technique to keep layer weights distributed in a non-crazy way, speeding up training. The rnn <code is:raw>sequence</code> hyperparameter just means you want the full sequence of outputs passed along (every single time slice of the audio) while generating a guess for the transcript. <em>Quick Summary:</em> The CNN layer ingests the FFT input then connects to RNNs and eventually to a fully connected layer which predicts from 28 letters. That\u2019s Deepspeech.</p>
<h2 id="overview-of-how-it-works">Overview of How it Works</h2>
<p>When training modern speech DNNs you generally slice up the audio into ~20 millisecond chunks, do something like a fast fourier transform (FFT) on the chunk of audio, feed those chunked FFTs into the DNN sequentially, and generate a prediction for the current chunk. You do that until you\u2019ve digested the whole file (while remembering your chunk predictions the whole way) and end up with a sequence.</p>
<h4 id="this-is-how-deepspeech-works-in-kur">This is how Deepspeech works in Kur</h4>
<p>Kur takes in normal <code is:raw>wav</code> audio files. Then it grabs the spectrogram (FFT over time) of the file and jams it into a DNN with a CNN layer and a stack of three RNN layers. Out pops probabilities of latin characters, which (when read by a human) form words. As the model trains, there will be validation steps that give you an updated prediction on a random test audio file. You\u2019ll see the true text listed next to each prediction. You can watch the predicted text outputs get better as the network trains. <em>It\u2019s learning.</em> At first it will learn about spaces (ya know, this ), then it\u2019ll figure out good ratios for vowels and consonants, then it\u2019ll learn common easy words like <code is:raw>the</code>, <code is:raw>it</code>, <code is:raw>a</code>, <code is:raw>good</code> and build up it\u2019s vocabulary from there. It\u2019s fascinating to watch.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661725773/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-02-at-12.21.53-PM.png" alt="missing"></p>
<div style="text-align: center;">Take in time slices of audio frequencies and infer the letters that are being spoken. Time goes to the right. Image by Baidu.</div>
<h2 id="tell-us-what-you-think">Tell us what you think</h2>
<p>At Deepgram, we\u2019re really open about what we\u2019re working on. We know that A.I. is going to be huuuge and there are not enough trained people or good tools in the world to help it along \u2026 yet. We hope <a href="http://kur.deepgram.com">Kur</a>, <a href="http://www.kurhub.com">KurHub</a>, our upcoming <a href="http://www.deeplearninghackathon.com">Deep Learning Hackathon</a>, and blog posts like this help out the community, gets people excited, and shows that the good stuff can now be used by everyone. We\u2019re a startup and our research team can only produce so much helpful material per unit time. The best way to help us is to implement your <a href="https://github.com/terryum/awesome-deep-learning-papers">favorite Deep Learning papers</a> in <a href="https://www.github.com/deepgram/kur">Kur</a> and upload it to <a href="http://www.kurhub.com/">KurHub</a>. You can also contribute to the Kur framework directly on <a href="https://github.com/deepgram/kur">GitHub</a>. You\u2019ll be showered with thanks from us and a pile of others that are hungry for good implementations.</p>` }, "file": "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-to-train-baidus-deepspeech-model-with-kur/index.md" };
function rawContent() {
  return "## You want to train a Deep Neural Network for Speech Recognition?\n\nMe too. It was two years ago and I was a particle physicist finishing a PhD at University of Michigan. I could code a little in C/C++ and Python and I knew Noah Shutty. Noah's my cofounder at [Deepgram](https://deepgram.com/) and an all around powerhouse of steep learning curve-ness. We both had no speech recognition background (at all), a little programming and applied machine learning (boosted decision trees and NN), and a lot of data juggling/system hacking experience. We found ourselves building the world's first deep learning based speech search engine. To get moving we needed a DNN that could understand speech. Here's the basic problem.\n\n## Turn this input audio \u2B07\u2B07\u2B07\n\n![missing](https://res.cloudinary.com/deepgram/image/upload/v1661725768/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-03-at-8.56.51-PM.png)\n\n<div style=\"text-align: center;\">A spectrogram of an ordinary squishy human saying \"I am a human saying human things.\"</div>\n\nListen to the audio.\n\n<iframe src=\"https://www.youtube.com/embed/TOZVpWL3ZGA\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\n## Into this text \u2B07\u2B07\u2B07\n\n![missing](https://res.cloudinary.com/deepgram/image/upload/v1661725769/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-02-at-12.51.52-PM.png)\n\n<div style=\"text-align: center; styl;margin-top: 0px;\">The prediction from a DNN that just heard the \"I am a human saying human things\" audio file.</div>\n\n# Why we did it\n\nWe'll probably write a \"This is Deepgram\" post sometime, but suffice to say: we are building a Google for Audio and **we needed a deep learning model for speech recognition** to accomplish that goal. Good thing Baidu had just released the first of the Deepspeech papers two years ago when we were starting \\[[1]](https://arxiv.org/abs/1412.5567). This gave us the push we needed to figure out how deep learning can work for audio search. Here's a picture of the Deepspeech RNN for inspiration. \n\n![](https://res.cloudinary.com/deepgram/image/upload/v1661725769/blog/how-to-train-baidus-deepspeech-model-with-kur/NVIDIA_Baidu_Deep_Speech_Neural_Network_600-1.jpg)\n\n*Baidu's Andrew Ng at NVidia's GTC conference talking about Deepspeech*\n\nDeep Learning is hard. There are a few reasons why that is but some obvious ones are that models and frameworks involving differentiable tensor graphs are complicated, the hardware to run them efficiently is finicky (GPUs), and it's difficult to get data. Getting a working Deepspeech model is pretty hard too, even with a paper outlining it. **The first step was to build an end-to-end deep learning speech recognition system.** We started working on that and based the DNN on the Baidu Deepspeech paper. After a lot of toil, we put together a genuinely good end-to-end DNN speech recognition model. We've [open sourced](https://kur.deepgram.com/) the Deepspeech model in the [Kur](https://kur.deepgram.com) framework running on [TensorFlow](https://www.github.com/tensorflow/tensorflow).\n\n> *Quick Aside:* We had to build Kur for Deepgram's survival. It's the wild west out here in A.I. and it's not possible to quickly build cutting edge models unless you have a simple way to do it.\n\nWe find that Kur lets you *describe your model* and then *it works* without having to do a lot of the plumbing that slows projects down. The [Kur](https://github.com/deepgram/kur) software package was [just released](https://techcrunch.com/2017/01/18/deepgram-open-sources-kur-to-make-diy-deep-learning-less-painful/). *It's free. It's open source. It's named after [the first mythical dragon](https://en.wikipedia.org/wiki/Kur).* Kur was crafted by the whole Deepgram A.I. team and we hope it helps the deep learning community in some small way.\n\n#### To get this working, download and install [Kur](https://github.com/deepgram/kur)\n\nTo install, all you really need to do is run `$ pip install kur` in your terminal if you have python 3.4 or above installed. If you need guidance or want easy environments to work in, we have an entire installation page at [kur.deepgram.com](https://kur.deepgram.com).\n\n#### Run the Deepspeech Example\n\nOnce Kur is installed, fire up your fingers and run `$ kur -v train speech.yml` from your `kur/examples/` directory. You can omit the `-v` if you want Kur to quietly train without outlining what it's up to in your terminal's standard out. We find that running with `-v` the first few times gives you an idea of how Kur works, however. Turn on `-vv` if you're really craving gory details.\n\n#### Your model will start training\n\nAt first, the outputs will be gibberish. But they get better :)\n\nHour 1:\n\n**True transcript:** `these vast buildings what were they`\n\n**DNN prediction:** `he s ma tol ln wt r hett jzxzjxzjqzjqjxzq`\n\nHour 6:\n\n**True transcript:** `the valkyrie kept off the coast steering to the westward`\n\n**DNN prediction:** `the bak gerly cap dof the cost stkuarinte the west werd`\n\nHour 24:\n\n**True transcript:** `it was a theatre ready made`\n\n**DNN prediction:** `it was it theater readi made`\n\n*Real English is spilling out.* I trained for 48 hours in total then ran the *\"i am a human saying human things\"* audio file through the network.\n\nListen to the audio.\n\n<iframe src=\"https://www.youtube.com/embed/TOZVpWL3ZGA\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\nHour 48:\n\n**True transcript:** `i am a human saying human things`\n\n**DNN prediction:** `i am a human saying human things`\n\nIt's just two days old and didn't make a single mistake on that utterance. **Our Speech A.I. is doing pretty well.**\n\n##### Training and Validation Loss of Kur Deepspeech Model\n\n![missing](https://res.cloudinary.com/deepgram/image/upload/v1661725770/blog/how-to-train-baidus-deepspeech-model-with-kur/loss-kur-up.png)\n\n<div style=\"text-align: center;\">Loss as a function of batch for both training and validation data in the [Kur](http://github.com/deepgram/kur) 'speech.yml' example. The validation data seems a little easier.</div>\n\n#### There's a lot of things to try\n\nWe abstracted away some of the time consuming bits. A little help comes from the descriptive nature of Kur, too. You can write down what you mean.\n\n![missing](https://res.cloudinary.com/deepgram/image/upload/v1661725771/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-02-at-10.15.51-AM.png)\n\n<div style=\"text-align: center;\">Hyperparameters for Deepspeech in the example Kurfile</div>\n\nThese are the handful of hyperparameters needed to construct the DNN. There's a single one dimensional CNN that operates on a time slice of FFT outputs. Then there's an RNN stack which is 3 layers deep and 1000 nodes wide each. The vocab size is how many 'letters' we'll be choosing from (`a` to `z`, a space and an apostrophe `'`-that's 28 total). The hyperparameters are grabbed in the model section of the Kurfile (that's the `speech.yml`). The CNN layer is built like this.\n\n![missing](https://res.cloudinary.com/deepgram/image/upload/v1661725772/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-02-at-10.20.24-AM.png)\n\n<div style=\"text-align: center;\">The CNN layer specification</div>\n\nThis puts in a single [CNN layer](https://en.wikipedia.org/wiki/Convolutional_neural_network#Convolutional_layer) with a few sensible hyperparameters and slaps on a [Rectified Linear Unit](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) activation layer. *Note: The hyperparameters are filled in using the [Jinja2](http://jinja.pocoo.org/docs/2.9/) templating engine.*\n\n> You can read more about defining Kurfiles in the docs at [kur.deepgram.com](https://kur.deepgram.com/).\n\nThe stack of [RNN layers](https://en.wikipedia.org/wiki/Recurrent_neural_network) is built with a `for` loop that stamps out three layers in a row-*three* because of the `depth` hyperparameter.\n\n![missing](https://res.cloudinary.com/deepgram/image/upload/v1661725772/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-09-at-5.05.32-PM.png)\n\n<div style=\"text-align: center;\">The RNN stack specification</div>\n\nThe batch normalization layer uses a technique to keep layer weights distributed in a non-crazy way, speeding up training. The rnn `sequence` hyperparameter just means you want the full sequence of outputs passed along (every single time slice of the audio) while generating a guess for the transcript. *Quick Summary:* The CNN layer ingests the FFT input then connects to RNNs and eventually to a fully connected layer which predicts from 28 letters. That's Deepspeech.\n\n## Overview of How it Works\n\nWhen training modern speech DNNs you generally slice up the audio into ~20 millisecond chunks, do something like a fast fourier transform (FFT) on the chunk of audio, feed those chunked FFTs into the DNN sequentially, and generate a prediction for the current chunk. You do that until you've digested the whole file (while remembering your chunk predictions the whole way) and end up with a sequence.\n\n#### This is how Deepspeech works in Kur\n\nKur takes in normal `wav` audio files. Then it grabs the spectrogram (FFT over time) of the file and jams it into a DNN with a CNN layer and a stack of three RNN layers. Out pops probabilities of latin characters, which (when read by a human) form words. As the model trains, there will be validation steps that give you an updated prediction on a random test audio file. You'll see the true text listed next to each prediction. You can watch the predicted text outputs get better as the network trains. *It's learning.* At first it will learn about spaces (ya know, this ), then it'll figure out good ratios for vowels and consonants, then it'll learn common easy words like `the`, `it`, `a`, `good` and build up it's vocabulary from there. It's fascinating to watch.\n\n![missing](https://res.cloudinary.com/deepgram/image/upload/v1661725773/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-02-at-12.21.53-PM.png)\n\n<div style=\"text-align: center;\">Take in time slices of audio frequencies and infer the letters that are being spoken. Time goes to the right. Image by Baidu.</div>\n\n## Tell us what you think\n\nAt Deepgram, we're really open about what we're working on. We know that A.I. is going to be huuuge and there are not enough trained people or good tools in the world to help it along ... yet. We hope [Kur](http://kur.deepgram.com), [KurHub](http://www.kurhub.com), our upcoming [Deep Learning Hackathon](http://www.deeplearninghackathon.com), and blog posts like this help out the community, gets people excited, and shows that the good stuff can now be used by everyone. We're a startup and our research team can only produce so much helpful material per unit time. The best way to help us is to implement your [favorite Deep Learning papers](https://github.com/terryum/awesome-deep-learning-papers) in [Kur](https://www.github.com/deepgram/kur) and upload it to [KurHub](http://www.kurhub.com/). You can also contribute to the Kur framework directly on [GitHub](https://github.com/deepgram/kur). You'll be showered with thanks from us and a pile of others that are hungry for good implementations.";
}
function compiledContent() {
  return `<h2 id="you-want-to-train-a-deep-neural-network-for-speech-recognition">You want to train a Deep Neural Network for Speech Recognition?</h2>
<p>Me too. It was two years ago and I was a particle physicist finishing a PhD at University of Michigan. I could code a little in C/C++ and Python and I knew Noah Shutty. Noah\u2019s my cofounder at <a href="https://deepgram.com/">Deepgram</a> and an all around powerhouse of steep learning curve-ness. We both had no speech recognition background (at all), a little programming and applied machine learning (boosted decision trees and NN), and a lot of data juggling/system hacking experience. We found ourselves building the world\u2019s first deep learning based speech search engine. To get moving we needed a DNN that could understand speech. Here\u2019s the basic problem.</p>
<h2 id="turn-this-input-audio">Turn this input audio \u2B07\u2B07\u2B07</h2>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661725768/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-03-at-8.56.51-PM.png" alt="missing"></p>
<div style="text-align: center;">A spectrogram of an ordinary squishy human saying \u201CI am a human saying human things.\u201D</div>
<p>Listen to the audio.</p>
<iframe src="https://www.youtube.com/embed/TOZVpWL3ZGA" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen" />
<h2 id="into-this-text">Into this text \u2B07\u2B07\u2B07</h2>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661725769/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-02-at-12.51.52-PM.png" alt="missing"></p>
<div style="text-align: center; styl;margin-top: 0px;">The prediction from a DNN that just heard the \u201CI am a human saying human things\u201D audio file.</div>
<h1 id="why-we-did-it">Why we did it</h1>
<p>We\u2019ll probably write a \u201CThis is Deepgram\u201D post sometime, but suffice to say: we are building a Google for Audio and <strong>we needed a deep learning model for speech recognition</strong> to accomplish that goal. Good thing Baidu had just released the first of the Deepspeech papers two years ago when we were starting [[1]](<a href="https://arxiv.org/abs/1412.5567">https://arxiv.org/abs/1412.5567</a>). This gave us the push we needed to figure out how deep learning can work for audio search. Here\u2019s a picture of the Deepspeech RNN for inspiration.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661725769/blog/how-to-train-baidus-deepspeech-model-with-kur/NVIDIA_Baidu_Deep_Speech_Neural_Network_600-1.jpg" alt=""></p>
<p><em>Baidu\u2019s Andrew Ng at NVidia\u2019s GTC conference talking about Deepspeech</em></p>
<p>Deep Learning is hard. There are a few reasons why that is but some obvious ones are that models and frameworks involving differentiable tensor graphs are complicated, the hardware to run them efficiently is finicky (GPUs), and it\u2019s difficult to get data. Getting a working Deepspeech model is pretty hard too, even with a paper outlining it. <strong>The first step was to build an end-to-end deep learning speech recognition system.</strong> We started working on that and based the DNN on the Baidu Deepspeech paper. After a lot of toil, we put together a genuinely good end-to-end DNN speech recognition model. We\u2019ve <a href="https://kur.deepgram.com/">open sourced</a> the Deepspeech model in the <a href="https://kur.deepgram.com">Kur</a> framework running on <a href="https://www.github.com/tensorflow/tensorflow">TensorFlow</a>.</p>
<blockquote>
<p><em>Quick Aside:</em> We had to build Kur for Deepgram\u2019s survival. It\u2019s the wild west out here in A.I. and it\u2019s not possible to quickly build cutting edge models unless you have a simple way to do it.</p>
</blockquote>
<p>We find that Kur lets you <em>describe your model</em> and then <em>it works</em> without having to do a lot of the plumbing that slows projects down. The <a href="https://github.com/deepgram/kur">Kur</a> software package was <a href="https://techcrunch.com/2017/01/18/deepgram-open-sources-kur-to-make-diy-deep-learning-less-painful/">just released</a>. <em>It\u2019s free. It\u2019s open source. It\u2019s named after <a href="https://en.wikipedia.org/wiki/Kur">the first mythical dragon</a>.</em> Kur was crafted by the whole Deepgram A.I. team and we hope it helps the deep learning community in some small way.</p>
<h4 id="to-get-this-working-download-and-install-kur">To get this working, download and install <a href="https://github.com/deepgram/kur">Kur</a></h4>
<p>To install, all you really need to do is run <code is:raw>$ pip install kur</code> in your terminal if you have python 3.4 or above installed. If you need guidance or want easy environments to work in, we have an entire installation page at <a href="https://kur.deepgram.com">kur.deepgram.com</a>.</p>
<h4 id="run-the-deepspeech-example">Run the Deepspeech Example</h4>
<p>Once Kur is installed, fire up your fingers and run <code is:raw>$ kur -v train speech.yml</code> from your <code is:raw>kur/examples/</code> directory. You can omit the <code is:raw>-v</code> if you want Kur to quietly train without outlining what it\u2019s up to in your terminal\u2019s standard out. We find that running with <code is:raw>-v</code> the first few times gives you an idea of how Kur works, however. Turn on <code is:raw>-vv</code> if you\u2019re really craving gory details.</p>
<h4 id="your-model-will-start-training">Your model will start training</h4>
<p>At first, the outputs will be gibberish. But they get better :)</p>
<p>Hour 1:</p>
<p><strong>True transcript:</strong> <code is:raw>these vast buildings what were they</code></p>
<p><strong>DNN prediction:</strong> <code is:raw>he s ma tol ln wt r hett jzxzjxzjqzjqjxzq</code></p>
<p>Hour 6:</p>
<p><strong>True transcript:</strong> <code is:raw>the valkyrie kept off the coast steering to the westward</code></p>
<p><strong>DNN prediction:</strong> <code is:raw>the bak gerly cap dof the cost stkuarinte the west werd</code></p>
<p>Hour 24:</p>
<p><strong>True transcript:</strong> <code is:raw>it was a theatre ready made</code></p>
<p><strong>DNN prediction:</strong> <code is:raw>it was it theater readi made</code></p>
<p><em>Real English is spilling out.</em> I trained for 48 hours in total then ran the <em>\u201Ci am a human saying human things\u201D</em> audio file through the network.</p>
<p>Listen to the audio.</p>
<iframe src="https://www.youtube.com/embed/TOZVpWL3ZGA" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen" />
<p>Hour 48:</p>
<p><strong>True transcript:</strong> <code is:raw>i am a human saying human things</code></p>
<p><strong>DNN prediction:</strong> <code is:raw>i am a human saying human things</code></p>
<p>It\u2019s just two days old and didn\u2019t make a single mistake on that utterance. <strong>Our Speech A.I. is doing pretty well.</strong></p>
<h5 id="training-and-validation-loss-of-kur-deepspeech-model">Training and Validation Loss of Kur Deepspeech Model</h5>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661725770/blog/how-to-train-baidus-deepspeech-model-with-kur/loss-kur-up.png" alt="missing"></p>
<div style="text-align: center;">Loss as a function of batch for both training and validation data in the <a href="http://github.com/deepgram/kur">Kur</a> \u2018speech.yml\u2019 example. The validation data seems a little easier.</div>
<h4 id="theres-a-lot-of-things-to-try">There\u2019s a lot of things to try</h4>
<p>We abstracted away some of the time consuming bits. A little help comes from the descriptive nature of Kur, too. You can write down what you mean.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661725771/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-02-at-10.15.51-AM.png" alt="missing"></p>
<div style="text-align: center;">Hyperparameters for Deepspeech in the example Kurfile</div>
<p>These are the handful of hyperparameters needed to construct the DNN. There\u2019s a single one dimensional CNN that operates on a time slice of FFT outputs. Then there\u2019s an RNN stack which is 3 layers deep and 1000 nodes wide each. The vocab size is how many \u2018letters\u2019 we\u2019ll be choosing from (<code is:raw>a</code> to <code is:raw>z</code>, a space and an apostrophe <code is:raw>'</code>-that\u2019s 28 total). The hyperparameters are grabbed in the model section of the Kurfile (that\u2019s the <code is:raw>speech.yml</code>). The CNN layer is built like this.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661725772/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-02-at-10.20.24-AM.png" alt="missing"></p>
<div style="text-align: center;">The CNN layer specification</div>
<p>This puts in a single <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#Convolutional_layer">CNN layer</a> with a few sensible hyperparameters and slaps on a <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">Rectified Linear Unit</a> activation layer. <em>Note: The hyperparameters are filled in using the <a href="http://jinja.pocoo.org/docs/2.9/">Jinja2</a> templating engine.</em></p>
<blockquote>
<p>You can read more about defining Kurfiles in the docs at <a href="https://kur.deepgram.com/">kur.deepgram.com</a>.</p>
</blockquote>
<p>The stack of <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">RNN layers</a> is built with a <code is:raw>for</code> loop that stamps out three layers in a row-<em>three</em> because of the <code is:raw>depth</code> hyperparameter.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661725772/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-09-at-5.05.32-PM.png" alt="missing"></p>
<div style="text-align: center;">The RNN stack specification</div>
<p>The batch normalization layer uses a technique to keep layer weights distributed in a non-crazy way, speeding up training. The rnn <code is:raw>sequence</code> hyperparameter just means you want the full sequence of outputs passed along (every single time slice of the audio) while generating a guess for the transcript. <em>Quick Summary:</em> The CNN layer ingests the FFT input then connects to RNNs and eventually to a fully connected layer which predicts from 28 letters. That\u2019s Deepspeech.</p>
<h2 id="overview-of-how-it-works">Overview of How it Works</h2>
<p>When training modern speech DNNs you generally slice up the audio into ~20 millisecond chunks, do something like a fast fourier transform (FFT) on the chunk of audio, feed those chunked FFTs into the DNN sequentially, and generate a prediction for the current chunk. You do that until you\u2019ve digested the whole file (while remembering your chunk predictions the whole way) and end up with a sequence.</p>
<h4 id="this-is-how-deepspeech-works-in-kur">This is how Deepspeech works in Kur</h4>
<p>Kur takes in normal <code is:raw>wav</code> audio files. Then it grabs the spectrogram (FFT over time) of the file and jams it into a DNN with a CNN layer and a stack of three RNN layers. Out pops probabilities of latin characters, which (when read by a human) form words. As the model trains, there will be validation steps that give you an updated prediction on a random test audio file. You\u2019ll see the true text listed next to each prediction. You can watch the predicted text outputs get better as the network trains. <em>It\u2019s learning.</em> At first it will learn about spaces (ya know, this ), then it\u2019ll figure out good ratios for vowels and consonants, then it\u2019ll learn common easy words like <code is:raw>the</code>, <code is:raw>it</code>, <code is:raw>a</code>, <code is:raw>good</code> and build up it\u2019s vocabulary from there. It\u2019s fascinating to watch.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661725773/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-02-at-12.21.53-PM.png" alt="missing"></p>
<div style="text-align: center;">Take in time slices of audio frequencies and infer the letters that are being spoken. Time goes to the right. Image by Baidu.</div>
<h2 id="tell-us-what-you-think">Tell us what you think</h2>
<p>At Deepgram, we\u2019re really open about what we\u2019re working on. We know that A.I. is going to be huuuge and there are not enough trained people or good tools in the world to help it along \u2026 yet. We hope <a href="http://kur.deepgram.com">Kur</a>, <a href="http://www.kurhub.com">KurHub</a>, our upcoming <a href="http://www.deeplearninghackathon.com">Deep Learning Hackathon</a>, and blog posts like this help out the community, gets people excited, and shows that the good stuff can now be used by everyone. We\u2019re a startup and our research team can only produce so much helpful material per unit time. The best way to help us is to implement your <a href="https://github.com/terryum/awesome-deep-learning-papers">favorite Deep Learning papers</a> in <a href="https://www.github.com/deepgram/kur">Kur</a> and upload it to <a href="http://www.kurhub.com/">KurHub</a>. You can also contribute to the Kur framework directly on <a href="https://github.com/deepgram/kur">GitHub</a>. You\u2019ll be showered with thanks from us and a pile of others that are hungry for good implementations.</p>`;
}
const $$Astro = createAstro("/Users/sandrarodgers/web-next/blog/src/content/blog/posts/how-to-train-baidus-deepspeech-model-with-kur/index.md", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Index = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro, $$props, $$slots);
  Astro2.self = $$Index;
  new Slugger();
  return renderTemplate`<head>${renderHead($$result)}</head><h2 id="you-want-to-train-a-deep-neural-network-for-speech-recognition">You want to train a Deep Neural Network for Speech Recognition?</h2>
<p>Me too. It was two years ago and I was a particle physicist finishing a PhD at University of Michigan. I could code a little in C/C++ and Python and I knew Noah Shutty. Noah’s my cofounder at <a href="https://deepgram.com/">Deepgram</a> and an all around powerhouse of steep learning curve-ness. We both had no speech recognition background (at all), a little programming and applied machine learning (boosted decision trees and NN), and a lot of data juggling/system hacking experience. We found ourselves building the world’s first deep learning based speech search engine. To get moving we needed a DNN that could understand speech. Here’s the basic problem.</p>
<h2 id="turn-this-input-audio">Turn this input audio ⬇⬇⬇</h2>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661725768/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-03-at-8.56.51-PM.png" alt="missing"></p>
<div style="text-align: center;">A spectrogram of an ordinary squishy human saying “I am a human saying human things.”</div>
<p>Listen to the audio.</p>
<iframe src="https://www.youtube.com/embed/TOZVpWL3ZGA" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe>
<h2 id="into-this-text">Into this text ⬇⬇⬇</h2>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661725769/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-02-at-12.51.52-PM.png" alt="missing"></p>
<div style="text-align: center; styl;margin-top: 0px;">The prediction from a DNN that just heard the “I am a human saying human things” audio file.</div>
<h1 id="why-we-did-it">Why we did it</h1>
<p>We’ll probably write a “This is Deepgram” post sometime, but suffice to say: we are building a Google for Audio and <strong>we needed a deep learning model for speech recognition</strong> to accomplish that goal. Good thing Baidu had just released the first of the Deepspeech papers two years ago when we were starting [[1]](<a href="https://arxiv.org/abs/1412.5567">https://arxiv.org/abs/1412.5567</a>). This gave us the push we needed to figure out how deep learning can work for audio search. Here’s a picture of the Deepspeech RNN for inspiration.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661725769/blog/how-to-train-baidus-deepspeech-model-with-kur/NVIDIA_Baidu_Deep_Speech_Neural_Network_600-1.jpg" alt=""></p>
<p><em>Baidu’s Andrew Ng at NVidia’s GTC conference talking about Deepspeech</em></p>
<p>Deep Learning is hard. There are a few reasons why that is but some obvious ones are that models and frameworks involving differentiable tensor graphs are complicated, the hardware to run them efficiently is finicky (GPUs), and it’s difficult to get data. Getting a working Deepspeech model is pretty hard too, even with a paper outlining it. <strong>The first step was to build an end-to-end deep learning speech recognition system.</strong> We started working on that and based the DNN on the Baidu Deepspeech paper. After a lot of toil, we put together a genuinely good end-to-end DNN speech recognition model. We’ve <a href="https://kur.deepgram.com/">open sourced</a> the Deepspeech model in the <a href="https://kur.deepgram.com">Kur</a> framework running on <a href="https://www.github.com/tensorflow/tensorflow">TensorFlow</a>.</p>
<blockquote>
<p><em>Quick Aside:</em> We had to build Kur for Deepgram’s survival. It’s the wild west out here in A.I. and it’s not possible to quickly build cutting edge models unless you have a simple way to do it.</p>
</blockquote>
<p>We find that Kur lets you <em>describe your model</em> and then <em>it works</em> without having to do a lot of the plumbing that slows projects down. The <a href="https://github.com/deepgram/kur">Kur</a> software package was <a href="https://techcrunch.com/2017/01/18/deepgram-open-sources-kur-to-make-diy-deep-learning-less-painful/">just released</a>. <em>It’s free. It’s open source. It’s named after <a href="https://en.wikipedia.org/wiki/Kur">the first mythical dragon</a>.</em> Kur was crafted by the whole Deepgram A.I. team and we hope it helps the deep learning community in some small way.</p>
<h4 id="to-get-this-working-download-and-install-kur">To get this working, download and install <a href="https://github.com/deepgram/kur">Kur</a></h4>
<p>To install, all you really need to do is run <code>$ pip install kur</code> in your terminal if you have python 3.4 or above installed. If you need guidance or want easy environments to work in, we have an entire installation page at <a href="https://kur.deepgram.com">kur.deepgram.com</a>.</p>
<h4 id="run-the-deepspeech-example">Run the Deepspeech Example</h4>
<p>Once Kur is installed, fire up your fingers and run <code>$ kur -v train speech.yml</code> from your <code>kur/examples/</code> directory. You can omit the <code>-v</code> if you want Kur to quietly train without outlining what it’s up to in your terminal’s standard out. We find that running with <code>-v</code> the first few times gives you an idea of how Kur works, however. Turn on <code>-vv</code> if you’re really craving gory details.</p>
<h4 id="your-model-will-start-training">Your model will start training</h4>
<p>At first, the outputs will be gibberish. But they get better :)</p>
<p>Hour 1:</p>
<p><strong>True transcript:</strong> <code>these vast buildings what were they</code></p>
<p><strong>DNN prediction:</strong> <code>he s ma tol ln wt r hett jzxzjxzjqzjqjxzq</code></p>
<p>Hour 6:</p>
<p><strong>True transcript:</strong> <code>the valkyrie kept off the coast steering to the westward</code></p>
<p><strong>DNN prediction:</strong> <code>the bak gerly cap dof the cost stkuarinte the west werd</code></p>
<p>Hour 24:</p>
<p><strong>True transcript:</strong> <code>it was a theatre ready made</code></p>
<p><strong>DNN prediction:</strong> <code>it was it theater readi made</code></p>
<p><em>Real English is spilling out.</em> I trained for 48 hours in total then ran the <em>“i am a human saying human things”</em> audio file through the network.</p>
<p>Listen to the audio.</p>
<iframe src="https://www.youtube.com/embed/TOZVpWL3ZGA" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe>
<p>Hour 48:</p>
<p><strong>True transcript:</strong> <code>i am a human saying human things</code></p>
<p><strong>DNN prediction:</strong> <code>i am a human saying human things</code></p>
<p>It’s just two days old and didn’t make a single mistake on that utterance. <strong>Our Speech A.I. is doing pretty well.</strong></p>
<h5 id="training-and-validation-loss-of-kur-deepspeech-model">Training and Validation Loss of Kur Deepspeech Model</h5>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661725770/blog/how-to-train-baidus-deepspeech-model-with-kur/loss-kur-up.png" alt="missing"></p>
<div style="text-align: center;">Loss as a function of batch for both training and validation data in the <a href="http://github.com/deepgram/kur">Kur</a> ‘speech.yml’ example. The validation data seems a little easier.</div>
<h4 id="theres-a-lot-of-things-to-try">There’s a lot of things to try</h4>
<p>We abstracted away some of the time consuming bits. A little help comes from the descriptive nature of Kur, too. You can write down what you mean.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661725771/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-02-at-10.15.51-AM.png" alt="missing"></p>
<div style="text-align: center;">Hyperparameters for Deepspeech in the example Kurfile</div>
<p>These are the handful of hyperparameters needed to construct the DNN. There’s a single one dimensional CNN that operates on a time slice of FFT outputs. Then there’s an RNN stack which is 3 layers deep and 1000 nodes wide each. The vocab size is how many ‘letters’ we’ll be choosing from (<code>a</code> to <code>z</code>, a space and an apostrophe <code>'</code>-that’s 28 total). The hyperparameters are grabbed in the model section of the Kurfile (that’s the <code>speech.yml</code>). The CNN layer is built like this.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661725772/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-02-at-10.20.24-AM.png" alt="missing"></p>
<div style="text-align: center;">The CNN layer specification</div>
<p>This puts in a single <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#Convolutional_layer">CNN layer</a> with a few sensible hyperparameters and slaps on a <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">Rectified Linear Unit</a> activation layer. <em>Note: The hyperparameters are filled in using the <a href="http://jinja.pocoo.org/docs/2.9/">Jinja2</a> templating engine.</em></p>
<blockquote>
<p>You can read more about defining Kurfiles in the docs at <a href="https://kur.deepgram.com/">kur.deepgram.com</a>.</p>
</blockquote>
<p>The stack of <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">RNN layers</a> is built with a <code>for</code> loop that stamps out three layers in a row-<em>three</em> because of the <code>depth</code> hyperparameter.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661725772/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-09-at-5.05.32-PM.png" alt="missing"></p>
<div style="text-align: center;">The RNN stack specification</div>
<p>The batch normalization layer uses a technique to keep layer weights distributed in a non-crazy way, speeding up training. The rnn <code>sequence</code> hyperparameter just means you want the full sequence of outputs passed along (every single time slice of the audio) while generating a guess for the transcript. <em>Quick Summary:</em> The CNN layer ingests the FFT input then connects to RNNs and eventually to a fully connected layer which predicts from 28 letters. That’s Deepspeech.</p>
<h2 id="overview-of-how-it-works">Overview of How it Works</h2>
<p>When training modern speech DNNs you generally slice up the audio into ~20 millisecond chunks, do something like a fast fourier transform (FFT) on the chunk of audio, feed those chunked FFTs into the DNN sequentially, and generate a prediction for the current chunk. You do that until you’ve digested the whole file (while remembering your chunk predictions the whole way) and end up with a sequence.</p>
<h4 id="this-is-how-deepspeech-works-in-kur">This is how Deepspeech works in Kur</h4>
<p>Kur takes in normal <code>wav</code> audio files. Then it grabs the spectrogram (FFT over time) of the file and jams it into a DNN with a CNN layer and a stack of three RNN layers. Out pops probabilities of latin characters, which (when read by a human) form words. As the model trains, there will be validation steps that give you an updated prediction on a random test audio file. You’ll see the true text listed next to each prediction. You can watch the predicted text outputs get better as the network trains. <em>It’s learning.</em> At first it will learn about spaces (ya know, this ), then it’ll figure out good ratios for vowels and consonants, then it’ll learn common easy words like <code>the</code>, <code>it</code>, <code>a</code>, <code>good</code> and build up it’s vocabulary from there. It’s fascinating to watch.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661725773/blog/how-to-train-baidus-deepspeech-model-with-kur/Screen-Shot-2017-02-02-at-12.21.53-PM.png" alt="missing"></p>
<div style="text-align: center;">Take in time slices of audio frequencies and infer the letters that are being spoken. Time goes to the right. Image by Baidu.</div>
<h2 id="tell-us-what-you-think">Tell us what you think</h2>
<p>At Deepgram, we’re really open about what we’re working on. We know that A.I. is going to be huuuge and there are not enough trained people or good tools in the world to help it along … yet. We hope <a href="http://kur.deepgram.com">Kur</a>, <a href="http://www.kurhub.com">KurHub</a>, our upcoming <a href="http://www.deeplearninghackathon.com">Deep Learning Hackathon</a>, and blog posts like this help out the community, gets people excited, and shows that the good stuff can now be used by everyone. We’re a startup and our research team can only produce so much helpful material per unit time. The best way to help us is to implement your <a href="https://github.com/terryum/awesome-deep-learning-papers">favorite Deep Learning papers</a> in <a href="https://www.github.com/deepgram/kur">Kur</a> and upload it to <a href="http://www.kurhub.com/">KurHub</a>. You can also contribute to the Kur framework directly on <a href="https://github.com/deepgram/kur">GitHub</a>. You’ll be showered with thanks from us and a pile of others that are hungry for good implementations.</p>`;
});

export { compiledContent, $$Index as default, frontmatter, metadata, rawContent };
