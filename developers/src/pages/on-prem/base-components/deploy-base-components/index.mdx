---
layout: ../../../../layouts/OnPrem.astro
title: Deploying Base Components
description: Learn how to configure your environment for on-premises deployments of Deepgram products.
order: 5
seo:
  metaTitle: Deploying Base Components
  metaDescription: Learn how to configure your environment for on-premises deployments of Deepgram products.
  keywords: on-premises, on-prem, deployment, configuration, configure, base components, base
---
import Alert from '../../../../shared/components/global/Alert.astro'; 

# {frontmatter.title}

Cloud-hosted on-premises (on-prem) deployments are the most common type of deployment performed by customers who want to leverage Deepgram within their own infrastructure. Specific configurations vary, but the Cloud Service Providers that are typically used include Amazon Web Services (AWS), Azure, and Google Cloud Platform (GCP).

## Prerequisites

Before you begin, you will need a Deepgram Account Representative. To reach one, [contact us](https://deepgram.com/contact-us/)!

Your Deepgram Account Representative will guide you through the process of setting up:

- a Deepgram product contract
- a [Deepgram Console account](https://console.deepgram.com/), so we can connect your license to your projects
- an account on [Docker Hub](https://hub.docker.com/), so you can conveniently get the latest Deepgram software and updates.

Ahead of your planned on-prem deployment, your Deepgram Account Representative will need:

- your Deepgram Console account email address
- your Deepgram Console Project ID
- your Docker Hub Account ID

Providing this information will allow Deepgram to supply:

- download links to your customized configuration files, which will include your license ID
- download links to required models, including at least one pre-trained AI model for testing purposes

## Deploying on AWS (Ubuntu Server 20.04 LTS)

This guide details steps to set up a deployment to the cloud on an AWS instance running Ubuntu Linux. Deploying Deepgram on AWS requires some preparation. In this section, you will learn how to expose and access your application in a secure and stable manner. You will need to perform some of these steps in the AWS Management Console and some in your local terminal.

### Accessing Your Cloud Environment

AWS uses public-key cryptography to secure login information for your instance. A Linux instance has no password; you use a key pair to log in to your instance securely.

#### Create an Amazon EC2 Key Pair

If you don’t already have an Amazon EC2 key pair, you will need to create one in order to access the AWS EC2 Virtual Machine. To learn how, read [Create a key pair using Amazon EC2](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair) in Amazon’s documentation.

At the end of this process, your browser should download a `private-key.pem` file for your key pair. Move this file to a secure and memorable location.

#### Create an Amazon EC2 Instance

​To begin your on-prem installation, you must create an Amazon EC2 instance.

1. Navigate to the EC2 Dashboard and confirm that the proper AWS Region is configured, then choose **Launch Instance** to open the wizard.

2. For the **Choose an Amazon Machine Image (AMI)** wizard step, choose a basic configuration to serve as a template for your instance:

   <ol type="a">
   	<li>
   		Search for <strong>Ubuntu Server 20.04 LTS (HVM), SSD Volume Type</strong>.
   	</li>
   	<li>
   		Ensure the <strong>64-bit (x86)</strong> option is selected.
   	</li>
   	<li>
   		Choose <strong>Select</strong>.
   	</li>
   </ol>

3. For the **Choose an Instance Type** wizard step, select the hardware configuration of your instance:

   <ol type="a">
   	<li>
   		Filter by the <code>g4dn</code> family type.
   	</li>
   	<li>
   		Select the <strong>g4dn.2xlarge</strong> instance.
   	</li>
   	<li>
   		Select <strong>Next: Configure Instance Details</strong>.
   	</li>
   </ol>

   <Alert type="warning">

   Avoid selecting **Review and Launch**. If you do so, the wizard will complete the other configuration settings for you and will miss some important settings.

   </Alert>

4. For the **Configure Instance Details** wizard step, review the default instance detail selections, then select **Next: Add Storage**.

5. For the **Add Storage** wizard step, increase the size of the **Root** volume to 32 GB, and then select **Next: Add Tags**.

6. For the **Add Tags** wizard step, select “Add Tag”:

   <ol type="a">
   	<li>
   		For the key, type <code>Name</code>.
   	</li>
   	<li>
   		For the value, type <code>On-Premises Base</code> .
   	</li>
   	<li>
   		Select <strong>Next: Configure Security Group</strong>.
   	</li>
   </ol>

7. For the **Configure Security Group** wizard step, select **Create a new security group**.

   <ol type="a">
		<li>For the description, type <code>On-Premises Base Security Group</code>.</li>
		<li>Ensure the first rule (of type <strong>SSH</strong>) is listed, and provide the description <code>SSH for Administration</code>.</li>
		<Alert type="warning">
   			Open SSH to only necessary IP addresses.
    	</Alert>
		<li>Select <strong>Add Rule</strong>, then select <strong>HTTPS</strong> from the dropdown, and provide the description <code>HTTPS for Deepgram API</code>.</li>
		<li>Select <strong>Review and Launch</strong>.</li>
	</ol>

8. For the **Review Instance Launch** wizard step, ensure all of the instance details match what you entered in the previous steps, and then select **Launch**.

9. In the **Select an existing key pair or create a new key pair** modal, ensure that **choose an existing key pair** is selected, then choose the key pair you created in the [Create an Amazon EC2 Key Pair](#create-an-amazon-ec2-key-pair) section. Check the acknowledgement box, and select **Launch Instances**.

10. Once the instance successfully launches, review its details in the EC2 instances list, and make note of its Public IPv4 DNS entry.

#### Log in to the AWS EC2 instance

To complete the rest of the installation, including configuring your environment and transferring files between your local computer and your AWS instance, you must connect to the AWS EC2 instance that you launched.

1. Open the terminal application for your computer.

2. Connect to your AWS instance:

   ```bash
   ssh -i private-key.pem ubuntu@AWS_HOSTNAME
   ```

   <Alert type="warning">

   Be sure to replace the AWS_HOSTNAME placeholder value with the hostname for your instance. Also check that the path to your `private-key.pem` file is correct.

   </Alert>

   <Alert type="info">

   For AWS Ubuntu instances, the default username is `ubuntu`, not `user` as their documentation would suggest.

   </Alert>

3. If you receive a message that indicates that the authenticity of the host can’t be established, type `yes`, then press the Enter key on your keyboard.

### Configuring Your Cloud Environment

Once you have successfully logged in to your instance, you must remove incompatible kernel drivers and install all of the necessary utilities to run GPU-accelerated containers in Docker.

#### Remove Nouveau Drivers

The Nouveau kernel driver is incompatible with NVIDIA drivers, so you will need to disable it before installing any NVIDIA drivers.

1. In your terminal, create a new configuration file:

   ```bash
   sudo touch /etc/modprobe.d/blacklist-nouveau.conf
   ```

2. Using sudo and the text editor of your choice, add the following lines to the `/etc/modprobe.d/blacklist-nouveau.conf` configuration file:

   ```bash
   blacklist nouveau
   options nouveau modeset=0
   ```

3. Regenerate the kernel with the new conf file added:

   ```bash
   sudo update-initramfs -u
   ```

4. Reboot the instance:

   ```bash
   sudo reboot
   ```

5. Once the instance has finished rebooting, reconnect via SSH and verify that Nouveau has been removed:

   ```bash
   lsmod | grep nouveau
   ```

   If you see no output, Nouveau was successfully removed.

#### Update Ubuntu’s Aptitude Package Manager

Update your AWS instance’s operating system package manager to get information on updated versions of packages or their dependencies:

```bash
sudo apt update
```

#### Install GNU Toolchain Components

Install the GNU Compiler Collection (`gcc`) and GNU Make (`make`) tool:

```bash
sudo apt install -y gcc make
```

#### Install Docker Engine

For ease of use, Deepgram provides its products in Docker containers, so you must make sure that you have installed the latest version of Docker Engine on all hosts.

1. Install the Docker Engine. To learn how, read [Install Using the Repository](https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository) in Docker’s [Install Docker Engine on Ubuntu](https://docs.docker.com/engine/install/ubuntu) documentation.

2. Ensure that your AWS instance user (`ubuntu`) has sufficient permissions to communicate with the Docker daemon service that runs on your host operating system. To learn how to run Docker commands without elevated privileges, read [Manage Docker as a Non-Root User](https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user) in Docker’s optional [Post-installation steps for Linux](https://docs.docker.com/engine/install/linux-postinstall/) documentation.

#### Cache Docker Credentials

Once you are satisfied that Docker is installed and configured correctly, cache your credentials locally by logging in to Docker Hub using your Docker credentials. Once your credentials are cached locally, you should not have to log in again (until after you log out (`docker logout`)).

<Alert type="warning">

We recommend that you use an access token when logging in to Docker Hub. Access tokens are used like passwords, but limit administrative access to your account. To learn how to create and use access tokens with Docker, read Docker’s documentation on [Access Tokens](https://docs.docker.com/docker-hub/access-tokens/).

</Alert>

#### Install Docker-Compose

Ensure that [Docker Compose](https://docs.docker.com/compose/install/) is installed.

1. Download Docker Compose, and send the output to your `/usr/local/bin` directory:

   ```bash
   sudo curl -L "https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
   ```

2. Modify the permissions for the bin file to allow your user to execute it:

   ```bash
   sudo chmod +x /usr/local/bin/docker-compose
   ```

3. Test the installation:

   ```bash
   docker-compose --version
   ```

   You should expect the command output to return version 1.29.2.

#### Install NVIDIA Drivers

Install the latest available NVIDIA drivers for the `g4dn` instance which has the Tesla T4 GPU:

1. Identify the latest driver for the GPU you are using and retrieve its download URL:

   <ol type="a">
   	<li>
   		Go to <a href="https://www.nvidia.com/download/index.aspx">NVIDIA Official Drivers</a>.
   	</li>
   	<li>Select the correct product and a corresponding CUDA toolkit 11 or greater.</li>
   	<li>
   		Select <strong>Search</strong> and check that the correct driver is displayed, then select <strong>Download</strong>.
   	</li>
   	<li>
   		Right-click <strong>Agree & Download</strong>, then copy the link to save the download URL to your clipboard.
   	</li>
   </ol>

2. Download the latest driver for your GPU:

   ```bash
   # Example: wget  https://us.download.nvidia.com/tesla/510.47.03/NVIDIA-Linux-x86_64-510.47.03.run
   wget LINK_TO_LATEST_NVIDIA_GPU_DRIVER
   ```

   <Alert type="warning">

   Be sure to replace the LINK_TO_LATEST_NVIDIA_GPU_DRIVER placeholder value with the URL to the latest driver for the GPU you are using.

   </Alert>

3. Change the permissions on the downloaded file to allow your user to execute it:

   ```bash
   # Example: chmod +x NVIDIA-Linux-x86_64-510.47.03.run
   chmod DOWNLOADED_FILE_NAME
   ```

4. Decompress and execute the driver file to install it:

   ```bash
   # Example: sudo ./NVIDIA-Linux-x86_64-510.47.03.run --silent
   sudo DOWNLOADED_FILE_NAME --silent
   ```

   With the `--silent` install, you will see warnings that are similar to the following (they can be ignored):

   ```bash
   WARNING: Ignoring CC version mismatch:

   The kernel was built with gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0, GNU ld (GNU Binutils for Ubuntu) 2.34, but the current compiler version is cc (Ubuntu 9.4.0-1ubuntu1~20.04) 9.4.0.

   WARNING: nvidia-installer was forced to guess the X library path '/usr/lib64' and X module path '/usr/lib64/xorg/modules'; these paths were not queryable from the system.  If X fails to find the NVIDIA X driver module, please install the `pkg-config` utility and the X.Org SDK/development package for your distribution and reinstall the driver.
   ```

5. Test that the NVIDIA drivers are installed:

   ```bash
   nvidia-smi
   ```

#### Install the NVIDIA-Docker Runtime

CUDA is NVIDIA's library for interacting with its GPU. CUDA support is made available to Docker containers using [`nvidia-docker`](https://github.com/NVIDIA/nvidia-docker), NVIDIA's custom runtime for Docker.

1. Retrieve the NVIDIA runtime GPG key:

   ```bash
   distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \
   && curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
   && curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \
   sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
   sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
   ```

2. Update the Ubuntu’s package manager to get the latest NVIDIA Docker components:

   ```bash
   sudo apt update
   ```

3. Install the NVIDIA Docker runtime:

   ```bash
   sudo apt install -y nvidia-docker2
   ```

4. Restart the Docker service:

   ```bash
   sudo systemctl restart docker
   ```

5. Download the `nvidia-docker` container image for Ubuntu 20.04 LTS:

   ```bash
   docker pull nvidia/cuda:11.6.0-base-ubuntu20.04
   ```

6. Run a test of the NVIDIA Docker runtime:

   ```bash
   docker run --runtime=nvidia --rm --gpus all nvidia/cuda:11.6.0-base-ubuntu20.04 nvidia-smi
   ```

### Setting up the Deepgram Engine and API Containers

Now that your environment is set up, you can download and set up your Deepgram products. For ease of use, Deepgram provides its products in Docker containers.

#### Get Deepgram Products

1. Download the latest Deepgram Engine image from Docker:

   ```bash
   docker pull deepgram/onprem-engine:3.26.7
   ```

2. Download the latest Deepgram API image from Docker:

   ```bash
   docker pull deepgram/onprem-api:1.58.0
   ```

#### Import Your Docker Compose, Container Configuration, and Model Files

Before you can run your on-prem deployment, you must configure the base components. To do this, you will need to customize your configuration files and create a directory to house models that have been encrypted for use in your requests.

For your deployment, we provide models and configuration files to you via Amazon S3 buckets, so you can download directly to your AWS Ubuntu instance. If you don’t have customized configuration files, you can create configuration files using the examples included in [Customize Your Configuration](#customize-your-configuration).

1. Replace the default `docker-compose.yml` file in your root directory with the customized `docker-compose.yml` file provided by Deepgram:

   ```bash
   wget LINK_TO_YAML_FILE_PROVIDED_BY_DEEPGRAM
   ```

   <Alert type="warning">

   Be sure to replace the LINK_TO_YAML_FILE_PROVIDED_BY_DEEPGRAM placeholder value with the URL to the `docker-compose.yml` file provided by your Deepgram Account Representative.

   </Alert>

2. To house your configuration files, in your root directory, create a directory named `config`:

   ```bash
   mkdir config
   ```

   <Alert type="info">

   You can name this directory whatever you like, as long as you update the `docker-compose.yml` file accordingly.

   </Alert>

3. In your new directory, download each TOML configuration file using the links provided by Deepgram:

   ```bash
   cd config
   wget LINK_TO_TOML_CONFIGURATION_FILE_PROVIDED_BY_DEEPGRAM
   ```

   <Alert type="warning">

   Be sure to replace each LINK_TO_TOML_CONFIGURATION_FILE_PROVIDED_BY_DEEPGRAM placeholder value with the URL to the TOML configuration file provided by your Deepgram Account Representative.

   </Alert>

4. To house models that have been encrypted for use in your requests, in your root directory, create a directory named `models`:

   ```bash
   mkdir models
   ```

   <Alert type="info">

   You can name this directory whatever you like, as long as you update the `docker-compose.yml` and `engine.toml` files accordingly.

   </Alert>

5. In your new directory, download each model using the links provided by Deepgram:

   ```bash
   cd models
   wget LINK_TO_MODEL_PROVIDED_BY_DEEPGRAM
   ```

   <Alert type="warning">

   Be sure to replace each LINK_TO_MODEL_PROVIDED_BY_DEEPGRAM placeholder value with the URL to the model provided by your Deepgram Account Representative.

   </Alert>

#### Customize Your Configuration

Once your configuration files have been created in your AWS instance, you can update your configuration to customize it for your use case.

##### Configuration Files

<Alert type="warning">

Your Deepgram Account Representative should provide you with download links to customized configuration files. Unless further customized, the example files included in this section are basic and should be used only to spin up a standard proof of concept (POC) deployment or to test your system.

</Alert>

###### `docker-compose.yml`

The Docker-compose configuration file makes it possible to spin up the containers with Docker using a single command. This makes spinning up a standard POC deployment quick and easy.

```toml
version: "3.7"

services:

  # The Deepgram API service.
  api:
    image: deepgram/onprem-api:1.58.0

    # The API port to the host machine. The container port (right-hand side)
    # must match the port that the API service is listening on (this can be
    # found in its configuration file).
    ports:
      - "8080:8080"

    volumes:
      # The API configuration file needs to be accessible; this should point to
      # the file on the host machine.
      - "/path/to/api.toml:/api.toml:ro"

    # Invoke the API server, passing it the path (inside the container) to its
    # configuration file.
    command: -v serve /api.toml

  # The Deepgram Engine service.
  engine:
    image: deepgram/onprem-engine:3.26.7

    # Change the default runtime.
    runtime: nvidia

    # The Engine configuration file and models need to be accessible; these paths
    # should point to files/directories on the host machine.
    #
    # You can combine the configuration file and models into a single volume mount.
    volumes:
      # The Engine configuration file needs to be accessible.
      - "/path/to/engine.toml:/engine.toml:ro"

      # The models will also need to be available. The in-container paths must
      # match the paths specified in the Engine configuration file.
      - "/path/to/models:/models:ro"

    # Invoke the Engine service, passing it the path (inside the container) to
    # its configuration file.
    command: -v serve /engine.toml
```

###### `api.toml`

The API configuration file determines how the API container built from the Docker image will be run. It includes important settings for exposing API endpoints and for enabling features and post-processing plugins for requests.

```toml
# Keep in mind that all paths are in-container paths and do not need to exist
# on the host machine.

# Configure license validation.
[license]
  # Your license key.
  key = "LICENSE_KEY"

# Configure how the API will listen for your requests
[server]
  # The base URL (prefix) for requests to the API.
  base_url = "/v2"
  # The IP address to listen on. Since this is likely running in a Docker
  # container, you will probably want to listen on all interfaces.
  host = "0.0.0.0"
  # The port to listen on
  port = 8080

  # How long to wait for a connection to a callback URL (in seconds)
  callback_conn_timeout = 1
  # How long to wait for a response to a callback URL (in seconds)
  callback_timeout = 10

  # How long to wait for a connection to a fetch URL (in seconds)
  fetch_conn_timeout = 1
  # How long to wait for a response to a fetch URL (in seconds)
  fetch_timeout = 60

[features]
  endpointing_on_by_default = true

# Configure the DNS resolver, overriding the system default.
# Typically not needed, although we document it here for completeness.
# [resolver]
#   # List of nameservers to use to resolve DNS queries.
#   nameservers = ["127.0.0.11 53 udp"]
#   # Override the TTL in the DNS response (in seconds).
#   max_ttl = 10

# Configure the backend pool of speech engines (generically referred to as
# "drivers" here). There is only one pool type: "standard". The API will
# load-balance among drivers in the pool; if a driver fails, the next one
# will be tried.
#
# Each driver URL will have its hostname resolved to an IP address. If a domain
# name resolves to multiple IP addresses, the API will load-balance across each
# IP address.
#
# This behavior is provided for convenience, and in a production environment
# other tools can be used, such as HAProxy.

# A new Speech Engine ("driver") in the "standard" pool.
[[driver_pool.standard]]
  # Host to connect to.
  # If you are using Docker Swarm, then you should replace "engine" with
  # "tasks.engine", which is the Docker Swarm method for resolving the IP
  # addresses of all "engine" services. If you rename the "engine" service
  # in the Docker Compose  file, then change it accordingly here.
  # Additionally, the port and prefix should match those defined in the
  # Engine configuration file.
  # NOTE: This must be HTTPS.
  url = "https://engine:8080/v2"

  # How long to wait for a connection to be established (in seconds).
  conn_timeout = 5
  # Once a connection is established, how many seconds to wait for a response.
  timeout = 400
  # Factor to increase the timeout by for each additional retry (for
  # exponential backoff).
  timeout_backoff = 1.2

  # If you fail to get a valid response (timeout or unexpected error), then
  # how many attempts should be made in total, including the initial attempt?
  # This is applied *per IP address* that the domain name in the URL resolves
  # to. If your domain resolves to multiple IPs, then "1" may be enough.
  retries_per_ip = 1

  # Before attempting a retry, sleep for this long (in seconds)
  retry_sleep = 2
  # Factor to increase the retry sleep by for each additional retry (for
  # exponential backoff).
  retry_backoff = 1.6

  # Maximum response to deserialize from Driver (in bytes)
  max_response_size = 1073741824

# Additional speech engines ("drivers") can be defined here, either in the
# standard pool using [[driver_pool.standard]]
```

###### `engine.toml`

The Deepgram Engine configuration file determines how the Deepgram Engine container built from the Docker image will be run. It includes important file paths and settings related to models that you will be using.

```toml
# All paths are in-container paths and do not need to exist on the host machine.

# Configure license validation.
[license]
  # Your license key
  key = "LICENSE_KEY"

# Configure the server to listen for requests from the API.
[server]
  # The base URL (prefix) for requests from the API.
  base_url = "/v2"
  # The IP address to listen on. Because this is likely running in a Docker
  # container, you will probably want to listen on all interfaces.
  host = "0.0.0.0"
  # The port to listen on
  port = 8080

# Speech models. You can place these in one or multiple directories.
[model_manager]
 search_paths = ["/models"]

# Enable supporting models
# To disable any of these features, just remove to comment out the respective
# feature section.
[features]
 [features.g2p]
  path = "/models/g2p.tar"
 [[features.diarizer]]
  weights = "/models/diarizer.v5.tar"
  type = "v2"
  [features.diarizer.features]
    max_frequency = 4000
    type = "mfcc"
```

### Starting and Using Your Container

To make sure your Deepgram on-prem deployment is properly configured and running, you will want to start Docker, run the container, and make a sample request to Deepgram.

#### Start Docker

Now that you have your configuration files set up and in the correct location to be used by the container, use Docker-Compose to run the container:

```bash
docker-compose up -d
```

#### Test Your Deepgram Setup with a Sample Request

Test your environment and container setup with a local file:

```bash
curl -X POST --data-binary @[FILE_PATH] http://localhost:8080/v2/listen
```

Finally, test that you can view the model that was automatically loaded by Deepgram Engine. To do this, download the American English Harvard Sentences file from the [Open Speech Repository](https://www.voiptroubleshooter.com/open_speech/index.html) and run it through the model:

```bash
wget https://www.voiptroubleshooter.com/open_speech/american/OSR_us_000_0019_8k.wav

curl -X POST --data-binary @OSR_us_000_0019_8k.wav "http://localhost:8080/v2/listen?model=general-dQw4w9WgXcQ&version=2021-08-18"; echo
```

## Adding the License Proxy Server

For customers deploying Deepgram’s on-premises solution in production, Deepgram recommends the License Proxy Server, which is a caching proxy that communicates with the Deepgram-hosted license server to ensure uptime and simplify network security.

<Alert type="warning">

If you aren't certain which products your contract includes or if you are interested in adding the License Proxy Server to your on-premises deployment, please consult a Deepgram Account Representative. To reach one, [contact us](https://deepgram.com/contact-us/)!

</Alert>

There are many benefits of using the License Proxy Server:

- For production deployments, the License Proxy Server allows your deployed on-premises services to continue to run even if your deployment loses connectivity to the Deepgram license server.

- If network security requirements dictate that traffic over the web is allowed from only certain hosts, the License Proxy Server can be deployed statically while ASR services scale elastically.

- If your customers will deploy Deepgram software as part of your on-premises solution and their traffic must flow back to your environment, you may deploy the License Proxy Server to relay traffic on to the Deepgram license server.

To see an architecture overview or learn about monitoring the License Proxy Server, see [License Proxy Server](/on-prem/optional-components/license-proxy-server/).

### Installing

Deepgram makes all of its products available through Docker Hub.

1. Log in to your Docker Hub account from one of your servers.
2. Run the following command:

   ```bash
   $ docker pull deepgram/onprem-license-proxy
   ```

### Deploying

Before you can run your on-prem deployment with the License Proxy Server, you must configure the License Proxy Server. To do this, you will need to update your Docker Compose and container configuration files, then restart Docker.

#### Update Your Docker Compose File

Replace the default `docker-compose.yml` file in your root directory with the following:

```toml
version: "3.7"

services:

  # The Deepgram API service.
  api:
    image: deepgram/onprem-api:1.58.0

    # The API port to the host machine. The container port (right-hand side)
    # must match the port that the API service is listening on (this can be
    # found in its configuration file).
    ports:
      - "8080:8080"

    depends_on:
      - "license-proxy"

    volumes:
      # The API configuration file needs to be accessible; this should point to
      # the file on the host machine.
      - "/path/to/api.toml:/api.toml:ro"

    # Invoke the API server, passing it the path (inside the container) to its
    # configuration file.
    command: -v serve /api.toml

  # The Deepgram Engine service.
  engine:
    image: deepgram/onprem-engine:3.26.7

    # Change the default runtime.
    runtime: nvidia

    depends_on:
      - "license-proxy"

    # The Engine configuration file and models need to be accessible; these paths
    # should point to files/directories on the host machine.
    #
    # You can combine the configuration file and models into a single volume mount.
    volumes:
      # The Engine configuration file needs to be accessible.
      - "/path/to/engine.toml:/engine.toml:ro"

      # The models will also need to be available. The in-container paths must
      # match the paths specified in the Engine configuration file.
      - "/path/to/models:/models:ro"

    # Invoke the Engine service, passing it the path (inside the container) to
    # its configuration file.
    command: -v serve /engine.toml

  license-proxy:
    image: deepgram/onprem-license-proxy:1.0.0-1

    ports:
      - "8443:8443"
      - "8089:8089"

    command: -v serve --license-key "YOUR_ONPREM_LICENSE_KEY" --host 0.0.0.0 --port 8443 --status-port 8089
```

#### Update Your Container Configuration Files

In your `api.toml` and `engine.toml` files, add a line that specifies the URL to your deployed proxy immediately below your license key configuration:

```toml
[license]
key = "YOUR_LICENSE_KEY"
server_url = "https://license-proxy:8443"
```

#### Restart Docker

Restart Docker to begin directing licensing requests through the proxy:

```bash
docker-compose up -d
```

After restarting, test that an ASR request is processed as expected.

## Deploying with Kubernetes

You can deploy a local Kubernetes cluster using Deepgram, which will provide an instance of Deepgram's API and Engine services running locally.

## Before You Begin

In addition to the information listed in the [Prerequisites](#prerequisites) section of this document, you'll need a few relevant Kubernetes deployment files. To get these, please [contact Support](/support/).

- Kubectl is the command line tool for interacting with a cluster. It can be installed using steps from Kubernetes's [Download Kubernetes](https://kubernetes.io/releases/download/) documentation.
- Minkube is a tool for quickly creating a Kubernetes cluster. It can be installed using steps from minikube's [minikube start](https://minikube.sigs.k8s.io/docs/start/) documentation.

## Terminology

If you are not overly familiar with Kubernetes, you should be aware of three main concepts:

- Node: A physical computer or virtual machine.
- Pod: A single container running on a node. One node can have many pods.
- Cluster: A group of nodes and their pods.

## Creating a Cluster

1. Use minkube to create a cluster and validate that it is running:

   ```bash
   minikube start
   kubectl cluster-info
   ```

2. Your Deepgram Account Representative should provide you with download links to customized configuration files. Create a configuration map using the `engine.toml` and `api.toml` files:

   ```bash
   kubectl create configmap engine --from-file=/path/to/engine.toml
   kubectl create configmap api --from-file=/path/to/api.toml
   ```

<Alert type="warning">
  
You'll have to delete, then recreate, configuration maps each time a change is made to a .toml file.

</Alert>

3. Use the customized YAML files provided by your Deepgram Account Representative to deploy Engine and API pods:

   ```bash
   kubectl apply -f engine.yaml
   kubectl apply -f api.yaml
   ```

4. Check the status of each deployment:

   ```bash
   kubectl get pods
   ```

The status will show `Running` if successful. If this is any other value, you can further diagnose the issue with the command `kubectl describe pods <pod-name>` or `kubectl logs <pod-name>`. Running the `apply` command again will apply the changes you made to the deployment files.

## Getting Model Files

Your Deepgram Account Representative should provide you with download links to at least one pre-trained AI model for testing purposes. Once the API and Engine pods are up and running, copy the provided model files into the directory in which you decided to place the models:

```bash
kubectl cp model.dg <engine-pod-name>:/models
```

The newly-added models will appear within a minute or so. To validate that they were added, you can query the `/v2/models` endpoint.

<Alert type="info">
	Note that this method is primarily for proof-of-concept. In production, Kubernetes lets you obtain these files automatically by creating `Volumes`. For more information, see
	Kubernetes' [Volumes](https://kubernetes.io/docs/concepts/storage/volumes/) documentation.
</Alert>

## Querying the API

To send a query:

1. Get the IP address of the API:

   ```bash
   kubectl get pods -o wide
   ```

2. Use the IP address in your queries. For example, to query the `/models` endpoint:

   ```bash
   curl http://<api-pod-ip-address>:<api-pod-port>/v2/models
   ```

## Additional Notes

- In a production environment, Kubernetes services are typically used to expose public endpoints and create load balancers. To learn more, read Kubernetes' [Service](https://kubernetes.io/docs/concepts/services-networking/service/) documentation.
- GPU/CPU utilization isn't a reliable metric for load balancing and scaling. We generally recommend a least-connected strategy and scaling based off the number of active requests.
- We recommend running a single pod per node i.e. one instance of the API per machine, and one instance of the Engine per machine
