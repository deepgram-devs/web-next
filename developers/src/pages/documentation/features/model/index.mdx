---
layout: ../../../../layouts/Documentation.astro
title: Model
description: Learn about Deepgram's Model feature, which allows you to supply a model to use to process submitted audio.
tags: [model, streaming, pre-recorded]
order: 2
seo:
  metaTitle: Model
  metaDescription: Learn about Deepgram's Model feature, which allows you to supply a model to use to process submitted audio.
  keywords: model, speech-recognition, process, custom
---
import Alert from '../../../../shared/components/global/Alert.astro';
import Callout from '../../../../shared/components/global/Callout.astro';
import CodeBlock from '../../../../shared/components/code/CodeBlock.astro';
import CodeGroup from '../../../../shared/components/code/CodeGroup.astro';

# {frontmatter.title}

<div class="badge">PRE-RECORDED</div>
<div class="badge">STREAMING</div>

Deepgram’s Model feature allows you to supply a model to use when processing submitted audio. Each model belongs to a tier. For self-serve customers, Deepgram provides the following models.

<Callout>

If you are a <b>premium customer</b> who would like access to Deepgram's Nova or Whisper models, please reach out to your customer support team to gain access.

</Callout>

## Models

* **Nova** models are our newest and most powerful speech-to-text model on the market today. Nova's vast and diverse training data sets it apart.  Nova's performance is an incredible 22% better than the competition, making it the top choice for applications that demand high accuracy in diverse contexts. It's also our most cost effective models to use.

* **Whisper** models are Deepgram's hosted version of OpenAI’s Whisper model. Whisper is a powerful speech-to-text model that is trained on a large corpus of text and audio data. It is designed to be used in a variety of applications, including voice assistants, transcription, and more.

* **Enhanced** models are still some of our most powerful ASR models. Enhanced models generally have higher accuracy with better word recognition than our Base models and they handle uncommon words significantly better.

* **Base** models are built on our signature end-to-end deep learning speech model architecture and offer a solid combination of accuracy and cost effectiveness.

Once you have chosen your tier and model, you can select an available language and a version. To learn more about languages, see [Language](/documentation/features/language/). To learn more about versions, see [Version](/documentation/features/version/).


<Alert type = "info" >

All model tiers have been trained with our patented AutoML (TM) training. To learn more about tiers, see [Tier](/documentation/features/tier/).
  </Alert>


## Use Cases

A use case is a specific application of Deepgram’s speech-to-text technology, e.g., a phone call, a meeting, financial conversations, etc. Deepgram provides a number of pre-trained use case models that are optimized to provide the best results for a particular use case.


Some examples of use cases for Model include:

- Customers with audio data with traits that match a specific Deepgram-provided use-case model.
- Customers with specialized audio data who want to apply a custom trained model that has been optimized to provide the best results for their particular data.

## Enable Feature

To enable Model, when you call Deepgram’s API, add a `model` parameter in the query string and set it to the model you would like to use:

`model=OPTION`

<Alert type="info">

By default, Deepgram applies its Base tier. If you would like to use a different tier, for a hosted deployment, add a `tier` parameter in the query string as well:

`tier=enhanced&model=OPTION`

For an on-premises deployment, use the `model` parameter in the query string and append `-enhanced` after the name of the model you would like to use:

`model=OPTION-enhanced`

</Alert>

To transcribe audio from a file on your computer, run the following cURL command in a terminal or your favorite API client.

<Alert type="info">

Be sure to replace the placeholder `OPTION` with your chosen model and `YOUR_DEEPGRAM_API_KEY` with your Deepgram API Key. You can [create an API Key](/documentation/getting-started/authentication/#create-an-api-key) in the [Deepgram Console](https://console.deepgram.com)

</Alert>

<CodeGroup title="curl">

<CodeBlock label="curl" active>

```bash
curl \
  --request POST \
  --header 'Authorization: Token YOUR_DEEPGRAM_API_KEY' \
  --header 'Content-Type: audio/wav' \
  --data-binary @youraudio.wav \
  --url 'https://api.deepgram.com/v1/listen?model=OPTION'
```

</CodeBlock>

</CodeGroup>

### Model Options

For self-serve customers, Deepgram provides Nova, Whisper, Enhanced and Base model tiers. To learn more about pricing, see [Deepgram Pricing & Plans](https://deepgram.com/pricing/).

#### Nova
Nova is our newest and most powerful speech-to-text model. Compared to its nearest competitor, Nova delivers:

- A remarkable 22% reduction in word error rate (WER)
- A blazing-fast 27x quicker inference time
- A budget-friendly 67% lower cost starting at only $0.0043/min

Nova's groundbreaking training spans over 100 domains and 47 billion tokens, making it our deepest trained automatic speech recognition (ASR) model to date. This extensive and diverse training has produced a model that consistently outperforms any other ASR model across a wide range of datasets.

While other models rely on smaller, more closely paired audio-text training datasets or unsupervised audio pre-training, Nova's vast and diverse training data sets it apart.  Nova's performance is 50% better than the competition, making it the top choice for applications that demand high accuracy in diverse contexts.

Nova's comprehensive training on diverse data makes it the most reliable and adaptable model on the market, perfect for a wide array of speech recognition applications. Nova also provides a solid foundation for fine-tuning in specific domains and tackling enterprise use cases.


The Nova model can be called with the following syntax:


```curl
https://api.deepgram.com/v1/listen?model=nova
```
```curl
https://api.deepgram.com/v1/listen?model=general&tier=nova
```
```curl
https://api.deepgram.com/v1/listen?model=phonecall&tier=nova
```

#### Whisper

Deepgram's Whisper Cloud is a fully managed API that gives you access to Deepgram's version of OpenAI’s Whisper model. Using Deepgram's fully hosted Whisper Cloud instead of running your own version of Whisper provides many benefits. Some of these benefits include:

- Pairing the Whisper model with Deepgram features that you can’t get using the OpenAI speech-to-text API, such as diarization and word timings.
- Scalable infrastructure that can handle high-traffic usage (up to 50 concurrent requests per minute or 15 concurrent requests).
- The ability to use all Whisper model sizes.
- Support for audio files up to 2GB.

 Deepgram's Whisper models have the following size options:

 - `tiny`: Contains 39 M parameters. The smallest model available.
 - `base`: Contains 74 M parameters.
 - `small`: Contains 244 M parameters.
 - `medium`: Contains 769 M parameters. The default model if you don't specify a size.
 - `large`: Contains 1550 M parameters. The largest model available. Defaults to OpenAI’s Whisper large-v2.


<Alert type="info">
Deepgram hosts and maintains these Whisper models. They aren't hosted or run by Open AI.

</Alert>

**There are some important things to understand about using Deepgram's Whisper Cloud:**

- Deepgram's Whisper models can perform automatic language detection by setting `detect_language=true`. If you know the language of your audio or video file and would like to specify the language, you can do so by setting the language parameter in the query string. To learn more about languages, see [Language](/documentation/features/language/). To learn more about language detection, see [Language Detection](/documentation/features//detect-language/).


```curl
https://api.deepgram.com/v1/listen?model=whisper-medium&language=en
```

- Whisper has English-only variants (e.g. en-au, en-gb, en-in, en-nz) and they can be called via the API.  Whisper works similarly to Deepgram’s English language models. To learn more about languages, see [Language](/documentation/features/language/).


```curl
https://api.deepgram.com/v1/listen?model=whisper-medium&language=en-au
```

- To request different size Whisper models you can use the following size options: tiny, base, small, medium, and large. The default model is medium.

```curl
https://api.deepgram.com/v1/listen?model=whisper-medium
```

<Alert type="info">
When using the Whisper models you don't need to specify a tier parameter.

</Alert>



#### Enhanced

Enhanced models are still some of our most powerful ASR models; they generally have higher accuracy and better word recognition than our Base models, and they handle uncommon words significantly better. Deepgram's enhanced models have the following options:

- `general`: Optimized for everyday audio processing. Generally, more accurate than any region-specific Base model for the language for which it is enabled. If you aren't sure which model to select, start here.
- `meeting` _beta_: Optimized for conference room settings, which include multiple speakers with a single microphone.
- `phonecall`: Optimized for low-bandwidth audio phone calls.

- `finance` _beta_: Optimized for multiple speakers with varying audio quality, such as might be found on a typical earnings call. Vocabulary is heavily finance oriented.

```curl
https://api.deepgram.com/v1/listen?tier=enhanced&model=OPTION
```

#### Base

Base models are built on our signature end-to-end deep learning speech model architecture. They offer a solid combination of accuracy and cost effectiveness in some cases. Deepgram's base models have the following options:

- `general`: (Default) Optimized for everyday audio processing.
- `meeting`: Optimized for conference room settings, which include multiple speakers with a single microphone.
- `phonecall`: Optimized for low-bandwidth audio phone calls.
- `voicemail`: Optimized for low-bandwidth audio clips with a single speaker. Derived from the phonecall model.
- `finance`: Optimized for multiple speakers with varying audio quality, such as might be found on a typical earnings call. Vocabulary is heavily finance oriented.
- `conversationalai`: Optimized to allow artificial intelligence technologies, such as chatbots, to interact with people in a human-like way.
- `video`: Optimized for audio sourced from videos.

```curl
https://api.deepgram.com/v1/listen?tier=base&model=OPTION
```

<Alert type="warning">

Not all models are supported for all languages. For a list of languages and their supported models, see [Language](/documentation/features/language/).

</Alert>

<Alert type="warning">

There is a 10 minute time out for all Deepgram models. Transcription requests that run longer than 10 minutes will return a 504 error.

</Alert>

#### Custom

You may also use a custom, trained model associated with your account by including its `custom_id`.
